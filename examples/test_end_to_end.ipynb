{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 PSSM generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate PSSM files to extract evolutionary features using Mmseqs's easy_generate_pssm function.\n",
    "\n",
    "The arguments are: we have the names of the arguments and inside () the format it accepts\n",
    "* input_file (FASTA): path to the fasta file \n",
    "* database_input (FASTA): path to the uniref50 database or any database you want to perform the search \n",
    "* generate_searchdb (bool): Set it to True only the first time you run it to create the search database from the database input which is necessary for Mmseqs to run correctly\n",
    "* database_output (str, optional): Optionally, you canname the database created from the database_input\n",
    "* pssm_filename (str): name of the pssm file which stores the PSSM for all the sequences in the input\n",
    "* output_dir (str): pssm_file will be split into individual PSSM files in this directory\n",
    "\n",
    "So the generated output is basically:\n",
    "the individual pssm_files stored in *output_dir* used by PossumFeatures(pssm_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.utilities.utils import MmseqsClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"../data/whole_sequence.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running createdb\n",
      "It took 0.07075858116149902 second to run\n",
      "start running search\n",
      "It took 0.008486747741699219 second to run\n",
      "start running generate_profile\n",
      "It took 0.13269639015197754 second to run\n",
      "start running convert profile to pssm\n",
      "It took 0.08297467231750488 second to run\n"
     ]
    }
   ],
   "source": [
    "pssm_file = MmseqsClustering.easy_generate_pssm(input_file=fasta_file, database_input=\"../data/db_seq.fasta\",\n",
    "                                                generate_searchdb=True, pssm_filename=\"esterase.pssm\", output_dir=\"pssm\") # actualizar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several features to extract:\n",
    "   1. Physicochemical features from iFeatures https://github.com/Superzchen/iFeature\n",
    "   2. Evolutionary features using PSSM and Possum https://possum.erc.monash.edu/\n",
    "   3. Features or embeddings from Large language models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.features.extraction import PossumFeatures, IfeatureFeatures, ExtractFeatures \n",
    "from BioML.utilities.utils import clean_fasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before substrate extraction you might want to clean the fast file of non canonical residues like X, U, B etc and to remove those shorter than 100 aamino acids for example.\n",
    "The arguments are:\n",
    "* possum_program (str): path to the POSSUM_Toolkit with the scripts to clean the fasta file\n",
    "* fasta_file (FASTA): path to the fasta file\n",
    "* out_fasta (FASTA): path to the cleaned fasta file\n",
    "* min_aa (int): minimum length of the sequence\n",
    "\n",
    "The generated output is:\n",
    "* out_fasta which is used by all the other feature extraction programs.\n",
    "Clean fasta should actually be used even before pssm_generation from Mmseqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_fasta(\"../POSSUM_Toolkit\", fasta_file, \"cleaned.fasta\", 100)\n",
    "fasta_file = \"cleaned.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class extract iFeature features and the most important arguments for the class are:\n",
    "* program (str): path to the iFeature program\n",
    "* output (str): path to the output directory\n",
    "\n",
    "The output of the function ifeatures.extract(fasta_file)\n",
    "*  A bunch of csv files stored in the output directory.\n",
    "\n",
    "Each of the csv files represent one feature from the iFeature program repertoire\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iFeature features to be extracted: {'long': ['Moran', 'Geary', 'NMBroto'], 'short': ['QSOrder', 'GDPC', 'CTDT', 'PAAC', 'GAAC', 'SOCNumber', 'KSCTriad', 'APAAC', 'CTDD', 'CTDC', 'CKSAAGP', 'CTriad', 'GTPC']}\n",
      "start running Ifeature programs\n",
      "It took 15.259301900863647 second to run\n"
     ]
    }
   ],
   "source": [
    "ifeatures = IfeatureFeatures(\"../iFeature\")\n",
    "ifeatures.extract(fasta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class extract possum features and the most important arguments for the class are:\n",
    "* pssm_dir (str): path to the individual pssm files generated by Mmseqs\n",
    "* output (str): path to the output directory\n",
    "* program (str): path to the POSSUM_Toolkit\n",
    "\n",
    "The output:\n",
    "with this function you generate a bunch of tsv files stored in the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possum features to be extracted: {'long': ['pssm_cc', 'tri_gram_pssm'], 'short': ['dpc_pssm', 'pse_pssm:1', 'smoothed_pssm:9', 'k_separated_bigrams_pssm', 'smoothed_pssm:5', 'smoothed_pssm:7', 'aac_pssm', 'dp_pssm', 's_fpssm', 'ab_pssm', 'pse_pssm:3', 'pse_pssm:2', 'tpc', 'rpm_pssm', 'pssm_composition', 'eedp', 'rpssm', 'pssm_ac', 'd_fpssm', 'edp']}\n",
      "start running Possum programs\n",
      "It took 187.8209524154663 second to run\n"
     ]
    }
   ],
   "source": [
    "possum = PossumFeatures(pssm_dir=\"pssm\", output=\"possum_features\", program=\"../POSSUM_Toolkit\")\n",
    "possum.extract(fasta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large fasta files of hundreds thousands of sequences I recommend using ExtractFeatures.\n",
    "ExtractFeatures takes a large fasta file and splits it into smaller fasta files called groups_*.fasta with separate_bunch(base=\"group\"). You could change the base to change the name of the files.\n",
    "\n",
    "The arguments for the run_extraction_parallel function are:\n",
    "* file (list[FASTA]): a list of fasta files\n",
    "* num_threads (int): number of threads to use\n",
    "* f (function): an arbitrary number of function to be applied to each fasta file. The function can be anyone that accepts a fasta file as an argument.\n",
    "\n",
    "The output is a list of csv files stored in the different output directories from each fasta file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possum features to be extracted: {'long': ['pssm_cc', 'tri_gram_pssm'], 'short': ['ab_pssm', 'edp', 'eedp', 'rpm_pssm', 'smoothed_pssm:5', 'pssm_composition', 'rpssm', 'pssm_ac', 'pse_pssm:1', 'pse_pssm:3', 's_fpssm', 'd_fpssm', 'tpc', 'smoothed_pssm:7', 'pse_pssm:2', 'dpc_pssm', 'k_separated_bigrams_pssm', 'dp_pssm', 'smoothed_pssm:9', 'aac_pssm']}\n",
      "iFeature features to be extracted: {'long': ['Moran', 'NMBroto', 'Geary'], 'short': ['CTDD', 'GTPC', 'PAAC', 'GAAC', 'APAAC', 'CTDC', 'CTDT', 'GDPC', 'QSOrder', 'KSCTriad', 'CTriad', 'SOCNumber', 'CKSAAGP']}\n",
      "start running Possum programs\n",
      "It took 0.9275431632995605 second to run\n",
      "start running Ifeature programs\n",
      "It took 16.410173177719116 second to run\n"
     ]
    }
   ],
   "source": [
    "extract = ExtractFeatures(fasta_file)\n",
    "extract.separate_bunch()\n",
    "file = list(extract.fasta_file.parent.glob(\"group_*.fasta\"))\n",
    "\n",
    "possum = PossumFeatures(pssm_dir=\"pssm\", output=\"possum_features\", program=\"../POSSUM_Toolkit\")\n",
    "ifeature = IfeatureFeatures(\"../iFeature\")\n",
    "\n",
    "extract.run_extraction_parallel(file, 100, possum.extract, ifeature.extract)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3  LLM embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want you can easily generate large language model embeddings from models supported by Hugging Face's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.deep import embeddings\n",
    "from BioML.deep. train_config import LLMConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LLMConfig()\n",
    "tokenizer = embeddings.TokenizeFasta(config)\n",
    "embed = embeddings.ExtractEmbeddings(config)\n",
    "tok = tokenizer.tokenize(fasta_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings from a FASTA file:\n",
    "* fasta_file (FASTA): path to the fasta file\n",
    "* model_name (str): name of the model\n",
    "* option (str): The embeddings has the dimensions L*H where L is the number of aminoacids and H is the dimension of the features. The option can be \"mean\", \"sum\", \"max\" or \"flatten\" so it turns L * H into H. \n",
    "* save_path (str): path to the output file\n",
    "* mode (str): write or return\n",
    "\n",
    "The saved output \n",
    "* embeddings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.generate_embeddings(fasta_file, model_name=\"facebook/esm2_t6_8M_UR50D\", option=\"mean\", \n",
    "                               save_path=\"embeddings.csv\", mode=\"write\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we combine features from multiple places the dimensions would be too high to handle, so normally ti would be wise to use some feature selection techniques:  \n",
    "BioML implements at the moment 8 algorithms for classification and 6 for regression \n",
    " \n",
    "    1. Supervised: using Tree methods Random Forest and Xgboost, Recursive feature elmination\n",
    "    2. Filter methods: Pearson correlation, Chi squared,\n",
    "    3. Unsupervised methods like: PCA, ICA (independent component analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from BioML.features import selection\n",
    "from BioML.features.extraction import read_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read features is a convenient function to read all the csv or tsv files generated by the feature extraction programs and returns a dataframe with all the features\n",
    "The arguments are:\n",
    "* program (str): the feature extraction program wether 'ifeature' or 'possum'\n",
    "* ifeature_out (str): path to the output directory if ifeature features\n",
    "* possum_out (str): path to the output directory if possum features\n",
    "* file_splits (int): This arguments was added for ExtractFeatures that splits the large fasta files into smaller ones, it is the number of splits performed. In this case no splits so file_splits = 1 \n",
    "* index (list): possum features don't save the sequence index with the tsvs unlike ifeature, which makes identifying each row back to the original sequence difficult (the order is kept) so you should provide the index if you are using possum features\n",
    "\n",
    "The output is a dataframe with all the features concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning possum_features/tri_gram_pssm_0.csv does not exist\n",
      "Warning possum_features/k_separated_bigrams_pssm_0.csv does not exist\n"
     ]
    }
   ],
   "source": [
    "ifeat = read_features(\"ifeature\", ifeature_out=\"ifeature_features\", file_splits=1)\n",
    "possum_feat = read_features(\"possum\", possum_out=\"possum_features\", file_splits=1, index=ifeat.index)\n",
    "emb = pd.read_csv(\"embeddings.csv\", index_col=0, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((320,), (2274,), (10330,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.columns.shape, ifeat.columns.shape, possum_feat.columns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataReader function takes in different formats and converts them into a single format (the conversion is necessary for the feature selection):\n",
    "\n",
    "* label in different formats: string, ndarray, a list of pd.Series.\n",
    "* It also takes features in different formats: a list or ndarray, dataframe, list of dataframes or a list of the different accepted formats and concatenates them\n",
    "\n",
    "It will automatically remove features with 0 variance (meaning every value is the same), you can control the threshold\n",
    "\n",
    "The output: \n",
    "* A class with the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data with:\n",
       "    num. samples: 147\n",
       "    num. columns: 12924\n",
       "    variance threshold: 0\n",
       "    sheet: None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = selection.DataReader(\"../data/esterase_labels.csv\", [ifeat, possum_feat, emb], variance_thres=0)\n",
    "#features.features.columns = features.features.columns.astype(str)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom splitting based on 30% sequence identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to split based on sequence identity so that the training, test or validation sets doesn't have sequence with more than 30% sequence identity.\n",
    "You can use mmseqs to generate such clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.utilities.utils import MmseqsClustering\n",
    "from BioML.utilities.split_methods import ClusterSpliter\n",
    "from BioML.models.base import DataParser\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate different clusters of sequences based on an identity threshold of 30%.\n",
    "It creates a cluster.tsv file. This tsv file has the first column the clusters names (which should be the same one as the features indices) and in the second column the idex of the sequence that resides within that cluster.  \n",
    "\n",
    "For example: This means EH2(71) is a cluster with 3 sequences that share more than 30% sequence identity.\n",
    "\n",
    "* EH2(71)\tEH2(71)\n",
    "* EH2(71)\tEH4(67)\n",
    "* EH2(71)\tEH14(48)\n",
    "\n",
    "This technique is actually very flexible, you can generate manually a cluster file if you want to customize the way you want to split your data into training and testing splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "easy_cluster function takes the following arguments:\n",
    "\n",
    "* input_file (str or Path): path to the input sequence file\n",
    "* cluster_at_sequence_identity (float): the sequence identity threshold\n",
    "* cluster_tsv (str or Path): path to the output cluster file\n",
    "\n",
    "The output:\n",
    "* cluster.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start running cluster\n",
      "It took 6.722121715545654 second to run\n",
      "start running create tsv\n",
      "It took 0.02847123146057129 second to run\n"
     ]
    }
   ],
   "source": [
    "cluster = MmseqsClustering.easy_cluster(fasta_file, cluster_at_sequence_identity=0.3, \n",
    "                                        cluster_tsv=\"cluster.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now split based on this sequence identity cluster, the clusters names can be strings or numbers as long as it is the same as the index of feautures dataframe.  \n",
    "But if they are different, cluster names are different to the index of the dataframe, there is an additional groups argument in the split and train_test_split function that accepts a list with the names present in the cluster file and ordered as the rows in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustersplitter splits the dataframes into training and testing sets based on the cluster file.\n",
    "The arguments are:\n",
    "\n",
    "* _cluster_info (str or Path): path to the cluster file\n",
    "* num_splits (int): number of splits\n",
    "* random_state (int): random seed\n",
    "* stratified (bool): whether to use stratified sampling or not, only for classification\n",
    "\n",
    "The function train_test_split is similar to the sklearn train_test_split function.\n",
    "* X (Any iterable): features dataframe\n",
    "* y (any iterable): labels array -> y is optional, if you don't provide it it returns X_train and X_test only\n",
    "* test_size (float): proportion of the dataset to include in the test split (it doesn't always work since it has to follow the clusters)\n",
    "\n",
    "The output is X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = ClusterSpliter(\"cluster.tsv\", num_splits=5, random_state=100, stratified=True)\n",
    "X_train, X_test, y_train, y_test = split.train_test_split(features.features, features.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117, 12924), 117)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(X_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_range_features arguments:\n",
    "* features (pd.DataFrame): features dataframe\n",
    "* num_features_min (int): minimum number of features\n",
    "* num_features_max (int): maximum number of features\n",
    "* step_range (int): step range between num_features_min and num_features_max\n",
    "\n",
    "The output:\n",
    "* feature_range (list): list of feature dimensions to extract for each of the feature selection algorithm\n",
    "\n",
    "You could skip this function and give an list of feature dimensions diectly to construct features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 40, 50, 60]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "problem =\"classification\"\n",
    "feature_range = selection.get_range_features(features.features, num_features_min = 20, num_features_max=60, step_range=10)\n",
    "feature_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Featureselection class arguments are:\n",
    "\n",
    "* excel_file (str or Path): path to the excel file to save the features\n",
    "* filter_args : a class with filter_args attributes, which is a dict containing the selection algorithms\n",
    "* num_thread (int): number of threads\n",
    "* seed (int): random seed\n",
    "* scaler (str): scaler to use to scale the features before runing the selection\n",
    "\n",
    "The construct_features function is the function that actually runs the selection algorithms.\n",
    "* X (Any iterable): features dataframe\n",
    "* X_train (Any iterable): training features dataframe\n",
    "* X_test (Any iterable): testing features dataframe\n",
    "* y_train (Any iterable): training labels array\n",
    "* feature_range (list): list of feature dimensions to extract for each of the feature selection algorithm\n",
    "\n",
    "The output is:\n",
    "* excel_file (saved): with the features, each sheet is a selection algorithm and feature_dimension\n",
    "* feature importance plot (saved): with the shap package https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/\n",
    "* A csv of the feature importance (saved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14-04-2025 14:29:31 INFO Reading the features\n",
      "14-04-2025 14:29:31 INFO Starting feature selection and using the following parameters\n",
      "14-04-2025 14:29:31 INFO seed: 10\n",
      "14-04-2025 14:29:31 INFO filtering the features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification filter: mutual_info\n",
      "classification filter: Fscore\n",
      "classification filter: chi2\n",
      "classification filter: FechnerCorr\n",
      "classification filter: KendallCorr\n",
      "generating a feature set of 20 dimensions\n",
      "generating a feature set of 30 dimensions\n",
      "generating a feature set of 40 dimensions\n",
      "generating a feature set of 50 dimensions\n",
      "generating a feature set of 60 dimensions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x950 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select features\n",
    "if problem == \"classification\":\n",
    "    filters = selection.FeatureClassification()\n",
    "elif problem == \"regression\":\n",
    "    filters = selection.FeatureRegression()\n",
    "\n",
    "select = selection.FeatureSelection(\"classification_results/filtered_features.xlsx\", filters, num_thread=2, seed=10, \n",
    "                                    scaler=\"robust\")\n",
    "\n",
    "select.construct_features(features.features, X_train, X_test, y_train, feature_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also eliminate outliers in your dataset which could decrease model performance.\n",
    "Although, generally, if you have a lot of data outliers won't matter much.  \n",
    "It is unsupervised so it only needs the features. It accepts the following formats\n",
    "\n",
    "1. excel files\n",
    "2. Pandas dataframe\n",
    "3. Series, ndarray\n",
    "4. csv file\n",
    "\n",
    "It leverages 8 different outlier detection algorithms and gives you a voting. \n",
    "If you give it an excel file where there are different sheets. It will iterate through all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.utilities.outlier import OutlierDetection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe OutlierDetection class arguments are:\n",
    "\n",
    "* feature_file (str or Path): path to the excel file\n",
    "* output (str or Path): path to the output file\n",
    "* num_thread (int): number of threads\n",
    "\n",
    "The run function outputs a dataframe with the outliers in it.\n",
    "* \"classification_results/outliers.csv\" (saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection = OutlierDetection(\"classification_results/filtered_features.xlsx\", output=\"classification_results/outliers.csv\", num_thread=4)\n",
    "outliers = detection.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EH144(1)</th>\n",
       "      <td>281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH115(8)</th>\n",
       "      <td>226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH121(5)</th>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH74(17)</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH125(4)</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH98(11)</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH85(13)</th>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH48(23)</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH61(20)</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH46(23)</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH31(29)</th>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH127(4)</th>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH111(8)</th>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH102(10)</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH119(6)</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH92(12)</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH6(66)</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH29(31)</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH36(28)</th>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH52(21)</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "EH144(1)   281\n",
       "EH115(8)   226\n",
       "EH121(5)   198\n",
       "EH74(17)   169\n",
       "EH125(4)   142\n",
       "EH98(11)   116\n",
       "EH85(13)   116\n",
       "EH48(23)   102\n",
       "EH61(20)    86\n",
       "EH46(23)    85\n",
       "EH31(29)    76\n",
       "EH127(4)    73\n",
       "EH111(8)    66\n",
       "EH102(10)   65\n",
       "EH119(6)    58\n",
       "EH92(12)    55\n",
       "EH6(66)     53\n",
       "EH29(31)    46\n",
       "EH36(28)    46\n",
       "EH52(21)    45"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through multiple feature sheets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have many features sets that were selected and saved in the excel file from the previous feature selection step. You want to test which feature set is better for your data relatively fast -> you could use iterate_excel from the utilities and the iterate_multiple_features method from Trainer.\n",
    "\n",
    "The iterate_multiple_features method from Trainer accepts an iterator of (feature dataframe, label column name, name) generated by iterate_excel: \n",
    "    1. name is used to identify the features later in the results\n",
    "    2. feature dataframe should contain a column with teh labels\n",
    "    3. label column name within the feature dataframe\n",
    "\n",
    "Then it calls the run_training method from the Trainer iteratively and gathers performance metrics for different sheets.\n",
    "run_training only runs the models without tuning, stacking or esembling.\n",
    "Finally it write the results for each sheet\n",
    "\n",
    "Once you have picked the best performing sheet you could run again the previous steps and perform tuning, stacking and ensembling\n",
    "\n",
    "Iterate excel reads all the sheets from an excel and yields (feature dataframe, label column name, sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.utilities.utils import write_results, iterate_excel\n",
    "from BioML.models.base import DataParser\n",
    "from BioML.models.classification import Classifier\n",
    "from BioML.models.regression import Regressor\n",
    "from BioML.models.base import PycaretInterface, Trainer, DataParser\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BioML.utilities.utils import write_results\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from BioML.utilities.split_methods import ClusterSpliter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default pycaret logs the hyperparameters and metrics in MlFLow (similar to wandb or tensorboard) but since we are only testing the features\n",
    "we don't need registry of all those runs.\n",
    "So we can disable log_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 12:08:37 INFO ------------------------------------------------------------------------------\n",
      "04-04-2025 12:08:37 INFO PycaretInterface parameters\n",
      "04-04-2025 12:08:37 INFO Seed: 250\n",
      "04-04-2025 12:08:37 INFO Budget time: 20\n",
      "04-04-2025 12:08:37 INFO The number of models to select: 4\n",
      "04-04-2025 12:08:37 INFO Output path: classification_results\n",
      "04-04-2025 12:08:37 INFO ----------------Trainer inputs-------------------------\n",
      "04-04-2025 12:08:37 INFO Number of kfolds: 5\n",
      "04-04-2025 12:08:37 INFO Number of retuning iterations: 50\n",
      "04-04-2025 12:08:37 INFO Test size: 0.2\n"
     ]
    }
   ],
   "source": [
    "experiment = PycaretInterface(\"classification\", 250, budget_time=20, best_model=4, \n",
    "                                  output_path=\"classification_results\", optimize=\"MCC\", log_experiment=False) \n",
    "classifier = Classifier(optimize=\"MCC\", drop=(), selected=(), add=())\n",
    "training = Trainer(experiment, classifier, num_splits=5, test_size=0.2,  num_iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments for iterate_excel are:\n",
    "\n",
    "* excel_file (str): the path to the excel file with all the selected features\n",
    "* parser (DataParser): the parser for the excel file\n",
    "* label (str): The labels for the features it can be any of the accepted formates by parser\n",
    "* outliers (Iterable[str]): An iterable containing the indices to remove from the training set\n",
    "\n",
    "The output is:\n",
    "* yield -> (feature dataframe, label column name, sheet name)\n",
    "\n",
    "The arguments for iterate_multiple_features are:\n",
    "\n",
    "* generator (Generator): A generator that yields (feature dataframe, label column name, name)\n",
    "* training_output (str): the path to the training output\n",
    "* split_strategy (ClusterSpliter): The strategy for splitting the training dataset\n",
    "\n",
    "The output is:\n",
    "* \"training_output / training_results.xlsx\" (saved)\n",
    "* \"output_path/ config_setup_pycaret.csv\" (saved) from PycaretInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/04 12:08:42 INFO mlflow.tracking.fluent: Experiment with name 'Classification' does not exist. Creating a new experiment.\n",
      "04-04-2025 12:08:42 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 12:08:42 INFO --------------------------------------------------------\n",
      "04-04-2025 12:08:42 INFO Training classification models\n",
      "04-04-2025 12:08:42 INFO The models used ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', 'ada', 'gbc', 'lda', 'et', 'xgboost', 'lightgbm', 'catboost', 'dummy']\n",
      "04-04-2025 12:08:42 INFO The number of models used 19\n",
      "04-04-2025 12:08:42 INFO Time budget is 20 minutes\n",
      "2025/04/04 12:08:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:46 INFO Model lr trained in 0.055 minutes\n",
      "2025/04/04 12:08:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:48 INFO Model knn trained in 0.039 minutes\n",
      "2025/04/04 12:08:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:49 INFO Model nb trained in 0.013 minutes\n",
      "2025/04/04 12:08:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:49 INFO Model dt trained in 0.013 minutes\n",
      "2025/04/04 12:08:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:50 INFO Model svm trained in 0.013 minutes\n",
      "2025/04/04 12:08:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:51 INFO Model rbfsvm trained in 0.013 minutes\n",
      "2025/04/04 12:08:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:53 INFO Model gpc trained in 0.027 minutes\n",
      "2025/04/04 12:08:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:54 INFO Model mlp trained in 0.024 minutes\n",
      "2025/04/04 12:08:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:55 INFO Model ridge trained in 0.014 minutes\n",
      "2025/04/04 12:08:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:56 INFO Model rf trained in 0.02 minutes\n",
      "2025/04/04 12:08:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:57 INFO Model qda trained in 0.014 minutes\n",
      "2025/04/04 12:08:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:58 INFO Model ada trained in 0.017 minutes\n",
      "2025/04/04 12:08:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:08:59 INFO Model gbc trained in 0.019 minutes\n",
      "2025/04/04 12:09:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:09:00 INFO Model lda trained in 0.013 minutes\n",
      "2025/04/04 12:09:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:09:01 INFO Model et trained in 0.02 minutes\n",
      "2025/04/04 12:09:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:09:02 INFO Model xgboost trained in 0.017 minutes\n",
      "2025/04/04 12:10:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:00 INFO Model lightgbm trained in 0.97 minutes\n",
      "2025/04/04 12:10:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:06 INFO Model catboost trained in 0.094 minutes\n",
      "2025/04/04 12:10:06 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:07 INFO Model dummy trained in 0.014 minutes\n",
      "04-04-2025 12:10:07 INFO Training over: Total runtime 1.408 minutes\n",
      "04-04-2025 12:10:08 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 12:10:08 INFO --------------------------------------------------------\n",
      "04-04-2025 12:10:08 INFO Training classification models\n",
      "04-04-2025 12:10:08 INFO The models used ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', 'ada', 'gbc', 'lda', 'et', 'xgboost', 'lightgbm', 'catboost', 'dummy']\n",
      "04-04-2025 12:10:08 INFO The number of models used 19\n",
      "04-04-2025 12:10:08 INFO Time budget is 20 minutes\n",
      "2025/04/04 12:10:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:09 INFO Model lr trained in 0.013 minutes\n",
      "2025/04/04 12:10:09 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:09 INFO Model knn trained in 0.014 minutes\n",
      "2025/04/04 12:10:10 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:10 INFO Model nb trained in 0.013 minutes\n",
      "2025/04/04 12:10:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:11 INFO Model dt trained in 0.013 minutes\n",
      "2025/04/04 12:10:11 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:12 INFO Model svm trained in 0.013 minutes\n",
      "2025/04/04 12:10:12 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:13 INFO Model rbfsvm trained in 0.014 minutes\n",
      "2025/04/04 12:10:14 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:14 INFO Model gpc trained in 0.025 minutes\n",
      "2025/04/04 12:10:15 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:16 INFO Model mlp trained in 0.028 minutes\n",
      "2025/04/04 12:10:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:17 INFO Model ridge trained in 0.013 minutes\n",
      "2025/04/04 12:10:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:18 INFO Model rf trained in 0.021 minutes\n",
      "2025/04/04 12:10:18 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:19 INFO Model qda trained in 0.014 minutes\n",
      "2025/04/04 12:10:19 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:20 INFO Model ada trained in 0.017 minutes\n",
      "2025/04/04 12:10:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:21 INFO Model gbc trained in 0.019 minutes\n",
      "2025/04/04 12:10:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:22 INFO Model lda trained in 0.013 minutes\n",
      "2025/04/04 12:10:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:23 INFO Model et trained in 0.02 minutes\n",
      "2025/04/04 12:10:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:10:24 INFO Model xgboost trained in 0.019 minutes\n",
      "2025/04/04 12:11:16 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:17 INFO Model lightgbm trained in 0.878 minutes\n",
      "2025/04/04 12:11:22 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:22 INFO Model catboost trained in 0.094 minutes\n",
      "2025/04/04 12:11:23 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:23 INFO Model dummy trained in 0.013 minutes\n",
      "04-04-2025 12:11:23 INFO Training over: Total runtime 1.255 minutes\n",
      "04-04-2025 12:11:24 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 12:11:24 INFO --------------------------------------------------------\n",
      "04-04-2025 12:11:24 INFO Training classification models\n",
      "04-04-2025 12:11:24 INFO The models used ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', 'ada', 'gbc', 'lda', 'et', 'xgboost', 'lightgbm', 'catboost', 'dummy']\n",
      "04-04-2025 12:11:24 INFO The number of models used 19\n",
      "04-04-2025 12:11:24 INFO Time budget is 20 minutes\n",
      "2025/04/04 12:11:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:25 INFO Model lr trained in 0.014 minutes\n",
      "2025/04/04 12:11:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:26 INFO Model knn trained in 0.015 minutes\n",
      "2025/04/04 12:11:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:27 INFO Model nb trained in 0.014 minutes\n",
      "2025/04/04 12:11:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:27 INFO Model dt trained in 0.013 minutes\n",
      "2025/04/04 12:11:28 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:28 INFO Model svm trained in 0.014 minutes\n",
      "2025/04/04 12:11:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:29 INFO Model rbfsvm trained in 0.013 minutes\n",
      "2025/04/04 12:11:30 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:31 INFO Model gpc trained in 0.026 minutes\n",
      "2025/04/04 12:11:32 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:32 INFO Model mlp trained in 0.025 minutes\n",
      "2025/04/04 12:11:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:33 INFO Model ridge trained in 0.013 minutes\n",
      "2025/04/04 12:11:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:34 INFO Model rf trained in 0.023 minutes\n",
      "2025/04/04 12:11:35 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:35 INFO Model qda trained in 0.013 minutes\n",
      "2025/04/04 12:11:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:36 INFO Model ada trained in 0.017 minutes\n",
      "2025/04/04 12:11:37 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:37 INFO Model gbc trained in 0.019 minutes\n",
      "2025/04/04 12:11:38 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:38 INFO Model lda trained in 0.012 minutes\n",
      "2025/04/04 12:11:39 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:39 INFO Model et trained in 0.022 minutes\n",
      "2025/04/04 12:11:40 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:11:40 INFO Model xgboost trained in 0.016 minutes\n",
      "2025/04/04 12:12:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:36 INFO Model lightgbm trained in 0.934 minutes\n",
      "2025/04/04 12:12:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:43 INFO Model catboost trained in 0.108 minutes\n",
      "2025/04/04 12:12:43 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:44 INFO Model dummy trained in 0.013 minutes\n",
      "04-04-2025 12:12:44 INFO Training over: Total runtime 1.323 minutes\n",
      "04-04-2025 12:12:45 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 12:12:45 INFO --------------------------------------------------------\n",
      "04-04-2025 12:12:45 INFO Training classification models\n",
      "04-04-2025 12:12:45 INFO The models used ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', 'ada', 'gbc', 'lda', 'et', 'xgboost', 'lightgbm', 'catboost', 'dummy']\n",
      "04-04-2025 12:12:45 INFO The number of models used 19\n",
      "04-04-2025 12:12:45 INFO Time budget is 20 minutes\n",
      "2025/04/04 12:12:45 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:45 INFO Model lr trained in 0.013 minutes\n",
      "2025/04/04 12:12:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:46 INFO Model knn trained in 0.013 minutes\n",
      "2025/04/04 12:12:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:47 INFO Model nb trained in 0.013 minutes\n",
      "2025/04/04 12:12:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:48 INFO Model dt trained in 0.013 minutes\n",
      "2025/04/04 12:12:48 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:48 INFO Model svm trained in 0.013 minutes\n",
      "2025/04/04 12:12:49 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:49 INFO Model rbfsvm trained in 0.013 minutes\n",
      "2025/04/04 12:12:50 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:51 INFO Model gpc trained in 0.023 minutes\n",
      "2025/04/04 12:12:52 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:52 INFO Model mlp trained in 0.026 minutes\n",
      "2025/04/04 12:12:53 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:53 INFO Model ridge trained in 0.013 minutes\n",
      "2025/04/04 12:12:54 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:54 INFO Model rf trained in 0.022 minutes\n",
      "2025/04/04 12:12:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:55 INFO Model qda trained in 0.013 minutes\n",
      "2025/04/04 12:12:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:56 INFO Model ada trained in 0.018 minutes\n",
      "2025/04/04 12:12:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:57 INFO Model gbc trained in 0.02 minutes\n",
      "2025/04/04 12:12:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:58 INFO Model lda trained in 0.013 minutes\n",
      "2025/04/04 12:12:59 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:12:59 INFO Model et trained in 0.02 minutes\n",
      "2025/04/04 12:13:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "04-04-2025 12:13:01 INFO Model xgboost trained in 0.025 minutes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m generator = iterate_excel(\u001b[33m\"\u001b[39m\u001b[33mclassification_results/filtered_features.xlsx\u001b[39m\u001b[33m\"\u001b[39m, parser=DataParser, \n\u001b[32m      2\u001b[39m                           label=\u001b[33m\"\u001b[39m\u001b[33m../data/esterase_labels.csv\u001b[39m\u001b[33m\"\u001b[39m, outliers=())\n\u001b[32m      4\u001b[39m split = ClusterSpliter(\u001b[33m\"\u001b[39m\u001b[33mcluster.tsv\u001b[39m\u001b[33m\"\u001b[39m, num_splits=\u001b[32m5\u001b[39m, random_state=\u001b[32m100\u001b[39m, stratified=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterate_multiple_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_output\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclassification_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BioML/BioML/models/base.py:1139\u001b[39m, in \u001b[36mTrainer.iterate_multiple_features\u001b[39m\u001b[34m(self, iterator, training_output, split_strategy, split_index, filter_sheet, test_size, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m split_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     X_train, X_test, _, _ = split_strategy.train_test_split(input_feature, input_feature[label_name], \n\u001b[32m   1137\u001b[39m                                                             test_size=test_size, \n\u001b[32m   1138\u001b[39m                                                             split_index=split_index)\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     sorted_results, sorted_models, top_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m                                                \u001b[49m\u001b[43mfold_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1142\u001b[39m     sorted_results, sorted_models, top_params = \u001b[38;5;28mself\u001b[39m.run_training(input_feature, label_name,\n\u001b[32m   1143\u001b[39m                                                                   **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BioML/BioML/models/base.py:996\u001b[39m, in \u001b[36mTrainer.run_training\u001b[39m\u001b[34m(self, feature, label_name, **kwargs)\u001b[39m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_training\u001b[39m(\u001b[38;5;28mself\u001b[39m, feature: pd.DataFrame, label_name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    973\u001b[39m                   **kwargs: Any) -> \u001b[38;5;28mtuple\u001b[39m[pd.DataFrame, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], pd.Series]:\n\u001b[32m    974\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    975\u001b[39m \u001b[33;03m        A function that splits the data into training and test sets and then trains the models\u001b[39;00m\n\u001b[32m    976\u001b[39m \u001b[33;03m        using cross-validation but only on the training data\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    994\u001b[39m \u001b[33;03m        \u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m        \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m         results, returned_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    997\u001b[39m         sorted_results, sorted_models = \u001b[38;5;28mself\u001b[39m.rank_results(results, returned_models, \u001b[38;5;28mself\u001b[39m.arguments._calculate_score_dataframe)\n\u001b[32m    998\u001b[39m         top_params = \u001b[38;5;28mself\u001b[39m.experiment.get_best_params_multiple(sorted_models)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BioML/BioML/models/base.py:851\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, features, label_name, **kwargs)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.arguments.add:\n\u001b[32m    849\u001b[39m     \u001b[38;5;28mself\u001b[39m.experiment.final_models += \u001b[38;5;28mself\u001b[39m.arguments.add\n\u001b[32m--> \u001b[39m\u001b[32m851\u001b[39m results, returned_models = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results, returned_models\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/BioML/BioML/models/base.py:439\u001b[39m, in \u001b[36mPycaretInterface.train\u001b[39m\u001b[34m(self, cross_validation)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.final_models:\n\u001b[32m    438\u001b[39m     model_time_start = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m     model = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpycaret\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    440\u001b[39m     model_results = \u001b[38;5;28mself\u001b[39m.pycaret.pull(pop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    441\u001b[39m     model_results = model_results.loc[[(\u001b[33m\"\u001b[39m\u001b[33mCV-Train\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMean\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mCV-Train\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mStd\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mCV-Val\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMean\u001b[39m\u001b[33m\"\u001b[39m), (\u001b[33m\"\u001b[39m\u001b[33mCV-Val\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mStd\u001b[39m\u001b[33m\"\u001b[39m)]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/pycaret/classification/oop.py:1344\u001b[39m, in \u001b[36mClassificationExperiment.create_model\u001b[39m\u001b[34m(self, estimator, fold, round, cross_validation, fit_kwargs, groups, experiment_custom_tags, probability_threshold, engine, verbose, return_train_score, **kwargs)\u001b[39m\n\u001b[32m   1341\u001b[39m     \u001b[38;5;28mself\u001b[39m._set_engine(estimator=estimator, engine=engine, severity=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1343\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     return_values = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1359\u001b[39m         \u001b[38;5;66;03m# Reset the models back to the default engines\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1766\u001b[39m, in \u001b[36m_SupervisedExperiment.create_model\u001b[39m\u001b[34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, return_train_score, **kwargs)\u001b[39m\n\u001b[32m   1754\u001b[39m \u001b[38;5;66;03m# TODO improve error message\u001b[39;00m\n\u001b[32m   1755\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m   1756\u001b[39m     x\n\u001b[32m   1757\u001b[39m     \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1764\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[32m   1765\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1769\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcross_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1771\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1772\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprobability_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexperiment_custom_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1779\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1780\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1533\u001b[39m, in \u001b[36m_SupervisedExperiment._create_model\u001b[39m\u001b[34m(self, estimator, fold, round, cross_validation, predict, fit_kwargs, groups, refit, probability_threshold, experiment_custom_tags, verbose, system, add_to_model_list, X_train_data, y_train_data, metrics, display, model_only, return_train_score, error_score, **kwargs)\u001b[39m\n\u001b[32m   1530\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m model, model_fit_time\n\u001b[32m   1531\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m-> \u001b[39m\u001b[32m1533\u001b[39m model, model_fit_time, model_results, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_model_with_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1535\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1536\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1537\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1538\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1539\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1541\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1543\u001b[39m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1544\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1545\u001b[39m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1546\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1547\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1549\u001b[39m \u001b[38;5;66;03m# end runtime\u001b[39;00m\n\u001b[32m   1550\u001b[39m runtime_end = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:1126\u001b[39m, in \u001b[36m_SupervisedExperiment._create_model_with_cv\u001b[39m\u001b[34m(self, model, data_X, data_y, fit_kwargs, round, cv, groups, metrics, refit, system, display, error_score, return_train_score)\u001b[39m\n\u001b[32m   1124\u001b[39m     model_fit_start = time.time()\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m redirect_output(\u001b[38;5;28mself\u001b[39m.logger):\n\u001b[32m-> \u001b[39m\u001b[32m1126\u001b[39m         scores = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpipeline_with_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m            \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m model_fit_end = time.time()\n\u001b[32m   1140\u001b[39m model_fit_time = np.array(model_fit_end - model_fit_start).round(\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    208\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    209\u001b[39m         skip_parameter_validation=(\n\u001b[32m    210\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    211\u001b[39m         )\n\u001b[32m    212\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    215\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    219\u001b[39m     msg = re.sub(\n\u001b[32m    220\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    221\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    222\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    223\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:430\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    427\u001b[39m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[32m    429\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     62\u001b[39m config = get_config()\n\u001b[32m     63\u001b[39m iterable_with_config = (\n\u001b[32m     64\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/bioml/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "generator = iterate_excel(\"classification_results/filtered_features.xlsx\", parser=DataParser, \n",
    "                          label=\"../data/esterase_labels.csv\", outliers=())\n",
    "\n",
    "split = ClusterSpliter(\"cluster.tsv\", num_splits=5, random_state=100, stratified=True)\n",
    "training.iterate_multiple_features(generator, training_output=\"classification_results\", split_strategy=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know which feature set is the best -> you train, tune and ensemble models using only that specific sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.models.classification import Classifier\n",
    "from BioML.models.regression import Regressor\n",
    "from BioML.models.base import PycaretInterface, Trainer, DataParser\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from BioML.utilities.utils import write_results\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from BioML.utilities.split_methods import ClusterSpliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Recall</th>\n",
       "      <th>Prec.</th>\n",
       "      <th>F1</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Average Precision Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr_Logistic Regression</th>\n",
       "      <th>0.8333</th>\n",
       "      <th>0.8054</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>0.7355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Voting Classifier</th>\n",
       "      <th>0.8333</th>\n",
       "      <th>0.8167</th>\n",
       "      <td>0.6923</td>\n",
       "      <td>0.9000</td>\n",
       "      <td>0.7826</td>\n",
       "      <td>0.6512</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>0.7564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassiveAggressiveClassifier_PassiveAggressiveClassifier</th>\n",
       "      <th>0.6667</th>\n",
       "      <th>0.6335</th>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.7143</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.2823</td>\n",
       "      <td>0.3128</td>\n",
       "      <td>0.5414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stacking Classifier</th>\n",
       "      <th>0.6000</th>\n",
       "      <th>0.7964</th>\n",
       "      <td>0.1538</td>\n",
       "      <td>0.6667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>0.1570</td>\n",
       "      <td>0.7232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nb_Naive Bayes</th>\n",
       "      <th>0.5333</th>\n",
       "      <th>0.6109</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0660</td>\n",
       "      <td>-0.1624</td>\n",
       "      <td>0.4872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    Recall  \\\n",
       "                                                   Accuracy AUC              \n",
       "lr_Logistic Regression                             0.8333   0.8054  0.6923   \n",
       "Voting Classifier                                  0.8333   0.8167  0.6923   \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335  0.3846   \n",
       "Stacking Classifier                                0.6000   0.7964  0.1538   \n",
       "nb_Naive Bayes                                     0.5333   0.6109  0.0000   \n",
       "\n",
       "                                                                     Prec.  \\\n",
       "                                                   Accuracy AUC              \n",
       "lr_Logistic Regression                             0.8333   0.8054  0.9000   \n",
       "Voting Classifier                                  0.8333   0.8167  0.9000   \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335  0.7143   \n",
       "Stacking Classifier                                0.6000   0.7964  0.6667   \n",
       "nb_Naive Bayes                                     0.5333   0.6109  0.0000   \n",
       "\n",
       "                                                                        F1  \\\n",
       "                                                   Accuracy AUC              \n",
       "lr_Logistic Regression                             0.8333   0.8054  0.7826   \n",
       "Voting Classifier                                  0.8333   0.8167  0.7826   \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335  0.5000   \n",
       "Stacking Classifier                                0.6000   0.7964  0.2500   \n",
       "nb_Naive Bayes                                     0.5333   0.6109  0.0000   \n",
       "\n",
       "                                                                     Kappa  \\\n",
       "                                                   Accuracy AUC              \n",
       "lr_Logistic Regression                             0.8333   0.8054  0.6512   \n",
       "Voting Classifier                                  0.8333   0.8167  0.6512   \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335  0.2823   \n",
       "Stacking Classifier                                0.6000   0.7964  0.1045   \n",
       "nb_Naive Bayes                                     0.5333   0.6109 -0.0660   \n",
       "\n",
       "                                                                       MCC  \\\n",
       "                                                   Accuracy AUC              \n",
       "lr_Logistic Regression                             0.8333   0.8054  0.6659   \n",
       "Voting Classifier                                  0.8333   0.8167  0.6659   \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335  0.3128   \n",
       "Stacking Classifier                                0.6000   0.7964  0.1570   \n",
       "nb_Naive Bayes                                     0.5333   0.6109 -0.1624   \n",
       "\n",
       "                                                                    Average Precision Score  \n",
       "                                                   Accuracy AUC                              \n",
       "lr_Logistic Regression                             0.8333   0.8054                   0.7355  \n",
       "Voting Classifier                                  0.8333   0.8167                   0.7564  \n",
       "PassiveAggressiveClassifier_PassiveAggressiveCl... 0.6667   0.6335                   0.5414  \n",
       "Stacking Classifier                                0.6000   0.7964                   0.7232  \n",
       "nb_Naive Bayes                                     0.5333   0.6109                   0.4872  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_excel(\"/home/ruite/Projects/BioML/examples/classification_results/not_tuned/training_results.xlsx\", sheet_name=[\"train\", \"test_results\"], index_col=[0,1,2])[\"test_results\"].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We first read the data and create train and test set based on the sequence identity threshold\n",
    "The DataParser returns a dataframe with the labels as a column in the same dataframe, which is why we don't need the y_train and the y_test.\n",
    "This is how Pycaret likes it (the library used by BioML to train machine learning models) \n",
    "\n",
    "Here I'm only using one of the sheets for this example, but we don't know which of the sheets would perform better so we might need to iterate through the sheets, train the models and collect the performance for each sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataParser(\"classification_results/filtered_features.xlsx\", \"../data/esterase_labels.csv\", sheets=\"chi2_60\")\n",
    "data = DataParser(\"../data/esterase_features.xlsx\", \"../data/esterase_labels.csv\", sheets=\"ch2_20\")\n",
    "split = ClusterSpliter(\"cluster.tsv\", num_splits=5, random_state=100, stratified=True)\n",
    "X_train, X_test, _, _ = split.train_test_split(data.features, data.features[data.label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hydrophobicity_FASG890101.2.residue25</th>\n",
       "      <th>alphaticr.negativecharger.gap5</th>\n",
       "      <th>BHAR880101.lag9_moran</th>\n",
       "      <th>tri_gram_pssm7077</th>\n",
       "      <th>tri_gram_pssm286</th>\n",
       "      <th>tri_gram_pssm466</th>\n",
       "      <th>tri_gram_pssm3493</th>\n",
       "      <th>tri_gram_pssm1298</th>\n",
       "      <th>tri_gram_pssm5728</th>\n",
       "      <th>tri_gram_pssm7468</th>\n",
       "      <th>tri_gram_pssm1297</th>\n",
       "      <th>tri_gram_pssm4023</th>\n",
       "      <th>tri_gram_pssm3159</th>\n",
       "      <th>tri_gram_pssm7678</th>\n",
       "      <th>tri_gram_pssm7234</th>\n",
       "      <th>tri_gram_pssm5801</th>\n",
       "      <th>tri_gram_pssm5367</th>\n",
       "      <th>tri_gram_pssm7177</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EH111(8)</th>\n",
       "      <td>19.762846</td>\n",
       "      <td>0.040486</td>\n",
       "      <td>0.084414</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.051697</td>\n",
       "      <td>0.048276</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.088146</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.035476</td>\n",
       "      <td>0.080685</td>\n",
       "      <td>0.070774</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH15(42)</th>\n",
       "      <td>26.279863</td>\n",
       "      <td>0.052265</td>\n",
       "      <td>-0.051729</td>\n",
       "      <td>0.010338</td>\n",
       "      <td>0.573238</td>\n",
       "      <td>0.169123</td>\n",
       "      <td>0.121183</td>\n",
       "      <td>0.019340</td>\n",
       "      <td>0.333862</td>\n",
       "      <td>0.171416</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.451274</td>\n",
       "      <td>0.081755</td>\n",
       "      <td>0.229442</td>\n",
       "      <td>0.082068</td>\n",
       "      <td>0.262047</td>\n",
       "      <td>0.289476</td>\n",
       "      <td>0.006036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH117(6)</th>\n",
       "      <td>25.267666</td>\n",
       "      <td>0.036876</td>\n",
       "      <td>-0.027442</td>\n",
       "      <td>0.007047</td>\n",
       "      <td>0.075657</td>\n",
       "      <td>0.082276</td>\n",
       "      <td>0.037046</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>0.030363</td>\n",
       "      <td>0.023863</td>\n",
       "      <td>0.003769</td>\n",
       "      <td>0.188441</td>\n",
       "      <td>0.026338</td>\n",
       "      <td>0.058306</td>\n",
       "      <td>0.080121</td>\n",
       "      <td>0.122729</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.009361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH11(53)</th>\n",
       "      <td>19.032258</td>\n",
       "      <td>0.032895</td>\n",
       "      <td>0.042455</td>\n",
       "      <td>0.065163</td>\n",
       "      <td>0.875810</td>\n",
       "      <td>0.447033</td>\n",
       "      <td>0.195845</td>\n",
       "      <td>0.096829</td>\n",
       "      <td>0.513410</td>\n",
       "      <td>0.182430</td>\n",
       "      <td>0.031378</td>\n",
       "      <td>0.635770</td>\n",
       "      <td>0.265902</td>\n",
       "      <td>0.460689</td>\n",
       "      <td>0.281517</td>\n",
       "      <td>0.613932</td>\n",
       "      <td>0.404746</td>\n",
       "      <td>0.028915</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH135(2)</th>\n",
       "      <td>24.555160</td>\n",
       "      <td>0.047273</td>\n",
       "      <td>0.035283</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.053927</td>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.007024</td>\n",
       "      <td>0.005613</td>\n",
       "      <td>0.022906</td>\n",
       "      <td>0.011953</td>\n",
       "      <td>0.005436</td>\n",
       "      <td>0.069167</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.040943</td>\n",
       "      <td>0.023389</td>\n",
       "      <td>0.035671</td>\n",
       "      <td>0.049431</td>\n",
       "      <td>0.004665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          hydrophobicity_FASG890101.2.residue25  \\\n",
       "EH111(8)                              19.762846   \n",
       "EH15(42)                              26.279863   \n",
       "EH117(6)                              25.267666   \n",
       "EH11(53)                              19.032258   \n",
       "EH135(2)                              24.555160   \n",
       "\n",
       "          alphaticr.negativecharger.gap5  BHAR880101.lag9_moran  \\\n",
       "EH111(8)                        0.040486               0.084414   \n",
       "EH15(42)                        0.052265              -0.051729   \n",
       "EH117(6)                        0.036876              -0.027442   \n",
       "EH11(53)                        0.032895               0.042455   \n",
       "EH135(2)                        0.047273               0.035283   \n",
       "\n",
       "          tri_gram_pssm7077  tri_gram_pssm286  tri_gram_pssm466  \\\n",
       "EH111(8)           0.002188          0.051697          0.048276   \n",
       "EH15(42)           0.010338          0.573238          0.169123   \n",
       "EH117(6)           0.007047          0.075657          0.082276   \n",
       "EH11(53)           0.065163          0.875810          0.447033   \n",
       "EH135(2)           0.004763          0.053927          0.031520   \n",
       "\n",
       "          tri_gram_pssm3493  tri_gram_pssm1298  tri_gram_pssm5728  \\\n",
       "EH111(8)           0.016844           0.004625           0.018720   \n",
       "EH15(42)           0.121183           0.019340           0.333862   \n",
       "EH117(6)           0.037046           0.010299           0.030363   \n",
       "EH11(53)           0.195845           0.096829           0.513410   \n",
       "EH135(2)           0.007024           0.005613           0.022906   \n",
       "\n",
       "          tri_gram_pssm7468  tri_gram_pssm1297  tri_gram_pssm4023  \\\n",
       "EH111(8)           0.010066           0.001115           0.088146   \n",
       "EH15(42)           0.171416           0.006026           0.451274   \n",
       "EH117(6)           0.023863           0.003769           0.188441   \n",
       "EH11(53)           0.182430           0.031378           0.635770   \n",
       "EH135(2)           0.011953           0.005436           0.069167   \n",
       "\n",
       "          tri_gram_pssm3159  tri_gram_pssm7678  tri_gram_pssm7234  \\\n",
       "EH111(8)           0.019392           0.030169           0.035476   \n",
       "EH15(42)           0.081755           0.229442           0.082068   \n",
       "EH117(6)           0.026338           0.058306           0.080121   \n",
       "EH11(53)           0.265902           0.460689           0.281517   \n",
       "EH135(2)           0.018054           0.040943           0.023389   \n",
       "\n",
       "          tri_gram_pssm5801  tri_gram_pssm5367  tri_gram_pssm7177  target  \n",
       "EH111(8)           0.080685           0.070774           0.001775       0  \n",
       "EH15(42)           0.262047           0.289476           0.006036       1  \n",
       "EH117(6)           0.122729           0.022790           0.009361       0  \n",
       "EH11(53)           0.613932           0.404746           0.028915       1  \n",
       "EH135(2)           0.035671           0.049431           0.004665       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 classes important for training machine learning models in BioML. \n",
    "\n",
    "1. The pycaretInterface which provides a thin layer between pycaret and BioML.\n",
    "2. Classifer or Regressor clases which has the training arguments\n",
    "3. Then the Trainer class that takes both classes to perform classification or regression task.\n",
    "\n",
    "This modularity is similar to other machine learning framewoeks like pytorch-lightning and Hugging Face\n",
    "\n",
    "I recommend learning more about pycaret since it does many things: https://pycaret.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently there are 19 supported models, but you can supply your custom models as well as long as it is compatible with the scikit learn API. A fit and a predict or predict_proba methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"classification\"\n",
    "passive = PassiveAggressiveClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PycaretInterface class takes the following arguments\n",
    "\n",
    "* problem (str): classification or regression\n",
    "* seed (int): random seed\n",
    "* budget_time (int): time budget\n",
    "* best_model (int): How many models to select for tuning and ensembling\n",
    "* output_path (str): path to the output folder where the plots are saved\n",
    "* optimize (str): optimization metric for tune model, ensembling and satcking -> it doesnt affect the training\n",
    "\n",
    "The Trainer class takes the following arguments:\n",
    "\n",
    "* caret_interface (PycaretInterface): an instance of the PycaretInterface class\n",
    "* args (Classifier or Regressor): an instance of the Classifier or Regressor class\n",
    "* num_splits (int): number of splits\n",
    "* test_size (float): test size\n",
    "* num_iter (int): number of iterations\n",
    "* cross_validation (bool): cross validation\n",
    "\n",
    "The Classifier and Regressor classes take the following arguments:\n",
    "\n",
    "* drop (list): list of models to drop\n",
    "* selected (list): list of models to select\n",
    "* add (list): list of models to add\n",
    "* plot (list): list of plots to plot\n",
    "* optimize (str): optimization metric for sorting the model results, so the best models according to the optimize are at the top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:02:58 INFO ------------------------------------------------------------------------------\n",
      "04-04-2025 09:02:58 INFO PycaretInterface parameters\n",
      "04-04-2025 09:02:58 INFO Seed: 250\n",
      "04-04-2025 09:02:58 INFO Budget time: 20\n",
      "04-04-2025 09:02:58 INFO The number of models to select: 3\n",
      "04-04-2025 09:02:58 INFO Output path: classification_results\n",
      "04-04-2025 09:02:58 INFO ----------------Trainer inputs-------------------------\n",
      "04-04-2025 09:02:58 INFO Number of kfolds: 5\n",
      "04-04-2025 09:02:58 INFO Number of retuning iterations: 50\n",
      "04-04-2025 09:02:58 INFO Test size: 0.2\n"
     ]
    }
   ],
   "source": [
    "if problem == \"classification\":\n",
    "    args = Classifier(optimize=\"MCC\", drop=(), selected=(), add=(passive), plot=(\"learning\", \"confusion_matrix\", \"class_report\"))\n",
    "elif problem == \"regression\":\n",
    "    args = Regressor(optimize=\"RMSE\", drop=(), selected=(), add=(), plot=(\"learning\", \"residuals\", \"error\"))\n",
    "    \n",
    "experiment = PycaretInterface(problem, 250, budget_time=20, best_model=3, \n",
    "                                  output_path=\"classification_results\", optimize=\"MCC\", log_experiment=False)\n",
    "training = Trainer(experiment, args, num_splits=5, test_size=0.2,  num_iter=50, cross_validation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments for generate_training_results are:\n",
    "\n",
    "* X_train (pd.DataFrame): training data\n",
    "* y_train (str): the name of the label column, since Pycaret needs the dataframe to have the labels and the features together in the X_train\n",
    "* X_test (pd.DataFrame): test data, it is a keyword argument (so you need to specify test_data=X_test,)\n",
    "* tune (bool): whether to tune the models\n",
    "* fold_strategy (ClusterSpliter): an instance of the ClusterSpliter class which has the cluster information\n",
    "\n",
    "The output: \n",
    "* results dict[str, dict[str, pd.DataFrame]] (returned): The dataframe has the different performance metric liek acc, precision, recall, etc\n",
    "* The models dict[str, dict[str, Any]] (returned): All the train models, 19 classifiers or 23 regressors\n",
    "* model_plots (saved): The plots of the best models saved in the output_path argument in PycaretInterface\n",
    "* config_setup_pycaret.csv (saved): The config file with the settings used by PycaretInterface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:03:01 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 09:03:01 INFO --------------------------------------------------------\n",
      "04-04-2025 09:03:01 INFO Training classification models\n",
      "04-04-2025 09:03:01 INFO The models used ['lr', 'knn', 'nb', 'dt', 'svm', 'rbfsvm', 'gpc', 'mlp', 'ridge', 'rf', 'qda', 'ada', 'gbc', 'lda', 'et', 'xgboost', 'lightgbm', 'catboost', 'dummy', PassiveAggressiveClassifier(C=1.0, average=False, class_weight=None,\n",
      "                            early_stopping=False, fit_intercept=True,\n",
      "                            loss='hinge', max_iter=1000, n_iter_no_change=5,\n",
      "                            n_jobs=None, random_state=None, shuffle=True,\n",
      "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
      "                            warm_start=False)]\n",
      "04-04-2025 09:03:01 INFO The number of models used 20\n",
      "04-04-2025 09:03:01 INFO Time budget is 20 minutes\n",
      "04-04-2025 09:03:04 INFO Model lr trained in 0.047 minutes\n",
      "04-04-2025 09:03:06 INFO Model knn trained in 0.034 minutes\n",
      "04-04-2025 09:03:06 INFO Model nb trained in 0.009 minutes\n",
      "04-04-2025 09:03:07 INFO Model dt trained in 0.009 minutes\n",
      "04-04-2025 09:03:07 INFO Model svm trained in 0.009 minutes\n",
      "04-04-2025 09:03:08 INFO Model rbfsvm trained in 0.009 minutes\n",
      "04-04-2025 09:03:09 INFO Model gpc trained in 0.021 minutes\n",
      "04-04-2025 09:03:10 INFO Model mlp trained in 0.022 minutes\n",
      "04-04-2025 09:03:11 INFO Model ridge trained in 0.009 minutes\n",
      "04-04-2025 09:03:12 INFO Model rf trained in 0.016 minutes\n",
      "04-04-2025 09:03:12 INFO Model qda trained in 0.009 minutes\n",
      "04-04-2025 09:03:13 INFO Model ada trained in 0.012 minutes\n",
      "04-04-2025 09:03:14 INFO Model gbc trained in 0.013 minutes\n",
      "04-04-2025 09:03:14 INFO Model lda trained in 0.009 minutes\n",
      "04-04-2025 09:03:15 INFO Model et trained in 0.014 minutes\n",
      "04-04-2025 09:03:16 INFO Model xgboost trained in 0.012 minutes\n",
      "04-04-2025 09:04:12 INFO Model lightgbm trained in 0.941 minutes\n",
      "04-04-2025 09:04:18 INFO Model catboost trained in 0.098 minutes\n",
      "04-04-2025 09:04:19 INFO Model dummy trained in 0.009 minutes\n",
      "04-04-2025 09:04:19 INFO Model PassiveAggressiveClassifier trained in 0.009 minutes\n",
      "04-04-2025 09:04:19 INFO Training over: Total runtime 1.313 minutes\n",
      "04-04-2025 09:04:19 INFO Analyse the best models and plotting them\n",
      "04-04-2025 09:04:19 INFO Analyse the top 1 model: nb\n",
      "04-04-2025 09:04:21 INFO Analyse the top 2 model: lr\n",
      "04-04-2025 09:04:23 INFO Analyse the top 3 model: PassiveAggressiveClassifier\n",
      "04-04-2025 09:04:24 INFO --------Stacking the best models--------\n",
      "04-04-2025 09:04:24 INFO ----------Stacking the best models--------------\n",
      "04-04-2025 09:04:25 INFO --------Creating an ensemble model--------\n",
      "04-04-2025 09:04:25 INFO ----------Creating a majority voting model--------------\n",
      "04-04-2025 09:04:25 INFO fold: 5\n",
      "04-04-2025 09:04:25 INFO weights: None\n",
      "04-04-2025 09:04:26 INFO --------Retuning the best models--------\n",
      "04-04-2025 09:04:26 INFO Retuning nb\n",
      "04-04-2025 09:04:26 INFO Retuning lr\n",
      "04-04-2025 09:04:26 INFO ---------Retuning the best models--------------\n",
      "04-04-2025 09:04:26 INFO num_iter: 50\n",
      "04-04-2025 09:04:26 INFO fold: 5\n",
      "04-04-2025 09:04:37 INFO Retuning PassiveAggressiveClassifier\n",
      "04-04-2025 09:04:37 INFO --------Stacking the best models--------\n",
      "04-04-2025 09:04:37 INFO ----------Stacking the best models--------------\n",
      "04-04-2025 09:04:38 INFO --------Creating an ensemble model--------\n",
      "04-04-2025 09:04:38 INFO ----------Creating a majority voting model--------------\n",
      "04-04-2025 09:04:38 INFO fold: 5\n",
      "04-04-2025 09:04:38 INFO weights: None\n",
      "04-04-2025 09:04:39 INFO Analyse the best models and plotting them\n",
      "04-04-2025 09:04:39 INFO Analyse the top 1 model: nb\n",
      "04-04-2025 09:04:40 INFO Analyse the top 2 model: lr\n",
      "04-04-2025 09:04:42 INFO Analyse the top 3 model: PassiveAggressiveClassifier\n"
     ]
    }
   ],
   "source": [
    "results, models = training.generate_training_results(X_train, data.label, tune=True, test_data=X_test, fold_strategy=split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pycaret also generates different plots for the models it trains, like learning curves, validation curves, Precision Recall Curve, AUC etc: \n",
    "You can add more plots by modifying the plots argument in the classifier or regressor class\n",
    "\n",
    "You can see the full list of plots here: https://pycaret.readthedocs.io/en/stable/api/classification.html#pycaret.classification.plot_model\n",
    "\n",
    "Let's explain the 3 plots that I included by default for the classification: (\"learning\", \"confusion_matrix\", \"class_report\")\n",
    "For regression these are the 3 plots: (\"residuals\", \"error\", \"learning\")\n",
    "Here is an example learning curve plot.\n",
    "\n",
    "You can access directly the pycaret class and play with it like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment.pycaret.plot_model(models[\"not_tuned\"][\"train\"][\"rbfsvm\"], \"learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we want we can generate the predictions for the test or holdout set by providing with the trained models and finally \n",
    "we save both the training and the test set results\n",
    "\n",
    "Looking at the results we can find out which models have performed well and see how we could combine them. Currently it combines automatically teh best 3 but that might not be the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument is :\n",
    "* the models dict[str, dict[str, Any]]: The trained models\n",
    "\n",
    "The output is:\n",
    "\n",
    "* test_set_predictions (returned -> dict[str, pd.DataFrame]): The test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_predictions = training.generate_holdout_prediction(models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The write_results argumets:\n",
    "* training_output (str): the path to the training output\n",
    "* sheet_name (str): the name of the sheet\n",
    "* sorted_results (pd.DataFrame): the sorted results\n",
    "* top_params (pd.Series, optional): the hyperparameters of the best models\n",
    "\n",
    "The output:\n",
    "* \"training_output / training_results.xlsx\" (saved)\n",
    "* \"training_output / top_hyperparameters.xlsx\" (saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_output = \"classification_results\"\n",
    "l = []\n",
    "for tune_status, result_dict in results.items():\n",
    "    for key, value in result_dict.items():\n",
    "        write_results(f\"{training_output}/{tune_status}\", *value, sheet_name=key)\n",
    "    write_results(f\"{training_output}/{tune_status}\", test_set_predictions[tune_status] , sheet_name=f\"test_results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Save Model from pre-trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you noticed, the previous step doesn't save any model only the training results. That is because both are separate blocks, and you need to use save_model to save them.\n",
    "\n",
    "To save the models you can train the model with all the data or save the model trained only with the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qda': QuadraticDiscriminantAnalysis(priors=None, reg_param=0.8258792813861613,\n",
       "                               store_covariance=False, tol=0.0001),\n",
       " 'knn': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                      metric_params=None, n_jobs=-1, n_neighbors=24, p=2,\n",
       "                      weights='uniform'),\n",
       " 'nb': GaussianNB(priors=None, var_smoothing=0.7918521648729947)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[\"tuned\"][\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.models import save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GenerateModel arguments are:\n",
    "* trainer (Trainer): the trainer object, it should have been used in the previous step\n",
    "\n",
    "The finalize_model arguments are:\n",
    "* sorted_model (dict or Any): the model to finalize, it can be a dictrionary or a model object\n",
    "* index (int, optional): if it is a dictionary you need to indicate the index of the model you want to finalize\n",
    "\n",
    "The output of finalize_model is:\n",
    "* The fitted model with all the data\n",
    "\n",
    "The save_model arguments are:\n",
    "* model (Any): the model to save\n",
    "* filename (str): the filename to save the model\n",
    "\n",
    "The output of save_model is:\n",
    "* \"classification_results/saved_models/{status}_{name}\" -> the program adds .pkl automatically so no need for an extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:12 INFO ----------Finalizing the model by training it with all the data including test set--------------\n",
      "04-04-2025 09:10:13 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:13 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:14 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:15 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:15 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:16 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:16 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:17 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:10:18 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n",
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "generate = save_model.GenerateModel(training)\n",
    "for status, model in models.items():\n",
    "    for key, value in model.items():\n",
    "        if key == \"train\":\n",
    "            for num, (name, mod) in enumerate(value.items()):\n",
    "                if num > training.experiment.best_model - 1 : break\n",
    "                final_model = generate.finalize_model(value, num)\n",
    "                generate.save_model(final_model, f\"classification_results/saved_models/{status}_{name}\")\n",
    "        else:\n",
    "            final_model = generate.finalize_model(value)\n",
    "            generate.save_model(final_model, f\"classification_results/saved_models/{key}_{status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Save Model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example on how to train on a few or a single model using the selected argument and save that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.models import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:11:38 INFO ------------------------------------------------------------------------------\n",
      "04-04-2025 09:11:38 INFO PycaretInterface parameters\n",
      "04-04-2025 09:11:38 INFO Seed: 200\n",
      "04-04-2025 09:11:38 INFO Budget time: 20\n",
      "04-04-2025 09:11:38 INFO The number of models to select: 3\n",
      "04-04-2025 09:11:38 INFO Output path: classification_results\n",
      "04-04-2025 09:11:38 INFO ----------------Trainer inputs-------------------------\n",
      "04-04-2025 09:11:38 INFO Number of kfolds: 5\n",
      "04-04-2025 09:11:38 INFO Number of retuning iterations: 30\n",
      "04-04-2025 09:11:38 INFO Test size: 0.2\n"
     ]
    }
   ],
   "source": [
    "data = DataParser(\"classification_results/filtered_features.xlsx\", \"../data/esterase_labels.csv\", sheets=\"chi2_60\")\n",
    "experiment = PycaretInterface(\"classification\", 200, budget_time=20, best_model=3, \n",
    "                                  output_path=\"classification_results\", optimize=\"MCC\", experiment_name=\"generate_model\", log_experiment=False)\n",
    "classifier = Classifier(optimize=\"MCC\", selected=(\"qda\", \"rbfsvm\", \"ridge\"))\n",
    "training = Trainer(experiment, classifier, num_splits=5, test_size=0.2,  num_iter=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:11:40 INFO Added metrics for classification: Average precision\n",
      "04-04-2025 09:11:40 INFO --------------------------------------------------------\n",
      "04-04-2025 09:11:40 INFO Training classification models\n",
      "04-04-2025 09:11:40 INFO The models used ('qda', 'rbfsvm', 'ridge')\n",
      "04-04-2025 09:11:40 INFO The number of models used 3\n",
      "04-04-2025 09:11:40 INFO Time budget is 20 minutes\n",
      "04-04-2025 09:11:41 INFO Model qda trained in 0.015 minutes\n",
      "04-04-2025 09:11:42 INFO Model rbfsvm trained in 0.011 minutes\n",
      "04-04-2025 09:11:43 INFO Model ridge trained in 0.009 minutes\n",
      "04-04-2025 09:11:43 INFO Training over: Total runtime 0.035 minutes\n"
     ]
    }
   ],
   "source": [
    "sorted_results, sorted_models, top_params = training.run_training(data.features, data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04-04-2025 09:12:05 INFO --------Creating an ensemble model--------\n",
      "04-04-2025 09:12:05 INFO ----------Creating a majority voting model--------------\n",
      "04-04-2025 09:12:05 INFO fold: 5\n",
      "04-04-2025 09:12:05 INFO weights: None\n",
      "04-04-2025 09:12:06 INFO ----------Finalizing the model by training it with all the data including test set--------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "generate = save_model.GenerateModel(training)\n",
    "models =  generate.train_by_strategy(sorted_models, \"majority\")\n",
    "final_model = generate.finalize_model(models)\n",
    "generate.save_model(final_model, \"classification_results/single_model/logistic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally once you have the models you can make predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.models import predict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = \"classification_results/filtered_features.xlsx\"\n",
    "label = \"../data/esterase_labels.csv\"\n",
    "outlier_train=()\n",
    "outlier_test=()\n",
    "sheet_name=\"chi2_60\"\n",
    "problem=\"classification\"\n",
    "model_path=\"classification_results/single_model/logistic\"\n",
    "scaler=\"robust\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediccions are appended to the features dataframe, if it is classification it returns\n",
    "the probability of being in class 0 or class 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument for the predict function are:\n",
    "\n",
    "* test_features (pandas.DataFrame): The test features\n",
    "* model_path (str): The path to the model, you don't need to scale the test features, becuase the saved model has then entire pipeline saved\n",
    "* problem (str): The problem type, classification (it returns the probability of being in class 0 or class 1), regression it returns the value\n",
    "\n",
    "The output of the predict function is:\n",
    "\n",
    "* The predictions (pd.DataFrame): The predictions are appended to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = predict.DataParser(training_features, label, outliers=outlier_train, sheets=sheet_name)\n",
    "test_features = feature.remove_outliers(feature.read_features(training_features, \"chi2_60\"), outlier_test)\n",
    "predictions = predict.predict(test_features, model_path, problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHAM810101.lag23_NMBroto</th>\n",
       "      <th>CHOC760101.lag22_Geary</th>\n",
       "      <th>CIDH920105.lag1_Moran</th>\n",
       "      <th>CHAM820102.lag4_Moran</th>\n",
       "      <th>Schneider.lag1_SOCNumber</th>\n",
       "      <th>pssm_cc360</th>\n",
       "      <th>pssm_cc542</th>\n",
       "      <th>pssm_cc662</th>\n",
       "      <th>pssm_cc744</th>\n",
       "      <th>pssm_cc1025</th>\n",
       "      <th>...</th>\n",
       "      <th>CHAM810101.lag7_NMBroto</th>\n",
       "      <th>CHAM810101.lag9_NMBroto</th>\n",
       "      <th>CHAM810101.lag24_NMBroto</th>\n",
       "      <th>CHAM810101.lag30_NMBroto</th>\n",
       "      <th>DAYM780201.lag26_NMBroto</th>\n",
       "      <th>CIDH920105.lag3_Geary</th>\n",
       "      <th>CIDH920105.lag11_Geary</th>\n",
       "      <th>CIDH920105.lag16_Geary</th>\n",
       "      <th>CIDH920105.lag23_Geary</th>\n",
       "      <th>prediction_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EH1(72)</th>\n",
       "      <td>-0.086296</td>\n",
       "      <td>1.037206</td>\n",
       "      <td>0.028302</td>\n",
       "      <td>-0.082859</td>\n",
       "      <td>0.371161</td>\n",
       "      <td>-0.000232</td>\n",
       "      <td>-0.000284</td>\n",
       "      <td>-0.000167</td>\n",
       "      <td>-0.000974</td>\n",
       "      <td>-0.001420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.168890</td>\n",
       "      <td>0.041882</td>\n",
       "      <td>0.029023</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.966769</td>\n",
       "      <td>1.092017</td>\n",
       "      <td>1.031714</td>\n",
       "      <td>1.037640</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH2(71)</th>\n",
       "      <td>-0.110262</td>\n",
       "      <td>1.025836</td>\n",
       "      <td>-0.045634</td>\n",
       "      <td>-0.026907</td>\n",
       "      <td>0.348699</td>\n",
       "      <td>-0.000486</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>-0.000954</td>\n",
       "      <td>-0.000170</td>\n",
       "      <td>-0.000966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053429</td>\n",
       "      <td>0.019795</td>\n",
       "      <td>-0.158698</td>\n",
       "      <td>-0.071886</td>\n",
       "      <td>0.005959</td>\n",
       "      <td>0.996057</td>\n",
       "      <td>0.985778</td>\n",
       "      <td>0.999326</td>\n",
       "      <td>1.039344</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH3(69)</th>\n",
       "      <td>-0.125391</td>\n",
       "      <td>1.050111</td>\n",
       "      <td>0.024718</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.346771</td>\n",
       "      <td>-0.000700</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.000458</td>\n",
       "      <td>-0.000875</td>\n",
       "      <td>-0.001095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076793</td>\n",
       "      <td>-0.041301</td>\n",
       "      <td>0.079643</td>\n",
       "      <td>-0.111475</td>\n",
       "      <td>0.029596</td>\n",
       "      <td>0.977896</td>\n",
       "      <td>0.919151</td>\n",
       "      <td>0.881323</td>\n",
       "      <td>1.109520</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CalB(68)</th>\n",
       "      <td>0.135638</td>\n",
       "      <td>1.005781</td>\n",
       "      <td>0.032854</td>\n",
       "      <td>-0.042622</td>\n",
       "      <td>0.251895</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>-0.001548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>-0.024326</td>\n",
       "      <td>-0.017794</td>\n",
       "      <td>0.059463</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>1.072358</td>\n",
       "      <td>0.999313</td>\n",
       "      <td>0.959426</td>\n",
       "      <td>1.012822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EH4(67)</th>\n",
       "      <td>-0.201467</td>\n",
       "      <td>1.008612</td>\n",
       "      <td>-0.042323</td>\n",
       "      <td>-0.007536</td>\n",
       "      <td>0.340954</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.000731</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133993</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>-0.143633</td>\n",
       "      <td>-0.117702</td>\n",
       "      <td>-0.054452</td>\n",
       "      <td>1.018159</td>\n",
       "      <td>1.068451</td>\n",
       "      <td>1.038840</td>\n",
       "      <td>1.027438</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          CHAM810101.lag23_NMBroto  CHOC760101.lag22_Geary  \\\n",
       "EH1(72)                  -0.086296                1.037206   \n",
       "EH2(71)                  -0.110262                1.025836   \n",
       "EH3(69)                  -0.125391                1.050111   \n",
       "CalB(68)                  0.135638                1.005781   \n",
       "EH4(67)                  -0.201467                1.008612   \n",
       "\n",
       "          CIDH920105.lag1_Moran  CHAM820102.lag4_Moran  \\\n",
       "EH1(72)                0.028302              -0.082859   \n",
       "EH2(71)               -0.045634              -0.026907   \n",
       "EH3(69)                0.024718               0.003147   \n",
       "CalB(68)               0.032854              -0.042622   \n",
       "EH4(67)               -0.042323              -0.007536   \n",
       "\n",
       "          Schneider.lag1_SOCNumber  pssm_cc360  pssm_cc542  pssm_cc662  \\\n",
       "EH1(72)                   0.371161   -0.000232   -0.000284   -0.000167   \n",
       "EH2(71)                   0.348699   -0.000486    0.000237   -0.000954   \n",
       "EH3(69)                   0.346771   -0.000700    0.000922   -0.000458   \n",
       "CalB(68)                  0.251895    0.000807   -0.000265    0.001517   \n",
       "EH4(67)                   0.340954   -0.000667   -0.000236   -0.000500   \n",
       "\n",
       "          pssm_cc744  pssm_cc1025  ...  CHAM810101.lag7_NMBroto  \\\n",
       "EH1(72)    -0.000974    -0.001420  ...                -0.168890   \n",
       "EH2(71)    -0.000170    -0.000966  ...                 0.053429   \n",
       "EH3(69)    -0.000875    -0.001095  ...                 0.076793   \n",
       "CalB(68)    0.001502    -0.001548  ...                 0.018115   \n",
       "EH4(67)    -0.000306    -0.000731  ...                 0.133993   \n",
       "\n",
       "          CHAM810101.lag9_NMBroto  CHAM810101.lag24_NMBroto  \\\n",
       "EH1(72)                  0.041882                  0.029023   \n",
       "EH2(71)                  0.019795                 -0.158698   \n",
       "EH3(69)                 -0.041301                  0.079643   \n",
       "CalB(68)                -0.024326                 -0.017794   \n",
       "EH4(67)                  0.030291                 -0.143633   \n",
       "\n",
       "          CHAM810101.lag30_NMBroto  DAYM780201.lag26_NMBroto  \\\n",
       "EH1(72)                   0.013333                  0.002101   \n",
       "EH2(71)                  -0.071886                  0.005959   \n",
       "EH3(69)                  -0.111475                  0.029596   \n",
       "CalB(68)                  0.059463                  0.012514   \n",
       "EH4(67)                  -0.117702                 -0.054452   \n",
       "\n",
       "          CIDH920105.lag3_Geary  CIDH920105.lag11_Geary  \\\n",
       "EH1(72)                0.966769                1.092017   \n",
       "EH2(71)                0.996057                0.985778   \n",
       "EH3(69)                0.977896                0.919151   \n",
       "CalB(68)               1.072358                0.999313   \n",
       "EH4(67)                1.018159                1.068451   \n",
       "\n",
       "          CIDH920105.lag16_Geary  CIDH920105.lag23_Geary  prediction_label  \n",
       "EH1(72)                 1.031714                1.037640                 1  \n",
       "EH2(71)                 0.999326                1.039344                 1  \n",
       "EH3(69)                 0.881323                1.109520                 1  \n",
       "CalB(68)                0.959426                1.012822                 1  \n",
       "EH4(67)                 1.038840                1.027438                 1  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional if you want to filter predictions based if that test sample is within the applicability domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicability domain compares eucleadian distance between the features from the training and features from the test.   \n",
    "If the distance is far from a set threshold then that prediction is discarded since it deviates from the samples the model have seen during training  \n",
    "Testing on the training dataset it discards a few as well so the error can be quite high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed, scaler_dict, test_x = predict.scale(scaler, feature.drop(), test_features)\n",
    "filtered_pred = predict.domain_filter(predictions, transformed, test_x, 5) # it returns the predictions appended to the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((130, 63), (147, 61))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_pred.shape, predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " if you don't apply the domain filter, you can just use the predictions but you have to add a new Index column to sample_0, sample_1, etc to filter the fasta files into positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"Index\"] = [f\"sample_{x}\" for x, _ in enumerate(predictions.index)]\n",
    "col_name = [\"prediction_score\", \"prediction_label\", \"AD_number\", \"Index\"]\n",
    "predictions = predictions.loc[:, predictions.columns.str.contains(\"|\".join(col_name))] # only keep the columns with the prediction scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the fasta file into positive or negative only make sense if it is a classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"../data/whole_sequence.fasta\"\n",
    "res_dir = \"classification_results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m extractor = \u001b[43mpredict\u001b[49m.FastaExtractor(fasta_file, res_dir)\n\u001b[32m      2\u001b[39m positive, negative = extractor.separate_negative_positive(predictions)\n",
      "\u001b[31mNameError\u001b[39m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "extractor = predict.FastaExtractor(fasta_file, res_dir)\n",
    "positive, negative = extractor.separate_negative_positive(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.extract(positive, negative, positive_fasta=\"positive.fasta\", negative_fasta=f\"negative.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning large language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.deep import finetuning as ft\n",
    "from BioML.deep.train_config import LLMConfig, SplitConfig, TrainConfig\n",
    "import pandas as pd\n",
    "from BioML.utilities.utils import load_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression tasks use num_classes = 1, and change the optimize to val_R2 for instance.\n",
    "Go to BioML.deep.finetuning to see all the available metrics\n",
    "You can change optimize_mode to 'min' if it has to decrease\n",
    "To see the hyperparameters and configurations you can open mlflow  \n",
    "The command should be run (terminal) in the directory where logs are, specified by train_config.save_dir\n",
    "mlflow server --host 127.0.0.1 --port 8080 \n",
    "\n",
    "! mlflow server --host 127.0.0.1 --port 8080 # to run terminal commands in jupyter you can use this and then go to \n",
    "http://127.0.0.1:8080 or http://localhost:5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_csv(\"../data/esterase_labels.csv\", index_col=0).to_numpy().flatten()\n",
    "fasta_file = \"../data/whole_sequence.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(num_classes=2, optimize=\"Val_MCC\", optimize_mode=\"max\", max_epochs=3, root_dir=\"LLM_results\") # for regression do num_classses = 1 \n",
    "llm_config = LLMConfig()\n",
    "split_config = SplitConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 846,402 || all params: 8,687,165 || trainable%: 9.7431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/ruite/miniforge3/envs/bioml/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/home/ruite/miniforge3/envs/bioml/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (12) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 8.7 M  | train\n",
      "--------------------------------------------\n",
      "846 K     Trainable params\n",
      "7.8 M     Non-trainable params\n",
      "8.7 M     Total params\n",
      "34.749    Total estimated model params size (MB)\n",
      "204       Modules in train mode\n",
      "126       Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e96c786b5c40afb6755d64e1992e8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruite/miniforge3/envs/bioml/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921ae3d6ce694c60828cf4a01f290d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f0ad2904de4da8b3ca2def110b46e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved. New best score: 0.034\n",
      "Epoch 0, global step 12: 'Val_MCC' reached 0.03414 (best 0.03414), saving model to '/home/ruite/Projects/BioML/examples/LLM_results/model_checkpoint/epoch=0-Val_MCC=0.03.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a6bf92312b4078950209a2853a5d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 24: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf56790ba0cb4c73bd9460d778de234b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 36: 'Val_MCC' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "/home/ruite/miniforge3/envs/bioml/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b611424fe44ac87774b6e8076bee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test_AUROC           0.8616070747375488\n",
      "        Test_Acc            0.6000000238418579\n",
      " Test_Average_Precision     0.8665806651115417\n",
      "    Test_Cohen_Kappa        0.15094339847564697\n",
      "         Test_F1            0.6000000238418579\n",
      "        Test_MCC            0.2857142984867096\n",
      "     Test_Precision         0.6000000238418579\n",
      "       Test_Recall          0.6000000238418579\n",
      "          loss              0.6424059271812439\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/ruite/Projects/BioML/examples/LLM_results/model_checkpoint/epoch=0-Val_MCC=0.03.ckpt\n",
      "Loaded model weights from the checkpoint at /home/ruite/Projects/BioML/examples/LLM_results/model_checkpoint/epoch=0-Val_MCC=0.03.ckpt\n",
      "/home/ruite/miniforge3/envs/bioml/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f8708c7a854e3c93c260cac2030321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test_AUROC                 0.78125\n",
      "        Test_Acc            0.5333333611488342\n",
      " Test_Average_Precision     0.7894397974014282\n",
      "    Test_Cohen_Kappa                0.0\n",
      "         Test_F1            0.5333333611488342\n",
      "        Test_MCC           0.024397501721978188\n",
      "     Test_Precision         0.5333333611488342\n",
      "       Test_Recall          0.5333333611488342\n",
      "          loss              0.6824653744697571\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "lr=1e-3\n",
    "use_best_model = True\n",
    "tokenizer_args: dict = load_config(\"\") # tokenizer_config.json\n",
    "lightning_trainer_args = {}\n",
    "# Placeholder for the training loop call\n",
    "model, data_module, best_model_path, peft_output = ft.training_loop(fasta_file, label, lr, train_config, llm_config, \n",
    "                                                       split_config, tokenizer_args, \n",
    "                                                       lightning_trainer_args, use_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be 2 model folders after training, they contain the exact same model but they are loaded differently: \n",
    "train_config.root_dir / train_config.adapter_output: Where PEFT adapters are saved so you can use the HuggingFace APi from load_adapter\n",
    "\n",
    "from BioML.uilities.deep_utils import load_adapter\n",
    "\n",
    "Use the load_adapter function to get the trained model\n",
    "\n",
    "model = load_adapter(best_model_path, llm_config)\n",
    "\n",
    "Then there is train_config.root_dir / train_config.model_checkpoint_dir -> saved by the pytorch lightning\n",
    "If you want to use the lighning API\n",
    "\n",
    "mod = ft.TransformerModule.load_from_checkpoint(best_model_path, model=model): load the lightning model like so -> you will need to provide model architecture (look at the function training_loop to get the same model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (initial): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (initial): Linear(in_features=320, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (initial): Linear(in_features=64, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict(\n",
       "                      (initial): lora.dora.DoraLinearLayer()\n",
       "                    )\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (initial): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (initial): Linear(in_features=320, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (initial): Linear(in_features=64, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict(\n",
       "                      (initial): lora.dora.DoraLinearLayer()\n",
       "                    )\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (initial): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (initial): Linear(in_features=320, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (initial): Linear(in_features=64, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict(\n",
       "                      (initial): lora.dora.DoraLinearLayer()\n",
       "                    )\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): EsmClassificationHead(\n",
       "        (dense): ModulesToSaveWrapper(\n",
       "          (original_module): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (initial): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): ModulesToSaveWrapper(\n",
       "          (original_module): Linear(in_features=320, out_features=2, bias=True)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (initial): Linear(in_features=320, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from BioML.utilities.deep_utils import load_adapter\n",
    "from pathlib import Path\n",
    "peft_output = Path(train_config.root_dir) / train_config.adapter_output\n",
    "mod_peft = load_adapter(peft_output, llm_config)\n",
    "mod_peft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 846,402 || all params: 8,687,165 || trainable%: 9.7431\n"
     ]
    }
   ],
   "source": [
    "peft = ft.PreparePEFT(train_config, llm_config, lora_init=True)\n",
    "model = peft.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ft.TransformerModule.load_from_checkpoint(best_model_path, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.deep.embeddings import TokenizeFasta\n",
    "from BioML.models.predict import DeepPredictor, predict_deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20,  4,  4,  ...,  1,  1,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = TokenizeFasta(llm_config, tokenizer_args)\n",
    "train_config = TrainConfig(num_classes=2, root_dir=\"LLM_results\") # for regression do num_classses = 1 \n",
    "peft_output = Path(train_config.root_dir) / train_config.adapter_output\n",
    "peft_model = load_adapter(peft_output, llm_config)\n",
    "test_fasta = \"../data/whole_sequence.fasta\"\n",
    "res_dir = \"LLM_results/predictions.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepPredictor(peft_model, test_fasta, llm_config) # you could also give it the lightning model\n",
    "predictions = predictor.predict(tok.tokenizer, \"classification\", batch_size=2)\n",
    "predictions.to_csv(res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_deep(test_fasta, \"classification\", peft_output, llm_config, tokenizer_args, batch_size=2, res_dir=res_dir, lightning_model=None) # it is the same as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will return a dataframe with the prediction class probablities for classification problems or the prediction for regression problems.\n",
    "It will named as res_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Several uses cases comes into mind \n",
    "\n",
    "1. Rapidly test different features sets people might have constructed (or generate one using BioML)\n",
    "2. Compare which of the classical algorithms is more suited for your features so you can focus your efforts on optimizing the hyperparameters of that algorithm\n",
    "3. Have a baseline model to compare the performance of your customized training pipeline\n",
    "4. Finetune your own large language models -> automatically finetune LLM's for a specific task using parameter efficient fine tuning techniques. I'll allow you to train it on a consumer GPU, models between 13B and 30B parameters. Although you might still need to control batch size, and gradient accumulation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ruite/Projects/BioML/examples/home/ruite/Projects/BioML/data/esterase_labels.csv')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "Path(\"home/ruite/Projects/BioML/data/esterase_labels.csv\").resolve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
