{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Paper: Efficient evolution of human antibodies from general protein language models\n",
    "https://www-nature-com.sire.ub.edu/articles/s41587-023-01763-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForMaskedLM, pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from datasets import Dataset\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_args = {}\n",
    "tokenizer_args[\"padding\"] = True\n",
    "tokenizer_args[\"truncation\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_args)\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_sequence = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = {}\n",
    "\tfor x in positions:\n",
    "\t\tmasked_input_ids = input_ids.clone()\n",
    "  # The plus + makes sure we are assigning the correct positions to the correct index (since CLS is at the begining of teh position)\n",
    "\t\tmasked_input_ids[0, x+1] = tokenizer.mask_token_id\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = model(masked_input_ids).logits # to remove the probabilities of the tokens [CLS] and [SEP]\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob[x] = torch.log(probabilities)\n",
    "\treturn all_prob\n",
    "\n",
    "def wild_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = {}\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids).logits\n",
    "\tfor x in positions:\n",
    "     # softmaxing the probabilities of the correct positions -> so it is shape 33 the probabilities\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob[x] = torch.log(probabilities)\n",
    "\treturn all_prob\n",
    "\n",
    "def get_probabilities(protein_sequence, model, tokenizer, positions=(), \n",
    "                      strategy=masked_marginal):\n",
    "    # Encode the protein sequence\n",
    "    input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "    # sequence_length = input_ids.shape[1] - 2 \n",
    "    # List of amino acids\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    if not isinstance(amino_acids, str):\n",
    "        raise TypeError(\"amino_acids should be a string\")\n",
    "    aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "    if not aa_ids:\n",
    "        raise ValueError(\"Could not convert tokens to ids\")\n",
    "    prob_mt = {}\n",
    "    # Get the probabilities\t\n",
    "    all_prob = strategy(positions, input_ids, tokenizer, model)\n",
    "    if not all_prob:\n",
    "        raise ValueError(\"Could not get the probabilities\")\n",
    "    if not positions:\n",
    "        positions = range(input_ids.shape[1]-2) # -2 to remove the CLS and EOS tokens\n",
    "        # This will get he probabilities of all the positions in the sequence\n",
    "        \n",
    "    for pos in positions:\n",
    "        wt_residue_id = input_ids[0, pos+1].item()\n",
    "        wt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "        # Get the probability of the wild type residue\n",
    "        prob_wt = all_prob[pos][wt_residue_id].item()\n",
    "        # Get the probability of the mutant residue relative to the wild type residue\n",
    "        prob_mt[f\"{wt_token}{pos}\"] = {f\"{key}\": all_prob[pos][value].item() - prob_wt for key, value in aa_ids.items()}\n",
    "    return pd.DataFrame(prob_mt).T\n",
    "\n",
    "def filter_probabilities_by_alpha(probability, alpha):\n",
    "\t# Filter the probabilities\n",
    "\tfiltered_prob = {}\n",
    "\tfor pos, probs in probability.items():\n",
    "\t\tfiltered_prob[pos] = {aa: prob for aa, prob in probs.items() if prob > alpha}\n",
    "\treturn filtered_prob\n",
    "\n",
    "\n",
    "def filter_probabilities_by_set(probability, aa_set):\n",
    "\t# Filter the probabilities\n",
    "\tfiltered_prob = {}\n",
    "\tfor aa in aa_set:\n",
    "\t\tprint(aa)\n",
    "\t\tfiltered_prob[aa[:2]] = {}\n",
    "\t\tfor mut, probs in probability[aa[:2]].items():\n",
    "\t\t\tif mut == aa[-1]:\n",
    "\t\t\t\tfiltered_prob[aa[:2]][mut] = probs\n",
    "\treturn filtered_prob\n",
    "\n",
    "def return_set(probabilities):\n",
    "\t# Return the set of the probabilities\n",
    "\tprob_set = []\n",
    "\tfor prob in probabilities:\n",
    "\t\tproba = []\n",
    "\t\tfor key, value in prob.items():\n",
    "\t\t\tproba.extend([f\"{key}{k}\" for k in value.keys()])\n",
    "\t\tprob_set.append(proba)\n",
    "\tprob = set(prob_set[0]).intersection(*prob_set[1:])\n",
    "\treturn prob\n",
    "\n",
    "def filter_by_k(probabilities, k):\n",
    "\t# Filter the probability based on how many models agree\n",
    "\tsets = list(itertools.combinations(probabilities, k))\n",
    "\tall_sets = [return_set(comb_probabilities) for comb_probabilities in sets]\n",
    "\tall_sets = all_sets[0].union(*all_sets[1:])\n",
    "\tnew_probabilities = [filter_probabilities_by_set(prob, all_sets) for prob in probabilities]\n",
    "\treturn new_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vote(probabilities):\n",
    "\t# Return the set of the probabilities\n",
    "\tmodel_count = {}\n",
    "\tmodel_names = {}\n",
    "\tfor model_name, prob in probabilities.items():\n",
    "\t\tfor key, value in prob.items():\n",
    "\t\t\taa = [f\"{key}{k}\" for k in value.keys()]\n",
    "\t\t\tfor a in aa:\n",
    "\t\t\t\tif a in model_count:\n",
    "\t\t\t\t\tmodel_count[a] += 1\n",
    "\t\t\t\t\tmodel_names[a].append(model_name)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tmodel_count[a] = 0\n",
    "\t\t\t\t\tmodel_names[a] = []\n",
    "\treturn model_count, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "# sequence_length = input_ids.shape[1] - 2 \n",
    "# List of amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "prob_mt = {}\n",
    "positions = 2 # zero indexed\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[0, positions + 1] = tokenizer.mask_token_id\n",
    "with torch.no_grad():\n",
    "    output = model(masked_input_ids).logits\n",
    "probabilities = torch.nn.functional.softmax(output[0, positions+1], dim=0)\n",
    "log_prob = torch.log(probabilities)\n",
    "wt_residue_id = input_ids[0, positions+1].item()\n",
    "wt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "# Get the probability of the wild type residue\n",
    "prob_wt = log_prob[wt_residue_id].item()\n",
    "# Get the probability of the mutant residue relative to the wild type residue\n",
    "prob_mt[f\"{wt_token}{positions}\"] = {f\"{key}\": log_prob[value].item() - prob_wt \n",
    "                                for key, value in aa_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = pd.DataFrame(prob_mt).T\n",
    "a = {\"seq1\": pro, \"seq2\": pro, \"seq3\": pro}\n",
    "u = pd.concat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.295406</td>\n",
       "      <td>-2.850163</td>\n",
       "      <td>0.606348</td>\n",
       "      <td>1.052304</td>\n",
       "      <td>-0.956331</td>\n",
       "      <td>-0.088888</td>\n",
       "      <td>-1.368847</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.083619</td>\n",
       "      <td>-1.641879</td>\n",
       "      <td>0.449908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.252202</td>\n",
       "      <td>-0.313501</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.660977</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>-3.000461</td>\n",
       "      <td>-1.120543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         C         D         E         F         G         H  \\\n",
       "P2  0.295406 -2.850163  0.606348  1.052304 -0.956331 -0.088888 -1.368847   \n",
       "\n",
       "           I         K         L         M         N    P         Q         R  \\\n",
       "P2  0.174118  0.886782  0.083619 -1.641879  0.449908  0.0 -0.252202 -0.313501   \n",
       "\n",
       "           S         T         V         W         Y  \n",
       "P2  0.748899  0.660977  0.073754 -3.000461 -1.120543  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 534.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dat = Dataset.from_dict({\"protein_sequence\": [protein_sequence, protein_sequence]})\n",
    "tok = dat.map(lambda examples: tokenizer(examples[\"protein_sequence\"], return_tensors=\"np\"), batched=True)\n",
    "tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = get_probabilities(protein_sequence, model, tokenizer, positions=range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.295406</td>\n",
       "      <td>-2.850163</td>\n",
       "      <td>0.606348</td>\n",
       "      <td>1.052304</td>\n",
       "      <td>-0.956331</td>\n",
       "      <td>-0.088888</td>\n",
       "      <td>-1.368847</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.083619</td>\n",
       "      <td>-1.641879</td>\n",
       "      <td>0.449908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.252202</td>\n",
       "      <td>-0.313501</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.660977</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>-3.000461</td>\n",
       "      <td>-1.120543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         C         D         E         F         G         H  \\\n",
       "P2  0.295406 -2.850163  0.606348  1.052304 -0.956331 -0.088888 -1.368847   \n",
       "\n",
       "           I         K         L         M         N    P         Q         R  \\\n",
       "P2  0.174118  0.886782  0.083619 -1.641879  0.449908  0.0 -0.252202 -0.313501   \n",
       "\n",
       "           S         T         V         W         Y  \n",
       "P2  0.748899  0.660977  0.073754 -3.000461 -1.120543  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = pd.read_csv(\"test.csv\", index_col=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>M0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.300082</td>\n",
       "      <td>0.125866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.295405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.606345</td>\n",
       "      <td>1.052300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.174117</td>\n",
       "      <td>0.886781</td>\n",
       "      <td>0.083617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.449907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.748898</td>\n",
       "      <td>0.660977</td>\n",
       "      <td>0.073753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L3</th>\n",
       "      <td>0.648111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189710</td>\n",
       "      <td>0.733877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.052003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.776066</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.697228</td>\n",
       "      <td>0.669650</td>\n",
       "      <td>0.257574</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4</th>\n",
       "      <td>0.679773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.872293</td>\n",
       "      <td>1.185394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.432633</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.433574</td>\n",
       "      <td>0.211695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.720788</td>\n",
       "      <td>1.464011</td>\n",
       "      <td>0.301461</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.198884</td>\n",
       "      <td>1.010109</td>\n",
       "      <td>0.255025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.684088</td>\n",
       "      <td>0.950507</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.469521</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.876373</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.129810</td>\n",
       "      <td>0.615412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.413907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y7</th>\n",
       "      <td>1.043069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.628400</td>\n",
       "      <td>2.099530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.601515</td>\n",
       "      <td>0.17293</td>\n",
       "      <td>0.300105</td>\n",
       "      <td>2.390928</td>\n",
       "      <td>0.589779</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.279382</td>\n",
       "      <td>2.155662</td>\n",
       "      <td>1.030976</td>\n",
       "      <td>1.308511</td>\n",
       "      <td>1.432712</td>\n",
       "      <td>1.386070</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.621612</td>\n",
       "      <td>0.815924</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.902529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.192710</td>\n",
       "      <td>0.709438</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.313684</td>\n",
       "      <td>0.670698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.058832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A   C         D         E   F         G        H         I  \\\n",
       "M0       NaN NaN       NaN       NaN NaN       NaN      NaN       NaN   \n",
       "A1       NaN NaN       NaN       NaN NaN       NaN      NaN       NaN   \n",
       "P2  0.295405 NaN  0.606345  1.052300 NaN       NaN      NaN  0.174117   \n",
       "L3  0.648111 NaN  0.189710  0.733877 NaN       NaN      NaN       NaN   \n",
       "R4  0.679773 NaN  0.872293  1.185394 NaN  0.432633      NaN       NaN   \n",
       "K5       NaN NaN       NaN       NaN NaN       NaN      NaN       NaN   \n",
       "T6       NaN NaN  0.684088  0.950507 NaN  0.469521      NaN       NaN   \n",
       "Y7  1.043069 NaN  1.628400  2.099530 NaN  1.601515  0.17293  0.300105   \n",
       "V8       NaN NaN  0.621612  0.815924 NaN       NaN      NaN       NaN   \n",
       "L9       NaN NaN       NaN       NaN NaN       NaN      NaN       NaN   \n",
       "\n",
       "           K         L   M         N         P         Q         R         S  \\\n",
       "M0       NaN       NaN NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "A1  0.249405       NaN NaN       NaN       NaN       NaN       NaN  0.300082   \n",
       "P2  0.886781  0.083617 NaN  0.449907       NaN       NaN       NaN  0.748898   \n",
       "L3  1.052003       NaN NaN       NaN  0.776066       NaN       NaN  0.697228   \n",
       "R4  1.433574  0.211695 NaN  0.720788  1.464011  0.301461       NaN  1.198884   \n",
       "K5       NaN       NaN NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "T6  0.876373       NaN NaN  0.129810  0.615412       NaN       NaN  0.413907   \n",
       "Y7  2.390928  0.589779 NaN  1.279382  2.155662  1.030976  1.308511  1.432712   \n",
       "V8  0.902529       NaN NaN  0.192710  0.709438       NaN       NaN  0.313684   \n",
       "L9       NaN       NaN NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "           T         V   W   Y  \n",
       "M0       NaN       NaN NaN NaN  \n",
       "A1  0.125866       NaN NaN NaN  \n",
       "P2  0.660977  0.073753 NaN NaN  \n",
       "L3  0.669650  0.257574 NaN NaN  \n",
       "R4  1.010109  0.255025 NaN NaN  \n",
       "K5       NaN       NaN NaN NaN  \n",
       "T6       NaN       NaN NaN NaN  \n",
       "Y7  1.386070  0.820189 NaN NaN  \n",
       "V8  0.670698       NaN NaN NaN  \n",
       "L9       NaN  0.058832 NaN NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.heatmap(u.loc[\"seq1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0037612915039"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "# Encode the protein sequence\n",
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "# sequence_length = input_ids.shape[1] - 2 \n",
    "# Get the probabilities\t\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids).logits[0,1:-1]\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "val = 0\n",
    "for num, i in enumerate(input_ids[0,1:-1]):\n",
    "    val += probabilities[num][i]\n",
    "val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7461e-07, 1.4035e-10, 3.2243e-07, 1.4058e-10, 1.7262e-04, 3.5684e-04,\n",
       "        2.4192e-04, 2.5626e-04, 2.5007e-04, 2.7253e-04, 1.2750e-04, 2.4001e-04,\n",
       "        1.3581e-04, 2.0171e-04, 5.6419e-04, 2.8589e-04, 1.1679e-04, 1.4464e-04,\n",
       "        1.0539e-04, 7.0962e-05, 9.9636e-01, 4.8931e-05, 1.0774e-05, 1.4499e-05,\n",
       "        1.7726e-05, 2.0059e-09, 1.5186e-09, 9.5016e-10, 4.3751e-11, 2.9994e-11,\n",
       "        3.2791e-11, 2.5319e-11, 1.4077e-10])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(output, dim=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([108, 33])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0,1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.7461e-07, 1.4035e-10, 3.2243e-07, 1.4058e-10, 1.7262e-04, 3.5684e-04,\n",
       "        2.4192e-04, 2.5626e-04, 2.5007e-04, 2.7253e-04, 1.2750e-04, 2.4001e-04,\n",
       "        1.3581e-04, 2.0171e-04, 5.6419e-04, 2.8589e-04, 1.1679e-04, 1.4464e-04,\n",
       "        1.0539e-04, 7.0962e-05, 9.9636e-01, 4.8931e-05, 1.0774e-05, 1.4499e-05,\n",
       "        1.7726e-05, 2.0059e-09, 1.5186e-09, 9.5016e-10, 4.3751e-11, 2.9994e-11,\n",
       "        3.2791e-11, 2.5319e-11, 1.4077e-10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.softmax(output[0, 1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wild_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = []\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids).logits\n",
    "\tfor x in positions:\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob.append(probabilities)\n",
    "\treturn all_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = input_ids.shape[1] - 2 \n",
    "# List of amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[0,56] = tokenizer.mask_token_id\n",
    "with torch.no_grad():\n",
    "    output = model(masked_input_ids).logits\n",
    "    newoutput = model(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prob = []\n",
    "for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "\tmasked_input_ids = input_ids.clone()\n",
    "\tmasked_input_ids[0, x+1] = tokenizer.mask_token_id\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(masked_input_ids).logits\n",
    "\twt_residue_id = input_ids[0, x+1].item()\n",
    "\twt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "\t# Get the probability of the wild type residue\n",
    "\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=-1)\n",
    "\tall_prob.append(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2098e-07, 5.5431e-12, 5.2519e-07, 5.5082e-12, 4.9137e-02, 9.3152e-02,\n",
       "        2.7678e-02, 4.3878e-02, 1.2575e-01, 7.7442e-02, 3.1851e-02, 1.0565e-01,\n",
       "        5.8779e-02, 5.1997e-02, 6.2361e-02, 1.1954e-01, 3.1374e-02, 7.0313e-02,\n",
       "        1.3865e-02, 1.0678e-02, 1.0953e-02, 1.1653e-02, 1.4777e-03, 2.3821e-03,\n",
       "        8.9663e-05, 2.6679e-07, 1.8436e-07, 9.9049e-08, 2.9584e-09, 2.1299e-09,\n",
       "        2.2080e-09, 2.2518e-09, 5.5910e-12])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(all_prob)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.7804, -27.3554, -15.7982, -27.3520,  -1.4277,  -2.5076,  -3.9805,\n",
       "         -1.9887,  -4.3450,  -3.8944,  -3.6909,  -4.1550,  -1.9134,  -4.8577,\n",
       "         -4.9729,  -3.4737,  -4.2161,  -4.5330,  -2.4678,  -2.4993,  -3.5960,\n",
       "         -4.5091,  -3.9432,  -5.1766, -11.6180, -15.6090, -15.8708, -16.4083,\n",
       "        -19.7412, -20.0578, -20.1161, -20.1449, -27.3477])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(output[0, 56], dim=0)\n",
    "log_probabilities = torch.log(probabilities)\n",
    "log_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_residue = input_ids[0, 56].item()\n",
    "\n",
    "log_prob_wt = log_probabilities[wt_residue].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 1.39\n",
      "C: -1.28\n",
      "D: -0.96\n",
      "E: 0.00\n",
      "F: 1.43\n",
      "G: -0.09\n",
      "H: -0.61\n",
      "I: 1.98\n",
      "K: 0.42\n",
      "L: 2.47\n",
      "M: 0.30\n",
      "N: -0.64\n",
      "P: -1.08\n",
      "Q: -0.32\n",
      "R: 0.20\n",
      "S: -0.45\n",
      "T: -0.26\n",
      "V: 1.91\n",
      "W: -0.05\n",
      "Y: 1.40\n"
     ]
    }
   ],
   "source": [
    "for i, amino_acid in enumerate(amino_acids):\n",
    "    log_prob_mt = log_probabilities[tokenizer.convert_tokens_to_ids(amino_acid)].item()\n",
    "    u = log_prob_mt - log_prob_wt\n",
    "    print(f\"{amino_acid}: {u:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E56A': 4.001879542035948,\n",
       " 'E56C': 0.2774215940895632,\n",
       " 'E56D': 0.38165176240465715,\n",
       " 'E56E': 1.0,\n",
       " 'E56F': 4.164444006026979,\n",
       " 'E56G': 0.9174956996554051,\n",
       " 'E56H': 0.5408050497639408,\n",
       " 'E56I': 7.24976954106049,\n",
       " 'E56K': 1.5230791107844535,\n",
       " 'E56L': 11.783465044635921,\n",
       " 'E56M': 1.3477541668824693,\n",
       " 'E56N': 0.5280566944547288,\n",
       " 'E56P': 0.34012259939748507,\n",
       " 'E56Q': 0.7249081595733019,\n",
       " 'E56R': 1.2257103964296387,\n",
       " 'E56S': 0.6372239090923898,\n",
       " 'E56T': 0.770610221597457,\n",
       " 'E56V': 6.724243731397887,\n",
       " 'E56W': 0.9524152847213364,\n",
       " 'E56Y': 4.035606249303408}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wt = probabilities[wt_residue].item()\n",
    "# Get the probability of the mutant residue\n",
    "prob_mt = {f\"E56{key}\": probabilities[value].item()/prob_wt for key, value in aa_ids.items()}\n",
    "prob_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.625357189342083"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mt[\"E56L\"] / prob_mt[\"E56I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"fill-mask\", model=model_name, tokenizer=model_name, top_k=20)\n",
    "\n",
    "def use_pipe_marginal(pipe, sequence, positions, tokenizer):\n",
    "\tseq = []\n",
    "\tfor pos in positions:\n",
    "\t\tsequence = sequence[:pos] + tokenizer.mask_token + sequence[pos+1:]\n",
    "\t\tseq.append(sequence)\n",
    "\treturn pipe(seq)\n",
    "\n",
    "\n",
    "def parse_pipe_output(output, sequence, positions):\n",
    "\tall_proba = {}\n",
    "\tfor num, pos in enumerate(positions):\n",
    "\t\twild = sequence[pos]\n",
    "\t\tall_proba[f\"{wild}{pos}\"] = {x[\"token_str\"]: x[\"score\"] for x in output[num]}\n",
    "\t\tall_proba[f\"{wild}{pos}\"] = {k: v / all_proba[f\"{wild}{pos}\"][wild] for k, v in all_proba[f\"{wild}{pos}\"].items()}\n",
    "\treturn pd.DataFrame(all_proba).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>0.558195</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.125095</td>\n",
       "      <td>0.630997</td>\n",
       "      <td>1.283265</td>\n",
       "      <td>0.527495</td>\n",
       "      <td>0.117577</td>\n",
       "      <td>0.754814</td>\n",
       "      <td>0.669457</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>0.341929</td>\n",
       "      <td>1.349972</td>\n",
       "      <td>1.134131</td>\n",
       "      <td>0.471038</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>0.114632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P3</th>\n",
       "      <td>1.343673</td>\n",
       "      <td>0.057835</td>\n",
       "      <td>1.833724</td>\n",
       "      <td>2.864244</td>\n",
       "      <td>0.384301</td>\n",
       "      <td>0.914949</td>\n",
       "      <td>0.254400</td>\n",
       "      <td>1.190197</td>\n",
       "      <td>2.427309</td>\n",
       "      <td>1.087215</td>\n",
       "      <td>0.193616</td>\n",
       "      <td>1.568169</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777088</td>\n",
       "      <td>0.730884</td>\n",
       "      <td>2.114671</td>\n",
       "      <td>1.936685</td>\n",
       "      <td>1.076543</td>\n",
       "      <td>0.049764</td>\n",
       "      <td>0.326103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L4</th>\n",
       "      <td>1.911923</td>\n",
       "      <td>0.046710</td>\n",
       "      <td>1.208896</td>\n",
       "      <td>2.083139</td>\n",
       "      <td>0.208951</td>\n",
       "      <td>0.732715</td>\n",
       "      <td>0.219586</td>\n",
       "      <td>0.899000</td>\n",
       "      <td>2.863380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.117386</td>\n",
       "      <td>0.897115</td>\n",
       "      <td>2.172903</td>\n",
       "      <td>0.915103</td>\n",
       "      <td>0.794621</td>\n",
       "      <td>2.008179</td>\n",
       "      <td>1.953554</td>\n",
       "      <td>1.293787</td>\n",
       "      <td>0.032136</td>\n",
       "      <td>0.188638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R5</th>\n",
       "      <td>1.973429</td>\n",
       "      <td>0.095662</td>\n",
       "      <td>2.392388</td>\n",
       "      <td>3.271975</td>\n",
       "      <td>0.326470</td>\n",
       "      <td>1.541312</td>\n",
       "      <td>0.364618</td>\n",
       "      <td>0.966675</td>\n",
       "      <td>4.193666</td>\n",
       "      <td>1.235769</td>\n",
       "      <td>0.193045</td>\n",
       "      <td>2.056053</td>\n",
       "      <td>4.323272</td>\n",
       "      <td>1.351832</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.316411</td>\n",
       "      <td>2.745901</td>\n",
       "      <td>1.290495</td>\n",
       "      <td>0.044551</td>\n",
       "      <td>0.291857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K6</th>\n",
       "      <td>0.276130</td>\n",
       "      <td>0.030226</td>\n",
       "      <td>0.507187</td>\n",
       "      <td>0.579249</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.641203</td>\n",
       "      <td>0.093900</td>\n",
       "      <td>0.119002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.173978</td>\n",
       "      <td>0.045722</td>\n",
       "      <td>0.484264</td>\n",
       "      <td>0.732273</td>\n",
       "      <td>0.255497</td>\n",
       "      <td>0.316409</td>\n",
       "      <td>0.588060</td>\n",
       "      <td>0.392850</td>\n",
       "      <td>0.151816</td>\n",
       "      <td>0.012046</td>\n",
       "      <td>0.076425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>0.819319</td>\n",
       "      <td>0.061215</td>\n",
       "      <td>1.981964</td>\n",
       "      <td>2.587020</td>\n",
       "      <td>0.171343</td>\n",
       "      <td>1.599228</td>\n",
       "      <td>0.271671</td>\n",
       "      <td>0.415586</td>\n",
       "      <td>2.402177</td>\n",
       "      <td>0.486920</td>\n",
       "      <td>0.107403</td>\n",
       "      <td>1.138613</td>\n",
       "      <td>1.850421</td>\n",
       "      <td>0.620420</td>\n",
       "      <td>0.794016</td>\n",
       "      <td>1.512719</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601088</td>\n",
       "      <td>0.026687</td>\n",
       "      <td>0.223367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y8</th>\n",
       "      <td>2.837910</td>\n",
       "      <td>0.146315</td>\n",
       "      <td>5.095696</td>\n",
       "      <td>8.162321</td>\n",
       "      <td>0.599036</td>\n",
       "      <td>4.960533</td>\n",
       "      <td>1.188780</td>\n",
       "      <td>1.350002</td>\n",
       "      <td>10.923614</td>\n",
       "      <td>1.803590</td>\n",
       "      <td>0.455732</td>\n",
       "      <td>3.594403</td>\n",
       "      <td>8.633596</td>\n",
       "      <td>2.803796</td>\n",
       "      <td>3.700657</td>\n",
       "      <td>4.190033</td>\n",
       "      <td>3.999094</td>\n",
       "      <td>2.270931</td>\n",
       "      <td>0.094145</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.783819</td>\n",
       "      <td>0.054291</td>\n",
       "      <td>1.861931</td>\n",
       "      <td>2.261264</td>\n",
       "      <td>0.269974</td>\n",
       "      <td>0.834610</td>\n",
       "      <td>0.400744</td>\n",
       "      <td>0.695419</td>\n",
       "      <td>2.465827</td>\n",
       "      <td>0.901744</td>\n",
       "      <td>0.134310</td>\n",
       "      <td>1.212532</td>\n",
       "      <td>2.032843</td>\n",
       "      <td>0.808421</td>\n",
       "      <td>0.995437</td>\n",
       "      <td>1.368458</td>\n",
       "      <td>1.955604</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.056016</td>\n",
       "      <td>0.372498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L10</th>\n",
       "      <td>0.292952</td>\n",
       "      <td>0.040045</td>\n",
       "      <td>0.059589</td>\n",
       "      <td>0.082817</td>\n",
       "      <td>0.258523</td>\n",
       "      <td>0.148658</td>\n",
       "      <td>0.027128</td>\n",
       "      <td>0.820341</td>\n",
       "      <td>0.107850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101818</td>\n",
       "      <td>0.053985</td>\n",
       "      <td>0.214230</td>\n",
       "      <td>0.035702</td>\n",
       "      <td>0.057471</td>\n",
       "      <td>0.073047</td>\n",
       "      <td>0.139872</td>\n",
       "      <td>1.060600</td>\n",
       "      <td>0.025686</td>\n",
       "      <td>0.128020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K11</th>\n",
       "      <td>0.686631</td>\n",
       "      <td>0.093666</td>\n",
       "      <td>1.227909</td>\n",
       "      <td>2.135774</td>\n",
       "      <td>0.138714</td>\n",
       "      <td>0.770501</td>\n",
       "      <td>0.386378</td>\n",
       "      <td>0.418194</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.459316</td>\n",
       "      <td>0.087919</td>\n",
       "      <td>0.490002</td>\n",
       "      <td>0.130282</td>\n",
       "      <td>0.345678</td>\n",
       "      <td>0.529347</td>\n",
       "      <td>0.699894</td>\n",
       "      <td>1.421845</td>\n",
       "      <td>0.921015</td>\n",
       "      <td>0.035992</td>\n",
       "      <td>0.310612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            A         C         D         E         F         G         H  \\\n",
       "A2   1.000000  0.025572  0.558195  0.831349  0.148847  0.297127  0.125095   \n",
       "P3   1.343673  0.057835  1.833724  2.864244  0.384301  0.914949  0.254400   \n",
       "L4   1.911923  0.046710  1.208896  2.083139  0.208951  0.732715  0.219586   \n",
       "R5   1.973429  0.095662  2.392388  3.271975  0.326470  1.541312  0.364618   \n",
       "K6   0.276130  0.030226  0.507187  0.579249  0.063158  0.641203  0.093900   \n",
       "T7   0.819319  0.061215  1.981964  2.587020  0.171343  1.599228  0.271671   \n",
       "Y8   2.837910  0.146315  5.095696  8.162321  0.599036  4.960533  1.188780   \n",
       "V9   0.783819  0.054291  1.861931  2.261264  0.269974  0.834610  0.400744   \n",
       "L10  0.292952  0.040045  0.059589  0.082817  0.258523  0.148658  0.027128   \n",
       "K11  0.686631  0.093666  1.227909  2.135774  0.138714  0.770501  0.386378   \n",
       "\n",
       "            I          K         L         M         N         P         Q  \\\n",
       "A2   0.630997   1.283265  0.527495  0.117577  0.754814  0.669457  0.336804   \n",
       "P3   1.190197   2.427309  1.087215  0.193616  1.568169  1.000000  0.777088   \n",
       "L4   0.899000   2.863380  1.000000  0.117386  0.897115  2.172903  0.915103   \n",
       "R5   0.966675   4.193666  1.235769  0.193045  2.056053  4.323272  1.351832   \n",
       "K6   0.119002   1.000000  0.173978  0.045722  0.484264  0.732273  0.255497   \n",
       "T7   0.415586   2.402177  0.486920  0.107403  1.138613  1.850421  0.620420   \n",
       "Y8   1.350002  10.923614  1.803590  0.455732  3.594403  8.633596  2.803796   \n",
       "V9   0.695419   2.465827  0.901744  0.134310  1.212532  2.032843  0.808421   \n",
       "L10  0.820341   0.107850  1.000000  0.101818  0.053985  0.214230  0.035702   \n",
       "K11  0.418194   1.000000  0.459316  0.087919  0.490002  0.130282  0.345678   \n",
       "\n",
       "            R         S         T         V         W         Y  \n",
       "A2   0.341929  1.349972  1.134131  0.471038  0.015863  0.114632  \n",
       "P3   0.730884  2.114671  1.936685  1.076543  0.049764  0.326103  \n",
       "L4   0.794621  2.008179  1.953554  1.293787  0.032136  0.188638  \n",
       "R5   1.000000  3.316411  2.745901  1.290495  0.044551  0.291857  \n",
       "K6   0.316409  0.588060  0.392850  0.151816  0.012046  0.076425  \n",
       "T7   0.794016  1.512719  1.000000  0.601088  0.026687  0.223367  \n",
       "Y8   3.700657  4.190033  3.999094  2.270931  0.094145  1.000000  \n",
       "V9   0.995437  1.368458  1.955604  1.000000  0.056016  0.372498  \n",
       "L10  0.057471  0.073047  0.139872  1.060600  0.025686  0.128020  \n",
       "K11  0.529347  0.699894  1.421845  0.921015  0.035992  0.310612  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probabilities(protein_sequence, model, tokenizer, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S</th>\n",
       "      <th>K</th>\n",
       "      <th>T</th>\n",
       "      <th>A</th>\n",
       "      <th>E</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>I</th>\n",
       "      <th>D</th>\n",
       "      <th>L</th>\n",
       "      <th>V</th>\n",
       "      <th>R</th>\n",
       "      <th>Q</th>\n",
       "      <th>G</th>\n",
       "      <th>F</th>\n",
       "      <th>H</th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>C</th>\n",
       "      <th>W</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>1.349972</td>\n",
       "      <td>1.283265</td>\n",
       "      <td>1.134131</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.831349</td>\n",
       "      <td>0.754814</td>\n",
       "      <td>0.669457</td>\n",
       "      <td>0.630997</td>\n",
       "      <td>0.558195</td>\n",
       "      <td>0.527495</td>\n",
       "      <td>0.471038</td>\n",
       "      <td>0.341929</td>\n",
       "      <td>0.336804</td>\n",
       "      <td>0.297127</td>\n",
       "      <td>0.148847</td>\n",
       "      <td>0.125095</td>\n",
       "      <td>0.117577</td>\n",
       "      <td>0.114632</td>\n",
       "      <td>0.025572</td>\n",
       "      <td>0.015863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>1.565557</td>\n",
       "      <td>1.799515</td>\n",
       "      <td>1.286391</td>\n",
       "      <td>0.988343</td>\n",
       "      <td>1.253368</td>\n",
       "      <td>1.099248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.043457</td>\n",
       "      <td>0.891353</td>\n",
       "      <td>1.304214</td>\n",
       "      <td>0.764856</td>\n",
       "      <td>0.858806</td>\n",
       "      <td>0.642436</td>\n",
       "      <td>0.888907</td>\n",
       "      <td>0.555891</td>\n",
       "      <td>0.334455</td>\n",
       "      <td>0.173488</td>\n",
       "      <td>0.455629</td>\n",
       "      <td>0.209028</td>\n",
       "      <td>0.141089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L3</th>\n",
       "      <td>1.200384</td>\n",
       "      <td>1.379770</td>\n",
       "      <td>0.986334</td>\n",
       "      <td>0.757808</td>\n",
       "      <td>0.961014</td>\n",
       "      <td>0.842844</td>\n",
       "      <td>0.766746</td>\n",
       "      <td>0.800066</td>\n",
       "      <td>0.683441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.586450</td>\n",
       "      <td>0.658485</td>\n",
       "      <td>0.492585</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>0.426227</td>\n",
       "      <td>0.256442</td>\n",
       "      <td>0.133021</td>\n",
       "      <td>0.349352</td>\n",
       "      <td>0.160271</td>\n",
       "      <td>0.108179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R4</th>\n",
       "      <td>1.822947</td>\n",
       "      <td>2.095370</td>\n",
       "      <td>1.497883</td>\n",
       "      <td>1.150835</td>\n",
       "      <td>1.459431</td>\n",
       "      <td>1.279974</td>\n",
       "      <td>1.164408</td>\n",
       "      <td>1.215010</td>\n",
       "      <td>1.037898</td>\n",
       "      <td>1.518637</td>\n",
       "      <td>0.890605</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.748057</td>\n",
       "      <td>1.035051</td>\n",
       "      <td>0.647284</td>\n",
       "      <td>0.389442</td>\n",
       "      <td>0.202011</td>\n",
       "      <td>0.530538</td>\n",
       "      <td>0.243394</td>\n",
       "      <td>0.164285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K5</th>\n",
       "      <td>0.869988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714854</td>\n",
       "      <td>0.549228</td>\n",
       "      <td>0.696503</td>\n",
       "      <td>0.610858</td>\n",
       "      <td>0.555705</td>\n",
       "      <td>0.579855</td>\n",
       "      <td>0.495330</td>\n",
       "      <td>0.724758</td>\n",
       "      <td>0.425035</td>\n",
       "      <td>0.477243</td>\n",
       "      <td>0.357005</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.308912</td>\n",
       "      <td>0.185858</td>\n",
       "      <td>0.096408</td>\n",
       "      <td>0.253196</td>\n",
       "      <td>0.116158</td>\n",
       "      <td>0.078404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>1.217015</td>\n",
       "      <td>1.398887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.768307</td>\n",
       "      <td>0.974329</td>\n",
       "      <td>0.854522</td>\n",
       "      <td>0.777369</td>\n",
       "      <td>0.811151</td>\n",
       "      <td>0.692910</td>\n",
       "      <td>1.013855</td>\n",
       "      <td>0.594576</td>\n",
       "      <td>0.667609</td>\n",
       "      <td>0.499410</td>\n",
       "      <td>0.691009</td>\n",
       "      <td>0.432132</td>\n",
       "      <td>0.259995</td>\n",
       "      <td>0.134864</td>\n",
       "      <td>0.354192</td>\n",
       "      <td>0.162492</td>\n",
       "      <td>0.109678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y7</th>\n",
       "      <td>3.436032</td>\n",
       "      <td>3.949515</td>\n",
       "      <td>2.823327</td>\n",
       "      <td>2.169183</td>\n",
       "      <td>2.750849</td>\n",
       "      <td>2.412593</td>\n",
       "      <td>2.194766</td>\n",
       "      <td>2.290144</td>\n",
       "      <td>1.956311</td>\n",
       "      <td>2.862444</td>\n",
       "      <td>1.678681</td>\n",
       "      <td>1.884877</td>\n",
       "      <td>1.409996</td>\n",
       "      <td>1.950943</td>\n",
       "      <td>1.220051</td>\n",
       "      <td>0.734051</td>\n",
       "      <td>0.380766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.458767</td>\n",
       "      <td>0.309657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>2.046864</td>\n",
       "      <td>2.352749</td>\n",
       "      <td>1.681872</td>\n",
       "      <td>1.292195</td>\n",
       "      <td>1.638697</td>\n",
       "      <td>1.437196</td>\n",
       "      <td>1.307435</td>\n",
       "      <td>1.364252</td>\n",
       "      <td>1.165386</td>\n",
       "      <td>1.705174</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.122832</td>\n",
       "      <td>0.839943</td>\n",
       "      <td>1.162188</td>\n",
       "      <td>0.726791</td>\n",
       "      <td>0.437278</td>\n",
       "      <td>0.226824</td>\n",
       "      <td>0.595706</td>\n",
       "      <td>0.273290</td>\n",
       "      <td>0.184465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L9</th>\n",
       "      <td>1.200384</td>\n",
       "      <td>1.379770</td>\n",
       "      <td>0.986334</td>\n",
       "      <td>0.757808</td>\n",
       "      <td>0.961014</td>\n",
       "      <td>0.842844</td>\n",
       "      <td>0.766746</td>\n",
       "      <td>0.800066</td>\n",
       "      <td>0.683441</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.586450</td>\n",
       "      <td>0.658485</td>\n",
       "      <td>0.492585</td>\n",
       "      <td>0.681566</td>\n",
       "      <td>0.426227</td>\n",
       "      <td>0.256442</td>\n",
       "      <td>0.133021</td>\n",
       "      <td>0.349352</td>\n",
       "      <td>0.160271</td>\n",
       "      <td>0.108179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>K10</th>\n",
       "      <td>0.869988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714854</td>\n",
       "      <td>0.549228</td>\n",
       "      <td>0.696503</td>\n",
       "      <td>0.610858</td>\n",
       "      <td>0.555705</td>\n",
       "      <td>0.579855</td>\n",
       "      <td>0.495330</td>\n",
       "      <td>0.724758</td>\n",
       "      <td>0.425035</td>\n",
       "      <td>0.477243</td>\n",
       "      <td>0.357005</td>\n",
       "      <td>0.493970</td>\n",
       "      <td>0.308912</td>\n",
       "      <td>0.185858</td>\n",
       "      <td>0.096408</td>\n",
       "      <td>0.253196</td>\n",
       "      <td>0.116158</td>\n",
       "      <td>0.078404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            S         K         T         A         E         N         P  \\\n",
       "A1   1.349972  1.283265  1.134131  1.000000  0.831349  0.754814  0.669457   \n",
       "P2   1.565557  1.799515  1.286391  0.988343  1.253368  1.099248  1.000000   \n",
       "L3   1.200384  1.379770  0.986334  0.757808  0.961014  0.842844  0.766746   \n",
       "R4   1.822947  2.095370  1.497883  1.150835  1.459431  1.279974  1.164408   \n",
       "K5   0.869988  1.000000  0.714854  0.549228  0.696503  0.610858  0.555705   \n",
       "T6   1.217015  1.398887  1.000000  0.768307  0.974329  0.854522  0.777369   \n",
       "Y7   3.436032  3.949515  2.823327  2.169183  2.750849  2.412593  2.194766   \n",
       "V8   2.046864  2.352749  1.681872  1.292195  1.638697  1.437196  1.307435   \n",
       "L9   1.200384  1.379770  0.986334  0.757808  0.961014  0.842844  0.766746   \n",
       "K10  0.869988  1.000000  0.714854  0.549228  0.696503  0.610858  0.555705   \n",
       "\n",
       "            I         D         L         V         R         Q         G  \\\n",
       "A1   0.630997  0.558195  0.527495  0.471038  0.341929  0.336804  0.297127   \n",
       "P2   1.043457  0.891353  1.304214  0.764856  0.858806  0.642436  0.888907   \n",
       "L3   0.800066  0.683441  1.000000  0.586450  0.658485  0.492585  0.681566   \n",
       "R4   1.215010  1.037898  1.518637  0.890605  1.000000  0.748057  1.035051   \n",
       "K5   0.579855  0.495330  0.724758  0.425035  0.477243  0.357005  0.493970   \n",
       "T6   0.811151  0.692910  1.013855  0.594576  0.667609  0.499410  0.691009   \n",
       "Y7   2.290144  1.956311  2.862444  1.678681  1.884877  1.409996  1.950943   \n",
       "V8   1.364252  1.165386  1.705174  1.000000  1.122832  0.839943  1.162188   \n",
       "L9   0.800066  0.683441  1.000000  0.586450  0.658485  0.492585  0.681566   \n",
       "K10  0.579855  0.495330  0.724758  0.425035  0.477243  0.357005  0.493970   \n",
       "\n",
       "            F         H         M         Y         C         W  \n",
       "A1   0.148847  0.125095  0.117577  0.114632  0.025572  0.015863  \n",
       "P2   0.555891  0.334455  0.173488  0.455629  0.209028  0.141089  \n",
       "L3   0.426227  0.256442  0.133021  0.349352  0.160271  0.108179  \n",
       "R4   0.647284  0.389442  0.202011  0.530538  0.243394  0.164285  \n",
       "K5   0.308912  0.185858  0.096408  0.253196  0.116158  0.078404  \n",
       "T6   0.432132  0.259995  0.134864  0.354192  0.162492  0.109678  \n",
       "Y7   1.220051  0.734051  0.380766  1.000000  0.458767  0.309657  \n",
       "V8   0.726791  0.437278  0.226824  0.595706  0.273290  0.184465  \n",
       "L9   0.426227  0.256442  0.133021  0.349352  0.160271  0.108179  \n",
       "K10  0.308912  0.185858  0.096408  0.253196  0.116158  0.078404  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use = use_pipe(pipe, protein_sequence, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], tokenizer)\n",
    "parse_pipe_output(use, protein_sequence, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.2398565262556076,\n",
       " 'token': 4,\n",
       " 'token_str': 'L',\n",
       " 'sequence': 'M A P L R K T Y V L K L Y V A G N T P N S V R A L K T L N N I L E K E F K G V Y A L K V I D V L K N P Q L A E L D K I L A T P T L A K V L P P P V R R I I G D L S N R E K V L I G L D L L Y E E I G D Q A E D D L G L E'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pipe(protein_sequence[:55] + tokenizer.mask_token + protein_sequence[56:], top_k=20)\n",
    "res[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
