{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproduce Paper: Efficient evolution of human antibodies from general protein language models\n",
    "https://www-nature-com.sire.ub.edu/articles/s41587-023-01763-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, EsmForMaskedLM, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from datasets import Dataset\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_args = {}\n",
    "tokenizer_args[\"padding\"] = True\n",
    "tokenizer_args[\"truncation\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model_name = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_args)\n",
    "model = EsmForMaskedLM.from_pretrained(model_name, low_cpu_mem_usage=True, offload_folder=\"offload\", device_map=device, \n",
    "                                       torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_sequence = \"MAPLRKTYVLKLYVAGNTPNSVRALKTLNNILEKEFKGVYALKVIDVLKNPQLAEEDKILATPTLAKVLPPPVRRIIGDLSNREKVLIGLDLLYEEIGDQAEDDLGLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = {}\n",
    "\tfor x in positions:\n",
    "\t\tmasked_input_ids = input_ids.clone()\n",
    "  # The plus + makes sure we are assigning the correct positions to the correct index (since CLS is at the begining of teh position)\n",
    "\t\tmasked_input_ids[0, x+1] = tokenizer.mask_token_id\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\toutput = model(masked_input_ids).logits # to remove the probabilities of the tokens [CLS] and [SEP]\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob[x] = torch.log(probabilities)\n",
    "\treturn all_prob\n",
    "\n",
    "def wild_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = {}\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids).logits\n",
    "\tfor x in positions:\n",
    "     # softmaxing the probabilities of the correct positions -> so it is shape 33 the probabilities\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob[x] = torch.log(probabilities)\n",
    "\treturn all_prob\n",
    "\n",
    "def get_probabilities(protein_sequence, model, tokenizer, positions=(), \n",
    "                      strategy=masked_marginal):\n",
    "    # Encode the protein sequence\n",
    "    input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "    # sequence_length = input_ids.shape[1] - 2 \n",
    "    # List of amino acids\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    if not isinstance(amino_acids, str):\n",
    "        raise TypeError(\"amino_acids should be a string\")\n",
    "    aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "    if not aa_ids:\n",
    "        raise ValueError(\"Could not convert tokens to ids\")\n",
    "    prob_mt = {}\n",
    "    # Get the probabilities\t\n",
    "    all_prob = strategy(positions, input_ids, tokenizer, model)\n",
    "    if not all_prob:\n",
    "        raise ValueError(\"Could not get the probabilities\")\n",
    "    if not positions:\n",
    "        positions = range(input_ids.shape[1]-2) # -2 to remove the CLS and EOS tokens\n",
    "        # This will get he probabilities of all the positions in the sequence\n",
    "        \n",
    "    for pos in positions:\n",
    "        wt_residue_id = input_ids[0, pos+1].item()\n",
    "        wt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "        # Get the probability of the wild type residue\n",
    "        prob_wt = all_prob[pos][wt_residue_id].item()\n",
    "        # Get the probability of the mutant residue relative to the wild type residue\n",
    "        prob_mt[f\"{wt_token}{pos}\"] = {f\"{key}\": all_prob[pos][value].item() - prob_wt for key, value in aa_ids.items()}\n",
    "    return pd.DataFrame(prob_mt).T\n",
    "\n",
    "def filter_probabilities_by_alpha(probability, alpha):\n",
    "\t# Filter the probabilities\n",
    "\tfiltered_prob = {}\n",
    "\tfor pos, probs in probability.items():\n",
    "\t\tfiltered_prob[pos] = {aa: prob for aa, prob in probs.items() if prob > alpha}\n",
    "\treturn filtered_prob\n",
    "\n",
    "\n",
    "def filter_probabilities_by_set(probability, aa_set):\n",
    "\t# Filter the probabilities\n",
    "\tfiltered_prob = {}\n",
    "\tfor aa in aa_set:\n",
    "\t\tprint(aa)\n",
    "\t\tfiltered_prob[aa[:2]] = {}\n",
    "\t\tfor mut, probs in probability[aa[:2]].items():\n",
    "\t\t\tif mut == aa[-1]:\n",
    "\t\t\t\tfiltered_prob[aa[:2]][mut] = probs\n",
    "\treturn filtered_prob\n",
    "\n",
    "def return_set(probabilities):\n",
    "\t# Return the set of the probabilities\n",
    "\tprob_set = []\n",
    "\tfor prob in probabilities:\n",
    "\t\tproba = []\n",
    "\t\tfor key, value in prob.items():\n",
    "\t\t\tproba.extend([f\"{key}{k}\" for k in value.keys()])\n",
    "\t\tprob_set.append(proba)\n",
    "\tprob = set(prob_set[0]).intersection(*prob_set[1:])\n",
    "\treturn prob\n",
    "\n",
    "def filter_by_k(probabilities, k):\n",
    "\t# Filter the probability based on how many models agree\n",
    "\tsets = list(itertools.combinations(probabilities, k))\n",
    "\tall_sets = [return_set(comb_probabilities) for comb_probabilities in sets]\n",
    "\tall_sets = all_sets[0].union(*all_sets[1:])\n",
    "\tnew_probabilities = [filter_probabilities_by_set(prob, all_sets) for prob in probabilities]\n",
    "\treturn new_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vote(probabilities):\n",
    "\t# Return the set of the probabilities\n",
    "\tmodel_count = {}\n",
    "\tmodel_names = {}\n",
    "\tfor model_name, prob in probabilities.items():\n",
    "\t\tfor key, value in prob.items():\n",
    "\t\t\taa = [f\"{key}{k}\" for k in value.keys()]\n",
    "\t\t\tfor a in aa:\n",
    "\t\t\t\tif a in model_count:\n",
    "\t\t\t\t\tmodel_count[a] += 1\n",
    "\t\t\t\t\tmodel_names[a].append(model_name)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tmodel_count[a] = 0\n",
    "\t\t\t\t\tmodel_names[a] = []\n",
    "\treturn model_count, model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "# sequence_length = input_ids.shape[1] - 2 \n",
    "# List of amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "prob_mt = {}\n",
    "positions = 2 # zero indexed\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[0, positions + 1] = tokenizer.mask_token_id\n",
    "with torch.no_grad():\n",
    "    output = model(masked_input_ids).logits # to remove the probabilities of the tokens [CLS] and [SEP] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.nn.functional.softmax(output[0, positions+1], dim=0)\n",
    "log_prob = torch.log(probabilities)\n",
    "wt_residue_id = input_ids[0, positions+1].item()\n",
    "wt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "# Get the probability of the wild type residue\n",
    "prob_wt = log_prob[wt_residue_id].item()\n",
    "# Get the probability of the mutant residue relative to the wild type residue\n",
    "prob_mt[f\"{wt_token}{positions}\"] = {f\"{key}\": log_prob[value].item() - prob_wt \n",
    "                                for key, value in aa_ids.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = pd.DataFrame(prob_mt).T\n",
    "a = {\"seq1\": pro, \"seq2\": pro, \"seq3\": pro}\n",
    "u = pd.concat(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 534.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dat = Dataset.from_dict({\"protein_sequence\": [protein_sequence, protein_sequence]})\n",
    "tok = dat.map(lambda examples: tokenizer(examples[\"protein_sequence\"], return_tensors=\"np\"), batched=True)\n",
    "tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "a = get_probabilities(protein_sequence, model, tokenizer, positions=range(0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>I</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>N</th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "      <th>R</th>\n",
       "      <th>S</th>\n",
       "      <th>T</th>\n",
       "      <th>V</th>\n",
       "      <th>W</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P2</th>\n",
       "      <td>0.295406</td>\n",
       "      <td>-2.850163</td>\n",
       "      <td>0.606348</td>\n",
       "      <td>1.052304</td>\n",
       "      <td>-0.956331</td>\n",
       "      <td>-0.088888</td>\n",
       "      <td>-1.368847</td>\n",
       "      <td>0.174118</td>\n",
       "      <td>0.886782</td>\n",
       "      <td>0.083619</td>\n",
       "      <td>-1.641879</td>\n",
       "      <td>0.449908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.252202</td>\n",
       "      <td>-0.313501</td>\n",
       "      <td>0.748899</td>\n",
       "      <td>0.660977</td>\n",
       "      <td>0.073754</td>\n",
       "      <td>-3.000461</td>\n",
       "      <td>-1.120543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           A         C         D         E         F         G         H  \\\n",
       "P2  0.295406 -2.850163  0.606348  1.052304 -0.956331 -0.088888 -1.368847   \n",
       "\n",
       "           I         K         L         M         N    P         Q         R  \\\n",
       "P2  0.174118  0.886782  0.083619 -1.641879  0.449908  0.0 -0.252202 -0.313501   \n",
       "\n",
       "           S         T         V         W         Y  \n",
       "P2  0.748899  0.660977  0.073754 -3.000461 -1.120543  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = pd.read_csv(\"test.csv\", index_col=[0,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0037612915039"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = EsmForMaskedLM.from_pretrained(model_name)\n",
    "# Encode the protein sequence\n",
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "# sequence_length = input_ids.shape[1] - 2 \n",
    "# Get the probabilities\t\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids).logits[0,1:-1]\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "val = 0\n",
    "for num, i in enumerate(input_ids[0,1:-1]):\n",
    "    val += probabilities[num][i]\n",
    "val.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wild_marginal(positions, input_ids, tokenizer, model):\n",
    "\tall_prob = []\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_ids).logits\n",
    "\tfor x in positions:\n",
    "\t\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=0)\n",
    "\t\tall_prob.append(probabilities)\n",
    "\treturn all_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(protein_sequence, return_tensors=\"pt\") # it will add a cls and eos tokens, so the lenght is less \n",
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = input_ids.shape[1] - 2 \n",
    "# List of amino acids\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_ids = {aa: tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "masked_input_ids = input_ids.clone()\n",
    "masked_input_ids[0,56] = tokenizer.mask_token_id\n",
    "with torch.no_grad():\n",
    "    output = model(masked_input_ids).logits\n",
    "    newoutput = model(input_ids).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_prob = []\n",
    "for x in [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]:\n",
    "\tmasked_input_ids = input_ids.clone()\n",
    "\tmasked_input_ids[0, x+1] = tokenizer.mask_token_id\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(masked_input_ids).logits\n",
    "\twt_residue_id = input_ids[0, x+1].item()\n",
    "\twt_token = tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "\t# Get the probability of the wild type residue\n",
    "\tprobabilities = torch.nn.functional.softmax(output[0, x+1], dim=-1)\n",
    "\tall_prob.append(probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.7804, -27.3554, -15.7982, -27.3520,  -1.4277,  -2.5076,  -3.9805,\n",
       "         -1.9887,  -4.3450,  -3.8944,  -3.6909,  -4.1550,  -1.9134,  -4.8577,\n",
       "         -4.9729,  -3.4737,  -4.2161,  -4.5330,  -2.4678,  -2.4993,  -3.5960,\n",
       "         -4.5091,  -3.9432,  -5.1766, -11.6180, -15.6090, -15.8708, -16.4083,\n",
       "        -19.7412, -20.0578, -20.1161, -20.1449, -27.3477])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = torch.nn.functional.softmax(output[0, 56], dim=0)\n",
    "log_probabilities = torch.log(probabilities)\n",
    "log_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt_residue = input_ids[0, 56].item()\n",
    "\n",
    "log_prob_wt = log_probabilities[wt_residue].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'E56A': 4.001879542035948,\n",
       " 'E56C': 0.2774215940895632,\n",
       " 'E56D': 0.38165176240465715,\n",
       " 'E56E': 1.0,\n",
       " 'E56F': 4.164444006026979,\n",
       " 'E56G': 0.9174956996554051,\n",
       " 'E56H': 0.5408050497639408,\n",
       " 'E56I': 7.24976954106049,\n",
       " 'E56K': 1.5230791107844535,\n",
       " 'E56L': 11.783465044635921,\n",
       " 'E56M': 1.3477541668824693,\n",
       " 'E56N': 0.5280566944547288,\n",
       " 'E56P': 0.34012259939748507,\n",
       " 'E56Q': 0.7249081595733019,\n",
       " 'E56R': 1.2257103964296387,\n",
       " 'E56S': 0.6372239090923898,\n",
       " 'E56T': 0.770610221597457,\n",
       " 'E56V': 6.724243731397887,\n",
       " 'E56W': 0.9524152847213364,\n",
       " 'E56Y': 4.035606249303408}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_wt = probabilities[wt_residue].item()\n",
    "# Get the probability of the mutant residue\n",
    "prob_mt = {f\"E56{key}\": probabilities[value].item()/prob_wt for key, value in aa_ids.items()}\n",
    "prob_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.625357189342083"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_mt[\"E56L\"] / prob_mt[\"E56I\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"fill-mask\", model=model_name, tokenizer=model_name, top_k=20)\n",
    "\n",
    "def use_pipe_marginal(pipe, sequence, positions, tokenizer):\n",
    "\tseq = []\n",
    "\tfor pos in positions:\n",
    "\t\tsequence = sequence[:pos] + tokenizer.mask_token + sequence[pos+1:]\n",
    "\t\tseq.append(sequence)\n",
    "\treturn pipe(seq)\n",
    "\n",
    "\n",
    "def parse_pipe_output(output, sequence, positions):\n",
    "\tall_proba = {}\n",
    "\tfor num, pos in enumerate(positions):\n",
    "\t\twild = sequence[pos]\n",
    "\t\tall_proba[f\"{wild}{pos}\"] = {x[\"token_str\"]: x[\"score\"] for x in output[num]}\n",
    "\t\tall_proba[f\"{wild}{pos}\"] = {k: v / all_proba[f\"{wild}{pos}\"][wild] for k, v in all_proba[f\"{wild}{pos}\"].items()}\n",
    "\treturn pd.DataFrame(all_proba).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BioML.applications import suggest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sug = suggest.SuggestMutations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stra = {\"masked_marginal\": suggest.masked_marginal, \"wild_marginal\": suggest.wild_marginal}\n",
    "sequences = sug.read_fasta(\"/home/ruite/Projects/Oxipro/TMM/mfmo.fasta\")\n",
    "input_ids = sug.tokenizer.encode(sequences[\"tr|Q83XK4|Q83XK4_9GAMM\"], return_tensors=\"pt\")\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_ids = {aa: sug.tokenizer.convert_tokens_to_ids(aa) for aa in amino_acids}\n",
    "prob_mt = {}\n",
    "\n",
    "positions = range(input_ids.shape[1]-2) # -2 to remove the CLS and EOS tokens\n",
    "all_prob = stra[\"masked_marginal\"](positions, input_ids, sug.tokenizer, sug.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for pos in positions:\n",
    "    wt_residue_id = input_ids[0, pos+1].item()\n",
    "    wt_token = sug.tokenizer.convert_ids_to_tokens(wt_residue_id)\n",
    "    # Get the probability of the wild type residue\n",
    "    prob_wt = all_prob[pos][wt_residue_id].item()\n",
    "    # Get the probability of the mutant residue relative to the wild type residue\n",
    "    prob_mt[f\"{wt_token}{pos}\"] = {f\"{key}\": all_prob[pos][value].item() - prob_wt for key, value in aa_ids.items()}\n",
    "\n",
    "\n",
    "suggestions = pd.DataFrame(prob_mt).T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
