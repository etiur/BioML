{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to finetune HuggingFace models on text data of any size and format with custom splitting (not random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to handle text data of any size and format with custom split because random splitting is not recommended for protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from datasets import Dataset, DatasetDict\n",
    "from BioML.utilities import split_methods\n",
    "from BioML.deep.embeddings import LLMConfig, TokenizeFasta\n",
    "from BioML.deep.utils import set_seed\n",
    "from peft import get_peft_model, LoraConfig\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to label the target values as labels so Trainer can recognize it.\n",
    "Dataset can actually be used for any usecases with large   files it doesn't depend on transformers  \n",
    "Although you would need to use PyTorch Dataloader to transform it into batches (but it only returns inputs ids and attention masks will it also return labels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2785aef9f041e4a57e908bc238bff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "y = np.random.randint(0, 2, size=len(b))\n",
    "dataset = b.add_column(\"labels\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TokenizeFasta()\n",
    "tokens = tok.tokenize(\"../data/whole_sequence.fasta\", ([\"labels\", y],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom spliting with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = split_methods.ClusterSpliter(\"../data/resultsDB_clu.tsv\")\n",
    "train, test = cluster.train_test_split(range(len(dataset)), groups=dataset[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(range(len(dataset)), stratify=dataset[\"labels\"], test_size=0.2) # random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = DatasetDict({\"train\":dataset.select(train), \"test\":dataset.select(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, validation = cluster.train_test_split(range(len(new[\"train\"])), groups=new[\"train\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2 = DatasetDict({\"train\":new[\"train\"].select(train_), \"test\":dataset.select(test), \"validation\": new[\"train\"].select(validation)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the protein language models model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2) # torch_dtype=torch.bfloat16 to load in bfloat16 which is accepted by CPUs unlike float16\n",
    "\n",
    "def model_init2(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "\treturn AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f68842954984e87abc3735b7d39e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3b4ce46174a68bbf041a21f15b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new[\"train\"] = new[\"train\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)\n",
    "new[\"test\"] = new[\"test\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 1\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se use cpu to False whe you wan to use GPUs (it will automatically use GPUs), when f16 is True it will only use GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.2, lr_scheduler_type='cosine', fp16=False if device==\"cpu\" else True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to=['mlflow'],\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"matthews_correlation\", \n",
    "    save_total_limit=2, save_strategy=\"epoch\", seed=3242342, gradient_accumulation_steps=4, use_cpu=True if device==\"cpu\" else False) \n",
    "\n",
    "## The warmup step together with cosine learning rate scheduler turns to onecycle learning rate scheduler\n",
    "## weight decay for the Adam (AdamW) -> this is fast.Ai does\n",
    "## fp16 is half precision -> mixed training (using fp32 and fp16)\n",
    "## save_total_limit to 3 -> so only 3 models will be saved\n",
    "## each 500 steps will be saved a model\n",
    "## Save the report to mlflow\n",
    "# How to evaluate mlflow?\n",
    "# LR finder does not give reliable results for Transformers models https://github.com/huggingface/transformers/issues/16013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using several evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own function as an evaluation metric -> then you have to retun as an dict  \n",
    "Or you can use the evaluate library from hugging face to load different functions: [evaluate](https://huggingface.co/docs/evaluate/a_quick_tour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred):\n",
    "    metrics = [\"accuracy\", \"f1\", \"matthews_correlation\", \"precision\", \"recall\"]\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    loaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "    results = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "               for metric in metrics}\n",
    "\n",
    "    # the predictions from the models are logits (it also returns the labels, \n",
    "    # it also returns loss, attentions and hidden state but that is the classification model, for evalaution Trainer will only \n",
    "    # return logits and labels)\n",
    "    return results\n",
    "\n",
    "def compute_regression_metrics(eval_pred):\n",
    "\tmetrics = [\"mse\", \"mae\"]\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = logits\n",
    "\tloaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "\tresults = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "\t\t\t   for metric in metrics}\n",
    "\tresults[\"r2\"] = evaluate.load(\"r_squared\").compute(predictions=predictions, references=labels)\n",
    "\tresults[\"rmse\"] = loaded[\"mse\"].compute(predictions=predictions, references=labels, squared=False)[\"mse\"]\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        print(inputs)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.compute_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, args, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, compute_metrics=compute_classification_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters learning rate and batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8, 16]),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_classification_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_matthews_correlation\"]\n",
    "\n",
    "def compute_regression_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_r2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "model_init should have 0 or 1 argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we need to pass tokenized datasets\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_classification_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStoppingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:389\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;241m=\u001b[39m model_init\n\u001b[1;32m--> 389\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_model_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:1457\u001b[0m, in \u001b[0;36mTrainer.call_model_init\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init(trial)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should have 0 or 1 argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should not return None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: model_init should have 0 or 1 argument."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(None, args, model_init=model_init2, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, \n",
    "                  compute_metrics=compute_classification_metrics, \n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:27:53,211] A new study created in RDB with name: no-name-305c8b7d-b552-4cd6-b1d9-2e327a424486\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bff178a0fc74c9b9ffc182ef25172a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fd50fa88104f34832ff3bb22d0fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6917587518692017, 'eval_accuracy': 0.5666666666666667, 'eval_f1': 0.0, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 18.5898, 'eval_samples_per_second': 1.614, 'eval_steps_per_second': 0.807, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ddca6ef5484552b6fbd08fc014c515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7241240739822388, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 17.8967, 'eval_samples_per_second': 1.676, 'eval_steps_per_second': 0.838, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d8f83bc982433cbba6a578c3844d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7805582880973816, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 18.3333, 'eval_samples_per_second': 1.636, 'eval_steps_per_second': 0.818, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:36:15,996] Trial 0 finished with values: [0.7805582880973816, -0.008988968316207744] and parameters: {'learning_rate': 7.813286994811102e-05, 'gradient_accumulation_steps': 4}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 501.8248, 'train_samples_per_second': 0.933, 'train_steps_per_second': 0.231, 'train_loss': 0.6711522244859016, 'epoch': 2.97}\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "\tdirection=[\"minimize\", \"maximize\"],\n",
    "\tbackend=\"optuna\",\n",
    "\thp_space=optuna_hp_space,\n",
    "\tn_trials=1,\n",
    "\tcompute_objective=compute_classification_objective,\n",
    "    storage='sqlite:///my_optuna_studies.db',\n",
    "    load_if_exists=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.learning_rate = best_trials[0].hyperparameters[\"learning_rate\"]\n",
    "trainer.args.gradient_accumulation_steps = best_trials[0].hyperparameters[\"gradient_accumulation_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914d1c5b36242109903c985c86c6faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f5a46314b493cb66386ad59cc7861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7221236824989319, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.508, 'eval_samples_per_second': 0.952, 'eval_steps_per_second': 0.476, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e19a75be81b47138b51a3809d41f9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7303746938705444, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2142, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.481, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b5477268b8416aae340e6bca02f7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7445932030677795, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2265, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.48, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b70848b104408281ab0169afb06ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.753971517086029, 'eval_accuracy': 0.6, 'eval_f1': 0.7272727272727273, 'eval_matthews_correlation': 0.2857142857142857, 'eval_precision': 0.5714285714285714, 'eval_recall': 1.0, 'eval_runtime': 31.5365, 'eval_samples_per_second': 0.951, 'eval_steps_per_second': 0.476, 'epoch': 3.97}\n",
      "{'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=116, training_loss=0.6006371070598734, metrics={'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that we get the same results by evaluating the results once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for hyperparameters like the learning rate which is the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it is actually batch size and learning rate -> smaller batch sizes tend to work better than large batch sizes -> but learning rate is affected by batch as well -> higher abtch need higher learning rate.\n",
    "\n",
    "Fix everything else and tune the learning rate -> learning rate finder doesn'0t seem to work very well for transformers?  \n",
    "But teh idea of learning rate finder is just test different learning rates -> so I cannot test them?\n",
    "\n",
    "Ktrains: A wrapper to do many tasks and has a learning rate finder: [ktrains](https://github.com/amaiya/ktrain)\n",
    "\n",
    "Use pytorch lightning perhaps: [pytorch_lighningt_huggingface](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a552ef6cc74743f4b55e8d0d0bfe2327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b6497c18947b4a880757af2b28b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/44.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigscience/T0pp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\modeling_utils.py:3190\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3175\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3189\u001b[0m     }\n\u001b[1;32m-> 3190\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   3192\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3193\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3195\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    537\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model = AutoModel.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from lightning import LightningModule, LightningDataModule\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning import Trainer\n",
    "from torchmetrics.functional.classification import (\n",
    "    accuracy,\n",
    "    f1_score,\n",
    "    precision,\n",
    "    recall,\n",
    "    auroc,\n",
    "    average_precision,\n",
    "    cohen_kappa,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef\n",
    ") \n",
    "\n",
    "from torchmetrics.functional.regression import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    pearson_corrcoef,\n",
    "    kendall_rank_corrcoef,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_log_error)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from BioML.deep.train_config import LLMConfig\n",
    "from BioML.utilities import split_methods as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                     target: torch.tensor, num_classes: int=2, threshold: float=0.5):\n",
    "    task = \"binary\" if num_classes == 2 else \"multiclass\"\n",
    "    metrics = {\n",
    "                f\"{split}_Loss\": loss,\n",
    "                f\"{split}_Acc\": accuracy(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_F1\":f1_score(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Precision\": precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Recall\": recall(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\"\n",
    "                ),\n",
    "                f\"{split}_MCC\": matthews_corrcoef(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    threshold=threshold,\n",
    "                    task=task,\n",
    "                ),\n",
    "                f\"{split}_Confusion_Matrix\": confusion_matrix(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    normalize=\"true\",\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                ),\n",
    "                f\"{split}_AUROC\": auroc(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    thresholds=None,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Average_Precision\": average_precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Cohen_Kappa\": cohen_kappa(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                )}\n",
    "    return metrics\n",
    "\n",
    "def calculate_regression_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                 target: torch.tensor):\n",
    "    metrics = {f\"{split}_Loss\": loss,\n",
    "                f\"{split}_MAE\": mean_absolute_error(preds, target),\n",
    "                f\"{split}_MSE\": mean_squared_error(preds, target),\n",
    "                f\"{split}_RMSE\": mean_squared_error(preds, target, squared=False),\n",
    "                f\"{split}_R2\": r2_score(preds, target),\n",
    "                f\"{split}_Pearson\": pearson_corrcoef(preds, target),\n",
    "                f\"{split}_Kendall\": kendall_rank_corrcoef(preds, target),\n",
    "                f\"{split}_MAPE\": mean_absolute_percentage_error(preds, target),\n",
    "                f\"{split}_MSLE\": mean_squared_log_error(preds, target)}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values = (8, 16, 32, 64, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(x, y):\n",
    "    return x + y\n",
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.x = test\n",
    "        self.tes_ = partial(self.x, y=2)\n",
    "    def __call__(self, x):\n",
    "        return self.tes_(x)\n",
    "\n",
    "t = Test()\n",
    "t(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.esm.encoder.layer.0.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dropout']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_module_names_for_peft(model, filter_=\"key\"):\n",
    "    if isinstance(filter_, str):\n",
    "        filter_ = [filter_] # if it is a string, convert it to a list\n",
    "    module_names = []\n",
    "    for num, (name, module) in enumerate(model.named_modules()):\n",
    "        n = name.split(\".\")\n",
    "        if filter_ and set(n).intersection(filter_):\n",
    "            module_names.append(name)\n",
    "        elif not filter_:\n",
    "            module_names.append(name)\n",
    "    return module_names\n",
    "\n",
    "names = get_target_module_names_for_peft(model, filter_=\"output\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, replace_lora_weights_loftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1, \n",
    "                         target_modules=\"all-linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 285,144 || all params: 8,125,907 || trainable%: 3.509072894878073\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from BioML.deep import finetuning as ft\n",
    "from BioML.deep.utils import loftq_initialization\n",
    "from datasets import Dataset\n",
    "from Bio import SeqIO\n",
    "from safetensors import SafetensorError\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"../data/esterase_labels.csv\"\n",
    "lab = pd.read_csv(label, index_col=0)\n",
    "split_config = ft.SplitConfig()\n",
    "llm_config = ft.LLMConfig()\n",
    "train_config = ft.TrainConfig(batch_size=1, max_epochs=1, lora_rank=8)\n",
    "fasta_file = \"../data/whole_sequence.fasta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":fasta_file})\n",
    "dataset = b.add_column(\"labels\", lab.to_numpy().flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ft.TokenizeFasta(llm_config)\n",
    "tokenizer.tokenize(fasta_file, add_columns=[(\"labels\",lab.to_numpy().flatten())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "splitter = ft.PrepareSplit(split_config.cluster_file, split_config.shuffle, split_config.random_seed, \n",
    "                            split_config.splitting_strategy, \n",
    "                            split_config.num_split, split_config.stratify)\n",
    "data_module = ft.DataModule(splitter, fasta_file, lab.values.flatten(), llm_config, train_config.batch_size)\n",
    "peft = ft.PreparePEFT(True)\n",
    "model = peft.get_model(llm_config)\n",
    "peft_config = peft.get_lora_config(rank=train_config.lora_rank, target_modules=train_config.target_modules, \n",
    "                                       lora_alpha=train_config.lora_alpha, lora_dropout=train_config.lora_dropout)\n",
    "model = ft.get_peft_model(model, peft_config)\n",
    "#try:\n",
    "    #ft.replace_lora_weights_loftq(model) # https://github.com/huggingface/peft/blob/main/examples/loftq_finetuning/LoftQ_weight_replacement.ipynb\n",
    "#except SafetensorError as e:\n",
    "#    print(e)\n",
    "light_mod = ft.TransformerModule(model, train_config, lr=1e-3)\n",
    "\n",
    "filename = f\"{{epoch}}-{{{train_config.optimize}:.2f}}\"\n",
    "checkpoint_callback = ft.ModelCheckpoint(filename=filename, monitor=train_config.optimize, \n",
    "                                              mode=train_config.optimize_mode, verbose=True, save_top_k=1)\n",
    "early_callback = ft.EarlyStopping(monitor=train_config.optimize, min_delta=train_config.min_delta, \n",
    "                                       patience=train_config.patience, verbose=True, mode=train_config.optimize_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "peft = ft.PreparePEFT(True)\n",
    "model2 = peft.get_model(llm_config)\n",
    "peft_config = peft.get_lora_config(rank=train_config.lora_rank, target_modules=train_config.target_modules, \n",
    "                                       lora_alpha=train_config.lora_alpha, lora_dropout=train_config.lora_dropout)\n",
    "model2 = ft.get_peft_model(model2, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0235,  0.0263, -0.0274,  ...,  0.0383,  0.0155,  0.0455],\n",
      "        [ 0.0005, -0.0314,  0.0201,  ...,  0.0262,  0.0409, -0.0128],\n",
      "        [ 0.0557,  0.0065,  0.0155,  ..., -0.0222, -0.0319, -0.0253],\n",
      "        ...,\n",
      "        [-0.0398, -0.0048, -0.0126,  ..., -0.0211, -0.0285, -0.0191],\n",
      "        [ 0.0077,  0.0053,  0.0307,  ...,  0.0496,  0.0328,  0.0542],\n",
      "        [ 0.0221,  0.0179,  0.0110,  ...,  0.0061, -0.0308,  0.0303]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5862, 0.9429, 0.8837, 1.0076, 0.8492, 1.7166, 0.8644, 1.3870, 0.7423,\n",
      "        0.6175, 0.8243, 1.1255, 1.2853, 0.9283, 1.2228, 1.8992, 2.3443, 1.6446,\n",
      "        1.3758, 0.8378, 2.1611, 0.5501, 1.0974, 1.0511, 0.8397, 0.7476, 1.4126,\n",
      "        1.6756, 1.0346, 2.0714, 1.1916, 2.2218, 1.3524, 1.6404, 1.6772, 1.6735,\n",
      "        1.5414, 0.9779, 1.3851, 1.1143, 1.3759, 0.7261, 0.9219, 0.7447, 0.9530,\n",
      "        1.7455, 1.5829, 1.0947, 0.5239, 0.8929, 0.7863, 0.7890, 1.0879, 1.2823,\n",
      "        0.8932, 1.1095, 0.6585, 0.8408, 1.0320, 1.2270, 1.3935, 0.4902, 1.3824,\n",
      "        1.1673, 1.1798, 0.8884, 0.9308, 1.6890, 1.8688, 0.6523, 1.6008, 1.1140,\n",
      "        1.7788, 1.4181, 1.4356, 0.7761, 0.4717, 1.7446, 1.6219, 1.4715, 1.1666,\n",
      "        1.2488, 0.9497, 1.2668, 0.6491, 1.1796, 1.5521, 1.6871, 1.6649, 1.1435,\n",
      "        1.5632, 1.3659, 1.6767, 1.8283, 1.0855, 1.4968, 0.5521, 0.5434, 0.7009,\n",
      "        0.8965, 0.6092, 1.2434, 2.0437, 1.5180, 0.6302, 0.5475, 0.5228, 1.4036,\n",
      "        1.0545, 1.1239, 1.2507, 1.6234, 0.5058, 0.4475, 0.6897, 0.5137, 0.6819,\n",
      "        1.0727, 1.0866, 0.9307, 0.4857, 0.5720, 0.6350, 0.6765, 1.2327, 0.5822,\n",
      "        1.3968, 1.2755, 0.8633, 0.6238, 1.5989, 1.5493, 1.0602, 0.9070, 1.0289,\n",
      "        0.7648, 0.3507, 0.8619, 1.4043, 0.9922, 1.2073, 1.2402, 1.0743, 0.7398,\n",
      "        1.8467, 1.2726, 1.5709, 1.6023, 2.3057, 0.4682, 1.6140, 1.8348, 1.5967,\n",
      "        1.2403, 1.2038, 0.5121, 0.4503, 2.5990, 1.4505, 0.9527, 0.7552, 0.8365,\n",
      "        1.1179, 1.1192, 1.3609, 1.5615, 1.6031, 0.7561, 1.1096, 1.0553, 0.8823,\n",
      "        1.3429, 1.8495, 1.4050, 0.7984, 0.7357, 1.3229, 0.9304, 1.2168, 1.9348,\n",
      "        1.4657, 1.3290, 0.9052, 1.6767, 0.7352, 0.8353, 1.0252, 1.9396, 1.3151,\n",
      "        1.2327, 1.4784, 1.2812, 0.5867, 1.1560, 0.8847, 1.1759, 1.2551, 0.6593,\n",
      "        1.0892, 1.7112, 0.4957, 0.9320, 1.2353, 0.9506, 0.7596, 1.0426, 1.7893,\n",
      "        0.9921, 0.4484, 0.6358, 1.4825, 0.9794, 0.8826, 0.7294, 0.6848, 0.8231,\n",
      "        0.5078, 0.5924, 0.3667, 1.1160, 1.1978, 1.3284, 0.7188, 0.6411, 3.4073,\n",
      "        2.2934, 2.3287, 2.2973, 1.4569, 2.1643, 1.5393, 1.7422, 3.1833, 2.2823,\n",
      "        2.9344, 1.6734, 2.8809, 1.8536, 2.1289, 2.2592, 1.1477, 0.8934, 0.5670,\n",
      "        1.4487, 1.9125, 0.8360, 1.1007, 1.2043, 0.7052, 1.1521, 1.4432, 1.2166,\n",
      "        0.4975, 1.9589, 1.0552, 0.8392, 0.5745, 1.1248, 1.4840, 1.2769, 1.6698,\n",
      "        0.5441, 1.7804, 0.8257, 1.0273, 0.9426, 0.5990, 1.5690, 0.7234, 1.8827,\n",
      "        1.8989, 0.8611, 1.3200, 1.5710, 1.7008, 1.3976, 1.2304, 0.7790, 1.7279,\n",
      "        1.2610, 0.9092, 1.0180, 2.0371, 1.9601, 2.2705, 2.4800, 1.1135, 1.1172,\n",
      "        1.6322, 0.7774, 1.6956, 1.9093, 1.2729, 0.7173, 1.6135, 1.0815, 0.8686,\n",
      "        1.3260, 0.5600, 1.0039, 1.1433, 2.0486, 1.9045, 1.1057, 0.5381, 0.5306,\n",
      "        1.4354, 0.4798, 0.4651, 2.1708, 1.5492, 1.1246, 1.5154, 1.9522, 1.1209,\n",
      "        1.5180, 2.3922, 0.5586, 0.8654, 1.0511], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.6018e-02, -3.1220e-02, -1.0276e-03,  ...,  5.5106e-02,\n",
      "         -4.4594e-02,  2.5359e-02],\n",
      "        [ 1.7646e-03,  3.2644e-02, -1.3617e-02,  ...,  1.9058e-02,\n",
      "          3.4773e-05, -4.6599e-02],\n",
      "        [ 1.3301e-02, -2.3965e-02,  4.9648e-02,  ...,  1.1154e-02,\n",
      "         -1.9746e-02, -8.4897e-03],\n",
      "        ...,\n",
      "        [ 9.3013e-03, -5.2203e-02,  4.0559e-02,  ...,  1.2691e-02,\n",
      "         -5.7501e-03, -1.3006e-02],\n",
      "        [-4.1859e-02, -2.4251e-02, -4.2353e-02,  ..., -3.9676e-02,\n",
      "          4.1222e-02, -4.7053e-02],\n",
      "        [ 3.5137e-02,  4.1790e-03, -1.2425e-02,  ...,  4.2986e-02,\n",
      "         -3.7714e-02, -3.7773e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8478, 1.4965, 1.3076, 1.4875, 1.3179, 1.5522, 0.8240, 1.3240, 1.1532,\n",
      "        1.3316, 1.5933, 1.9032, 1.3815, 1.2156, 1.2397, 1.8229, 2.2529, 1.5213,\n",
      "        1.4318, 1.9215, 1.6892, 1.8160, 1.1300, 1.0768, 1.4711, 1.1317, 2.2967,\n",
      "        2.0824, 2.0584, 1.5540, 1.1587, 2.2666, 2.3109, 2.3845, 2.4080, 1.5889,\n",
      "        1.5185, 1.2397, 1.6919, 1.1430, 1.2908, 1.9239, 1.5610, 1.8111, 1.3454,\n",
      "        1.7474, 1.8425, 1.1583, 0.9442, 0.8059, 1.3115, 1.8933, 1.6322, 1.5446,\n",
      "        0.9709, 1.2652, 1.0774, 1.5023, 1.6979, 1.2072, 2.1618, 1.0550, 1.3565,\n",
      "        1.6929, 1.7812, 0.8990, 1.7082, 1.9598, 1.9057, 1.7797, 1.6153, 1.0299,\n",
      "        1.2934, 2.1556, 2.0294, 1.7755, 1.3463, 1.3405, 1.7910, 1.8695, 2.0361,\n",
      "        2.3234, 1.5621, 1.7095, 1.6019, 1.2569, 2.0161, 1.9803, 2.3412, 1.5434,\n",
      "        2.3673, 1.8573, 1.2167, 1.7541, 0.9040, 2.0985, 0.8366, 1.0930, 1.2617,\n",
      "        1.0773, 1.2256, 1.6233, 1.9353, 1.8480, 0.9491, 0.9974, 1.1079, 1.5546,\n",
      "        1.4845, 1.6497, 1.5999, 1.6419, 0.5624, 0.8276, 0.8974, 0.8489, 1.3092,\n",
      "        1.8070, 1.1411, 0.7907, 0.6280, 0.7666, 1.1891, 1.6078, 1.8518, 1.0459,\n",
      "        1.3360, 1.8340, 1.4418, 0.8339, 1.2550, 1.3489, 0.6919, 0.6963, 1.0518,\n",
      "        0.9414, 0.4443, 1.1598, 1.2221, 0.8108, 0.7379, 0.5495, 1.0927, 0.6286,\n",
      "        2.1190, 1.7302, 1.9393, 1.9609, 1.4557, 2.6250, 1.5304, 1.8769, 2.3922,\n",
      "        2.2058, 2.3147, 1.9202, 2.1419, 2.0313, 1.5890, 0.9214, 1.6091, 1.8182,\n",
      "        1.8322, 1.6996, 1.9643, 1.5410, 2.1490, 0.7088, 1.4362, 1.4373, 1.6259,\n",
      "        2.2336, 1.7949, 1.7293, 1.0748, 0.7752, 1.2169, 1.5681, 2.1432, 2.1661,\n",
      "        1.7278, 1.4980, 1.2478, 1.8937, 1.8201, 1.5992, 1.4620, 2.0191, 1.7328,\n",
      "        1.5289, 1.2310, 1.4932, 0.7956, 1.5318, 1.1820, 1.3262, 1.7898, 1.0852,\n",
      "        1.1153, 2.1663, 0.9311, 0.8985, 1.4675, 1.7139, 1.4143, 1.5927, 2.0170,\n",
      "        1.3242, 0.9296, 0.8199, 1.6955, 1.1445, 1.0452, 0.6766, 0.8192, 0.8343,\n",
      "        0.8152, 0.9886, 0.4908, 1.2168, 1.2173, 0.7831, 0.6315, 0.8225, 2.9372,\n",
      "        2.7239, 2.8589, 1.7102, 2.3174, 1.3986, 1.4204, 1.6910, 2.2551, 1.7845,\n",
      "        1.8485, 2.2803, 1.4430, 2.2149, 2.0290, 1.8286, 2.1062, 1.6624, 1.4873,\n",
      "        1.6571, 1.5070, 1.3585, 1.3648, 1.3204, 0.9181, 1.8802, 2.2725, 1.9339,\n",
      "        1.7663, 1.9206, 0.9720, 0.9086, 1.4240, 0.8461, 1.8216, 1.6548, 1.6317,\n",
      "        1.2630, 1.4269, 0.7407, 0.9957, 1.6741, 1.2006, 1.5965, 1.5432, 1.6285,\n",
      "        2.1168, 0.9096, 1.8020, 1.8720, 1.1545, 1.5248, 1.7755, 2.7695, 1.5409,\n",
      "        1.3871, 0.9830, 1.4320, 2.7218, 2.1875, 1.9937, 1.5291, 1.2329, 1.0888,\n",
      "        2.2280, 2.0866, 1.8274, 2.1331, 1.4064, 1.1456, 1.7418, 1.1542, 2.0455,\n",
      "        1.8637, 2.2227, 1.8890, 1.6301, 2.0033, 1.9405, 1.2246, 2.1718, 1.8097,\n",
      "        2.1228, 1.5776, 2.5457, 1.8543, 1.9696, 1.5330, 1.7268, 2.9122, 1.7255,\n",
      "        1.7153, 1.5749, 2.1756, 0.9060, 0.9757], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0522, -0.0309, -0.0090,  ..., -0.0245, -0.0419, -0.0367],\n",
      "        [ 0.0509,  0.0144, -0.0435,  ..., -0.0215,  0.0549,  0.0294],\n",
      "        [-0.0390, -0.0480, -0.0320,  ..., -0.0553, -0.0232,  0.0103],\n",
      "        ...,\n",
      "        [-0.0274, -0.0129,  0.0242,  ...,  0.0068,  0.0458, -0.0113],\n",
      "        [-0.0514,  0.0451, -0.0063,  ..., -0.0263,  0.0282,  0.0022],\n",
      "        [ 0.0504,  0.0419, -0.0322,  ..., -0.0472,  0.0266,  0.0229]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4814, 1.6077, 1.4001, 1.6225, 1.3481, 1.5382, 1.5926, 1.3350, 1.5171,\n",
      "        1.4979, 1.3260, 1.4033, 1.4921, 1.5024, 1.4713, 1.3863, 0.9298, 1.1069,\n",
      "        1.0242, 1.1283, 0.8803, 1.0907, 1.2020, 0.3930, 1.2059, 0.8637, 1.2244,\n",
      "        0.9819, 1.2443, 0.9304, 1.1733, 1.1784, 0.9154, 1.1095, 1.1201, 1.0209,\n",
      "        1.0684, 1.0949, 1.0835, 1.0419, 0.9907, 0.9597, 0.9928, 1.1975, 1.1156,\n",
      "        1.1844, 1.0525, 0.9629, 1.5726, 1.6138, 1.4270, 1.1307, 1.5021, 1.3843,\n",
      "        1.5141, 1.5866, 1.0014, 1.4198, 1.2534, 1.7632, 1.5415, 1.5019, 1.0348,\n",
      "        1.3807, 0.9291, 1.2902, 1.1577, 1.1545, 1.1762, 1.2567, 1.3934, 1.1478,\n",
      "        1.2845, 1.2884, 1.1504, 1.3195, 1.3017, 1.2068, 1.3105, 1.1871, 0.9732,\n",
      "        1.0017, 1.2545, 1.0540, 1.1979, 1.1796, 1.2457, 1.1855, 1.2896, 1.0182,\n",
      "        1.1920, 1.1203, 1.1193, 1.0838, 1.0711, 1.1351, 1.7125, 1.6579, 1.4608,\n",
      "        1.5461, 1.4895, 1.5192, 1.7486, 1.6703, 1.4715, 1.3665, 1.0663, 1.6621,\n",
      "        1.5964, 1.2675, 1.4495, 1.6934, 1.7329, 1.8238, 1.6447, 1.7920, 2.0522,\n",
      "        1.8246, 1.9788, 2.1421, 1.6785, 1.8669, 1.6668, 1.7534, 1.6113, 1.7879,\n",
      "        1.7411, 2.1028, 0.9960, 1.1526, 1.1986, 1.1088, 1.2201, 1.4889, 1.1933,\n",
      "        1.5490, 1.1309, 1.4473, 1.3306, 1.2104, 1.1061, 1.2175, 1.1002, 1.4226,\n",
      "        1.1499, 1.0341, 1.0874, 1.1223, 1.1062, 1.0988, 1.2045, 1.2276, 1.1050,\n",
      "        1.2049, 1.2166, 1.0914, 1.1201, 1.0096, 1.0788, 1.0483, 1.2278, 1.4683,\n",
      "        1.1742, 1.2811, 1.3447, 1.2587, 1.3016, 1.2617, 1.5040, 1.6671, 1.3443,\n",
      "        1.3528, 1.2596, 1.1355, 1.2716, 1.1829, 1.0287, 1.0273, 0.9032, 1.0473,\n",
      "        0.9755, 1.0713, 1.2970, 1.0581, 0.8725, 0.9847, 1.2098, 0.9959, 1.1010,\n",
      "        0.9484, 1.1648, 1.1365, 1.7532, 1.3972, 0.8712, 1.3171, 1.8754, 1.2665,\n",
      "        1.3175, 1.6811, 1.3677, 1.7820, 1.7233, 1.9505, 1.6949, 1.6918, 1.8269,\n",
      "        1.6858, 1.2804, 1.2682, 1.3992, 1.4749, 1.3752, 1.5512, 1.2965, 1.3963,\n",
      "        1.6035, 1.5256, 1.3309, 1.4295, 1.2354, 1.5708, 1.1627, 1.1858, 0.7366,\n",
      "        0.8240, 0.6748, 0.7892, 0.7230, 0.6876, 0.9180, 0.7898, 0.8900, 0.9544,\n",
      "        0.7396, 0.5850, 0.7262, 0.7645, 0.7086, 0.6737, 1.1840, 1.2570, 1.2490,\n",
      "        1.3624, 1.0340, 1.2662, 1.3008, 1.2063, 1.3218, 1.1598, 1.2142, 1.2784,\n",
      "        1.3799, 1.3627, 1.2091, 1.1767, 1.3212, 1.2607, 1.3596, 1.3409, 1.2491,\n",
      "        1.2890, 1.8117, 1.2449, 1.3214, 1.2024, 1.2865, 1.3264, 1.5586, 1.4158,\n",
      "        1.4000, 1.1216, 0.8905, 0.8019, 0.8863, 1.0487, 0.9838, 0.9592, 1.0017,\n",
      "        0.9702, 0.9367, 0.9242, 0.9464, 1.0847, 0.9148, 0.9844, 0.9338, 0.8764,\n",
      "        1.0475, 1.0040, 1.0029, 1.0294, 1.0492, 0.9026, 0.9753, 0.9785, 0.9783,\n",
      "        1.0685, 0.9197, 1.0438, 1.1544, 1.0312, 1.1362, 0.9640, 1.1600, 1.0978,\n",
      "        1.0498, 1.0299, 1.0972, 1.1813, 1.2136, 1.1738, 1.0779, 1.1843, 1.1739,\n",
      "        1.0872, 1.2761, 1.1155, 1.2007, 1.1933], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-4.7256e-02, -3.6541e-02,  6.3068e-05,  ...,  2.9776e-03,\n",
      "          3.1868e-02, -4.0448e-02],\n",
      "        [ 3.4106e-02, -4.3047e-03,  3.4759e-02,  ..., -3.8708e-02,\n",
      "          2.5713e-02, -2.9477e-02],\n",
      "        [ 1.6640e-02,  1.9410e-02,  3.0707e-02,  ..., -3.6100e-02,\n",
      "         -4.9287e-03, -4.1981e-02],\n",
      "        ...,\n",
      "        [-1.0762e-02, -3.6908e-02,  5.3540e-02,  ...,  3.0926e-02,\n",
      "          5.5087e-02, -3.1118e-02],\n",
      "        [ 1.7421e-02,  2.5241e-02, -2.1804e-02,  ...,  3.2472e-02,\n",
      "          1.6340e-02,  3.8335e-02],\n",
      "        [ 4.3367e-02,  4.4096e-02, -5.4629e-02,  ...,  3.5032e-03,\n",
      "         -5.5509e-02, -7.2302e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3720, 1.3759, 1.4964, 1.5290, 1.4602, 1.5630, 1.3994, 1.4003, 1.2986,\n",
      "        1.4397, 1.3445, 1.5038, 1.5165, 1.3627, 1.3552, 1.3771, 1.4274, 1.3521,\n",
      "        1.4887, 1.3440, 1.4216, 1.3974, 1.4173, 1.3619, 1.3882, 1.4385, 1.4546,\n",
      "        1.4087, 1.4196, 1.4604, 1.3732, 1.2993, 1.4353, 1.2845, 1.3508, 1.4046,\n",
      "        1.4323, 1.3855, 1.4170, 1.4631, 1.4203, 1.4204, 1.3504, 1.4417, 1.4370,\n",
      "        1.4410, 1.8064, 1.3587, 1.5679, 1.4312, 1.4341, 1.3715, 1.4576, 1.5573,\n",
      "        1.3918, 1.2312, 1.3884, 1.4200, 1.3761, 1.3224, 1.5072, 1.6890, 1.2578,\n",
      "        1.5156, 1.3940, 1.3801, 1.4499, 1.4349, 1.3804, 1.2998, 1.4136, 1.4727,\n",
      "        1.3612, 1.5192, 1.5154, 1.4049, 1.4298, 1.3172, 1.3812, 1.4775, 1.3979,\n",
      "        1.3647, 1.3198, 1.4339, 1.4477, 1.5610, 1.3700, 1.4677, 1.3843, 1.4050,\n",
      "        1.4907, 1.3991, 1.4633, 1.4339, 1.3692, 1.4252, 1.2956, 1.4503, 1.3942,\n",
      "        1.3478, 1.3696, 1.4303, 1.3764, 1.2815, 1.5146, 1.3278, 1.5258, 1.4907,\n",
      "        1.4110, 1.3608, 1.3642, 1.6105, 1.4782, 1.4610, 1.5107, 1.4070, 1.3099,\n",
      "        1.4478, 1.3408, 1.5347, 1.5871, 1.4140, 1.3655, 1.3402, 1.3283, 1.4530,\n",
      "        1.3552, 1.4347, 1.3637, 1.3263, 1.3841, 1.5747, 1.4340, 1.4781, 1.3677,\n",
      "        1.4595, 1.3917, 1.3116, 1.3878, 1.6410, 1.3498, 1.5467, 1.3671, 1.3365,\n",
      "        1.1516, 1.3917, 1.3776, 1.5009, 1.3862, 1.3684, 1.4492, 1.5408, 1.5997,\n",
      "        1.3759, 1.3943, 1.3016, 1.4260, 1.3762, 1.4762, 1.4008, 1.3382, 1.4330,\n",
      "        1.3958, 1.4832, 1.4805, 1.4953, 1.4996, 1.4420, 1.4106, 1.3743, 1.4511,\n",
      "        1.5927, 1.4656, 1.3896, 1.4053, 1.4514, 1.3308, 1.3636, 1.3799, 1.4226,\n",
      "        1.4005, 1.5238, 1.3601, 1.3606, 1.4486, 1.3199, 1.4298, 1.2384, 1.4374,\n",
      "        1.3722, 1.3805, 1.3357, 1.4810, 0.9609, 1.3887, 1.3305, 1.3917, 1.3009,\n",
      "        1.3623, 1.3417, 1.4429, 1.3743, 1.3276, 1.3508, 1.3971, 1.4452, 1.3711,\n",
      "        1.4171, 1.4246, 1.4667, 1.4156, 1.3188, 1.2998, 1.4126, 1.5008, 1.4162,\n",
      "        1.3854, 1.4152, 1.3773, 1.3708, 1.4080, 1.3757, 1.4623, 1.3233, 1.3358,\n",
      "        1.4104, 1.3981, 1.4795, 1.3842, 1.4547, 1.4356, 1.3394, 1.4621, 1.3369,\n",
      "        1.4398, 1.3993, 1.4679, 1.3771, 1.4125, 1.4341, 1.4466, 1.4619, 1.5350,\n",
      "        1.4254, 1.4268, 1.4038, 1.3911, 1.4307, 1.4917, 1.3218, 1.3409, 1.4258,\n",
      "        1.4673, 1.4127, 1.3165, 1.4553, 1.4650, 1.6837, 1.4342, 1.3766, 1.4199,\n",
      "        1.4514, 1.4636, 1.3466, 1.4857, 1.4139, 1.5178, 1.4063, 1.4184, 1.4893,\n",
      "        1.2898, 1.2427, 1.4707, 1.3554, 1.2741, 1.3780, 1.4556, 1.4533, 1.4295,\n",
      "        1.4114, 1.4623, 1.4496, 1.3470, 1.6340, 1.4527, 1.2673, 1.2923, 1.5230,\n",
      "        1.3749, 1.5059, 1.3935, 1.7477, 1.3921, 1.4250, 1.4833, 1.3409, 1.2761,\n",
      "        1.4008, 1.4157, 1.4210, 1.3868, 1.4279, 1.3872, 1.3592, 1.5226, 1.4222,\n",
      "        1.3803, 1.4201, 1.5662, 1.3911, 1.4866, 1.4924, 1.4276, 1.4520, 1.5168,\n",
      "        1.4443, 1.3887, 1.3877, 1.3908, 1.3706], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0378,  0.0326,  0.0039,  ..., -0.0221, -0.0001,  0.0025],\n",
      "        [-0.0047,  0.0255,  0.0294,  ...,  0.0504,  0.0335, -0.0075],\n",
      "        [ 0.0175,  0.0105,  0.0429,  ...,  0.0445, -0.0155, -0.0090],\n",
      "        ...,\n",
      "        [-0.0395, -0.0557, -0.0403,  ...,  0.0021, -0.0177, -0.0093],\n",
      "        [-0.0312,  0.0529,  0.0159,  ...,  0.0239,  0.0160, -0.0231],\n",
      "        [-0.0161,  0.0449,  0.0246,  ...,  0.0129,  0.0490, -0.0233]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3894, 2.1256, 1.3952,  ..., 2.5732, 1.6360, 1.6064], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0072,  0.0107, -0.0024,  ..., -0.0131,  0.0041,  0.0003],\n",
      "        [ 0.0041, -0.0060,  0.0164,  ...,  0.0160,  0.0064, -0.0025],\n",
      "        [ 0.0073,  0.0010,  0.0232,  ...,  0.0111, -0.0201,  0.0174],\n",
      "        ...,\n",
      "        [ 0.0231,  0.0041, -0.0199,  ...,  0.0175,  0.0230, -0.0132],\n",
      "        [ 0.0204,  0.0243,  0.0094,  ...,  0.0172,  0.0163,  0.0185],\n",
      "        [ 0.0010,  0.0184, -0.0200,  ..., -0.0088, -0.0131, -0.0255]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.8338, 1.9838, 2.0233, 1.9828, 1.8142, 2.0106, 1.9657, 1.9431, 1.9806,\n",
      "        1.9533, 2.0518, 2.0164, 2.0742, 2.1152, 1.8870, 1.9721, 1.9364, 1.9527,\n",
      "        2.0715, 1.9898, 1.9630, 1.9106, 2.0118, 1.8087, 2.0458, 1.8511, 2.1104,\n",
      "        1.9815, 1.8185, 1.8410, 1.8782, 2.8088, 2.0715, 2.0131, 2.0393, 1.8747,\n",
      "        1.8913, 2.0008, 2.1127, 1.9495, 1.9497, 2.0442, 1.9475, 1.9969, 1.9155,\n",
      "        1.9899, 2.7862, 2.0065, 2.0212, 1.9743, 2.0611, 1.9558, 2.0103, 1.9265,\n",
      "        1.9807, 1.9296, 2.0406, 1.9395, 1.9779, 1.8833, 2.0077, 2.3301, 2.8845,\n",
      "        1.9844, 1.9229, 1.8885, 2.1232, 1.9629, 1.9245, 2.0150, 2.0113, 2.0016,\n",
      "        1.9625, 1.9347, 1.9956, 1.8668, 2.0452, 2.0278, 1.9793, 2.0324, 2.0811,\n",
      "        2.0026, 1.9297, 1.9446, 2.0095, 1.9371, 1.9531, 1.8749, 2.4291, 1.9904,\n",
      "        1.9288, 2.0296, 1.8603, 1.9663, 1.9720, 1.9761, 1.9125, 1.9282, 2.0361,\n",
      "        1.9309, 1.8999, 1.9589, 1.9485, 2.0060, 1.9387, 1.8694, 2.0476, 1.8757,\n",
      "        1.9137, 1.9879, 1.8110, 2.0160, 1.9506, 1.8867, 1.8716, 2.0735, 1.9394,\n",
      "        1.9378, 1.9906, 1.8598, 1.9818, 2.1107, 1.8594, 2.0510, 2.2021, 1.8476,\n",
      "        1.8804, 1.9097, 1.9316, 1.9881, 2.1347, 2.0213, 1.8992, 1.9023, 1.8122,\n",
      "        1.8811, 1.9800, 2.0354, 1.9495, 1.9111, 2.0048, 1.8897, 1.9125, 1.9232,\n",
      "        1.8718, 2.0349, 1.9365, 1.9277, 1.8460, 2.0154, 2.1212, 1.8668, 2.0953,\n",
      "        1.9994, 1.9707, 1.9750, 1.9496, 1.9204, 1.9030, 1.9260, 1.9418, 1.9277,\n",
      "        1.8987, 2.0279, 1.9767, 1.9861, 1.9216, 1.9401, 1.9096, 1.9916, 1.9280,\n",
      "        2.0110, 2.0387, 2.0020, 2.0132, 1.9849, 1.9856, 1.8435, 2.0033, 1.9945,\n",
      "        1.9322, 1.9295, 2.0194, 1.9898, 1.9607, 2.0520, 2.0174, 1.8966, 1.8963,\n",
      "        1.9394, 2.0506, 1.9740, 3.8655, 1.9105, 1.9345, 1.8831, 1.9257, 1.9436,\n",
      "        1.8657, 1.8835, 1.9442, 2.0231, 1.8914, 1.9974, 2.0052, 2.0327, 1.8405,\n",
      "        2.1257, 1.8745, 1.9115, 1.9877, 1.9915, 1.9151, 1.9276, 1.9232, 1.9179,\n",
      "        2.0287, 1.8596, 2.3805, 2.0048, 1.9857, 1.9470, 2.0170, 1.8142, 2.0003,\n",
      "        1.9890, 2.0724, 2.0011, 2.1624, 2.0154, 2.0909, 2.1783, 1.9480, 1.9375,\n",
      "        1.9654, 1.9930, 1.9619, 2.0303, 1.9069, 1.8864, 2.1445, 1.9587, 1.9584,\n",
      "        2.0439, 1.9274, 1.9367, 1.9132, 2.0117, 1.9839, 1.9240, 1.9477, 2.0493,\n",
      "        1.9055, 2.0229, 1.9894, 1.9788, 1.9729, 2.0128, 1.9383, 1.8703, 2.0353,\n",
      "        1.9430, 1.9759, 1.9738, 2.1715, 2.0170, 1.9934, 2.0060, 1.9699, 1.9728,\n",
      "        2.0181, 2.0072, 1.9604, 1.9475, 1.8860, 2.0377, 1.9110, 2.1624, 1.9813,\n",
      "        1.9103, 1.9559, 2.0022, 1.8537, 2.0915, 1.9934, 1.9933, 1.9614, 2.0540,\n",
      "        2.0644, 1.9654, 1.9248, 2.2492, 2.0048, 2.0892, 2.2067, 2.1563, 1.9071,\n",
      "        2.0273, 1.9164, 1.9305, 1.9565, 2.0479, 1.9494, 2.0537, 1.9024, 1.9355,\n",
      "        1.8790, 2.0997, 2.0364, 2.1157, 2.0418, 2.1493, 1.9572, 2.0192, 1.9429,\n",
      "        1.8991, 1.9055, 1.9267, 1.8738, 3.4743], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0510,  0.0251,  0.0362,  ...,  0.0411, -0.0268,  0.0415],\n",
      "        [ 0.0391, -0.0031,  0.0472,  ...,  0.0545, -0.0147, -0.0397],\n",
      "        [ 0.0239, -0.0457,  0.0131,  ...,  0.0228,  0.0450, -0.0012],\n",
      "        ...,\n",
      "        [ 0.0314, -0.0033,  0.0069,  ...,  0.0340, -0.0287,  0.0456],\n",
      "        [ 0.0130,  0.0485, -0.0065,  ...,  0.0007, -0.0394, -0.0083],\n",
      "        [-0.0514,  0.0294, -0.0294,  ..., -0.0215,  0.0515, -0.0052]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3104, 1.7141, 1.7630, 1.7053, 2.5131, 3.0028, 2.8903, 2.6558, 2.2892,\n",
      "        1.9625, 1.9550, 2.6178, 2.1814, 2.4017, 2.8148, 2.6082, 3.4403, 2.5040,\n",
      "        1.9113, 2.3488, 2.5087, 2.6690, 3.1377, 3.6248, 1.8821, 2.0318, 2.8682,\n",
      "        2.0156, 2.3802, 2.5602, 3.1609, 3.2066, 2.4361, 2.0999, 2.8076, 2.7844,\n",
      "        2.9848, 2.3860, 3.4064, 3.9536, 1.9249, 2.1166, 2.2069, 2.5713, 2.9042,\n",
      "        2.8540, 3.3695, 3.0325, 1.3515, 1.9140, 0.9951, 1.3004, 2.2391, 2.3913,\n",
      "        2.7544, 2.6721, 3.4009, 1.9196, 2.7225, 2.7288, 2.5719, 2.4761, 3.0132,\n",
      "        2.9450, 1.9319, 2.0116, 3.3956, 2.7162, 2.4950, 3.1314, 2.7216, 3.2993,\n",
      "        2.3368, 1.2359, 1.0417, 1.5311, 1.9979, 2.4419, 3.1510, 3.0158, 2.3059,\n",
      "        2.3592, 2.9697, 2.9409, 2.8145, 3.2091, 3.3362, 3.4858, 2.3076, 2.4374,\n",
      "        2.4572, 2.6827, 2.7429, 2.6726, 3.3185, 2.9080, 2.4848, 2.3930, 3.4052,\n",
      "        3.8274, 3.7500, 2.3185, 2.3387, 3.7393, 2.6533, 3.0507, 3.8338, 3.6674,\n",
      "        2.9880, 3.8703, 3.2220, 4.1873, 1.3942, 1.2973, 2.6708, 2.4776, 2.4575,\n",
      "        2.5180, 2.7433, 2.6707, 1.8364, 2.5494, 1.9929, 1.9781, 2.2077, 2.6031,\n",
      "        2.7324, 3.1610, 1.9371, 1.9308, 2.1085, 1.8189, 2.3408, 2.9016, 2.7730,\n",
      "        2.8393, 2.0186, 1.9080, 2.4667, 3.4850, 2.5544, 2.4903, 2.8578, 2.8349,\n",
      "        3.2932, 1.9629, 2.5869, 2.1480, 2.3391, 2.2745, 3.0238, 2.8059, 1.0406,\n",
      "        1.5737, 1.8331, 2.1681, 2.2002, 2.9436, 2.9802, 3.2201, 1.9564, 2.3462,\n",
      "        2.0197, 2.2140, 2.3697, 2.9331, 2.8399, 2.8700, 3.0744, 2.0520, 2.6540,\n",
      "        2.3402, 2.6756, 2.5425, 3.0100, 3.3426, 1.4458, 1.4741, 2.4017, 1.6372,\n",
      "        1.9864, 2.7082, 2.4441, 2.7183, 1.2458, 1.5733, 1.4371, 2.3731, 2.0374,\n",
      "        2.1310, 2.5460, 2.7073, 1.2965, 1.7122, 1.7912, 1.4718, 2.0317, 1.9110,\n",
      "        2.8360, 2.6500, 2.2354, 2.0787, 2.1840, 2.6195, 2.1081, 2.2840, 2.8488,\n",
      "        2.8470, 2.6092, 2.3216, 2.3217, 3.2488, 3.4129, 1.9793, 3.0377, 3.1498,\n",
      "        1.8164, 3.1218, 3.6283, 4.3218, 3.5228, 4.1034, 3.2566, 3.7471, 1.3645,\n",
      "        1.4842, 2.5856, 1.3795, 1.7929, 2.1589, 2.4173, 3.5480, 4.4061, 2.1861,\n",
      "        1.3060, 2.3242, 2.1598, 3.0743, 2.5252, 3.0432, 2.1891, 2.4802, 2.1996,\n",
      "        3.2966, 2.0206, 2.6371, 3.0570, 3.0538, 2.2653, 1.9708, 2.0156, 1.3852,\n",
      "        2.7164, 2.5759, 2.9398, 2.9819, 3.0565, 2.1638, 2.2482, 2.0069, 2.1191,\n",
      "        2.3681, 3.3579, 3.1400, 2.5195, 2.4685, 2.2894, 3.0067, 2.7699, 2.2354,\n",
      "        3.4420, 2.4938, 1.9791, 2.4415, 2.9435, 2.3375, 1.9749, 2.5458, 3.3455,\n",
      "        3.3807, 2.0088, 2.6801, 2.9651, 3.0914, 3.1180, 2.6439, 3.1360, 3.0249,\n",
      "        2.2071, 3.0108, 2.9148, 3.2369, 2.1775, 3.2074, 3.3626, 3.4732, 2.9046,\n",
      "        3.4217, 2.8485, 2.3340, 3.2409, 2.4393, 3.8665, 3.3114, 1.4815, 2.1158,\n",
      "        2.3603, 2.0530, 2.0928, 2.4063, 2.4377, 2.8937, 1.6294, 1.4160, 1.4357,\n",
      "        2.7104, 2.0739, 2.5605, 2.6162, 2.7294], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0361, -0.0160,  0.0552,  ...,  0.0059, -0.0389, -0.0211],\n",
      "        [-0.0040,  0.0113, -0.0503,  ..., -0.0096,  0.0132, -0.0255],\n",
      "        [ 0.0144,  0.0553, -0.0437,  ...,  0.0243, -0.0218, -0.0361],\n",
      "        ...,\n",
      "        [-0.0429,  0.0323,  0.0454,  ...,  0.0381,  0.0181,  0.0164],\n",
      "        [-0.0183, -0.0095, -0.0137,  ...,  0.0317,  0.0487, -0.0148],\n",
      "        [-0.0244,  0.0043,  0.0054,  ...,  0.0474,  0.0045, -0.0123]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9312, 1.6560, 1.5025, 3.1015, 1.3156, 1.9814, 2.5277, 2.6336, 1.9142,\n",
      "        2.2558, 2.5849, 1.2664, 2.9048, 2.0867, 2.8378, 2.1356, 1.6963, 2.0704,\n",
      "        2.7106, 1.7995, 1.7215, 1.6940, 3.2480, 3.2050, 1.9310, 2.2181, 1.6052,\n",
      "        1.6832, 3.4282, 3.6349, 3.4557, 3.0808, 2.7000, 1.9037, 2.5099, 2.6531,\n",
      "        2.2798, 1.9575, 2.1395, 2.9575, 1.6684, 2.2187, 2.6083, 2.2316, 2.1844,\n",
      "        2.2781, 2.4180, 2.2915, 2.6304, 1.6635, 1.5177, 1.9025, 2.9682, 1.3307,\n",
      "        2.4196, 2.4561, 1.5203, 2.2448, 1.7559, 1.4543, 1.3035, 3.0633, 2.6558,\n",
      "        2.4679, 1.8064, 1.7375, 1.3743, 1.2425, 1.4645, 1.7114, 2.6392, 2.0872,\n",
      "        1.9829, 1.5329, 1.6450, 2.6210, 1.7122, 2.8205, 2.5080, 2.0684, 2.5156,\n",
      "        2.5847, 2.7209, 2.3664, 2.3682, 2.4310, 3.3034, 3.1730, 1.7336, 2.3239,\n",
      "        2.2372, 2.5145, 2.2317, 2.2727, 3.1602, 2.6132, 2.4380, 2.7107, 3.1735,\n",
      "        3.2669, 2.7625, 3.5837, 3.4606, 3.3799, 2.8331, 3.3233, 3.0440, 3.1949,\n",
      "        2.9180, 1.5721, 2.1803, 3.5255, 1.3090, 2.1215, 1.5921, 1.4672, 1.4230,\n",
      "        2.8573, 2.7264, 2.7338, 1.6511, 1.4987, 1.9909, 2.4212, 2.6597, 1.4796,\n",
      "        2.6403, 2.2590, 2.1686, 2.2166, 1.8278, 3.1845, 1.6452, 1.5280, 2.6079,\n",
      "        2.5026, 1.9819, 1.8816, 2.9063, 1.5530, 1.7573, 3.0625, 2.5847, 2.7725,\n",
      "        2.0413, 2.2674, 1.5518, 1.6024, 1.3709, 3.0381, 2.5933, 2.6730, 1.3697,\n",
      "        1.5330, 2.7347, 1.6535, 2.9922, 1.6024, 2.7015, 2.2714, 2.1838, 2.5030,\n",
      "        2.9637, 2.9847, 3.2738, 1.8628, 3.2288, 3.0367, 2.2855, 2.0455, 1.6372,\n",
      "        1.8146, 1.7554, 3.2882, 3.3710, 3.0813, 1.2555, 1.4622, 1.4254, 2.6514,\n",
      "        1.2953, 1.4116, 2.2405, 2.2022, 1.6476, 1.8457, 2.1349, 1.2015, 1.5303,\n",
      "        2.3462, 2.4142, 1.9270, 1.9625, 1.8636, 1.5789, 3.1025, 1.4579, 1.4719,\n",
      "        2.6630, 2.7715, 2.4022, 2.2523, 1.4769, 1.3367, 2.9694, 3.0570, 2.5579,\n",
      "        2.8308, 1.8783, 2.7321, 3.4147, 3.1242, 2.8634, 3.1580, 2.5782, 4.0256,\n",
      "        2.3529, 2.0401, 2.2517, 2.8545, 2.7627, 1.8473, 3.9106, 3.9481, 1.3837,\n",
      "        1.9616, 1.2603, 2.1160, 2.7447, 2.6057, 2.2500, 2.0770, 1.3076, 1.6387,\n",
      "        1.2627, 1.3047, 1.2148, 1.5335, 2.3153, 2.0635, 2.3206, 2.1383, 1.9802,\n",
      "        1.6081, 3.1433, 1.6794, 2.7601, 2.8216, 2.0414, 2.2026, 2.1116, 2.1251,\n",
      "        1.3875, 2.3393, 2.6504, 2.8752, 2.4258, 2.2400, 2.2036, 2.2063, 2.7984,\n",
      "        2.1927, 2.9763, 2.5214, 2.7947, 2.3749, 2.2754, 2.2125, 1.9459, 2.0575,\n",
      "        2.9073, 2.2779, 1.7675, 2.7788, 2.7039, 2.4637, 3.2157, 2.1244, 3.1141,\n",
      "        3.2243, 2.0312, 2.3673, 2.4527, 2.5355, 1.8380, 2.0462, 3.2585, 3.0566,\n",
      "        2.7938, 2.9489, 2.6235, 2.9744, 3.6921, 2.2351, 3.3414, 3.2011, 2.3180,\n",
      "        2.7270, 2.6749, 2.2782, 1.7321, 2.4280, 3.0868, 3.4698, 1.7618, 1.9611,\n",
      "        1.2558, 2.6987, 2.6617, 1.9394, 2.4054, 2.0618, 1.7352, 1.5318, 2.6446,\n",
      "        1.3029, 1.1733, 2.1066, 2.3062, 2.3852], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0410,  0.0426,  0.0473,  ..., -0.0217,  0.0022,  0.0088],\n",
      "        [ 0.0468,  0.0072, -0.0206,  ..., -0.0455,  0.0219,  0.0200],\n",
      "        [-0.0232, -0.0552,  0.0422,  ...,  0.0364, -0.0205, -0.0195],\n",
      "        ...,\n",
      "        [ 0.0297,  0.0224,  0.0407,  ...,  0.0307,  0.0472,  0.0249],\n",
      "        [-0.0328,  0.0364,  0.0499,  ..., -0.0069,  0.0544,  0.0375],\n",
      "        [ 0.0233,  0.0321, -0.0018,  ...,  0.0236, -0.0493, -0.0430]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5050, 1.3412, 1.4288, 1.4298, 1.3720, 1.4111, 1.3445, 1.3918, 1.4826,\n",
      "        1.4283, 1.4170, 1.4238, 1.4044, 1.4328, 1.4728, 1.4500, 1.2011, 1.2052,\n",
      "        1.1724, 1.2298, 1.1928, 1.1444, 1.2066, 1.1504, 1.2783, 1.1497, 1.1706,\n",
      "        1.1081, 1.1681, 1.1975, 1.1196, 1.2589, 1.8297, 1.9646, 1.9975, 1.9516,\n",
      "        2.1116, 2.2050, 2.1667, 1.9194, 2.1079, 2.0340, 1.9354, 2.1148, 2.0181,\n",
      "        1.9648, 1.9549, 2.1306, 1.3872, 1.5118, 1.3850, 1.4301, 1.5022, 1.5086,\n",
      "        1.3672, 1.5129, 1.4830, 1.4466, 1.4797, 1.4572, 1.5068, 1.5012, 1.4942,\n",
      "        1.4108, 1.2722, 1.2519, 1.2347, 1.3036, 1.3167, 1.3353, 1.2639, 1.3299,\n",
      "        1.2757, 1.2228, 1.3267, 1.2780, 1.1952, 1.2751, 1.2408, 1.1598, 2.0413,\n",
      "        2.1023, 2.0849, 1.9971, 2.1012, 2.1154, 2.1912, 2.1484, 2.2323, 2.1022,\n",
      "        2.0235, 2.1103, 2.0525, 2.1880, 2.0951, 1.9092, 1.6371, 1.7169, 1.7543,\n",
      "        1.7262, 1.7532, 1.7550, 1.7456, 1.7417, 1.6577, 1.6409, 1.7054, 1.7364,\n",
      "        1.7272, 1.6601, 1.6457, 1.7498, 1.2026, 1.2542, 1.2642, 1.2276, 1.2474,\n",
      "        1.3024, 1.2074, 1.2137, 1.3080, 1.2534, 1.2074, 1.2985, 1.2911, 1.2566,\n",
      "        1.2573, 1.2293, 1.6031, 1.4525, 1.5253, 1.5349, 1.5236, 1.4379, 1.5432,\n",
      "        1.5485, 1.5583, 1.4689, 1.5467, 1.4357, 1.5351, 1.6026, 1.5233, 1.4652,\n",
      "        1.2479, 1.2755, 1.1794, 1.2870, 1.2188, 1.2548, 1.2906, 1.2397, 1.3013,\n",
      "        1.3192, 1.1487, 1.3057, 1.2831, 1.2644, 1.2398, 1.2105, 1.1336, 1.2319,\n",
      "        1.3076, 1.1410, 1.1822, 1.1682, 1.2595, 1.1702, 1.1887, 1.1488, 1.1662,\n",
      "        1.2142, 1.2297, 1.3078, 1.1841, 1.2467, 1.4534, 1.4408, 1.4517, 1.4359,\n",
      "        1.4833, 1.4536, 1.4490, 1.4681, 1.4770, 1.3837, 1.4941, 1.4846, 1.4471,\n",
      "        1.4393, 1.4230, 1.5248, 1.5609, 1.5627, 1.6118, 1.6373, 1.5654, 1.5487,\n",
      "        1.5027, 1.5891, 1.6036, 1.6244, 1.5618, 1.6006, 1.6125, 1.5851, 1.5650,\n",
      "        1.5467, 1.4564, 1.4830, 1.5523, 1.5073, 1.4312, 1.5230, 1.5524, 1.4956,\n",
      "        1.6658, 1.5482, 1.4938, 1.4314, 1.5591, 1.4074, 1.5250, 1.5014, 1.3578,\n",
      "        1.3742, 1.3320, 1.3831, 1.3459, 1.3984, 1.4448, 1.4106, 1.3934, 1.4148,\n",
      "        1.4593, 1.3825, 1.3614, 1.3879, 1.4216, 1.3744, 1.6251, 1.6315, 1.6740,\n",
      "        1.6975, 1.7115, 1.6250, 1.6662, 1.6094, 1.6864, 1.6862, 1.7089, 1.6910,\n",
      "        1.6212, 1.7739, 1.6519, 1.7086, 1.8631, 1.8234, 1.7444, 1.7165, 1.7614,\n",
      "        1.8221, 1.7991, 1.8291, 1.8080, 1.8188, 1.8732, 1.8236, 1.8796, 1.7804,\n",
      "        1.6772, 1.7541, 1.7783, 1.7598, 1.9505, 1.8196, 1.8243, 1.7888, 1.8892,\n",
      "        1.7742, 1.7872, 1.7603, 1.8920, 1.7115, 1.7271, 1.8264, 1.8399, 1.8078,\n",
      "        1.7497, 1.7990, 1.7923, 1.8119, 1.8667, 1.7544, 1.7932, 1.8068, 1.8348,\n",
      "        1.6859, 1.7741, 1.8283, 1.7780, 1.8134, 1.7358, 1.8584, 1.4941, 1.4063,\n",
      "        1.4239, 1.3852, 1.4647, 1.3974, 1.3997, 1.3709, 1.4413, 1.3792, 1.4110,\n",
      "        1.3449, 1.3828, 1.4950, 1.4506, 1.4849], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0422, -0.0411, -0.0490,  ...,  0.0340, -0.0116, -0.0429],\n",
      "        [ 0.0173, -0.0016, -0.0015,  ...,  0.0026,  0.0056, -0.0115],\n",
      "        [ 0.0510, -0.0351, -0.0420,  ...,  0.0160,  0.0207,  0.0355],\n",
      "        ...,\n",
      "        [ 0.0158,  0.0392, -0.0134,  ..., -0.0410, -0.0037, -0.0537],\n",
      "        [ 0.0123, -0.0503,  0.0015,  ..., -0.0493, -0.0095,  0.0218],\n",
      "        [ 0.0060,  0.0482, -0.0389,  ...,  0.0029, -0.0228, -0.0004]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5182, 1.3366, 1.4452, 1.5578, 1.5781, 1.3782, 1.4307, 1.4030, 1.3956,\n",
      "        1.5392, 1.4049, 1.5424, 1.4235, 1.3929, 1.4204, 1.5161, 1.5383, 1.5262,\n",
      "        1.4092, 1.4106, 1.5467, 1.3979, 1.5477, 1.4213, 1.4434, 1.5220, 1.4763,\n",
      "        1.4391, 1.5416, 1.5211, 1.4687, 1.4914, 1.4371, 1.4126, 1.4679, 1.5788,\n",
      "        1.6083, 1.5001, 1.5510, 1.4941, 1.4378, 1.4928, 1.6759, 1.5651, 1.4231,\n",
      "        1.3777, 1.3118, 1.5162, 1.5366, 1.5503, 1.4433, 1.4289, 1.4351, 1.4808,\n",
      "        1.4637, 1.5147, 1.4136, 1.4784, 1.4784, 1.4223, 1.5072, 1.7830, 1.4340,\n",
      "        1.5779, 1.5148, 1.4621, 1.4189, 1.4414, 1.4641, 1.5716, 1.4644, 1.6185,\n",
      "        1.6025, 1.5276, 1.4267, 1.3351, 1.4597, 1.4452, 1.4230, 1.4779, 1.5124,\n",
      "        1.5607, 1.4482, 1.5363, 1.6044, 1.5273, 1.4308, 1.3625, 1.3576, 1.5975,\n",
      "        1.4584, 1.5385, 1.5445, 1.4219, 1.5000, 1.4864, 1.3054, 1.5387, 1.6205,\n",
      "        1.3920, 1.4756, 1.3705, 1.4178, 1.3223, 1.4354, 1.5449, 1.4813, 1.4093,\n",
      "        1.6309, 1.4055, 1.4253, 1.4951, 1.5546, 1.5273, 1.4382, 1.4641, 1.6035,\n",
      "        1.5470, 1.4484, 1.4687, 1.5177, 1.5826, 1.4420, 1.4778, 1.4888, 1.3776,\n",
      "        1.4175, 1.4349, 1.4670, 1.6567, 1.4621, 1.5241, 1.4666, 1.4799, 1.4409,\n",
      "        1.2788, 1.5139, 1.3347, 1.5573, 1.3917, 1.4549, 1.5198, 1.4670, 1.3974,\n",
      "        1.4225, 1.4071, 1.3826, 1.4129, 1.4907, 1.4848, 1.7317, 1.5302, 1.4641,\n",
      "        1.3462, 1.4440, 1.4942, 1.5771, 1.5341, 1.5495, 1.5005, 1.3863, 1.5109,\n",
      "        1.4870, 1.4588, 1.4852, 1.4844, 1.4773, 1.4372, 1.4059, 1.4777, 1.4668,\n",
      "        1.4655, 1.5055, 1.5339, 1.5569, 1.4343, 1.5029, 1.4768, 1.3866, 1.4469,\n",
      "        1.4716, 1.3901, 1.6421, 1.5552, 1.4477, 1.4996, 1.5427, 1.3390, 1.4022,\n",
      "        1.4134, 1.3921, 1.4768, 2.3183, 1.4221, 1.4900, 1.4289, 1.4467, 1.4291,\n",
      "        1.4613, 1.4765, 1.4677, 1.4864, 1.3952, 1.4817, 1.4247, 1.3936, 1.5728,\n",
      "        1.4473, 1.5164, 1.3501, 1.5127, 1.3906, 1.4533, 1.4714, 1.4324, 1.5176,\n",
      "        1.4693, 1.4334, 1.4705, 1.5173, 1.4735, 1.5462, 1.5126, 1.4526, 1.3725,\n",
      "        1.3420, 1.4608, 1.4537, 1.4973, 1.5339, 1.5004, 1.4706, 1.4751, 1.4055,\n",
      "        1.4360, 1.5123, 1.4806, 1.2851, 1.4492, 1.5379, 1.5761, 1.3477, 1.5074,\n",
      "        1.5081, 1.4160, 1.3632, 1.3501, 1.4541, 1.4540, 1.4935, 1.4111, 1.5290,\n",
      "        1.4934, 1.5602, 1.4195, 1.5025, 1.5771, 1.3995, 1.3335, 1.5111, 1.5102,\n",
      "        1.4395, 1.4446, 1.4974, 1.6232, 1.5785, 1.5276, 1.5547, 1.4638, 1.4861,\n",
      "        1.4445, 1.4832, 1.6828, 1.4359, 1.5108, 1.5060, 1.4832, 1.4109, 1.4376,\n",
      "        1.4550, 1.4801, 1.5254, 1.4070, 1.5407, 1.4185, 1.4569, 1.4193, 1.4493,\n",
      "        1.3051, 1.5326, 1.5453, 1.5726, 1.3734, 1.4894, 1.4973, 1.4928, 1.5814,\n",
      "        1.4894, 1.4258, 1.4995, 1.4506, 1.5399, 1.3944, 1.3439, 1.4536, 1.4659,\n",
      "        1.5821, 1.5112, 1.4598, 1.3430, 1.4768, 1.5326, 1.4429, 1.5462, 1.4058,\n",
      "        1.5654, 1.5074, 1.4295, 1.4190, 2.1234], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0239, -0.0007, -0.0305,  ...,  0.0260, -0.0239,  0.0361],\n",
      "        [ 0.0059, -0.0552,  0.0065,  ...,  0.0318,  0.0323,  0.0275],\n",
      "        [ 0.0303,  0.0017, -0.0475,  ...,  0.0385, -0.0486,  0.0363],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0554,  0.0189,  ...,  0.0199,  0.0335,  0.0431],\n",
      "        [-0.0390, -0.0531, -0.0478,  ..., -0.0509,  0.0281,  0.0494],\n",
      "        [-0.0195,  0.0172, -0.0037,  ..., -0.0527,  0.0539,  0.0144]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.2311, 1.1910, 1.2649,  ..., 1.1348, 1.8970, 3.0340], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0150,  0.0275,  0.0074,  ..., -0.0120,  0.0182,  0.0250],\n",
      "        [-0.0084, -0.0040, -0.0226,  ...,  0.0213,  0.0188,  0.0117],\n",
      "        [-0.0242, -0.0035,  0.0003,  ..., -0.0104,  0.0066, -0.0205],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0261, -0.0135,  ..., -0.0089,  0.0141,  0.0227],\n",
      "        [-0.0171, -0.0078,  0.0057,  ...,  0.0205,  0.0077,  0.0106],\n",
      "        [-0.0065, -0.0156,  0.0141,  ...,  0.0101,  0.0048, -0.0172]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.9676, 3.1091, 3.2689, 3.2900, 3.2342, 2.9883, 2.8677, 3.0428, 3.2095,\n",
      "        3.0305, 2.8528, 2.9969, 3.2439, 2.9156, 2.9493, 3.2031, 2.9203, 2.9546,\n",
      "        2.9031, 2.9368, 3.1434, 3.0678, 2.9457, 2.7971, 2.8970, 2.9207, 3.0174,\n",
      "        2.9717, 2.9061, 3.0519, 3.1955, 2.8348, 3.0893, 2.8279, 3.0561, 3.1930,\n",
      "        3.0490, 3.0140, 3.0400, 2.9917, 3.2240, 2.8658, 2.9175, 3.0790, 2.9873,\n",
      "        3.0939, 3.5258, 3.0208, 2.9768, 2.9723, 3.0941, 3.0221, 2.7844, 3.0329,\n",
      "        3.0994, 2.9659, 2.8902, 3.2147, 3.1329, 3.2659, 2.9704, 3.1234, 3.8791,\n",
      "        2.9474, 3.1038, 3.0202, 2.8716, 3.1907, 3.1476, 3.1157, 2.7840, 3.0592,\n",
      "        3.0594, 3.0037, 3.0850, 3.1107, 2.9149, 2.8688, 3.0262, 3.1772, 2.9239,\n",
      "        2.9703, 3.1032, 2.9483, 2.8390, 3.0838, 2.9909, 3.0508, 2.6012, 3.0086,\n",
      "        3.0392, 2.9030, 3.2557, 2.7629, 3.0982, 3.1274, 2.9047, 2.9094, 3.2297,\n",
      "        3.2185, 2.9742, 2.9031, 2.9815, 2.8830, 2.9266, 3.3168, 3.0423, 2.9394,\n",
      "        2.9211, 3.1658, 3.1429, 2.9654, 3.1356, 2.9917, 3.0325, 3.0532, 2.9810,\n",
      "        3.1742, 3.0561, 2.8999, 3.0136, 3.0203, 2.9212, 3.0997, 2.8827, 3.1068,\n",
      "        3.0274, 3.1395, 2.9567, 3.0354, 2.9082, 3.1015, 3.0209, 3.0527, 3.1573,\n",
      "        3.0792, 3.0003, 3.0547, 3.0900, 3.0763, 2.9769, 2.9070, 3.0880, 2.8846,\n",
      "        3.0303, 3.1234, 3.0129, 3.2725, 3.1111, 2.9968, 3.5699, 2.9382, 2.9625,\n",
      "        3.0895, 3.0811, 3.1877, 2.8088, 3.0356, 2.9381, 3.0107, 3.0650, 3.1529,\n",
      "        2.8059, 3.1452, 3.1255, 3.1120, 3.1684, 3.2176, 3.0419, 2.9287, 3.0710,\n",
      "        2.8077, 3.0397, 2.9546, 3.0535, 3.1689, 3.0756, 3.1549, 3.1413, 2.9242,\n",
      "        2.9784, 2.8635, 2.9383, 2.8278, 3.1445, 3.1041, 3.1945, 3.2077, 3.1428,\n",
      "        3.0674, 3.1539, 2.9950, 5.2807, 2.9450, 3.0943, 3.0977, 3.2497, 2.9892,\n",
      "        3.2182, 2.9953, 3.1558, 2.9074, 3.0738, 3.0064, 2.9811, 3.0604, 3.2391,\n",
      "        2.9630, 3.1438, 2.9741, 2.8683, 3.0241, 2.8392, 3.0562, 3.2009, 3.1125,\n",
      "        3.0253, 2.9186, 2.9083, 2.8912, 2.9779, 3.2884, 3.0810, 3.1916, 2.9462,\n",
      "        3.1927, 3.3085, 3.0104, 3.0118, 3.1244, 2.9147, 2.9083, 3.1547, 2.7859,\n",
      "        3.0636, 2.9682, 3.1581, 2.8804, 2.9523, 3.2394, 3.0698, 2.8730, 2.8884,\n",
      "        2.9047, 2.8931, 2.9375, 3.0611, 2.9584, 3.1209, 3.2135, 2.8814, 3.1067,\n",
      "        2.9470, 2.9455, 3.0483, 2.9228, 3.0291, 2.5509, 2.4521, 3.1228, 2.9969,\n",
      "        2.7896, 2.8780, 2.9090, 2.9872, 3.0819, 2.9944, 3.2323, 2.9363, 2.9183,\n",
      "        2.9071, 2.9882, 3.0925, 3.0159, 2.9690, 3.2182, 3.1341, 3.0226, 3.0195,\n",
      "        2.9175, 3.1193, 3.2404, 3.1444, 3.1954, 3.0076, 3.0884, 2.8119, 3.1150,\n",
      "        2.6310, 3.2634, 3.0590, 2.8496, 3.0474, 3.0430, 2.6883, 3.0586, 2.9579,\n",
      "        3.0348, 2.8233, 3.2324, 2.9160, 3.0493, 3.0838, 2.9203, 3.2560, 3.0286,\n",
      "        2.8774, 3.0848, 3.1279, 3.2239, 3.0601, 3.1812, 2.9693, 3.0565, 3.1453,\n",
      "        2.9989, 3.0124, 2.9586, 3.0326, 3.7541], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0412,  0.0525,  0.0045,  ..., -0.0557, -0.0447,  0.0136],\n",
      "        [-0.0066,  0.0226, -0.0206,  ...,  0.0243, -0.0373,  0.0109],\n",
      "        [-0.0491, -0.0270, -0.0141,  ...,  0.0499,  0.0374, -0.0061],\n",
      "        ...,\n",
      "        [ 0.0025, -0.0370, -0.0175,  ...,  0.0233,  0.0204, -0.0265],\n",
      "        [ 0.0016, -0.0286, -0.0365,  ..., -0.0390, -0.0174,  0.0059],\n",
      "        [-0.0135, -0.0552,  0.0187,  ...,  0.0161, -0.0249,  0.0274]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6364, 2.6390, 2.0944, 1.2652, 2.4686, 4.0931, 4.0519, 4.1087, 1.6234,\n",
      "        1.7464, 2.3974, 2.2292, 1.8248, 3.9704, 3.8726, 3.9401, 1.6910, 2.0033,\n",
      "        2.5521, 2.4998, 2.4315, 2.8289, 2.4898, 2.9163, 2.1409, 2.0417, 1.9754,\n",
      "        1.9498, 1.9808, 2.1200, 2.6522, 3.0651, 2.1056, 1.9643, 1.9586, 1.3718,\n",
      "        1.9455, 2.9884, 2.5272, 2.5670, 2.3139, 1.6175, 1.7928, 2.2782, 2.6153,\n",
      "        1.9655, 2.4290, 2.3931, 1.2619, 1.3964, 2.0686, 1.9218, 1.6280, 2.4835,\n",
      "        2.3906, 2.6732, 1.7436, 2.3112, 1.9873, 1.8901, 1.9661, 1.9705, 2.2395,\n",
      "        2.6618, 2.1720, 2.0727, 1.7508, 2.0072, 2.1007, 2.3100, 2.7728, 3.8755,\n",
      "        2.4129, 2.2285, 2.1135, 1.9256, 1.8053, 2.3419, 2.9929, 2.7835, 1.9217,\n",
      "        2.1208, 2.0780, 1.9929, 2.0803, 2.3699, 2.5774, 2.0393, 1.8339, 1.7092,\n",
      "        2.2835, 2.1641, 2.0069, 2.0672, 2.6587, 2.8190, 2.7390, 2.0057, 2.3089,\n",
      "        2.3541, 2.1238, 2.4661, 2.7346, 2.7648, 1.6161, 2.2131, 1.8029, 2.2882,\n",
      "        2.6033, 3.3191, 2.8231, 3.1153, 3.1539, 1.7625, 2.4417, 2.0257, 2.1885,\n",
      "        2.8687, 2.4908, 2.5448, 1.7583, 2.2836, 1.6609, 2.4423, 2.7646, 2.4037,\n",
      "        2.8301, 2.8687, 1.2769, 1.7879, 1.7641, 2.0596, 2.2267, 2.5727, 2.2271,\n",
      "        2.8411, 2.3698, 1.7364, 2.2663, 1.7427, 1.9744, 2.1931, 2.5126, 3.2081,\n",
      "        2.2521, 1.5078, 1.7417, 2.3257, 2.7109, 2.5379, 2.6063, 2.6232, 1.9164,\n",
      "        1.4639, 1.8369, 1.7459, 2.0580, 2.0620, 2.5262, 2.6260, 1.8816, 2.0346,\n",
      "        2.1639, 2.2454, 2.0329, 1.7281, 2.6057, 2.8484, 1.8403, 2.0948, 2.4362,\n",
      "        2.0903, 2.6062, 2.4560, 2.6719, 1.9641, 2.0986, 2.4448, 1.9692, 2.6073,\n",
      "        3.3678, 3.4097, 2.8969, 3.0830, 2.4151, 2.1252, 2.4094, 2.2619, 2.2182,\n",
      "        2.7284, 2.7975, 3.0168, 1.9455, 1.6708, 1.9965, 1.6050, 2.2160, 3.1495,\n",
      "        2.7658, 2.9006, 2.7101, 2.2568, 2.1394, 2.7212, 2.8581, 2.3452, 2.5273,\n",
      "        2.6438, 2.3477, 2.1827, 2.2393, 2.1477, 1.9436, 2.6527, 2.8940, 2.7421,\n",
      "        2.0614, 2.1811, 2.0123, 2.2799, 2.3622, 2.6914, 2.8982, 2.9276, 1.2834,\n",
      "        1.4906, 2.3089, 1.6049, 2.3016, 1.7326, 2.2454, 2.2151, 1.6348, 1.6884,\n",
      "        1.3607, 2.0792, 1.6069, 2.3638, 2.1205, 2.3262, 2.1855, 1.8477, 1.8446,\n",
      "        1.8586, 1.7805, 1.8662, 2.4126, 2.5826, 1.8942, 1.8062, 2.0818, 2.4619,\n",
      "        2.1452, 2.2530, 2.4484, 2.3065, 1.9321, 2.2932, 2.7363, 2.3035, 2.1176,\n",
      "        2.7247, 2.8584, 3.2607, 2.6077, 2.0601, 1.9174, 2.2119, 2.5767, 3.0825,\n",
      "        3.1141, 3.5762, 2.1825, 2.1459, 2.1885, 2.0560, 1.8502, 2.2381, 3.7271,\n",
      "        3.8786, 2.1951, 2.2558, 2.6577, 2.2646, 2.0682, 2.0150, 3.6247, 3.7154,\n",
      "        2.8938, 1.8903, 1.6515, 2.2105, 2.3003, 2.3973, 2.9187, 2.8900, 1.7587,\n",
      "        1.7403, 2.1632, 1.9215, 1.9667, 2.7426, 2.7604, 3.0110, 2.0377, 1.9718,\n",
      "        2.2795, 2.8472, 2.3816, 2.3218, 3.2355, 2.8049, 2.2205, 2.0097, 2.1191,\n",
      "        1.9181, 2.8458, 2.7819, 2.7914, 2.3329], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0096,  0.0169, -0.0558,  ...,  0.0225, -0.0460, -0.0460],\n",
      "        [ 0.0501, -0.0004,  0.0516,  ...,  0.0478, -0.0125, -0.0476],\n",
      "        [ 0.0552, -0.0278, -0.0555,  ...,  0.0302, -0.0302, -0.0153],\n",
      "        ...,\n",
      "        [-0.0227, -0.0290,  0.0290,  ..., -0.0156,  0.0103, -0.0215],\n",
      "        [ 0.0144, -0.0177,  0.0423,  ..., -0.0189,  0.0322,  0.0247],\n",
      "        [ 0.0231,  0.0273, -0.0113,  ...,  0.0074, -0.0069,  0.0168]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0765, 2.5408, 2.3526, 1.9824, 2.0977, 2.9838, 3.1092, 3.2165, 2.0370,\n",
      "        1.9273, 2.1878, 1.7016, 1.8061, 2.9055, 3.0309, 3.1815, 1.7630, 1.6878,\n",
      "        1.7816, 1.4597, 1.4368, 1.5555, 2.4318, 2.1590, 1.8673, 2.3699, 2.1836,\n",
      "        2.7774, 2.7201, 2.8083, 2.1936, 2.1107, 2.2372, 1.7439, 2.1271, 2.4231,\n",
      "        1.8976, 1.4587, 2.4317, 1.8740, 2.1105, 1.8819, 1.8455, 1.5286, 1.6699,\n",
      "        2.5525, 2.3122, 2.1250, 1.2378, 2.1860, 1.9858, 1.3942, 2.5593, 1.4662,\n",
      "        2.2744, 1.9708, 1.5138, 1.3570, 1.5383, 2.1842, 1.3004, 2.4476, 2.0740,\n",
      "        1.9916, 2.0103, 2.1097, 1.9125, 2.1963, 1.8097, 1.9272, 2.2464, 2.1612,\n",
      "        2.2586, 2.2298, 1.9800, 1.7482, 1.9602, 1.9611, 2.3406, 1.8746, 1.7990,\n",
      "        2.1364, 2.4526, 2.0383, 2.0060, 1.7538, 2.1470, 2.1273, 1.8844, 1.5257,\n",
      "        1.9340, 2.0358, 1.9429, 2.3786, 2.4349, 2.5060, 1.8652, 1.8629, 1.9705,\n",
      "        2.8884, 2.9914, 2.9530, 2.6767, 2.6803, 2.3777, 2.2590, 2.0348, 1.7970,\n",
      "        1.5435, 1.6642, 2.5901, 2.1689, 2.0565, 1.8697, 2.2341, 2.8094, 3.2886,\n",
      "        1.6145, 2.2524, 2.4820, 2.5120, 2.0733, 1.6069, 1.5789, 1.3825, 2.7499,\n",
      "        2.3293, 2.1207, 1.8518, 1.7886, 2.2369, 1.5007, 1.6208, 1.8312, 2.2347,\n",
      "        2.1320, 1.2281, 1.7165, 1.7379, 2.0056, 1.9773, 2.1719, 2.3638, 2.1369,\n",
      "        2.1738, 1.8475, 2.3084, 1.4090, 1.3525, 1.6167, 2.2576, 2.0339, 1.8903,\n",
      "        1.5805, 1.6164, 2.4363, 2.7430, 2.2113, 2.3560, 1.9520, 1.9774, 2.0554,\n",
      "        2.4268, 2.0452, 2.3000, 2.2972, 2.1652, 2.5555, 1.5367, 2.1512, 2.2499,\n",
      "        2.3050, 2.0007, 2.1711, 2.4442, 2.1673, 2.2846, 2.1165, 1.9002, 1.9196,\n",
      "        1.6349, 1.5919, 2.8377, 2.7244, 2.2149, 2.3595, 2.2693, 2.4992, 2.3285,\n",
      "        2.6272, 2.7037, 2.6743, 2.1327, 1.7734, 2.0147, 2.1846, 2.4195, 1.5452,\n",
      "        2.1489, 2.1214, 1.9037, 2.0490, 1.9586, 1.3811, 1.3554, 2.1178, 2.5161,\n",
      "        2.3703, 2.0669, 2.2160, 2.1187, 2.3077, 2.0318, 1.9623, 2.6676, 2.7385,\n",
      "        2.1651, 2.1472, 2.0449, 1.9658, 1.7615, 1.9185, 2.6848, 2.7488, 1.4351,\n",
      "        1.7839, 1.3499, 2.3567, 1.2237, 2.2845, 2.1018, 2.0661, 1.5861, 1.5432,\n",
      "        2.4486, 1.3138, 2.4077, 1.3087, 1.9775, 1.8867, 1.9588, 1.8228, 1.7777,\n",
      "        2.1213, 2.4333, 2.5496, 2.2527, 2.1380, 2.0958, 1.7880, 2.0004, 1.5471,\n",
      "        1.3875, 1.4362, 2.3632, 1.9479, 1.9572, 1.9936, 1.7768, 1.7956, 2.8338,\n",
      "        2.3627, 2.6741, 2.1537, 2.2110, 2.0818, 2.3969, 2.4299, 1.5893, 2.2398,\n",
      "        2.6306, 2.1893, 2.0799, 2.1423, 2.0194, 2.1198, 1.9263, 2.2288, 3.0496,\n",
      "        3.0728, 2.3141, 2.3782, 2.5201, 2.2066, 2.0528, 2.0087, 3.0066, 3.0127,\n",
      "        2.0551, 1.8981, 2.4359, 1.7355, 1.3715, 2.3960, 2.3254, 2.2504, 2.1068,\n",
      "        1.7720, 1.5561, 1.6461, 2.5138, 1.6675, 2.2104, 2.2546, 2.2672, 1.8749,\n",
      "        2.1885, 2.2695, 1.9406, 2.3520, 2.3238, 2.7735, 1.7960, 2.0847, 2.1190,\n",
      "        2.2766, 2.3209, 2.2903, 2.8318, 2.3242], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0107,  0.0325, -0.0467,  ..., -0.0396,  0.0425,  0.0316],\n",
      "        [-0.0290, -0.0340, -0.0066,  ..., -0.0440,  0.0298, -0.0366],\n",
      "        [ 0.0280, -0.0469, -0.0247,  ...,  0.0156,  0.0292,  0.0047],\n",
      "        ...,\n",
      "        [-0.0365, -0.0227,  0.0554,  ..., -0.0051,  0.0196, -0.0256],\n",
      "        [-0.0380,  0.0306, -0.0346,  ...,  0.0050,  0.0207, -0.0388],\n",
      "        [-0.0134,  0.0133,  0.0008,  ..., -0.0542, -0.0063, -0.0215]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5330, 1.6486, 1.5922, 1.5471, 1.6414, 1.6285, 1.5005, 1.5494, 1.5380,\n",
      "        1.5717, 1.5119, 1.5630, 1.5218, 1.5110, 1.5679, 1.5317, 1.1657, 1.2331,\n",
      "        1.2078, 1.1145, 1.2381, 1.1856, 1.2431, 1.1825, 1.2421, 1.1928, 1.2126,\n",
      "        1.1719, 1.2074, 1.1736, 1.1586, 1.2219, 1.6539, 1.4847, 1.5970, 1.6200,\n",
      "        1.5895, 1.5523, 1.6636, 1.6017, 1.5956, 1.5636, 1.6135, 1.6454, 1.6159,\n",
      "        1.6430, 1.6304, 1.6089, 1.2744, 1.1541, 1.2596, 1.2225, 1.1737, 1.2295,\n",
      "        1.2202, 1.2448, 1.2646, 1.2936, 1.1856, 1.2450, 1.2155, 1.2196, 1.2606,\n",
      "        1.2209, 1.8218, 1.8238, 1.8211, 1.7579, 1.8900, 1.7854, 1.8361, 1.9394,\n",
      "        1.8692, 1.9210, 1.8353, 1.9726, 1.8625, 1.9522, 1.8599, 1.9588, 1.8154,\n",
      "        1.7944, 1.8507, 1.7514, 1.7393, 1.8898, 1.7577, 1.7423, 1.8539, 1.6910,\n",
      "        1.7789, 1.8181, 1.7744, 1.8570, 1.8248, 1.8378, 1.6608, 1.7902, 1.6306,\n",
      "        1.6954, 1.7746, 1.6736, 1.6967, 1.7074, 1.7342, 1.6533, 1.6523, 1.6665,\n",
      "        1.6600, 1.7225, 1.6367, 1.6528, 1.6156, 1.5541, 1.6002, 1.5512, 1.5579,\n",
      "        1.5647, 1.6463, 1.5876, 1.6151, 1.5820, 1.6169, 1.5098, 1.6251, 1.5739,\n",
      "        1.5650, 1.6458, 1.2277, 1.1833, 1.2362, 1.2260, 1.2180, 1.1820, 1.1718,\n",
      "        1.1631, 1.2122, 1.1508, 1.1650, 1.1787, 1.2059, 1.2007, 1.1561, 1.1976,\n",
      "        1.5959, 1.6176, 1.5903, 1.5971, 1.5287, 1.5951, 1.5967, 1.5436, 1.5887,\n",
      "        1.6103, 1.6295, 1.6258, 1.5870, 1.6397, 1.6187, 1.6538, 1.7573, 1.8354,\n",
      "        1.7497, 1.8513, 1.8189, 1.7780, 1.7666, 1.8372, 1.7040, 1.8116, 1.6934,\n",
      "        1.7929, 1.7143, 1.7952, 1.6900, 1.7681, 1.7362, 1.7480, 1.6537, 1.6301,\n",
      "        1.7768, 1.7154, 1.7670, 1.8288, 1.7272, 1.7657, 1.6931, 1.6219, 1.7223,\n",
      "        1.6838, 1.7425, 1.6546, 1.5216, 1.5675, 1.4576, 1.5160, 1.5203, 1.5492,\n",
      "        1.5441, 1.5528, 1.5505, 1.5682, 1.6630, 1.5788, 1.6164, 1.5891, 1.5107,\n",
      "        1.5092, 1.7979, 1.8259, 1.7397, 1.7698, 1.8091, 1.9034, 1.7028, 1.7232,\n",
      "        1.7026, 1.8237, 1.8537, 1.7282, 1.7720, 1.7641, 1.8126, 1.8067, 1.3516,\n",
      "        1.3482, 1.2830, 1.3655, 1.3467, 1.4541, 1.3583, 1.3757, 1.4560, 1.4979,\n",
      "        1.3811, 1.3514, 1.3880, 1.3153, 1.3510, 1.3761, 1.6688, 1.6522, 1.6907,\n",
      "        1.6339, 1.5694, 1.6467, 1.6361, 1.6905, 1.6138, 1.6526, 1.6783, 1.6346,\n",
      "        1.7069, 1.6522, 1.7019, 1.6671, 1.2600, 1.2771, 1.2925, 1.2302, 1.2447,\n",
      "        1.2038, 1.2473, 1.2056, 1.2622, 1.2346, 1.2598, 1.1633, 1.2800, 1.2607,\n",
      "        1.2687, 1.2165, 1.8148, 1.6619, 1.6820, 1.6820, 1.6908, 1.6574, 1.6226,\n",
      "        1.8360, 1.5560, 1.6419, 1.7166, 1.7731, 1.7373, 1.6076, 1.7456, 1.7763,\n",
      "        1.3124, 1.3052, 1.2614, 1.2769, 1.2746, 1.3116, 1.3453, 1.2180, 1.2679,\n",
      "        1.3330, 1.2758, 1.3187, 1.2894, 1.3063, 1.2438, 1.3233, 1.4716, 1.5394,\n",
      "        1.5679, 1.4053, 1.5583, 1.5187, 1.4287, 1.5063, 1.5263, 1.4699, 1.5011,\n",
      "        1.4379, 1.4383, 1.5312, 1.4596, 1.6494], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0053, -0.0411, -0.0412,  ..., -0.0429,  0.0247, -0.0085],\n",
      "        [-0.0407,  0.0019,  0.0191,  ..., -0.0012, -0.0298,  0.0052],\n",
      "        [-0.0085, -0.0439, -0.0022,  ...,  0.0383, -0.0461, -0.0308],\n",
      "        ...,\n",
      "        [ 0.0091, -0.0471,  0.0146,  ..., -0.0100, -0.0285,  0.0143],\n",
      "        [-0.0176, -0.0310, -0.0380,  ..., -0.0420, -0.0478, -0.0224],\n",
      "        [-0.0238,  0.0442,  0.0485,  ...,  0.0356,  0.0286,  0.0425]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4822, 1.5958, 1.5226, 1.4844, 1.6312, 1.4061, 1.4287, 1.4948, 1.4405,\n",
      "        1.4338, 1.4328, 1.5247, 1.5395, 1.4083, 1.6657, 1.4291, 1.3430, 1.5321,\n",
      "        1.4450, 1.4708, 1.5434, 1.4091, 1.5879, 1.4397, 1.4820, 1.4877, 1.4753,\n",
      "        1.5286, 1.5151, 1.4916, 1.4469, 1.4372, 1.6063, 1.4844, 1.4224, 1.4422,\n",
      "        1.6234, 1.4236, 1.3931, 1.4732, 1.6349, 1.5232, 1.4745, 1.4977, 1.4154,\n",
      "        1.3963, 1.3905, 1.6176, 1.4972, 1.5041, 1.4620, 1.5044, 1.3152, 1.3573,\n",
      "        1.3813, 1.3352, 1.3392, 1.4417, 1.6144, 1.5470, 1.3889, 1.6838, 2.7645,\n",
      "        1.4663, 1.4679, 1.4835, 1.4246, 1.5512, 1.5106, 1.5435, 1.3332, 1.4409,\n",
      "        1.4046, 1.4235, 1.4994, 1.5649, 1.4313, 1.5218, 1.4660, 1.4991, 1.4691,\n",
      "        1.5554, 1.4913, 1.5766, 1.4099, 1.4593, 1.4441, 1.5465, 1.3061, 1.5114,\n",
      "        1.4293, 1.5289, 1.5909, 1.5140, 1.4307, 1.5709, 1.4918, 1.5674, 1.5494,\n",
      "        1.5344, 1.5788, 1.5546, 1.6100, 1.4552, 1.5416, 1.6217, 1.4440, 1.4080,\n",
      "        1.4208, 1.4302, 1.5730, 1.3582, 1.4527, 1.4370, 1.5436, 1.4907, 1.5845,\n",
      "        1.4625, 1.4736, 1.4255, 1.5150, 1.5781, 1.5514, 1.5432, 1.3402, 1.5780,\n",
      "        1.5116, 1.5330, 1.4693, 1.4833, 1.3283, 1.3924, 1.5130, 1.5728, 1.5464,\n",
      "        1.3942, 1.4479, 1.5043, 1.3502, 1.3853, 1.4330, 1.4035, 1.4749, 1.4128,\n",
      "        1.3964, 1.4956, 1.4774, 1.5710, 1.4716, 1.5430, 2.1351, 1.5707, 1.5643,\n",
      "        1.5058, 1.4899, 1.5119, 1.4562, 1.5855, 1.5581, 1.4801, 1.3473, 1.4627,\n",
      "        1.5705, 1.5802, 1.6325, 1.5482, 1.3855, 1.4475, 1.3946, 1.4991, 1.4308,\n",
      "        1.6283, 1.5180, 1.4553, 1.4833, 1.3210, 1.5071, 1.5688, 1.4502, 1.5450,\n",
      "        1.4347, 1.3942, 1.4395, 1.4998, 1.5475, 1.4305, 1.4495, 1.4558, 1.4937,\n",
      "        1.6229, 1.3519, 1.3493, 2.1439, 1.3159, 1.4705, 1.4760, 1.5008, 1.4229,\n",
      "        1.5051, 1.5507, 1.5921, 1.3069, 1.4143, 1.3600, 1.3771, 1.5240, 1.5124,\n",
      "        1.3443, 1.5106, 1.6412, 1.5381, 1.5013, 1.4412, 1.6358, 1.4699, 1.5243,\n",
      "        1.5473, 1.5135, 1.3606, 1.5644, 1.4330, 1.5808, 1.5253, 1.4473, 1.4750,\n",
      "        1.4442, 1.3796, 1.4843, 1.4243, 1.4672, 1.3324, 1.3869, 1.3633, 1.4686,\n",
      "        1.4695, 1.7236, 1.4538, 1.3484, 1.4734, 1.6222, 1.5228, 1.4438, 1.3212,\n",
      "        1.4092, 1.3569, 1.5233, 1.4217, 1.5196, 1.4740, 1.4130, 1.5175, 1.4222,\n",
      "        1.4796, 1.4115, 1.4425, 1.4490, 1.3794, 1.4583, 1.2202, 1.4952, 1.4576,\n",
      "        1.4899, 1.6244, 1.5007, 1.4017, 1.4707, 1.5477, 1.6794, 1.3866, 1.5920,\n",
      "        1.3159, 1.5489, 1.2813, 1.6257, 1.4487, 1.5893, 1.5283, 1.3507, 1.5920,\n",
      "        1.4814, 1.4727, 1.4570, 1.5876, 1.6081, 1.5529, 1.4029, 1.4153, 1.4633,\n",
      "        1.2542, 1.3400, 1.5409, 1.5385, 1.5073, 1.5081, 1.2876, 1.4573, 1.5935,\n",
      "        1.5774, 1.4604, 1.5127, 1.5044, 1.5618, 1.4733, 1.3449, 1.4596, 1.4511,\n",
      "        1.4489, 1.5359, 1.4150, 1.4504, 1.4693, 1.3479, 1.5628, 1.5491, 1.5894,\n",
      "        1.5016, 1.5856, 1.5577, 1.3735, 1.6977], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0224, -0.0014, -0.0493,  ..., -0.0185, -0.0481, -0.0483],\n",
      "        [-0.0448,  0.0129, -0.0300,  ..., -0.0239, -0.0481, -0.0149],\n",
      "        [ 0.0556, -0.0044,  0.0247,  ...,  0.0218, -0.0498, -0.0550],\n",
      "        ...,\n",
      "        [ 0.0051,  0.0088, -0.0100,  ...,  0.0274,  0.0176, -0.0221],\n",
      "        [ 0.0269, -0.0059, -0.0148,  ..., -0.0016,  0.0437, -0.0214],\n",
      "        [ 0.0514,  0.0502,  0.0314,  ...,  0.0549,  0.0071,  0.0273]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3860, 4.9371, 1.3569,  ..., 5.0185, 0.9925, 1.6615], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0195, -0.0269,  0.0058,  ..., -0.0023, -0.0102,  0.0257],\n",
      "        [ 0.0201,  0.0127,  0.0237,  ..., -0.0059,  0.0216,  0.0243],\n",
      "        [ 0.0135, -0.0101,  0.0028,  ..., -0.0123, -0.0178, -0.0003],\n",
      "        ...,\n",
      "        [ 0.0006,  0.0005, -0.0265,  ..., -0.0178, -0.0214, -0.0108],\n",
      "        [ 0.0229, -0.0124,  0.0147,  ..., -0.0230, -0.0180,  0.0167],\n",
      "        [-0.0158,  0.0086,  0.0090,  ..., -0.0230,  0.0258, -0.0138]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([3.8156, 3.7318, 3.7658, 3.6703, 3.8009, 3.5653, 3.6010, 3.6835, 3.7011,\n",
      "        3.7597, 3.6229, 3.5804, 3.6766, 3.6809, 3.7006, 3.8606, 3.7022, 3.7562,\n",
      "        3.6232, 3.6355, 3.6814, 3.7043, 3.6485, 3.7166, 3.5826, 3.8381, 3.4540,\n",
      "        3.5951, 3.9736, 3.5796, 3.7590, 4.7860, 3.7635, 3.5586, 3.6514, 3.8335,\n",
      "        3.7422, 3.7442, 3.3981, 3.7595, 3.8106, 3.8870, 3.8664, 3.4265, 3.7199,\n",
      "        3.7573, 3.9653, 3.6232, 3.6368, 3.7607, 4.0718, 3.7576, 3.4805, 3.7186,\n",
      "        4.0232, 3.5450, 3.8160, 3.8713, 3.8223, 3.6687, 3.3962, 3.8698, 8.6929,\n",
      "        3.7126, 3.6971, 3.7281, 3.6506, 3.8314, 3.7726, 3.8047, 3.6459, 3.7255,\n",
      "        3.3615, 3.7357, 3.7446, 3.5651, 3.5431, 3.6640, 3.6911, 3.6391, 3.6241,\n",
      "        3.7805, 3.6800, 3.7794, 3.7226, 3.9689, 3.5967, 3.7730, 3.2532, 3.5793,\n",
      "        3.6864, 3.7222, 4.1310, 3.5837, 3.8732, 3.7198, 3.7713, 3.6102, 3.8832,\n",
      "        3.9915, 3.8324, 3.7562, 3.6180, 3.5487, 3.6614, 3.8117, 3.6019, 3.8240,\n",
      "        3.7450, 3.6002, 3.8262, 3.5692, 3.7211, 3.6851, 3.8626, 4.0331, 3.9082,\n",
      "        3.6322, 3.8705, 3.7501, 3.6760, 3.5611, 3.8132, 3.7414, 3.7063, 3.7802,\n",
      "        3.7516, 3.9408, 3.7700, 3.7049, 3.3526, 3.7046, 3.7445, 4.0554, 3.7304,\n",
      "        3.6878, 3.5472, 3.7716, 3.5512, 3.5589, 3.5623, 3.6487, 3.6171, 3.6331,\n",
      "        3.7159, 3.4257, 3.7230, 3.7029, 3.6837, 3.7618, 4.0004, 3.5157, 3.7529,\n",
      "        3.7283, 3.7842, 3.6840, 3.5267, 3.6064, 3.5896, 3.9064, 3.4998, 3.6318,\n",
      "        3.8813, 3.7241, 3.8851, 4.0860, 3.6650, 3.8731, 3.7233, 3.8326, 3.7833,\n",
      "        3.4829, 3.6700, 3.7113, 3.6111, 3.6105, 3.6923, 3.7809, 3.8320, 3.5668,\n",
      "        3.8279, 3.7983, 3.6095, 3.6807, 3.9741, 3.7624, 3.6958, 3.7794, 3.7788,\n",
      "        3.6570, 3.5454, 3.6818, 7.5607, 3.6316, 3.6122, 3.5982, 3.7604, 3.7444,\n",
      "        3.7824, 3.9929, 3.6337, 3.5872, 3.7030, 3.5855, 3.7937, 3.5464, 3.7475,\n",
      "        3.3010, 3.6591, 3.6402, 3.6448, 3.5768, 3.7341, 3.6213, 3.6477, 3.6528,\n",
      "        3.5338, 3.7142, 3.4452, 3.6674, 3.8344, 3.4752, 3.8515, 3.7407, 3.7213,\n",
      "        3.6994, 3.7497, 3.6515, 3.5449, 3.8483, 3.7427, 3.8832, 3.7963, 3.7758,\n",
      "        3.7461, 3.9131, 3.4659, 3.6581, 3.7751, 3.7891, 3.6718, 3.6198, 3.6484,\n",
      "        3.6366, 3.6643, 3.6273, 3.7269, 3.6746, 3.7571, 3.8172, 3.6774, 3.5943,\n",
      "        3.8332, 3.3588, 3.7145, 3.7697, 3.7280, 3.1592, 3.4920, 3.7232, 3.6888,\n",
      "        3.6270, 3.7368, 3.9297, 4.0058, 3.4715, 3.9209, 4.2855, 3.7622, 3.6616,\n",
      "        3.6321, 3.8954, 3.7500, 3.7283, 3.6715, 3.6658, 3.8111, 3.6885, 3.6900,\n",
      "        3.6297, 3.5067, 3.7346, 3.7238, 3.7168, 3.9086, 3.7562, 3.6053, 3.8580,\n",
      "        3.3979, 3.8299, 3.8378, 3.6149, 3.7642, 3.7663, 3.5602, 3.6357, 3.5749,\n",
      "        3.7023, 3.5727, 3.7296, 3.6933, 3.6125, 3.8976, 3.6522, 3.7185, 3.8771,\n",
      "        3.6126, 3.6230, 3.7499, 3.7113, 3.6383, 3.6189, 3.7692, 3.5026, 3.8417,\n",
      "        3.5186, 3.8992, 3.7765, 3.6193, 3.8478], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0153, -0.0041,  0.0525,  ...,  0.0247,  0.0263,  0.0096],\n",
      "        [-0.0250,  0.0255,  0.0318,  ...,  0.0540, -0.0176, -0.0531],\n",
      "        [ 0.0382,  0.0182, -0.0486,  ..., -0.0197,  0.0418,  0.0376],\n",
      "        ...,\n",
      "        [-0.0463, -0.0372,  0.0260,  ..., -0.0391, -0.0039, -0.0002],\n",
      "        [-0.0092, -0.0035,  0.0347,  ..., -0.0459, -0.0145, -0.0426],\n",
      "        [-0.0188, -0.0470,  0.0533,  ...,  0.0009, -0.0515, -0.0549]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6810, 1.4546, 1.4416, 1.4017, 1.4289, 2.3834, 2.0811, 2.5731, 1.8156,\n",
      "        1.4560, 1.3636, 1.3742, 1.4964, 1.2853, 2.6002, 1.9869, 1.0954, 1.1288,\n",
      "        1.4620, 1.9583, 2.4079, 4.6500, 4.2633, 3.4266, 1.0139, 1.2243, 1.2734,\n",
      "        2.0229, 2.6234, 4.4774, 4.1505, 3.6105, 1.4930, 1.5214, 1.7788, 1.8114,\n",
      "        1.8868, 1.5464, 2.2280, 1.8814, 1.5041, 1.5614, 2.1873, 1.8147, 1.8577,\n",
      "        1.7299, 2.2170, 2.2791, 1.6256, 1.9182, 1.9487, 1.8068, 2.1088, 1.7528,\n",
      "        2.3088, 2.3006, 1.5356, 1.9443, 1.8348, 1.9581, 2.2457, 2.5306, 2.5654,\n",
      "        2.3424, 2.1369, 1.7621, 2.3613, 2.2607, 2.2633, 2.1485, 2.7339, 2.4163,\n",
      "        1.4708, 1.8974, 2.1504, 2.0231, 2.0088, 2.5092, 2.6878, 2.4050, 2.2347,\n",
      "        1.8587, 1.4279, 1.7478, 1.6198, 2.0976, 2.3000, 2.2294, 2.1521, 1.8505,\n",
      "        1.7075, 1.6126, 2.4350, 2.4438, 2.0853, 2.2611, 1.2891, 1.4413, 1.3987,\n",
      "        1.4153, 1.4189, 2.3290, 1.9849, 2.6058, 1.4015, 1.4950, 1.3950, 1.2888,\n",
      "        1.4158, 1.4396, 2.0575, 2.1927, 2.0347, 1.6263, 1.5155, 1.5715, 1.2400,\n",
      "        2.0329, 2.2012, 2.1487, 1.9370, 1.3377, 1.3806, 1.5595, 2.0373, 1.5844,\n",
      "        1.9624, 2.5229, 1.8478, 1.3962, 1.5035, 1.4000, 1.4333, 2.0731, 1.9440,\n",
      "        2.5190, 1.4940, 1.6723, 1.6141, 1.4135, 1.9499, 1.6721, 2.0549, 2.4818,\n",
      "        1.7446, 1.6247, 2.1155, 1.9661, 2.1978, 1.8785, 2.3858, 2.1813, 2.4151,\n",
      "        1.9494, 1.7446, 2.2358, 1.7387, 2.4922, 2.5267, 2.6897, 1.9578, 1.7424,\n",
      "        2.0556, 2.0041, 2.4343, 2.6063, 2.9768, 2.7918, 1.6644, 1.9173, 1.9822,\n",
      "        2.2251, 2.0906, 2.5236, 2.7814, 2.8176, 2.0266, 1.5878, 1.7567, 2.0874,\n",
      "        1.7204, 1.9276, 2.3684, 2.5285, 1.7233, 1.7762, 1.9974, 1.8583, 1.7476,\n",
      "        1.7855, 2.1772, 2.3995, 1.5066, 1.5603, 1.4235, 1.4633, 1.8015, 2.2283,\n",
      "        2.0422, 2.7486, 1.6425, 1.5147, 1.7435, 1.6939, 1.7981, 1.7167, 1.9424,\n",
      "        2.3281, 1.1410, 1.1454, 1.4833, 1.7414, 2.5135, 3.6743, 3.8198, 3.3158,\n",
      "        0.8154, 1.2210, 1.2809, 2.0500, 2.2213, 3.6776, 3.5391, 3.3324, 1.6391,\n",
      "        1.9296, 1.9038, 2.0484, 1.9601, 2.1591, 2.4385, 2.3472, 1.9399, 1.8308,\n",
      "        1.8882, 1.9901, 1.9433, 1.9258, 2.1908, 2.4572, 1.8591, 1.4437, 1.7765,\n",
      "        1.8085, 1.9342, 1.6388, 2.2913, 2.0137, 1.4411, 1.7414, 1.8989, 1.7400,\n",
      "        1.9512, 1.6757, 2.3579, 2.3641, 1.3788, 1.3442, 1.7949, 1.7002, 1.6518,\n",
      "        1.7517, 2.4580, 2.1261, 1.3810, 1.5690, 1.7191, 1.7633, 1.7192, 1.9086,\n",
      "        2.0526, 2.0738, 1.4183, 1.4870, 1.7941, 1.6683, 1.6423, 1.8286, 2.2739,\n",
      "        2.3420, 1.4842, 1.3700, 1.4980, 1.6440, 1.5580, 1.7753, 2.5280, 2.5017,\n",
      "        1.8881, 1.7310, 1.6886, 1.5008, 1.6623, 1.8393, 2.3361, 2.3251, 1.5914,\n",
      "        1.4615, 1.6093, 1.8441, 1.5382, 1.9858, 2.2207, 2.4260, 1.5991, 1.1789,\n",
      "        1.2312, 1.4231, 1.3593, 1.8604, 2.0767, 2.3582, 1.5203, 1.3202, 1.4460,\n",
      "        1.6679, 1.6356, 1.7615, 1.9609, 2.0853], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0217,  0.0019,  0.0319,  ...,  0.0339, -0.0420, -0.0477],\n",
      "        [-0.0183, -0.0055,  0.0313,  ...,  0.0172,  0.0346,  0.0279],\n",
      "        [-0.0444,  0.0070,  0.0466,  ..., -0.0354,  0.0169, -0.0146],\n",
      "        ...,\n",
      "        [-0.0232,  0.0153,  0.0355,  ..., -0.0422,  0.0435, -0.0072],\n",
      "        [-0.0193, -0.0516, -0.0023,  ...,  0.0549,  0.0087,  0.0177],\n",
      "        [-0.0265,  0.0183,  0.0328,  ...,  0.0181,  0.0531, -0.0064]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6764, 1.4213, 1.7597, 1.6834, 1.6684, 1.3012, 1.8301, 1.7546, 1.8038,\n",
      "        1.7605, 1.6567, 1.7182, 1.6517, 2.2730, 1.6472, 1.7561, 1.1317, 1.4099,\n",
      "        1.3759, 2.0092, 1.8973, 3.2235, 2.9139, 2.8174, 1.1048, 0.9403, 1.4485,\n",
      "        1.8982, 2.2880, 3.0665, 2.9987, 2.7093, 1.4693, 1.5954, 1.9958, 1.8306,\n",
      "        1.9015, 2.2845, 2.0059, 1.9131, 1.4340, 1.4526, 1.8923, 1.8878, 1.8559,\n",
      "        1.6496, 2.0993, 2.1363, 1.6745, 2.0761, 1.8281, 1.7009, 1.7866, 2.2509,\n",
      "        2.2982, 2.0137, 1.3957, 1.6619, 1.9225, 1.9850, 1.8895, 1.5496, 2.1717,\n",
      "        2.0414, 1.8851, 1.7104, 2.0618, 2.0716, 1.8540, 2.0913, 2.4629, 2.1451,\n",
      "        1.7411, 1.9526, 2.2914, 2.0431, 1.9983, 1.9144, 2.5692, 2.2694, 1.7944,\n",
      "        1.8768, 1.7270, 1.7264, 1.9832, 1.9636, 2.0107, 2.0597, 2.2691, 1.8377,\n",
      "        1.9055, 1.8757, 1.3259, 1.4715, 2.0978, 1.9269, 1.4838, 1.4952, 1.7280,\n",
      "        1.8880, 1.4292, 1.2664, 1.8931, 1.5423, 1.3212, 1.7197, 1.7180, 1.5558,\n",
      "        1.7916, 1.9618, 1.8608, 1.6571, 1.8431, 1.6406, 1.7708, 1.6701, 1.9573,\n",
      "        1.4383, 1.9729, 2.0729, 2.0148, 1.7871, 1.7188, 1.8036, 1.4007, 2.2957,\n",
      "        2.1404, 1.7805, 1.8138, 1.5988, 1.6590, 1.6974, 1.8553, 1.3742, 1.9068,\n",
      "        1.8199, 1.4081, 1.6469, 1.8591, 1.7411, 1.3247, 1.9931, 2.0451, 1.8314,\n",
      "        2.0352, 1.7550, 1.9365, 2.0100, 1.7565, 2.1420, 1.9920, 2.0173, 1.5936,\n",
      "        1.7383, 1.8232, 1.9518, 2.0308, 1.8812, 2.2625, 2.2560, 1.9064, 1.8769,\n",
      "        1.8793, 2.0405, 2.0190, 1.9716, 2.7964, 2.6157, 1.6537, 1.7284, 2.1479,\n",
      "        2.0522, 2.0151, 2.3088, 2.5441, 2.3571, 1.7695, 1.5033, 1.9490, 1.9707,\n",
      "        1.9809, 2.0205, 2.4416, 2.4079, 1.8734, 1.8739, 1.7711, 2.0069, 1.7492,\n",
      "        1.7818, 2.0897, 2.2555, 1.7785, 1.6239, 1.7317, 1.8663, 1.5126, 1.2284,\n",
      "        1.9705, 1.8510, 1.9311, 1.7485, 1.5484, 1.3174, 1.3337, 2.1878, 1.9251,\n",
      "        1.9800, 1.1278, 1.2869, 1.5366, 1.9480, 2.2079, 2.7648, 2.7794, 2.5380,\n",
      "        0.8762, 1.1795, 1.1238, 1.7155, 1.8380, 2.8528, 2.5639, 2.6236, 1.7206,\n",
      "        1.8295, 1.9285, 1.8886, 1.6388, 1.7992, 2.2853, 2.2716, 1.7456, 1.9042,\n",
      "        1.8338, 2.0147, 1.8783, 1.7302, 1.9922, 2.2151, 1.7672, 1.5429, 1.7614,\n",
      "        1.8890, 1.8547, 1.9538, 2.1734, 1.9753, 1.3376, 1.6335, 1.8404, 1.7498,\n",
      "        1.9519, 1.8432, 2.2650, 2.3240, 1.2786, 1.5662, 1.8568, 1.8544, 1.7646,\n",
      "        1.8545, 1.9914, 2.1362, 1.5661, 1.4620, 1.7900, 1.8073, 1.9187, 1.7451,\n",
      "        2.4912, 1.9751, 1.2785, 1.3799, 1.7183, 1.7661, 1.7000, 1.7071, 2.1361,\n",
      "        1.8733, 1.0038, 1.3251, 1.6760, 1.9601, 1.7498, 1.7357, 1.9702, 1.9525,\n",
      "        1.7843, 1.7492, 1.7429, 1.7552, 1.7913, 1.7086, 2.3015, 2.0914, 1.6649,\n",
      "        1.6655, 1.8253, 1.8114, 1.6976, 1.5951, 2.0872, 2.2372, 1.4198, 1.5163,\n",
      "        1.4656, 1.3446, 1.8050, 1.2448, 1.4661, 1.5293, 1.7733, 1.4339, 1.3994,\n",
      "        1.2830, 1.1154, 1.2571, 1.6184, 1.5668], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0222, -0.0009, -0.0186,  ...,  0.0550, -0.0219,  0.0372],\n",
      "        [-0.0524, -0.0430,  0.0173,  ...,  0.0257, -0.0185, -0.0338],\n",
      "        [-0.0144,  0.0371,  0.0173,  ...,  0.0107, -0.0102,  0.0165],\n",
      "        ...,\n",
      "        [ 0.0249, -0.0250, -0.0522,  ...,  0.0519,  0.0480, -0.0040],\n",
      "        [-0.0302, -0.0482, -0.0374,  ..., -0.0482,  0.0344, -0.0408],\n",
      "        [ 0.0398,  0.0210,  0.0463,  ...,  0.0483, -0.0235, -0.0497]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0185, 1.8057, 1.7194, 1.8730, 1.9329, 1.7872, 1.7726, 1.8372, 1.7655,\n",
      "        1.8783, 1.7175, 1.8061, 1.7640, 1.8216, 1.9684, 1.8586, 1.5485, 1.4491,\n",
      "        1.5414, 1.4802, 1.5085, 1.4737, 1.4155, 1.4282, 1.4714, 1.4185, 1.4272,\n",
      "        1.4797, 1.5525, 1.4728, 1.4971, 1.4516, 1.7749, 1.8700, 1.8197, 1.7401,\n",
      "        1.8915, 1.7175, 1.8688, 1.6899, 1.8777, 1.9268, 1.7359, 1.7846, 1.6814,\n",
      "        1.7745, 1.7928, 1.8391, 1.7949, 1.7574, 2.2112, 1.9516, 1.8392, 1.8869,\n",
      "        1.8592, 1.8607, 1.9668, 1.9871, 1.7944, 1.8071, 2.0192, 2.1605, 1.8880,\n",
      "        1.8760, 2.1645, 2.0823, 1.7815, 2.3201, 1.7952, 2.2814, 1.7596, 1.9236,\n",
      "        1.9398, 1.9791, 1.9796, 1.9130, 1.8428, 1.9228, 2.0440, 1.9659, 1.8092,\n",
      "        1.9137, 1.7223, 1.7659, 1.7491, 1.7393, 1.7457, 1.7460, 1.8704, 1.7521,\n",
      "        1.7872, 1.6596, 1.7505, 1.7788, 1.7276, 1.7846, 1.7672, 1.7973, 1.8651,\n",
      "        1.8864, 1.8661, 1.8861, 1.7283, 1.9123, 1.9332, 1.8771, 1.9306, 1.8466,\n",
      "        1.9085, 1.9096, 1.8815, 1.8465, 1.8359, 1.8219, 1.7083, 1.8787, 1.8637,\n",
      "        1.8824, 1.7746, 1.8374, 1.7673, 1.8084, 1.8162, 1.7530, 1.8167, 1.7082,\n",
      "        1.7912, 1.8472, 1.8501, 1.8263, 1.8032, 1.7040, 1.8530, 1.7434, 1.7337,\n",
      "        1.9052, 1.7151, 1.8122, 1.7296, 1.7832, 1.7495, 1.8451, 1.9123, 1.7888,\n",
      "        1.6614, 1.5908, 1.7134, 1.6655, 1.7782, 1.6805, 1.7275, 1.6500, 1.6990,\n",
      "        1.6972, 1.7525, 1.7977, 1.7477, 1.7388, 1.6800, 1.7847, 1.8909, 1.8615,\n",
      "        1.8785, 1.8307, 2.0581, 2.0594, 1.8807, 1.8303, 1.9280, 1.8462, 1.9308,\n",
      "        1.9061, 1.9382, 1.8262, 1.8840, 1.9454, 1.8812, 2.0261, 1.9022, 1.9516,\n",
      "        2.0687, 1.8792, 1.9388, 1.9408, 1.9192, 1.9151, 1.9608, 1.9056, 1.9978,\n",
      "        1.9322, 1.9543, 1.9356, 1.3801, 1.3935, 1.3107, 1.4196, 1.2942, 1.3255,\n",
      "        1.4207, 1.3312, 1.3807, 1.3417, 1.5081, 1.4164, 1.4223, 1.3972, 1.3431,\n",
      "        1.3707, 1.6048, 1.5338, 1.4021, 1.5645, 1.5170, 1.5293, 1.5172, 1.5121,\n",
      "        1.4933, 1.3771, 1.4301, 1.5181, 1.4282, 1.4568, 1.5029, 1.4444, 2.1246,\n",
      "        1.9335, 2.0200, 1.8882, 2.1295, 1.9814, 1.8516, 1.8929, 1.8558, 1.9036,\n",
      "        1.7878, 2.1207, 2.1760, 1.9803, 1.9067, 1.8612, 1.7303, 1.8052, 1.7222,\n",
      "        1.7817, 1.7148, 1.6938, 1.7814, 1.7757, 1.7690, 1.7814, 1.8282, 1.6557,\n",
      "        1.6926, 1.7485, 1.7214, 1.8215, 1.8344, 1.7549, 1.8191, 1.7601, 1.7014,\n",
      "        1.6798, 1.7618, 1.7852, 1.7467, 1.7305, 1.7219, 1.8184, 1.8148, 1.8031,\n",
      "        1.6579, 1.7460, 1.8776, 1.9845, 1.8864, 2.0510, 1.8473, 1.8767, 1.9113,\n",
      "        1.8465, 1.7790, 1.9204, 1.8608, 1.8868, 1.9656, 2.0261, 1.9352, 1.9562,\n",
      "        1.9070, 1.8127, 1.8908, 1.8804, 1.9039, 2.0701, 1.8665, 1.9908, 1.8303,\n",
      "        1.9099, 1.9711, 1.8684, 1.9350, 1.9219, 1.8309, 1.9260, 1.7481, 1.7815,\n",
      "        1.8057, 1.6253, 2.0715, 1.7430, 1.6742, 1.8379, 1.8030, 1.7491, 1.9297,\n",
      "        1.6799, 1.7456, 1.7375, 1.7602, 1.8630], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0054,  0.0425, -0.0083,  ...,  0.0423,  0.0302, -0.0390],\n",
      "        [-0.0345,  0.0275, -0.0057,  ...,  0.0100,  0.0083,  0.0103],\n",
      "        [ 0.0194,  0.0260, -0.0226,  ...,  0.0272, -0.0065, -0.0383],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0505, -0.0174,  ...,  0.0005,  0.0175,  0.0345],\n",
      "        [ 0.0352, -0.0100,  0.0392,  ...,  0.0134,  0.0054,  0.0041],\n",
      "        [-0.0261, -0.0107,  0.0169,  ..., -0.0347, -0.0230, -0.0001]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5984, 1.7716, 1.6446, 1.8599, 1.7668, 1.6453, 1.6833, 1.5692, 1.7799,\n",
      "        1.8150, 1.3649, 1.5986, 1.7082, 1.7006, 1.7535, 1.6667, 1.8358, 1.7890,\n",
      "        1.6636, 1.8314, 1.8229, 1.6563, 1.7033, 1.5858, 1.6467, 1.5278, 1.6039,\n",
      "        1.7004, 1.7420, 1.8666, 1.6613, 1.4351, 1.6460, 1.4489, 1.6992, 1.7655,\n",
      "        1.6817, 1.6896, 1.6119, 1.6661, 1.6866, 1.6871, 1.7712, 1.6215, 1.7617,\n",
      "        1.5799, 1.6177, 1.5404, 1.6776, 1.7599, 1.7532, 1.7521, 1.5067, 1.6564,\n",
      "        1.5915, 1.6810, 1.5451, 1.7115, 1.7767, 1.6385, 1.7163, 1.7527, 5.9953,\n",
      "        1.7402, 1.7131, 1.6963, 1.5816, 1.7103, 1.7279, 1.8672, 1.7117, 1.8198,\n",
      "        1.5624, 1.7635, 1.6368, 1.7765, 1.7650, 1.6773, 1.7423, 1.7523, 1.5301,\n",
      "        1.6578, 1.7033, 1.6381, 1.5898, 1.6932, 1.6753, 1.7273, 1.4770, 1.8173,\n",
      "        1.8365, 1.6143, 1.9056, 1.6457, 1.7115, 1.7300, 1.7312, 1.8271, 1.7734,\n",
      "        1.8551, 1.9074, 1.6805, 1.7479, 1.6072, 1.7766, 1.8220, 1.5209, 1.6631,\n",
      "        1.7424, 1.6507, 1.6545, 1.6659, 1.5981, 1.5406, 1.8352, 1.5647, 1.6774,\n",
      "        1.7139, 1.6008, 1.8301, 1.6126, 1.7003, 1.7059, 1.7573, 1.6230, 1.8097,\n",
      "        1.6699, 1.6639, 1.5082, 1.7813, 1.4990, 1.8382, 1.7230, 1.7717, 1.7316,\n",
      "        1.6422, 1.6975, 1.6722, 1.5787, 1.7544, 1.5509, 1.6667, 1.7107, 1.6396,\n",
      "        1.6618, 1.8771, 1.6829, 1.6595, 1.6794, 1.6768, 1.9656, 1.7392, 1.9354,\n",
      "        1.7562, 1.8325, 1.6780, 1.5591, 1.7462, 1.9405, 1.6875, 1.6516, 1.7227,\n",
      "        1.6434, 1.7039, 1.7573, 1.6621, 1.7027, 1.7316, 1.7493, 1.7341, 1.5751,\n",
      "        1.7089, 1.6713, 1.6218, 1.7224, 1.7106, 1.5392, 1.7049, 1.7351, 1.6195,\n",
      "        1.6138, 1.6604, 1.6690, 1.7366, 1.6067, 1.6335, 1.5560, 1.6364, 1.6162,\n",
      "        1.6589, 1.6703, 1.6035, 2.5913, 1.6008, 1.4746, 1.6611, 1.6625, 1.7345,\n",
      "        1.6959, 1.8672, 1.7674, 1.7899, 1.6976, 1.5630, 1.8275, 1.6612, 1.7268,\n",
      "        1.3480, 1.7005, 1.6418, 1.6646, 1.6246, 1.5970, 1.7688, 1.6606, 1.8105,\n",
      "        1.6506, 1.8219, 1.4080, 1.5616, 1.5026, 1.6518, 1.6199, 1.7262, 1.7299,\n",
      "        1.6464, 1.6514, 1.5979, 1.6211, 1.6709, 1.6272, 1.5952, 1.6825, 1.7180,\n",
      "        1.5960, 1.6430, 1.6049, 1.5787, 1.7375, 1.7828, 1.5580, 1.6778, 1.6723,\n",
      "        1.6556, 1.5592, 1.8052, 1.6131, 1.6316, 1.7594, 1.7181, 1.3929, 1.6960,\n",
      "        1.7019, 1.5905, 1.6275, 1.7628, 1.6897, 1.5486, 1.4903, 1.7346, 1.6133,\n",
      "        1.6604, 1.7407, 1.6003, 1.6469, 1.7392, 1.7475, 2.0595, 1.5515, 1.6602,\n",
      "        1.5797, 1.7846, 1.6154, 1.6407, 1.8112, 1.6428, 1.7256, 1.5770, 1.7497,\n",
      "        1.8180, 1.6888, 1.6685, 1.7020, 1.7490, 1.7523, 1.7036, 1.8000, 1.6973,\n",
      "        1.4583, 1.6565, 1.6611, 1.7948, 1.7024, 1.7262, 1.4650, 1.7885, 1.7533,\n",
      "        1.6400, 1.5465, 1.7949, 1.7784, 1.6840, 1.6163, 1.5529, 1.6306, 1.6628,\n",
      "        1.7000, 1.6761, 1.5945, 1.6824, 1.6905, 1.4883, 1.6192, 1.6989, 1.6975,\n",
      "        1.7266, 1.7882, 1.6204, 1.6891, 1.6420], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0450, -0.0133,  0.0552,  ...,  0.0313, -0.0284, -0.0448],\n",
      "        [-0.0319,  0.0058,  0.0555,  ..., -0.0454,  0.0124,  0.0497],\n",
      "        [-0.0318,  0.0224, -0.0249,  ...,  0.0430,  0.0253, -0.0292],\n",
      "        ...,\n",
      "        [-0.0333, -0.0503, -0.0160,  ...,  0.0360,  0.0549,  0.0022],\n",
      "        [-0.0371, -0.0214,  0.0199,  ..., -0.0039,  0.0395,  0.0163],\n",
      "        [-0.0504,  0.0017,  0.0467,  ...,  0.0484,  0.0433,  0.0383]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6506, 1.5330, 4.6475,  ..., 1.6842, 1.6208, 1.7517], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0164, -0.0212,  0.0248,  ..., -0.0074,  0.0141, -0.0127],\n",
      "        [-0.0031, -0.0260,  0.0188,  ...,  0.0017,  0.0267,  0.0274],\n",
      "        [ 0.0031,  0.0276,  0.0229,  ...,  0.0137,  0.0147, -0.0008],\n",
      "        ...,\n",
      "        [-0.0076,  0.0045,  0.0043,  ...,  0.0264,  0.0180, -0.0221],\n",
      "        [ 0.0156,  0.0244, -0.0241,  ...,  0.0144,  0.0131,  0.0134],\n",
      "        [ 0.0014,  0.0027, -0.0018,  ..., -0.0049, -0.0119, -0.0071]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 3.6018,  3.7403,  3.5410,  3.7891,  3.9623,  3.6038,  3.6558,  3.5409,\n",
      "         3.5942,  3.7095,  3.4917,  3.8465,  3.7726,  3.5436,  3.8554,  3.6658,\n",
      "         3.5903,  3.5451,  3.5394,  3.6018,  3.7809,  3.9299,  3.6094,  3.6540,\n",
      "         3.4076,  3.6573,  3.5533,  3.6247,  3.6048,  3.6486,  3.6485,  4.5138,\n",
      "         3.7496,  3.5079,  3.6672,  3.6407,  3.6225,  3.4917,  3.5159,  3.8676,\n",
      "         3.5298,  3.6476,  3.6430,  3.5128,  3.9418,  3.5473,  5.2920,  3.5721,\n",
      "         3.5155,  3.9465,  3.7454,  3.6530,  3.4252,  3.6160,  3.6241,  3.5207,\n",
      "         3.8255,  3.8809,  4.0644,  3.5085,  3.5907,  3.8630, 12.3795,  3.7743,\n",
      "         3.8265,  3.7034,  3.7543,  3.5973,  3.8506,  3.8463,  3.7825,  3.7276,\n",
      "         3.4458,  3.9082,  3.7246,  3.5389,  3.5490,  3.5170,  3.6533,  3.4229,\n",
      "         3.6293,  3.5701,  3.7347,  3.7307,  3.6226,  3.5070,  3.6243,  3.7295,\n",
      "         3.3779,  3.7917,  3.8335,  3.7890,  4.4699,  3.7093,  3.7534,  3.7844,\n",
      "         3.6104,  3.8090,  3.7388,  3.8642,  3.7350,  3.7674,  3.7503,  3.5764,\n",
      "         3.5283,  3.8337,  3.6957,  3.6866,  3.7222,  3.6465,  3.7814,  3.8380,\n",
      "         3.7266,  3.6204,  3.7478,  3.5035,  3.7325,  3.7096,  3.5846,  3.7189,\n",
      "         3.7200,  3.6818,  3.7194,  3.8216,  3.7725,  3.6173,  3.6099,  3.6192,\n",
      "         3.6504,  3.4531,  3.4424,  3.6632,  3.8894,  3.7553,  3.6263,  3.6529,\n",
      "         3.6147,  3.7973,  3.6428,  3.6895,  3.5485,  3.5792,  3.5262,  3.7506,\n",
      "         3.7540,  3.7736,  3.7465,  3.6712,  3.7936,  3.7670,  5.2146,  3.6291,\n",
      "         3.6689,  3.7569,  3.9550,  3.6433,  3.7677,  3.9043,  4.0388,  3.6442,\n",
      "         3.7075,  3.8414,  3.5643,  3.6579,  3.8060,  3.9962,  3.5956,  3.6109,\n",
      "         3.5577,  3.5977,  3.4951,  3.5659,  3.5873,  3.8938,  3.6109,  3.4763,\n",
      "         3.6697,  3.6971,  3.8477,  3.8352,  3.6109,  3.5950,  3.6648,  3.5510,\n",
      "         3.7046,  3.7635,  3.5727,  3.7754,  3.7648,  3.7523,  3.6263,  3.3873,\n",
      "         6.3470,  3.6069,  3.6235,  3.5210,  3.6888,  3.7040,  3.8065,  3.8533,\n",
      "         3.5958,  3.6571,  3.8298,  3.6677,  3.7875,  3.7608,  3.5166,  3.2716,\n",
      "         3.6497,  3.5675,  3.7946,  3.5790,  3.5387,  4.1970,  3.4534,  3.6759,\n",
      "         3.6250,  3.7672,  3.3593,  3.5307,  3.4860,  3.6090,  3.9875,  3.6940,\n",
      "         3.5790,  3.7477,  3.5547,  3.7146,  3.6239,  3.4713,  3.5834,  3.4783,\n",
      "         3.6034,  3.7192,  3.7531,  3.8407,  3.7649,  3.4580,  3.7410,  3.8713,\n",
      "         3.5835,  3.6510,  3.5604,  3.5852,  3.3920,  3.5244,  3.6136,  3.5717,\n",
      "         3.7081,  3.8242,  3.6830,  3.7167,  3.9054,  3.4903,  3.6577,  3.8018,\n",
      "         3.5511,  3.3268,  3.2459,  3.9985,  3.6750,  3.7033,  3.6665,  3.6178,\n",
      "         3.8193,  3.7294,  3.9092,  5.4879,  3.5585,  3.5876,  3.6450,  3.5832,\n",
      "         3.6132,  3.8495,  3.6793,  3.7902,  3.7172,  3.4155,  3.8098,  3.6961,\n",
      "         3.5185,  3.6574,  3.6042,  3.6489,  3.8826,  3.8687,  3.7792,  3.6347,\n",
      "         3.5801,  3.9334,  3.6163,  3.8077,  3.6184,  3.9857,  3.4887,  3.5591,\n",
      "         3.6319,  3.6474,  3.4827,  3.6028,  3.5202,  3.6744,  3.6368,  3.5751,\n",
      "         3.6715,  3.4610,  3.8880,  3.5246,  3.5810,  3.8177,  3.5157,  3.5982,\n",
      "         3.5785,  3.6360,  3.5434,  4.0926,  3.7711,  3.6435,  3.4586,  3.8898],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0294,  0.0229, -0.0423,  ...,  0.0011, -0.0413, -0.0425],\n",
      "        [ 0.0147,  0.0477,  0.0098,  ...,  0.0283,  0.0524, -0.0177],\n",
      "        [-0.0467, -0.0008,  0.0411,  ...,  0.0252,  0.0360,  0.0533],\n",
      "        ...,\n",
      "        [ 0.0478,  0.0191,  0.0157,  ..., -0.0223,  0.0057, -0.0194],\n",
      "        [ 0.0294,  0.0308, -0.0056,  ..., -0.0282, -0.0093,  0.0391],\n",
      "        [ 0.0484,  0.0017,  0.0143,  ...,  0.0499,  0.0386,  0.0288]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6399, 1.7532, 1.7401, 1.8787, 1.9230, 1.6786, 2.1896, 2.5693, 1.5460,\n",
      "        1.6934, 1.9450, 2.0813, 1.9811, 1.6468, 2.4505, 2.1049, 1.5252, 1.3479,\n",
      "        1.7098, 1.6677, 2.5352, 3.4434, 3.6038, 4.1420, 1.0332, 1.4892, 1.6809,\n",
      "        1.9756, 2.3140, 3.5056, 3.6203, 3.8188, 1.3657, 1.3241, 0.9850, 1.4209,\n",
      "        1.8986, 1.9482, 1.6422, 1.8361, 1.1159, 0.7433, 1.3883, 1.2681, 1.2944,\n",
      "        1.4203, 1.6976, 1.8463, 1.5352, 1.6592, 2.0446, 1.9660, 2.2150, 3.5244,\n",
      "        3.3069, 3.5561, 1.3109, 1.7472, 1.9958, 1.8844, 2.2730, 3.4284, 3.2347,\n",
      "        3.3037, 1.6240, 2.2476, 1.6686, 1.3729, 1.7999, 1.7009, 1.9781, 2.0563,\n",
      "        1.9609, 1.9101, 1.7582, 1.7060, 1.4039, 1.8833, 2.0767, 2.1336, 1.3957,\n",
      "        1.1564, 1.3805, 1.5844, 2.2057, 2.0548, 1.8731, 2.0882, 1.5799, 1.5085,\n",
      "        1.2395, 1.2237, 1.3745, 1.6826, 1.9747, 1.9520, 1.7189, 1.7347, 1.8014,\n",
      "        1.7389, 1.6225, 1.6147, 2.3629, 2.2485, 2.0264, 2.0594, 1.9970, 1.5499,\n",
      "        2.0467, 1.4772, 2.4785, 2.1946, 1.6994, 1.5328, 1.5023, 1.0039, 1.4094,\n",
      "        2.1070, 1.8524, 1.8899, 1.6407, 1.2759, 0.9799, 1.7389, 1.4326, 1.5630,\n",
      "        1.9087, 1.8779, 1.4330, 1.5100, 1.3487, 1.3864, 1.2457, 1.8189, 1.7919,\n",
      "        1.3486, 1.7582, 1.6835, 1.6497, 1.2925, 1.6369, 1.3154, 1.9913, 1.8808,\n",
      "        1.0577, 1.1046, 1.0937, 1.6284, 1.4016, 1.5316, 1.7904, 2.3111, 1.5954,\n",
      "        1.1247, 1.5253, 1.4146, 2.1064, 1.9507, 1.8259, 1.9706, 1.8471, 1.3858,\n",
      "        1.0218, 1.2784, 1.7961, 1.5362, 1.7342, 1.9235, 1.2142, 1.3945, 1.4937,\n",
      "        1.5193, 1.4447, 1.9944, 1.9142, 1.9544, 2.1126, 2.3211, 2.2225, 1.9661,\n",
      "        1.9786, 1.7591, 2.3781, 2.3465, 1.7807, 1.8158, 1.9272, 1.8607, 1.6505,\n",
      "        2.6532, 2.3795, 2.3321, 1.8441, 1.2193, 1.5984, 1.2776, 1.9333, 2.0502,\n",
      "        1.7745, 1.9912, 0.9144, 1.3271, 1.0742, 1.7728, 1.4558, 1.4807, 1.7407,\n",
      "        2.0101, 1.9951, 1.7268, 1.5418, 1.6000, 1.7148, 1.8276, 1.9428, 2.2936,\n",
      "        1.7690, 1.9717, 2.0186, 1.8240, 1.6491, 2.2010, 1.5317, 2.2551, 1.0349,\n",
      "        1.2077, 1.6398, 1.9618, 1.8454, 1.4260, 1.8593, 1.8200, 1.6689, 1.0413,\n",
      "        1.1333, 1.2397, 1.3742, 1.9926, 1.7921, 2.2204, 1.9864, 1.7324, 1.8486,\n",
      "        1.8682, 1.7959, 2.3611, 2.9822, 3.0240, 2.1033, 2.0210, 2.0893, 1.9155,\n",
      "        2.4735, 2.3574, 3.0370, 2.9679, 1.5837, 1.3275, 1.1664, 1.0881, 2.1642,\n",
      "        1.5821, 1.9833, 1.8820, 1.7725, 1.0236, 1.5322, 2.1045, 1.5301, 2.0006,\n",
      "        1.7831, 1.9235, 1.6434, 1.3536, 1.5288, 1.2298, 1.2770, 1.9021, 1.8886,\n",
      "        1.9364, 1.5411, 1.5281, 1.3582, 1.7217, 1.8525, 1.4789, 1.7658, 1.9164,\n",
      "        1.6292, 1.2865, 1.2296, 1.7494, 1.9714, 2.2781, 2.0492, 2.0185, 1.9224,\n",
      "        1.3075, 1.3280, 1.2710, 1.6704, 1.4781, 1.9662, 1.9411, 2.1537, 1.4944,\n",
      "        1.5965, 1.9430, 2.2675, 2.1759, 1.8899, 2.2379, 1.0664, 1.4775, 1.5062,\n",
      "        1.6311, 1.5692, 1.8051, 1.9906, 2.2601], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0080,  0.0319, -0.0518,  ...,  0.0270,  0.0304, -0.0488],\n",
      "        [ 0.0424,  0.0138,  0.0395,  ..., -0.0428, -0.0217,  0.0440],\n",
      "        [-0.0230,  0.0056, -0.0364,  ..., -0.0081, -0.0459,  0.0340],\n",
      "        ...,\n",
      "        [ 0.0059, -0.0049, -0.0045,  ..., -0.0122,  0.0030, -0.0481],\n",
      "        [-0.0430,  0.0104, -0.0096,  ...,  0.0183, -0.0210,  0.0116],\n",
      "        [ 0.0150,  0.0382, -0.0018,  ..., -0.0438, -0.0031,  0.0151]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5309, 1.6931, 1.9261, 2.0828, 1.8593, 1.7166, 2.2565, 2.1145, 1.6875,\n",
      "        1.6549, 1.8035, 1.7787, 1.9966, 2.2490, 2.0969, 2.0211, 1.4882, 1.3877,\n",
      "        1.6289, 1.8107, 2.2115, 2.5412, 2.6763, 2.5874, 0.8751, 1.4086, 1.6166,\n",
      "        1.7843, 1.9915, 2.4754, 2.6760, 2.2740, 1.5758, 1.3231, 1.4229, 1.2359,\n",
      "        1.0395, 1.2115, 1.5916, 1.7042, 1.5247, 1.5291, 1.6045, 1.4420, 1.9579,\n",
      "        1.8754, 1.6039, 1.5901, 1.4641, 1.6338, 1.9279, 2.0953, 2.0096, 2.5621,\n",
      "        2.7428, 2.8156, 1.4154, 1.7971, 2.0818, 1.7845, 2.0726, 2.4232, 2.6847,\n",
      "        2.6718, 1.7588, 1.9996, 1.7536, 1.5809, 1.5263, 2.0492, 1.9141, 1.9497,\n",
      "        1.7724, 1.9682, 1.8924, 2.0216, 1.7462, 1.4728, 1.9382, 1.9295, 1.8665,\n",
      "        1.4939, 1.6831, 1.3205, 1.1467, 1.2686, 1.8304, 1.7464, 1.6372, 1.7575,\n",
      "        1.5297, 1.7791, 2.2907, 1.9087, 1.8050, 1.8142, 1.7619, 1.5962, 1.7379,\n",
      "        1.8792, 2.1115, 2.0625, 2.1574, 2.1005, 1.8917, 2.0196, 2.0236, 1.8531,\n",
      "        1.5987, 1.7175, 2.0690, 2.0709, 1.6899, 1.7640, 1.5843, 2.0010, 2.1373,\n",
      "        1.2837, 1.7552, 1.8098, 1.7074, 1.5789, 1.8204, 1.3155, 1.2603, 2.0813,\n",
      "        1.7908, 1.7504, 1.5806, 1.6545, 1.7405, 1.7510, 1.7344, 1.3628, 1.7240,\n",
      "        1.7804, 1.5890, 1.7528, 1.6554, 1.5532, 1.5696, 1.8348, 1.8815, 1.8062,\n",
      "        1.4526, 1.4226, 1.7177, 1.4029, 2.1150, 1.8771, 1.7170, 1.5375, 1.7636,\n",
      "        1.6100, 1.4903, 1.3616, 1.1485, 1.2252, 1.6983, 1.7065, 1.6219, 1.4383,\n",
      "        1.5358, 1.5397, 1.2007, 2.1919, 1.6914, 1.6745, 2.0379, 1.8482, 1.5907,\n",
      "        1.5008, 1.8042, 1.2340, 1.7363, 2.0732, 1.9503, 2.0849, 2.0506, 1.8897,\n",
      "        1.4795, 2.2823, 2.2541, 2.2776, 1.8925, 1.9656, 1.9645, 1.9647, 2.2284,\n",
      "        1.3699, 2.1567, 2.1344, 1.7646, 1.5307, 1.4546, 1.9314, 1.2545, 1.1431,\n",
      "        1.6371, 1.7795, 1.6551, 1.7058, 1.7538, 1.2051, 1.7675, 2.1046, 1.5886,\n",
      "        1.7472, 1.6998, 1.8586, 1.7274, 1.6286, 1.4503, 1.9361, 1.9855, 2.0396,\n",
      "        1.9892, 1.8138, 1.7858, 1.5776, 1.6772, 1.4767, 1.6805, 1.9908, 1.2172,\n",
      "        1.5634, 1.6086, 1.1619, 1.1010, 2.0406, 1.7009, 1.6884, 2.1298, 1.4200,\n",
      "        1.4657, 1.8107, 1.9875, 1.2372, 1.7489, 1.8665, 2.1701, 1.8381, 2.0496,\n",
      "        1.7656, 1.8957, 1.7588, 2.2853, 2.4793, 1.9064, 1.9628, 1.8846, 2.0270,\n",
      "        1.7675, 1.7389, 2.4557, 2.2686, 1.7228, 1.6518, 1.8330, 1.9934, 1.1377,\n",
      "        1.9731, 1.7001, 1.8143, 1.8599, 1.6181, 1.5369, 1.0908, 1.9761, 1.1879,\n",
      "        1.7457, 1.8771, 1.7028, 1.6062, 1.7726, 1.7157, 2.2365, 1.1758, 1.7710,\n",
      "        1.8453, 1.7239, 1.8168, 1.5406, 1.2757, 1.1536, 2.0793, 1.7885, 1.7869,\n",
      "        1.9619, 1.6829, 1.6188, 1.3668, 1.2203, 1.1384, 1.9284, 1.7988, 1.7185,\n",
      "        1.6424, 1.7992, 1.5953, 1.6589, 2.1209, 1.7519, 1.8725, 1.8261, 1.6776,\n",
      "        1.6234, 1.4318, 1.2036, 1.2949, 1.8718, 1.8330, 1.5246, 1.5061, 1.6595,\n",
      "        1.6851, 2.0671, 2.1689, 1.7885, 1.7964], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0392,  0.0293,  0.0457,  ...,  0.0281, -0.0468,  0.0367],\n",
      "        [-0.0077, -0.0537,  0.0257,  ..., -0.0051, -0.0434, -0.0227],\n",
      "        [-0.0416, -0.0075,  0.0034,  ..., -0.0169, -0.0505,  0.0220],\n",
      "        ...,\n",
      "        [ 0.0400, -0.0275, -0.0515,  ...,  0.0534, -0.0261,  0.0361],\n",
      "        [ 0.0320,  0.0444,  0.0256,  ...,  0.0344, -0.0077,  0.0448],\n",
      "        [-0.0157,  0.0157,  0.0288,  ...,  0.0286, -0.0331,  0.0074]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0770, 2.1868, 2.1022, 2.1854, 2.1122, 2.0832, 2.2038, 2.2273, 2.1921,\n",
      "        2.2248, 2.1408, 2.0402, 2.2058, 2.3088, 2.1050, 2.0521, 1.9018, 2.0451,\n",
      "        1.9653, 1.9934, 1.9726, 2.0453, 2.0089, 1.9860, 2.0174, 1.9298, 1.9474,\n",
      "        1.9847, 1.9136, 1.9503, 2.0072, 1.9159, 2.2982, 2.0947, 2.2144, 2.0272,\n",
      "        2.2389, 2.1956, 2.2662, 2.1090, 2.1554, 2.1549, 2.1717, 2.3194, 2.2245,\n",
      "        2.2550, 2.2022, 2.2045, 2.1382, 2.0672, 2.0555, 2.0330, 2.1097, 2.1014,\n",
      "        2.0494, 2.1116, 2.0780, 2.1001, 2.0058, 2.1321, 2.1345, 2.2069, 2.0986,\n",
      "        2.1025, 2.0310, 2.1676, 2.0431, 2.1549, 2.2531, 2.1262, 2.0618, 2.1047,\n",
      "        2.0603, 2.1272, 2.1321, 2.0953, 2.1069, 2.0605, 2.0703, 2.0714, 2.1637,\n",
      "        2.1645, 2.1459, 2.1828, 1.9858, 2.1222, 2.0283, 1.9757, 2.2361, 2.0802,\n",
      "        2.0594, 1.9104, 2.1481, 1.9930, 2.2373, 2.0512, 2.2672, 2.0625, 1.9609,\n",
      "        2.0820, 2.1634, 2.0489, 2.0347, 2.0689, 2.1276, 2.1139, 2.0712, 2.1248,\n",
      "        2.1202, 2.0430, 2.1655, 2.1044, 2.1660, 2.1258, 2.1839, 2.1655, 2.1589,\n",
      "        2.1284, 2.0306, 2.1263, 2.2546, 2.1828, 2.1948, 2.1693, 2.0905, 2.1400,\n",
      "        2.0633, 2.1730, 2.2752, 2.3350, 2.3809, 2.2109, 2.4155, 2.3477, 2.2968,\n",
      "        2.3404, 2.2203, 2.4030, 2.1255, 2.3743, 2.2424, 2.3942, 2.2158, 2.4082,\n",
      "        1.8951, 2.0980, 2.1671, 2.2687, 1.9103, 2.0767, 2.1250, 2.0545, 1.9379,\n",
      "        2.1263, 2.0658, 2.0749, 2.1370, 2.1591, 1.9838, 2.1803, 1.9496, 2.0693,\n",
      "        2.1359, 2.1926, 2.0580, 1.9730, 2.0684, 2.0542, 2.0955, 2.0726, 2.0678,\n",
      "        2.0273, 2.1981, 1.9993, 2.0510, 2.1530, 2.0863, 2.1501, 2.1773, 2.1979,\n",
      "        2.2988, 2.0910, 2.1831, 2.2213, 2.0981, 2.3209, 2.1127, 2.1143, 2.2905,\n",
      "        2.1164, 2.0824, 2.1752, 1.9726, 1.9557, 2.0186, 2.0770, 2.0749, 2.1383,\n",
      "        2.0322, 1.9994, 2.0897, 1.9702, 2.1816, 2.0273, 2.1075, 2.0124, 2.0299,\n",
      "        1.9848, 2.1881, 2.0103, 2.1236, 2.0629, 2.1666, 2.1783, 2.0521, 2.1310,\n",
      "        2.1731, 2.3236, 2.2978, 2.1847, 2.1439, 2.1406, 2.2068, 2.2094, 1.9548,\n",
      "        1.9800, 2.0595, 1.9299, 1.9281, 2.0339, 2.0070, 1.8651, 2.0356, 1.9922,\n",
      "        2.0647, 1.9510, 1.9135, 2.0453, 2.0418, 1.9997, 2.2066, 2.1614, 2.0919,\n",
      "        2.3066, 2.1342, 2.3125, 2.0857, 1.7429, 2.1276, 2.3420, 2.0698, 2.1958,\n",
      "        2.1939, 2.1435, 2.2575, 2.0988, 2.1592, 2.1348, 2.1238, 2.0459, 2.0066,\n",
      "        2.1365, 2.0739, 2.0361, 2.0002, 2.1446, 1.9522, 2.0823, 2.1652, 2.0183,\n",
      "        2.0954, 2.1262, 1.9798, 2.0231, 2.0891, 2.0205, 2.0961, 2.1467, 2.2060,\n",
      "        2.1201, 2.1436, 2.0971, 2.0731, 2.0799, 2.0854, 1.9297, 2.0079, 2.1356,\n",
      "        2.0707, 2.0824, 2.1047, 2.1811, 2.0798, 2.1564, 2.1726, 2.0348, 2.0793,\n",
      "        2.0238, 2.1072, 2.0278, 2.0219, 2.1454, 1.9184, 2.0645, 1.9349, 1.7613,\n",
      "        1.8310, 1.7929, 1.7692, 1.9554, 1.7835, 1.8297, 1.8013, 1.8338, 1.8866,\n",
      "        1.7978, 1.8381, 1.8735, 1.7833, 1.7948], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0364, -0.0097, -0.0290,  ..., -0.0056, -0.0272, -0.0528],\n",
      "        [-0.0547, -0.0344,  0.0287,  ...,  0.0020,  0.0393, -0.0061],\n",
      "        [ 0.0459, -0.0472,  0.0160,  ..., -0.0310,  0.0039,  0.0504],\n",
      "        ...,\n",
      "        [-0.0455,  0.0553, -0.0357,  ..., -0.0263, -0.0302, -0.0244],\n",
      "        [-0.0239,  0.0302,  0.0278,  ...,  0.0190,  0.0512, -0.0261],\n",
      "        [-0.0240,  0.0468, -0.0364,  ..., -0.0216,  0.0133,  0.0250]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9887, 1.9667, 1.9686, 2.1404, 2.1756, 1.9522, 2.0768, 2.0013, 2.0420,\n",
      "        1.9534, 1.8161, 1.9717, 1.9485, 1.7971, 2.1435, 2.1176, 1.9313, 1.9949,\n",
      "        1.8323, 1.9093, 2.1829, 1.9569, 1.9322, 2.0143, 1.8329, 1.9281, 1.8434,\n",
      "        2.0422, 2.2039, 2.0914, 1.9506, 1.6324, 2.1541, 1.8678, 1.9440, 1.8237,\n",
      "        1.9048, 1.8813, 1.8777, 2.1595, 1.9345, 2.0269, 2.0369, 2.0196, 2.0317,\n",
      "        2.0272, 2.9470, 1.8834, 1.9671, 1.9844, 2.0147, 1.9019, 1.9457, 1.8433,\n",
      "        2.0010, 1.9130, 2.1656, 2.1402, 2.3505, 2.0482, 1.8881, 1.9594, 4.8799,\n",
      "        1.8860, 2.0750, 2.0965, 2.1530, 2.0291, 1.8897, 2.0138, 1.9442, 1.9360,\n",
      "        1.8623, 2.2090, 1.9383, 2.1361, 1.9659, 1.9765, 1.9239, 1.7239, 1.8168,\n",
      "        1.9595, 1.8813, 2.1749, 1.8817, 1.8579, 1.7259, 2.0130, 1.7269, 2.0472,\n",
      "        2.1896, 2.0073, 2.2534, 1.9600, 1.9461, 2.0850, 1.9474, 2.1537, 2.1236,\n",
      "        2.1673, 2.0568, 2.2109, 1.9573, 1.9979, 1.9569, 2.0239, 2.0930, 2.0678,\n",
      "        1.8298, 1.7851, 2.1934, 2.1291, 2.0753, 1.9874, 2.1711, 1.9325, 2.0575,\n",
      "        1.8937, 1.9175, 2.2114, 2.0737, 1.9726, 2.1772, 2.0696, 1.8004, 2.1130,\n",
      "        1.8933, 1.9801, 1.9419, 1.9057, 1.8254, 2.1051, 2.1422, 2.0047, 2.2137,\n",
      "        1.9405, 1.8594, 1.9792, 1.9190, 1.9863, 1.9697, 1.8971, 2.3270, 2.1053,\n",
      "        1.9599, 1.9776, 1.9554, 2.0714, 1.9706, 1.8698, 2.1657, 2.0410, 1.9869,\n",
      "        1.9246, 2.1226, 1.9061, 1.8382, 1.9740, 2.2156, 1.9233, 2.0194, 2.0118,\n",
      "        2.1034, 1.9347, 2.0281, 2.1334, 2.1112, 2.0508, 2.0594, 1.9717, 1.9389,\n",
      "        1.7377, 2.0743, 2.0280, 1.8292, 1.8218, 1.8197, 2.0463, 2.3273, 1.9584,\n",
      "        1.8108, 1.9661, 2.0483, 1.9929, 2.0183, 2.0136, 1.9630, 1.9780, 2.0108,\n",
      "        1.9488, 2.0114, 1.8289, 2.7553, 2.0908, 1.9550, 2.0223, 1.9311, 1.8829,\n",
      "        2.0330, 1.9760, 1.9633, 2.0334, 1.9606, 2.1304, 1.9852, 2.0732, 2.0284,\n",
      "        1.7470, 1.9775, 1.9311, 1.8720, 2.0909, 2.0700, 2.1411, 2.2182, 2.0365,\n",
      "        1.8958, 2.0880, 1.6633, 1.8833, 1.8717, 2.0032, 1.9434, 2.1623, 2.1681,\n",
      "        2.0086, 2.0884, 2.0070, 1.9510, 2.0639, 1.8960, 1.7580, 2.0434, 1.8812,\n",
      "        2.0147, 2.0469, 1.8734, 1.8851, 2.0870, 2.0102, 1.9013, 1.8546, 2.0447,\n",
      "        2.0570, 2.0088, 1.8260, 2.0221, 2.0226, 1.9385, 1.9742, 1.9454, 1.9184,\n",
      "        2.0739, 1.7781, 1.9147, 1.9652, 1.9739, 1.7608, 1.7765, 2.0608, 2.2146,\n",
      "        2.1411, 2.0763, 1.9726, 1.9751, 2.0170, 2.0166, 2.5355, 2.0198, 1.9096,\n",
      "        1.8987, 2.1949, 1.8665, 1.9914, 1.9949, 1.9378, 2.1967, 1.8492, 1.9875,\n",
      "        2.0846, 1.9368, 1.9247, 2.0473, 2.0010, 2.0318, 2.0240, 2.0404, 1.8529,\n",
      "        1.9264, 1.8822, 1.8953, 2.1519, 1.9661, 2.1247, 1.7502, 2.0385, 2.1029,\n",
      "        1.9563, 1.8361, 1.9037, 2.0742, 2.0230, 2.1955, 1.9699, 2.1024, 1.9458,\n",
      "        2.0636, 1.7316, 1.8480, 2.1103, 2.0585, 1.8158, 1.8071, 2.2024, 2.1193,\n",
      "        1.9509, 2.0265, 1.9656, 2.1872, 1.9272], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0539,  0.0214,  0.0407,  ..., -0.0033, -0.0280, -0.0427],\n",
      "        [ 0.0116,  0.0044, -0.0420,  ..., -0.0456,  0.0142, -0.0376],\n",
      "        [ 0.0425,  0.0041, -0.0060,  ...,  0.0057,  0.0167, -0.0332],\n",
      "        ...,\n",
      "        [-0.0239, -0.0259, -0.0080,  ..., -0.0372, -0.0447, -0.0199],\n",
      "        [-0.0546, -0.0461, -0.0256,  ..., -0.0500, -0.0238,  0.0141],\n",
      "        [ 0.0038,  0.0366, -0.0070,  ..., -0.0434, -0.0145, -0.0423]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.1035, 1.8902, 2.1295,  ..., 2.6341, 1.8450, 1.9355], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-1.3801e-02, -1.2418e-03, -1.2519e-02,  ...,  1.5442e-02,\n",
      "          1.5804e-02,  2.0976e-02],\n",
      "        [ 1.2518e-02,  8.8998e-03,  1.4091e-02,  ...,  2.2418e-02,\n",
      "         -2.2070e-02,  9.0416e-03],\n",
      "        [-1.2818e-02,  1.4682e-02, -7.1138e-03,  ..., -3.9129e-03,\n",
      "         -3.2569e-03,  1.5279e-02],\n",
      "        ...,\n",
      "        [ 2.5349e-02, -1.3193e-02, -6.3088e-03,  ...,  9.9767e-04,\n",
      "          1.6058e-02,  1.3515e-02],\n",
      "        [ 4.1566e-03, -2.1671e-02,  5.4709e-03,  ...,  1.7408e-02,\n",
      "         -1.3723e-02,  9.4440e-03],\n",
      "        [-1.7585e-02, -7.1325e-05,  1.5183e-02,  ...,  9.7971e-03,\n",
      "          2.4214e-02, -2.3660e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([4.3365, 4.4910, 4.1932, 4.4708, 4.8677, 4.3201, 4.4398, 4.2075, 4.5104,\n",
      "        4.4101, 4.1962, 4.3025, 4.4620, 4.0274, 4.5948, 4.5125, 4.1666, 4.3014,\n",
      "        4.0578, 4.4230, 4.6034, 4.6799, 4.1816, 4.3287, 4.1907, 4.2324, 4.1814,\n",
      "        4.3826, 4.5019, 4.6307, 4.2049, 3.8998, 4.6969, 4.2819, 4.3067, 4.2324,\n",
      "        3.9502, 4.2322, 4.2696, 4.8974, 4.0937, 4.4063, 4.3352, 4.2798, 4.6537,\n",
      "        4.3568, 5.5422, 4.1373, 4.4195, 4.6006, 4.6372, 4.4636, 4.1063, 3.9942,\n",
      "        4.3123, 4.3651, 4.5027, 4.3766, 5.4256, 4.3583, 4.5579, 4.3673, 9.4339,\n",
      "        4.3311, 4.5433, 4.3234, 4.4696, 4.0663, 4.7019, 4.5882, 4.0626, 4.1265,\n",
      "        4.0503, 4.6782, 4.3031, 4.3019, 4.2117, 4.3817, 4.5240, 3.8781, 4.2126,\n",
      "        4.0856, 4.2789, 4.8610, 4.3397, 4.2279, 4.0894, 4.4401, 3.9451, 4.4110,\n",
      "        4.5479, 4.0697, 5.1690, 4.4374, 4.4058, 4.7290, 4.3152, 4.4211, 4.3590,\n",
      "        4.8193, 4.5421, 4.6362, 4.3804, 4.4611, 4.4397, 4.3598, 4.1741, 4.3810,\n",
      "        4.3592, 4.1172, 4.6639, 4.3306, 4.2095, 4.2386, 4.2700, 4.0900, 4.6047,\n",
      "        4.2940, 4.6185, 4.7467, 4.4043, 4.2281, 4.4012, 4.3954, 4.3006, 4.8003,\n",
      "        4.3360, 4.3008, 4.3579, 3.8749, 4.1746, 4.6093, 4.4811, 4.5865, 4.6563,\n",
      "        4.3804, 4.1872, 4.5227, 4.5311, 4.7935, 3.9695, 4.1488, 4.5197, 4.3863,\n",
      "        4.6371, 4.6356, 4.5869, 4.3511, 4.2194, 4.2992, 5.0311, 4.3517, 4.3371,\n",
      "        4.2752, 4.4410, 4.5052, 4.3046, 4.4847, 4.9412, 4.5899, 4.3069, 4.5940,\n",
      "        4.4834, 4.2313, 4.2854, 4.4473, 4.2243, 4.4109, 4.3833, 4.4276, 4.1149,\n",
      "        4.3353, 4.3169, 4.4047, 4.4576, 4.1022, 4.1567, 4.4350, 4.7317, 4.2355,\n",
      "        4.5311, 4.0633, 4.6543, 4.3593, 4.3435, 4.6995, 4.3343, 4.5373, 4.3475,\n",
      "        4.3233, 4.3016, 4.0784, 6.6798, 4.3197, 4.3691, 4.2075, 4.5271, 4.2079,\n",
      "        4.6079, 4.6843, 4.4250, 4.4521, 4.6515, 4.2621, 4.5702, 4.5508, 4.1984,\n",
      "        3.8257, 4.4877, 4.1397, 4.4638, 4.4387, 4.4973, 4.4731, 4.7222, 4.4277,\n",
      "        4.2393, 4.6488, 3.7931, 4.1747, 4.3157, 4.1779, 4.4120, 4.8020, 4.7867,\n",
      "        4.4131, 4.5257, 4.3028, 4.2489, 4.3093, 4.2179, 3.9671, 4.4511, 4.3508,\n",
      "        4.1761, 4.6307, 4.3126, 4.1306, 4.3518, 4.4154, 4.0972, 4.3251, 4.1088,\n",
      "        4.1819, 4.2155, 4.1914, 5.0033, 4.2059, 4.6359, 4.4262, 4.3824, 4.5383,\n",
      "        4.3221, 4.1278, 4.3298, 4.4177, 4.5346, 4.0078, 3.9841, 4.6724, 4.2435,\n",
      "        4.5775, 4.6529, 4.3233, 4.2558, 4.5348, 4.7071, 6.7723, 4.1021, 4.0479,\n",
      "        4.4890, 4.0780, 4.3554, 4.2033, 4.2261, 4.3817, 4.5846, 4.3193, 4.4047,\n",
      "        4.3839, 4.0766, 4.3128, 4.3471, 4.6576, 4.3232, 4.2868, 4.5946, 4.2110,\n",
      "        4.0502, 4.3902, 4.6060, 4.6066, 4.1612, 4.6684, 4.1095, 4.3878, 4.4613,\n",
      "        4.1272, 4.2003, 4.2314, 4.2930, 4.1856, 4.4179, 4.3635, 4.5913, 4.3382,\n",
      "        4.7721, 4.0839, 4.1736, 4.3944, 4.2119, 4.2414, 3.9799, 4.3892, 4.2905,\n",
      "        4.2576, 4.4301, 4.3827, 4.5907, 4.3110], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0147, -0.0525,  0.0473,  ..., -0.0488,  0.0337,  0.0331],\n",
      "        [ 0.0068,  0.0261, -0.0145,  ..., -0.0257, -0.0037,  0.0152],\n",
      "        [ 0.0312, -0.0143, -0.0441,  ..., -0.0552,  0.0052, -0.0484],\n",
      "        ...,\n",
      "        [-0.0305,  0.0326,  0.0481,  ..., -0.0065, -0.0302, -0.0317],\n",
      "        [-0.0296, -0.0229, -0.0517,  ..., -0.0162, -0.0379, -0.0030],\n",
      "        [ 0.0060, -0.0353, -0.0347,  ..., -0.0018, -0.0529, -0.0557]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5322, 1.3287, 1.3713, 1.3015, 1.5693, 1.7081, 1.6232, 1.6133, 1.5950,\n",
      "        1.4249, 1.1314, 1.3564, 1.4261, 1.3076, 1.6162, 1.5731, 1.5001, 1.3453,\n",
      "        1.0226, 1.3359, 1.7058, 1.4944, 1.8993, 1.9775, 1.0753, 1.0356, 1.3219,\n",
      "        1.1723, 1.1183, 1.9610, 2.0764, 1.8562, 1.0663, 1.1798, 1.2020, 1.6886,\n",
      "        2.1612, 1.5533, 1.8104, 2.5779, 2.0246, 1.5053, 1.6653, 1.4728, 1.4952,\n",
      "        2.1919, 1.9534, 1.9754, 1.6558, 1.7535, 1.6999, 1.5500, 1.4422, 1.5105,\n",
      "        1.8827, 1.9673, 1.7029, 1.7616, 1.5748, 1.7463, 1.7219, 1.5488, 1.8871,\n",
      "        1.9495, 1.1743, 1.6711, 1.4352, 1.9107, 2.0157, 2.0974, 2.2838, 2.0134,\n",
      "        1.2027, 1.2480, 1.5167, 2.0816, 2.0611, 1.6452, 2.2622, 2.1049, 1.7933,\n",
      "        1.2997, 1.6489, 1.0752, 1.8476, 1.7220, 1.3067, 1.4524, 1.6227, 1.7275,\n",
      "        1.4030, 1.7887, 1.1518, 1.1827, 1.2897, 1.3187, 1.1511, 0.9951, 1.0729,\n",
      "        1.1334, 1.4878, 1.4861, 1.4424, 1.5946, 1.1493, 0.9325, 1.2852, 1.3888,\n",
      "        1.2561, 1.5554, 1.4155, 1.4553, 1.5731, 1.6623, 1.9309, 1.6977, 1.8057,\n",
      "        1.6709, 2.2307, 2.3321, 1.5050, 1.6866, 1.7719, 1.8206, 1.5858, 1.9902,\n",
      "        2.2302, 2.3435, 2.0250, 1.6987, 1.6789, 1.5127, 1.6969, 2.2795, 1.9657,\n",
      "        1.9459, 1.6433, 1.7249, 1.6492, 1.3740, 1.6860, 1.4038, 1.8742, 1.9072,\n",
      "        1.9144, 1.2787, 1.7018, 1.2282, 1.2884, 1.4051, 1.6113, 2.1323, 1.2959,\n",
      "        1.2636, 1.1729, 1.8344, 1.9662, 2.0100, 1.7293, 1.7973, 1.4682, 1.4027,\n",
      "        1.5970, 1.7454, 1.5399, 2.3303, 1.7911, 1.8393, 1.6471, 1.4292, 1.6975,\n",
      "        1.7112, 1.9004, 1.4925, 1.6694, 1.9795, 0.7596, 1.3487, 1.5117, 1.4582,\n",
      "        2.0792, 2.0030, 2.3852, 1.9873, 1.3954, 1.1159, 1.2219, 1.6577, 1.9104,\n",
      "        2.0570, 2.3384, 1.9305, 1.6674, 1.2083, 1.6214, 1.8822, 1.8589, 1.8714,\n",
      "        1.6483, 1.9403, 1.2433, 1.4256, 1.0696, 1.2111, 1.2407, 1.3871, 1.6565,\n",
      "        1.6131, 0.9884, 1.3256, 1.2953, 1.2617, 1.2331, 1.2102, 1.7620, 1.7839,\n",
      "        1.2031, 1.2077, 1.2855, 1.3889, 1.1792, 1.7561, 1.9076, 1.7976, 1.4384,\n",
      "        1.3163, 1.7273, 1.7167, 1.2724, 1.3137, 1.6883, 1.6174, 1.6633, 1.4678,\n",
      "        1.0952, 1.1017, 1.6596, 1.9326, 1.5659, 1.8822, 0.7006, 1.5608, 1.4552,\n",
      "        1.6686, 2.1867, 1.7548, 2.1382, 2.2612, 1.3559, 1.1918, 1.4555, 1.5387,\n",
      "        2.2191, 1.7764, 2.2746, 2.3878, 1.9256, 1.9907, 1.9365, 1.6284, 1.3955,\n",
      "        2.1240, 1.8167, 2.0163, 1.8503, 1.8222, 1.8137, 1.3295, 1.7565, 1.5241,\n",
      "        2.0326, 1.8893, 1.8174, 1.9105, 1.8947, 1.8591, 1.4823, 2.1557, 2.1055,\n",
      "        2.0801, 1.6811, 1.7419, 1.9799, 1.9735, 1.7417, 1.5740, 2.2179, 2.0054,\n",
      "        1.3009, 1.3189, 1.3929, 1.2809, 1.3458, 1.1439, 2.0034, 2.0512, 1.2375,\n",
      "        1.2693, 1.2661, 1.3753, 1.2589, 1.5827, 1.6108, 1.3963, 0.7866, 0.7888,\n",
      "        0.8338, 1.2125, 1.9412, 2.0820, 2.0319, 2.0030, 1.7633, 1.4010, 1.5158,\n",
      "        1.5368, 1.7716, 2.0763, 1.9702, 1.6632], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0513, -0.0343, -0.0328,  ..., -0.0513,  0.0038,  0.0170],\n",
      "        [-0.0511,  0.0454,  0.0026,  ...,  0.0437, -0.0478,  0.0153],\n",
      "        [ 0.0376, -0.0360,  0.0377,  ...,  0.0125, -0.0373, -0.0289],\n",
      "        ...,\n",
      "        [-0.0199, -0.0040, -0.0158,  ...,  0.0067,  0.0365,  0.0557],\n",
      "        [-0.0115,  0.0405,  0.0131,  ..., -0.0552, -0.0200, -0.0262],\n",
      "        [-0.0476, -0.0489,  0.0274,  ...,  0.0099, -0.0244, -0.0310]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3995, 1.2959, 1.4102, 1.4478, 1.2159, 1.1735, 1.5659, 1.5781, 1.4691,\n",
      "        1.4185, 1.3074, 1.3325, 1.4212, 1.8655, 1.5511, 1.5322, 1.1092, 1.4849,\n",
      "        1.0690, 1.2211, 1.3932, 1.3808, 1.5816, 1.4445, 1.2976, 1.0483, 1.5252,\n",
      "        1.2520, 1.1959, 1.3464, 1.7678, 1.6390, 1.5404, 1.4908, 1.5533, 1.3140,\n",
      "        1.2879, 2.0822, 1.7387, 1.8679, 1.4773, 1.4550, 1.4456, 1.7047, 2.0766,\n",
      "        1.3216, 1.8760, 1.8494, 1.6586, 1.7455, 1.7720, 1.4964, 1.5276, 1.4536,\n",
      "        1.7426, 1.8610, 1.6134, 1.8452, 1.7740, 1.7096, 1.4388, 2.0495, 1.7516,\n",
      "        1.9308, 1.2004, 1.5721, 1.5876, 1.7234, 1.5864, 1.3313, 1.8518, 1.7407,\n",
      "        1.2236, 1.2653, 1.6476, 1.7577, 1.4020, 1.6353, 1.6879, 1.8899, 1.8034,\n",
      "        1.4717, 1.6030, 1.5678, 0.9466, 1.0095, 1.3914, 1.4799, 1.5859, 1.6932,\n",
      "        1.4528, 1.2650, 1.5829, 1.3789, 1.3536, 1.3023, 1.0210, 1.2409, 1.2452,\n",
      "        1.3209, 1.0526, 1.1026, 1.3821, 1.4795, 1.1985, 1.1367, 1.3503, 1.0754,\n",
      "        1.1289, 1.3039, 1.3669, 1.4462, 1.4690, 1.6013, 1.9506, 1.7830, 1.7239,\n",
      "        1.8123, 1.9730, 1.7766, 1.5541, 1.7137, 1.8047, 1.8217, 1.7528, 1.6207,\n",
      "        2.0093, 1.9066, 1.9251, 1.7413, 1.8014, 1.6218, 1.2507, 1.1359, 1.8372,\n",
      "        1.8320, 1.7124, 1.8510, 1.7800, 1.5076, 1.4248, 2.3201, 1.7533, 1.7664,\n",
      "        1.8135, 1.4666, 1.3273, 1.8500, 1.8296, 1.9462, 1.5446, 1.7149, 1.4717,\n",
      "        1.4815, 1.5729, 1.1264, 1.0456, 1.1884, 1.7073, 1.6840, 1.5950, 1.5372,\n",
      "        1.4451, 1.3926, 1.6571, 1.1761, 1.6628, 1.7248, 1.5198, 1.6092, 1.4856,\n",
      "        1.4340, 1.1889, 1.9720, 1.7138, 1.7053, 1.5608, 1.0689, 1.3158, 1.4873,\n",
      "        1.6190, 1.8840, 1.9548, 1.8462, 0.8326, 1.5021, 1.3406, 1.3597, 1.7217,\n",
      "        1.8104, 1.9394, 1.8134, 1.7806, 1.4176, 1.3906, 1.1332, 1.0304, 1.1607,\n",
      "        1.6014, 1.7563, 1.4047, 1.5165, 1.5236, 1.5911, 1.9748, 1.8447, 1.6622,\n",
      "        1.5332, 0.8934, 0.9964, 1.0100, 1.2984, 1.1559, 1.2920, 1.4799, 1.4542,\n",
      "        1.0402, 1.1596, 1.3891, 1.2013, 1.1918, 1.2257, 1.7550, 1.3429, 1.5577,\n",
      "        1.4367, 1.6198, 1.0606, 1.8716, 1.9182, 1.6293, 1.5925, 1.7916, 1.5909,\n",
      "        1.3260, 1.7117, 0.9965, 1.0774, 1.5440, 1.6447, 1.5005, 1.5412, 1.5537,\n",
      "        1.6704, 1.6272, 1.7585, 1.9342, 1.9405, 1.1234, 0.9565, 1.1468, 1.2585,\n",
      "        1.8633, 1.7206, 2.0673, 1.9533, 1.9119, 1.9259, 1.9723, 1.7099, 1.9524,\n",
      "        1.2408, 1.6320, 1.7556, 1.7750, 1.8907, 1.8237, 1.5864, 1.4724, 2.1804,\n",
      "        1.9062, 1.7116, 1.7718, 1.8999, 1.9300, 1.9058, 1.6150, 1.4685, 1.9602,\n",
      "        1.8581, 1.6730, 1.7099, 1.8579, 1.8588, 1.5774, 2.1275, 2.0620, 1.9776,\n",
      "        0.9871, 0.9270, 1.1866, 1.1240, 1.3166, 1.2614, 1.6739, 1.4633, 0.8530,\n",
      "        1.0256, 1.2664, 1.3363, 1.0959, 1.1890, 1.4761, 1.4628, 1.1724, 1.5129,\n",
      "        1.6290, 1.6201, 1.6811, 1.7998, 1.8743, 1.8513, 1.4634, 1.1311, 1.0280,\n",
      "        1.5072, 1.7326, 1.8693, 1.8698, 1.7403], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0332, -0.0190, -0.0093,  ..., -0.0489,  0.0068,  0.0135],\n",
      "        [ 0.0434,  0.0071, -0.0002,  ...,  0.0553,  0.0503, -0.0314],\n",
      "        [-0.0519,  0.0019,  0.0425,  ..., -0.0441, -0.0022, -0.0341],\n",
      "        ...,\n",
      "        [ 0.0308, -0.0020,  0.0122,  ...,  0.0128, -0.0256,  0.0248],\n",
      "        [ 0.0516,  0.0258,  0.0086,  ...,  0.0523,  0.0309, -0.0266],\n",
      "        [ 0.0267, -0.0410,  0.0457,  ...,  0.0552, -0.0123,  0.0161]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0657, 2.0785, 2.0676, 2.0695, 2.3478, 2.2481, 2.0824, 1.9778, 2.1133,\n",
      "        2.1494, 2.1680, 2.0302, 2.1144, 2.1558, 2.0468, 2.1599, 2.2824, 2.2292,\n",
      "        2.2259, 2.3669, 2.2609, 2.1286, 2.3002, 2.4073, 2.3116, 2.2774, 2.1875,\n",
      "        2.1140, 2.2783, 2.2080, 2.2720, 2.2485, 2.0430, 1.9345, 2.1356, 2.0834,\n",
      "        1.9732, 2.0210, 2.0908, 2.0771, 2.0724, 2.0631, 2.0122, 2.0069, 2.0436,\n",
      "        2.0227, 2.0209, 1.9650, 2.2054, 2.2588, 2.2462, 2.3188, 1.8916, 2.1577,\n",
      "        2.2226, 2.0541, 2.0283, 2.1731, 2.2595, 2.2528, 2.2854, 2.0241, 2.3201,\n",
      "        2.2264, 2.2505, 2.3336, 2.1078, 2.1393, 2.4342, 2.2008, 2.2322, 2.1851,\n",
      "        2.2332, 2.2491, 2.2417, 2.1535, 2.2299, 2.1308, 2.1687, 2.2507, 2.1146,\n",
      "        2.1266, 2.1921, 2.1355, 2.2889, 2.1244, 2.1089, 2.1099, 2.2978, 2.1006,\n",
      "        2.2142, 2.1877, 2.1276, 2.1914, 2.0778, 2.0871, 2.3377, 2.0274, 2.2325,\n",
      "        2.0907, 2.1946, 2.1169, 2.0601, 2.1136, 2.1643, 2.1445, 2.2498, 2.2065,\n",
      "        2.3980, 2.1848, 2.1425, 2.1759, 2.1687, 2.2285, 2.0741, 2.1375, 2.0289,\n",
      "        2.1865, 2.2231, 2.2702, 2.5273, 2.1065, 2.1886, 2.2369, 2.0204, 2.2002,\n",
      "        2.2493, 2.2946, 2.1891, 2.0688, 2.2207, 2.2179, 2.2601, 2.2962, 2.0702,\n",
      "        2.2231, 2.1452, 2.2406, 2.1071, 2.3051, 2.2573, 2.2331, 2.1229, 2.1415,\n",
      "        2.0598, 2.1146, 1.8182, 1.9498, 2.0475, 2.1046, 2.1370, 1.9533, 2.2619,\n",
      "        2.1768, 2.0503, 2.1036, 2.0108, 2.2037, 2.0662, 2.0277, 2.1596, 2.0848,\n",
      "        2.0421, 2.1011, 2.0757, 2.1190, 2.1298, 2.0874, 2.0608, 2.0034, 2.1266,\n",
      "        2.1261, 2.0708, 1.9519, 2.0311, 2.1350, 1.9819, 1.8290, 2.0159, 1.9534,\n",
      "        1.8445, 1.8958, 1.9468, 1.9471, 2.0154, 1.9934, 1.9373, 1.9092, 1.9183,\n",
      "        1.9637, 1.9520, 1.8603, 2.0725, 1.9178, 2.0254, 2.2075, 2.1738, 2.0153,\n",
      "        2.0385, 1.9725, 2.1871, 2.2203, 2.0111, 2.1198, 2.1930, 2.0104, 1.8545,\n",
      "        2.1801, 2.5897, 2.5605, 2.5351, 2.5149, 2.5498, 2.9697, 2.5294, 2.2781,\n",
      "        2.1281, 2.3527, 2.2823, 2.2901, 2.2330, 2.4675, 2.3667, 2.3378, 2.1347,\n",
      "        2.1620, 2.2378, 2.1831, 2.1161, 1.9512, 2.1955, 2.1507, 2.2602, 2.0893,\n",
      "        2.1519, 2.1719, 2.0769, 2.1728, 2.1017, 1.9324, 2.0184, 2.0348, 1.8871,\n",
      "        1.9376, 1.9706, 1.9539, 1.9022, 1.9940, 1.9336, 1.9271, 1.9799, 1.9046,\n",
      "        1.9106, 1.8595, 1.9805, 1.9662, 2.2734, 2.2044, 1.9976, 2.3193, 2.2096,\n",
      "        2.1024, 2.0765, 2.2196, 2.2219, 2.2228, 2.1115, 2.1969, 2.2352, 2.1124,\n",
      "        2.1860, 2.1059, 2.2079, 2.1742, 2.1830, 2.2900, 2.1535, 2.2009, 2.2127,\n",
      "        2.0939, 2.2520, 2.1785, 2.0875, 2.0289, 2.0152, 2.3102, 2.1300, 2.2125,\n",
      "        3.5395, 2.6963, 2.6321, 3.2084, 2.3722, 2.8298, 2.5297, 2.7103, 3.5759,\n",
      "        2.5188, 2.7190, 2.6799, 3.0126, 2.7961, 2.8011, 2.7505, 2.0947, 1.9773,\n",
      "        1.8664, 1.8836, 1.9189, 1.8989, 1.9191, 2.1288, 1.9690, 1.9698, 1.9115,\n",
      "        1.9069, 2.0790, 1.9444, 1.9552, 1.9607], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0549,  0.0116,  0.0164,  ...,  0.0044,  0.0163,  0.0248],\n",
      "        [-0.0466,  0.0304,  0.0427,  ...,  0.0352, -0.0377,  0.0392],\n",
      "        [-0.0365, -0.0326,  0.0313,  ...,  0.0204, -0.0219,  0.0539],\n",
      "        ...,\n",
      "        [-0.0495, -0.0030, -0.0503,  ...,  0.0108, -0.0121, -0.0058],\n",
      "        [ 0.0370, -0.0108,  0.0512,  ...,  0.0321,  0.0048,  0.0353],\n",
      "        [ 0.0066,  0.0223, -0.0027,  ..., -0.0146, -0.0088, -0.0209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.1950, 2.0796, 2.1976, 2.2173, 2.5501, 2.2756, 2.1236, 2.3138, 2.2977,\n",
      "        2.1657, 2.1068, 2.4646, 2.5181, 2.0444, 2.3841, 2.1924, 2.2644, 2.1490,\n",
      "        2.3533, 2.0837, 2.4601, 2.4260, 2.0452, 2.1849, 2.1550, 1.9889, 2.2334,\n",
      "        2.1646, 2.4606, 2.2237, 2.0750, 2.1085, 2.4677, 2.1813, 2.1133, 2.0784,\n",
      "        2.0952, 2.2553, 2.3456, 2.1600, 2.1055, 2.3648, 2.3313, 2.1969, 2.3154,\n",
      "        2.1441, 3.1410, 1.9616, 2.2736, 2.5440, 2.2182, 2.2177, 2.0848, 2.3466,\n",
      "        2.0585, 2.1936, 2.1982, 2.3910, 2.6087, 2.3830, 2.3073, 2.0785, 6.8979,\n",
      "        2.1908, 2.2077, 2.1640, 2.3990, 2.6359, 2.3405, 2.1419, 2.2254, 2.1583,\n",
      "        2.0122, 2.3110, 2.1540, 2.3781, 2.2231, 2.4419, 1.9088, 1.9739, 2.0241,\n",
      "        2.1073, 2.3082, 2.2637, 2.0061, 2.1520, 2.4936, 2.4253, 1.9614, 2.2138,\n",
      "        2.1708, 2.0397, 2.4328, 2.2518, 2.2294, 2.3114, 2.1305, 2.3091, 2.1867,\n",
      "        2.3113, 2.3704, 2.2551, 2.4534, 2.4093, 2.3042, 2.3786, 2.1241, 2.2828,\n",
      "        1.9977, 2.3156, 2.2617, 2.2921, 2.2089, 2.1527, 2.4583, 2.1340, 2.2345,\n",
      "        2.3181, 2.4635, 2.5142, 2.4255, 2.2639, 2.2885, 2.2363, 1.9873, 2.5436,\n",
      "        2.1057, 2.1601, 2.2571, 2.3535, 2.1398, 2.0234, 2.1795, 2.1968, 2.4075,\n",
      "        2.1425, 2.2363, 2.4519, 2.1769, 2.3354, 2.1222, 2.0811, 2.2397, 2.3746,\n",
      "        2.3608, 2.2968, 2.1404, 2.3381, 2.1728, 2.2849, 2.4438, 2.2570, 2.3707,\n",
      "        2.1321, 2.2494, 2.3116, 2.0870, 2.3461, 2.2676, 2.1515, 2.0726, 2.2039,\n",
      "        2.3286, 2.3901, 2.3475, 2.3471, 2.2778, 2.1221, 2.3046, 2.1665, 2.2876,\n",
      "        2.3215, 2.2611, 2.2019, 2.2329, 2.0736, 2.2756, 2.2638, 2.1551, 1.9872,\n",
      "        2.0286, 2.1084, 2.1899, 2.1900, 2.3084, 2.1778, 2.3019, 2.2346, 1.9178,\n",
      "        2.2242, 2.1535, 2.0849, 3.3454, 2.1798, 2.2239, 2.2741, 2.4394, 2.2432,\n",
      "        2.3690, 2.3077, 2.2977, 2.3400, 2.2820, 2.2664, 2.2592, 2.3565, 1.9593,\n",
      "        2.2843, 2.6104, 2.1894, 2.1687, 2.3565, 2.3998, 2.3129, 2.3196, 2.1748,\n",
      "        2.1705, 2.0274, 2.0073, 2.1452, 2.0776, 2.0987, 2.4038, 2.4470, 2.3962,\n",
      "        2.2420, 2.3455, 2.1151, 1.9444, 2.3953, 2.1166, 2.1088, 2.0402, 2.3179,\n",
      "        2.0248, 2.1503, 2.2581, 2.2423, 2.0714, 2.2609, 2.1182, 2.1079, 2.3298,\n",
      "        2.0863, 2.2737, 2.0280, 2.3699, 1.9975, 2.0390, 2.4194, 2.2933, 2.1714,\n",
      "        2.0581, 1.9837, 2.2471, 2.2126, 2.3342, 2.0425, 2.3212, 2.2752, 2.1007,\n",
      "        2.1751, 2.2346, 2.1550, 2.1678, 2.2087, 2.3177, 2.8002, 2.3434, 2.1028,\n",
      "        1.9480, 2.2001, 2.0561, 2.4584, 2.4455, 2.3499, 2.1432, 2.1032, 2.2826,\n",
      "        2.1882, 2.2317, 2.1125, 2.2637, 2.5743, 2.1783, 2.2591, 2.2868, 2.3154,\n",
      "        2.1249, 2.1544, 2.1657, 2.2682, 2.2433, 2.3270, 2.0693, 2.0515, 2.1967,\n",
      "        2.0879, 2.0386, 2.3096, 2.2174, 2.1616, 2.1916, 2.1859, 2.3670, 2.2354,\n",
      "        2.3232, 2.0281, 2.0040, 2.0220, 2.3620, 1.9888, 1.9273, 2.3367, 2.3175,\n",
      "        2.1491, 2.3649, 2.1205, 2.2215, 1.9783], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0376,  0.0487, -0.0095,  ...,  0.0046, -0.0471,  0.0228],\n",
      "        [ 0.0527,  0.0185,  0.0144,  ..., -0.0464, -0.0011, -0.0390],\n",
      "        [-0.0471, -0.0502, -0.0517,  ..., -0.0219, -0.0481,  0.0269],\n",
      "        ...,\n",
      "        [-0.0097, -0.0196,  0.0206,  ...,  0.0544, -0.0222, -0.0360],\n",
      "        [ 0.0127, -0.0374,  0.0363,  ...,  0.0484, -0.0387, -0.0105],\n",
      "        [-0.0129, -0.0180, -0.0303,  ..., -0.0312,  0.0107, -0.0038]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.7293, 1.9122, 1.6004,  ..., 2.2096, 2.0481, 2.0325], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0015, -0.0086, -0.0026,  ...,  0.0017,  0.0074,  0.0150],\n",
      "        [-0.0175,  0.0160,  0.0250,  ..., -0.0203,  0.0185, -0.0217],\n",
      "        [ 0.0167,  0.0023,  0.0056,  ..., -0.0137, -0.0193,  0.0256],\n",
      "        ...,\n",
      "        [ 0.0097, -0.0270,  0.0264,  ..., -0.0177, -0.0276, -0.0155],\n",
      "        [-0.0277, -0.0103, -0.0021,  ...,  0.0200, -0.0194, -0.0133],\n",
      "        [ 0.0134,  0.0198,  0.0165,  ..., -0.0007, -0.0065, -0.0229]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 5.2838,  4.9963,  4.9711,  3.5833,  5.1552,  5.9426,  6.1616,  6.2339,\n",
      "         5.7748,  5.4205,  5.9237,  5.5825,  5.7407,  5.2781,  5.4173,  5.9613,\n",
      "         5.3184,  5.9201,  5.2907,  5.7537,  5.4361,  6.0948,  4.7069,  6.1293,\n",
      "         5.5831,  5.8848,  4.9793,  5.4906,  5.4290,  5.5899,  5.6038,  4.1914,\n",
      "         6.1415,  4.6386,  5.7186,  5.7481,  4.5930,  6.0114,  5.8535,  5.1684,\n",
      "         5.6134,  5.8804,  5.5777,  5.7252,  6.0576,  5.7577,  5.4043,  5.3896,\n",
      "         5.3369,  5.9635,  5.2642,  6.1025,  5.5471,  5.8451,  5.4164,  6.0571,\n",
      "         5.3112,  6.2964,  5.2504,  4.7866,  6.3851,  5.5430, 12.0029,  5.7796,\n",
      "         6.0472,  5.5287,  5.6425,  5.9188,  5.7040,  5.7570,  5.4182,  5.8208,\n",
      "         5.1088,  5.4265,  5.4827,  6.1843,  5.6877,  6.3520,  4.9429,  5.6272,\n",
      "         5.1136,  5.0615,  5.6672,  5.0317,  5.7291,  5.8960,  5.9235,  5.6418,\n",
      "         5.4866,  5.3753,  5.7789,  4.9961,  5.7366,  5.9204,  5.2493,  6.6370,\n",
      "         6.0835,  5.4826,  4.1855,  5.4494,  5.7173,  6.0563,  5.9601,  5.5297,\n",
      "         5.8146,  5.8555,  5.4472,  5.9562,  5.6704,  5.4498,  6.3773,  5.6880,\n",
      "         5.6270,  6.0766,  5.6824,  5.8045,  5.4037,  5.2919,  6.0884,  5.5748,\n",
      "         5.9348,  5.5505,  5.4181,  4.4431,  4.0944,  5.3186,  6.0208,  5.4692,\n",
      "         5.6645,  5.4248,  5.8207,  5.0342,  3.9631,  5.2825,  6.1638,  5.2886,\n",
      "         5.7411,  6.2174,  5.4566,  4.8838,  5.1231,  4.9722,  5.9016,  6.0205,\n",
      "         6.6397,  6.0973,  6.2681,  6.2523,  6.1005,  5.9257,  5.8661,  5.8956,\n",
      "         5.3551,  5.8907,  5.4151,  6.1109,  5.0319,  5.9331,  6.0766,  5.8600,\n",
      "         5.6053,  5.5575,  5.4149,  6.1254,  5.6531,  6.1349,  6.0200,  5.6762,\n",
      "         5.3727,  5.9928,  4.9040,  5.9177,  5.5215,  5.6332,  5.5013,  5.0323,\n",
      "         5.6163,  6.2341,  5.2454,  5.0410,  5.2225,  5.0837,  5.5679,  6.0701,\n",
      "         5.7333,  5.7757,  5.7917,  6.2277,  5.4246,  5.6110,  5.2917,  5.5963,\n",
      "         4.9330,  5.7683,  5.5455,  5.9402,  6.2271,  5.9109,  5.9180,  5.6058,\n",
      "         5.6163,  6.1175,  6.1141,  5.9105,  5.8417,  6.0239,  5.6607,  3.5147,\n",
      "         6.0139,  4.9196,  5.7786,  6.0411,  5.9471,  5.6669,  5.9353,  6.2322,\n",
      "         5.6127,  5.3326,  4.8366,  5.2641,  5.7634,  5.6476,  5.5418,  5.7026,\n",
      "         6.5285,  4.6853,  5.9367,  5.5032,  5.8780,  5.8817,  5.9187,  5.2776,\n",
      "         5.4291,  5.1131,  5.6682,  5.7957,  5.5021,  6.1111,  5.5274,  5.8997,\n",
      "         5.0702,  5.9061,  5.8296,  5.5695,  6.0133,  5.6835,  5.1161,  5.7777,\n",
      "         4.9116,  5.8937,  5.5857,  5.4990,  5.2516,  5.3286,  5.7863,  5.0983,\n",
      "         5.4398,  5.5096,  5.1766,  5.3858,  5.7402,  5.7012,  5.8674,  5.6966,\n",
      "         5.9467,  5.7606,  5.8120,  5.3916,  5.3800,  5.6626,  3.9190,  5.7918,\n",
      "         5.2986,  5.9066,  5.7481,  5.7586,  6.3480,  5.4802,  5.6798,  5.9028,\n",
      "         5.4962,  5.9336,  6.0329,  6.2861,  5.2169,  5.5299,  6.0939,  5.9211,\n",
      "         3.8323,  5.4365,  5.8944,  6.0094,  6.0177,  5.6209,  5.6850,  6.2233,\n",
      "         5.4157,  4.7825,  5.7321,  5.6887,  5.7404,  5.6773,  5.4225,  5.8670,\n",
      "         6.3293,  5.9119,  6.2331,  4.9898,  4.9076,  6.0051,  5.7050,  5.0615,\n",
      "         5.0289,  5.8065,  5.2101,  5.4340,  5.9941,  5.6751,  5.4966,  3.4099],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 5.2048e-02,  5.7168e-02,  3.0671e-03,  7.6395e-03,  4.9073e-02,\n",
      "         -3.1210e-02,  6.1013e-02, -2.0448e-02, -4.3302e-02, -9.0638e-02,\n",
      "         -3.1305e-02,  8.1594e-03, -1.9025e-02,  6.1880e-02,  3.3327e-02,\n",
      "         -3.2257e-02, -5.1502e-02, -6.3603e-02,  4.7971e-02,  1.8586e-02,\n",
      "         -1.0727e-02, -9.0833e-02, -2.3802e-02, -4.8193e-02, -2.7362e-02,\n",
      "          4.1894e-02,  2.4672e-02,  3.4746e-02, -2.1268e-03,  6.4929e-02,\n",
      "         -4.7017e-02, -5.3023e-02,  7.5545e-02, -8.5475e-03, -1.8681e-02,\n",
      "          1.9075e-02, -3.6381e-02, -3.6849e-02,  6.7679e-02, -6.5267e-03,\n",
      "          5.4711e-02, -9.1222e-02, -8.6994e-03,  6.5640e-02,  7.8718e-02,\n",
      "         -1.8303e-02, -3.3819e-02, -5.8391e-02,  6.2718e-02, -7.2402e-02,\n",
      "         -2.5109e-02,  8.3493e-03,  6.9597e-02,  1.1976e-02, -2.4708e-02,\n",
      "         -3.9660e-02,  9.0227e-03,  4.8732e-03, -5.2985e-02,  5.2058e-02,\n",
      "         -4.4509e-02, -6.5427e-03, -7.9234e-02, -2.9316e-02, -4.5654e-02,\n",
      "          7.0541e-02, -2.9225e-02, -4.6966e-02,  1.9730e-02,  4.3822e-02,\n",
      "         -7.7667e-02,  3.8662e-02, -3.4669e-02,  2.9455e-02,  4.8630e-02,\n",
      "         -1.8747e-03, -4.0188e-02,  8.0200e-02,  3.1501e-02,  2.1120e-03,\n",
      "         -3.8209e-02, -1.7818e-02, -2.3472e-02,  2.2498e-02, -2.3097e-02,\n",
      "          2.0090e-03, -1.2906e-02, -9.4848e-03, -9.5199e-04, -3.1765e-02,\n",
      "          6.8914e-02, -2.4542e-02,  5.6995e-02,  7.3282e-02,  8.2201e-02,\n",
      "          1.6637e-02,  4.4703e-03,  6.5569e-03,  8.0486e-02,  8.2753e-02,\n",
      "          4.9726e-02,  4.7019e-03,  2.7874e-02, -2.8243e-02,  2.3701e-02,\n",
      "         -2.1732e-02, -8.0983e-02, -2.4880e-02,  1.1293e-02,  4.1167e-02,\n",
      "          1.5900e-02, -2.6957e-02, -2.3409e-03, -2.8293e-02,  6.9298e-02,\n",
      "          8.6540e-02,  2.7610e-02, -5.4482e-02,  7.8792e-02, -1.0626e-02],\n",
      "        [-4.3524e-02,  1.8156e-02,  8.7717e-02, -8.0032e-03, -8.4494e-02,\n",
      "          3.1344e-03, -6.1531e-02,  2.6168e-02, -4.7147e-02,  8.0173e-02,\n",
      "         -6.0429e-02, -1.4185e-02,  4.8383e-02,  9.0423e-02, -7.8544e-02,\n",
      "         -5.4365e-02, -6.4657e-02, -3.7951e-02, -6.7491e-02,  2.3218e-02,\n",
      "          3.1010e-02, -7.9554e-02,  8.9294e-02, -3.6038e-02, -6.0452e-02,\n",
      "         -8.4869e-02, -5.4067e-02,  1.2882e-02, -8.3364e-03,  3.1729e-02,\n",
      "          5.0105e-02, -1.4880e-02,  7.0933e-02,  7.4586e-02,  4.5191e-02,\n",
      "          2.1829e-02, -4.9204e-02, -3.6967e-02, -4.4102e-02,  1.2695e-03,\n",
      "          5.5328e-02, -2.6898e-02,  2.8673e-02,  7.6428e-02, -4.9325e-02,\n",
      "         -3.9762e-02, -5.5717e-02, -5.3646e-02,  5.1280e-02,  6.5921e-02,\n",
      "         -7.3198e-02,  4.0161e-02, -7.5979e-02,  4.1211e-03, -3.6509e-02,\n",
      "         -2.1323e-02,  1.8490e-03, -2.8367e-02, -6.8964e-02,  8.1847e-02,\n",
      "         -7.5619e-03, -5.9387e-02,  2.3774e-02, -1.0436e-03,  4.6711e-02,\n",
      "          3.2849e-02,  3.5498e-02, -6.8003e-02, -4.6913e-02,  6.8553e-02,\n",
      "          8.9541e-02,  7.6555e-02,  7.4681e-02,  5.8823e-02,  4.2131e-02,\n",
      "         -3.8928e-02, -6.3926e-02,  4.8324e-02, -3.6324e-02,  4.3068e-02,\n",
      "         -6.4895e-02, -5.1385e-02,  2.1957e-02,  2.9131e-02, -1.3269e-03,\n",
      "         -4.4508e-02,  8.5692e-02,  5.5540e-02,  6.6900e-02,  4.8650e-02,\n",
      "         -7.7568e-02, -6.1602e-02,  2.7745e-02,  2.5610e-02, -6.8598e-02,\n",
      "         -8.6124e-02, -8.1652e-02, -1.2628e-02, -6.9427e-02,  1.2360e-02,\n",
      "          7.5987e-02, -8.5291e-02, -2.0682e-02, -5.1821e-03, -2.8576e-02,\n",
      "         -3.9674e-02,  6.3789e-02,  6.9545e-02,  6.2593e-02,  8.1183e-03,\n",
      "          9.0493e-02,  8.0044e-02, -4.6054e-02, -1.4563e-02, -4.1635e-02,\n",
      "         -7.2867e-02,  3.0071e-02,  8.3747e-02, -2.4794e-02,  5.5450e-02],\n",
      "        [ 2.5866e-02,  7.9405e-02, -1.3064e-02,  6.1466e-02, -4.9401e-02,\n",
      "          5.4963e-02,  7.8275e-02, -2.8372e-02, -8.8238e-02,  1.7308e-02,\n",
      "          7.1522e-02, -2.3194e-03, -3.3775e-03, -2.5514e-02,  4.9239e-02,\n",
      "         -1.2804e-02,  1.4174e-02,  9.0242e-02,  1.7368e-03, -1.6282e-02,\n",
      "         -4.2620e-04,  6.4290e-02,  5.5246e-02,  5.0454e-02,  5.6458e-02,\n",
      "         -8.4477e-02, -4.5525e-03,  2.8170e-02, -5.2229e-02, -2.4550e-02,\n",
      "         -5.0321e-02, -2.2997e-02,  6.1522e-02, -1.9320e-02, -7.3430e-02,\n",
      "         -5.7278e-02,  5.0821e-02, -1.2768e-02,  7.8118e-02, -3.1079e-02,\n",
      "          7.7682e-02, -1.6414e-02,  6.5233e-02, -2.3002e-02, -6.5459e-02,\n",
      "         -8.0094e-02, -4.3684e-02,  7.1106e-02, -6.8804e-02, -7.2889e-02,\n",
      "          6.7741e-02,  8.4391e-02,  7.6311e-02,  7.9899e-02,  8.8938e-02,\n",
      "          8.3535e-02, -8.2436e-02, -3.5915e-03,  3.6392e-02,  2.2132e-02,\n",
      "          3.1260e-02,  6.6301e-02,  8.7614e-02,  2.0461e-02,  6.1568e-02,\n",
      "         -4.9511e-02,  3.3442e-02,  7.8538e-02,  2.1029e-02,  4.0005e-02,\n",
      "         -2.1998e-02, -9.1218e-02,  5.1963e-02, -4.5896e-02,  2.2432e-02,\n",
      "         -1.4790e-02,  4.9881e-02,  8.4726e-02, -1.6578e-02, -2.9716e-02,\n",
      "          8.3143e-02, -7.5306e-02,  2.5014e-02, -8.9941e-02, -3.9233e-02,\n",
      "          2.3438e-02, -7.7145e-04,  4.2188e-03, -8.8095e-02, -6.7135e-02,\n",
      "          4.9220e-02, -1.2141e-02, -6.6777e-02, -1.5566e-02, -2.3173e-02,\n",
      "         -5.1371e-02, -2.5879e-02, -8.1535e-02, -4.1013e-02, -4.7851e-02,\n",
      "         -6.6397e-02,  8.7765e-02,  8.3044e-02, -4.6970e-02, -2.6849e-02,\n",
      "          7.5462e-02,  7.9259e-03, -1.6544e-02, -6.8735e-02, -4.4598e-02,\n",
      "          4.9845e-04, -3.7067e-02, -8.1111e-02,  5.1616e-02,  6.2984e-02,\n",
      "          4.7545e-02,  3.6439e-02,  1.2973e-02, -5.8141e-02, -4.1015e-02],\n",
      "        [-7.0770e-02, -2.3912e-02, -5.4811e-02, -6.8518e-02,  5.3705e-02,\n",
      "         -1.7017e-02, -3.7094e-02, -3.2247e-02, -7.7896e-02,  8.7733e-02,\n",
      "         -7.5835e-02,  1.9084e-02, -1.7063e-02,  5.7932e-02,  7.0905e-02,\n",
      "          3.0538e-02,  7.1833e-02, -1.8382e-02, -8.6879e-02, -2.4391e-03,\n",
      "         -1.8961e-04,  6.5857e-02, -1.3579e-02, -8.7461e-02,  7.2151e-03,\n",
      "         -4.0687e-02, -4.0339e-02, -8.6915e-03,  6.8748e-02,  3.1219e-02,\n",
      "          6.6404e-02, -4.3203e-02, -9.0433e-02, -1.6474e-02,  9.1238e-02,\n",
      "          7.1505e-03, -4.5745e-02,  1.5214e-02,  7.8110e-02, -9.1125e-02,\n",
      "         -5.9617e-02,  3.8887e-02, -7.7777e-02,  3.2791e-02, -8.0137e-02,\n",
      "         -3.3836e-02,  5.9641e-03, -5.0180e-02,  6.7538e-02,  7.5382e-02,\n",
      "         -2.5732e-02, -5.5171e-02,  6.5320e-02,  4.2669e-02,  7.9218e-02,\n",
      "         -5.5700e-02, -8.2238e-02,  8.0124e-03,  3.1590e-03, -2.5704e-02,\n",
      "         -5.6741e-03,  4.9395e-04,  1.2604e-02,  3.7754e-02, -5.2252e-02,\n",
      "         -6.3988e-05, -1.9274e-02,  6.4993e-02,  3.3794e-02, -3.0669e-02,\n",
      "          8.3265e-02, -8.6009e-03, -2.9982e-02,  2.9370e-02,  5.1132e-02,\n",
      "         -2.7723e-03, -6.1257e-02,  5.1481e-03,  7.6030e-02, -1.1829e-02,\n",
      "          9.0769e-02, -3.5690e-02, -8.0731e-02,  3.9631e-02, -4.0781e-02,\n",
      "         -6.1122e-02, -2.0530e-02,  2.9503e-02, -8.8690e-02,  7.4762e-02,\n",
      "          4.4173e-02, -5.6355e-02, -8.4142e-02,  4.5109e-02,  5.6040e-02,\n",
      "          2.7298e-02, -4.0730e-02, -3.6586e-02, -6.3828e-02,  7.0806e-03,\n",
      "          5.3247e-02, -2.2195e-02,  3.6842e-02,  8.9839e-02, -4.8552e-03,\n",
      "         -8.5568e-02, -5.2697e-04, -7.2547e-02,  6.8513e-02,  8.0152e-02,\n",
      "         -4.0386e-02,  3.2646e-02, -6.3814e-02, -2.5210e-02,  5.9936e-02,\n",
      "         -7.8991e-02, -8.7769e-02,  3.5507e-02,  5.9511e-02, -8.5726e-02],\n",
      "        [ 4.8205e-02,  4.3737e-02,  5.5488e-02,  5.3117e-02, -6.8854e-02,\n",
      "         -1.3530e-02,  6.8259e-02, -2.2976e-02,  5.1076e-02,  4.4870e-02,\n",
      "         -5.5534e-02,  8.6940e-02,  7.3180e-02,  4.7798e-02,  7.6495e-02,\n",
      "         -6.4625e-02,  4.9923e-02,  4.1502e-02, -5.2620e-02, -1.0773e-03,\n",
      "          1.4424e-02, -1.8933e-02, -7.4799e-02,  2.4285e-02, -4.0002e-03,\n",
      "         -7.5474e-02, -7.4636e-03, -4.3678e-02, -2.8748e-02,  5.2301e-02,\n",
      "         -3.6687e-02,  8.5370e-02,  5.5577e-03, -1.4829e-02, -4.3796e-02,\n",
      "          7.7980e-02,  3.8031e-03, -5.7689e-02, -2.8371e-02, -1.1608e-02,\n",
      "          4.0987e-03, -6.7840e-02, -3.0961e-02, -6.1162e-02,  2.4371e-02,\n",
      "          6.4245e-02, -2.8524e-03, -3.7693e-03,  6.6016e-02, -7.4197e-02,\n",
      "         -6.9313e-02, -6.9040e-02,  6.8146e-02,  2.4434e-02,  3.0487e-02,\n",
      "          8.0080e-03, -2.9285e-03,  8.8121e-02, -6.8492e-02, -7.0580e-02,\n",
      "          4.6320e-02,  1.0411e-02, -3.9706e-02, -6.7674e-02, -3.0895e-02,\n",
      "          8.5238e-02,  8.3126e-02, -2.1042e-02, -5.0605e-02,  5.7306e-02,\n",
      "         -8.0013e-02,  3.9645e-02, -8.2224e-02,  6.4132e-02, -4.4297e-02,\n",
      "         -6.4583e-02,  6.2014e-03, -3.9227e-03,  2.5545e-02, -9.0680e-02,\n",
      "         -6.4731e-02, -8.2919e-02, -3.7788e-02,  9.1263e-02,  6.1929e-02,\n",
      "         -6.1832e-02,  1.5195e-02,  2.7057e-02,  1.3705e-02,  1.9044e-02,\n",
      "         -1.6152e-02, -5.4144e-02, -4.3002e-02, -6.1121e-02, -6.3252e-02,\n",
      "          6.1997e-02, -2.0995e-02,  1.8157e-03, -7.9598e-02,  6.1480e-02,\n",
      "         -3.8141e-03,  6.0415e-02, -2.9940e-02, -2.6105e-02, -2.5436e-02,\n",
      "          1.5003e-02,  7.9838e-02, -3.8747e-02,  8.9901e-02, -3.8925e-02,\n",
      "          1.9897e-02, -3.8338e-02,  4.5365e-02, -3.5450e-02,  4.2659e-02,\n",
      "         -3.1645e-02,  9.0453e-02, -7.4355e-02,  8.6922e-02, -3.2312e-02],\n",
      "        [ 7.8611e-02,  5.0023e-02,  3.1569e-02,  7.4060e-02,  3.4692e-02,\n",
      "         -3.8813e-02, -1.3483e-03,  6.5831e-02,  8.4281e-02, -6.6874e-02,\n",
      "         -2.8215e-02, -8.0277e-02, -4.5427e-02, -4.2700e-02,  4.4488e-02,\n",
      "         -8.3317e-02, -6.9992e-02, -3.6561e-02, -8.5886e-03,  5.8279e-04,\n",
      "          2.4995e-02,  5.7487e-02, -5.3198e-02,  9.0176e-03,  8.1387e-04,\n",
      "         -7.6916e-02,  8.6139e-02, -8.7993e-02,  8.3792e-02,  5.5021e-02,\n",
      "         -7.8043e-02, -1.0653e-02, -6.6272e-02,  7.9602e-02,  3.5707e-02,\n",
      "         -4.4018e-03,  2.7479e-02,  1.6917e-02, -5.4429e-02, -6.8105e-02,\n",
      "         -2.8116e-02,  3.8720e-02, -8.8406e-02,  2.8788e-02,  7.8700e-02,\n",
      "          3.1168e-02,  5.0301e-02,  7.2557e-02,  7.7166e-02,  5.1948e-02,\n",
      "         -1.3104e-02,  1.7543e-03, -4.1497e-02, -6.6288e-02,  2.4006e-02,\n",
      "         -6.7753e-02, -4.6466e-02,  7.9295e-02,  4.0833e-02,  6.2728e-02,\n",
      "         -5.4178e-02,  2.7313e-02, -7.6762e-02,  7.8231e-02, -4.7464e-02,\n",
      "         -3.3610e-02,  3.9732e-02,  8.1485e-02, -4.1542e-02,  2.6831e-02,\n",
      "         -2.6617e-02, -4.0696e-02,  6.0360e-02, -5.9014e-02,  3.0745e-02,\n",
      "         -8.7112e-03,  4.4217e-02,  8.5085e-02,  5.0352e-02,  8.7947e-02,\n",
      "         -7.5486e-02, -2.8877e-02,  7.8583e-02, -8.3780e-02,  9.0747e-03,\n",
      "         -7.3395e-04, -3.2927e-02,  1.5885e-02, -2.2105e-02,  7.7039e-02,\n",
      "         -4.3619e-02, -1.2271e-02, -4.4721e-02,  4.5569e-02,  6.4266e-02,\n",
      "         -3.8143e-02, -9.6399e-03, -5.2199e-02, -8.3583e-02, -6.2214e-02,\n",
      "          5.2032e-02, -2.3657e-02, -4.2494e-02,  2.2489e-02,  6.0503e-02,\n",
      "          8.6333e-02, -4.2516e-02, -8.4919e-02,  2.6706e-02, -6.3692e-02,\n",
      "          8.8765e-02,  1.0036e-02, -5.0520e-02, -5.6726e-02, -2.9083e-02,\n",
      "         -5.2542e-02,  8.8274e-02, -2.4867e-03, -8.9578e-02, -7.2576e-02],\n",
      "        [ 8.9572e-02, -2.3469e-02,  6.2593e-02, -9.8068e-03,  4.4818e-02,\n",
      "         -1.7806e-02,  7.7258e-02, -8.7955e-02,  2.0717e-02,  3.1078e-02,\n",
      "          5.9258e-02,  7.0024e-02,  1.4849e-02, -5.8861e-02, -1.1326e-02,\n",
      "         -6.5491e-02,  2.6469e-02, -2.5098e-02,  5.2014e-02,  1.3741e-02,\n",
      "         -1.3696e-02,  1.1992e-02, -5.7642e-02, -4.6052e-02, -3.6499e-02,\n",
      "         -3.3648e-02, -6.9920e-02, -3.0771e-02,  7.1880e-02,  8.9643e-02,\n",
      "          7.9920e-03,  1.2432e-02, -8.8859e-02, -2.9080e-02,  8.1511e-02,\n",
      "          3.3576e-02,  2.4451e-02, -8.1077e-02, -4.2202e-02,  6.4616e-02,\n",
      "          3.9724e-02, -5.9294e-02,  1.7537e-02,  2.1365e-02, -3.0884e-02,\n",
      "         -7.9861e-02,  5.2485e-02, -7.4168e-02, -8.7185e-02, -7.2453e-02,\n",
      "         -6.0077e-02, -6.7730e-02, -5.2165e-03, -6.1453e-02, -4.5829e-03,\n",
      "         -8.4484e-02, -3.0384e-02,  2.6585e-02,  8.9533e-02,  7.0750e-02,\n",
      "         -8.6600e-02, -5.7153e-02,  7.1428e-02, -5.2724e-02,  4.8475e-02,\n",
      "         -2.1627e-02, -6.5967e-02, -4.3553e-02, -1.8233e-02, -6.9964e-02,\n",
      "         -7.8359e-02, -6.5171e-02, -6.6544e-03, -4.6473e-02, -3.8572e-02,\n",
      "         -3.3438e-02,  7.8866e-02,  6.8237e-02,  7.6397e-02, -8.9299e-03,\n",
      "          2.1678e-02, -4.4581e-02,  8.8017e-02, -1.1254e-02, -1.0109e-02,\n",
      "          6.7245e-02,  8.8134e-02,  6.3138e-02,  2.1344e-02,  6.6490e-02,\n",
      "         -2.1411e-02, -4.8203e-03, -8.1019e-02,  8.3602e-03,  7.8788e-02,\n",
      "          6.3118e-02,  4.8445e-02,  2.6266e-02,  6.5160e-02,  3.6574e-02,\n",
      "         -4.3626e-02,  3.6461e-02, -7.2867e-02,  8.9647e-02, -4.8377e-02,\n",
      "         -2.7007e-02,  6.1859e-02, -7.2727e-02,  7.8305e-02, -1.4361e-02,\n",
      "          5.1413e-02,  7.4190e-02, -6.3165e-02, -8.4459e-02,  5.3581e-02,\n",
      "         -2.6601e-02, -8.2763e-02,  4.4337e-02,  6.2857e-02, -6.2388e-03],\n",
      "        [ 7.1211e-02,  6.6658e-02, -5.9428e-02, -4.2631e-02,  6.9394e-02,\n",
      "         -7.0819e-03, -3.5744e-02, -3.8549e-02, -6.2739e-02,  2.1319e-02,\n",
      "          5.5233e-02,  6.3317e-02, -3.8381e-02, -2.7255e-02, -1.4849e-02,\n",
      "          4.3682e-02, -4.0378e-02, -6.4587e-02,  6.8455e-02,  1.8486e-02,\n",
      "          6.8346e-02, -4.1347e-02, -1.5260e-02,  7.0921e-02, -8.5917e-02,\n",
      "         -6.5156e-02,  8.3500e-02, -2.0661e-02,  4.1561e-03,  1.4112e-02,\n",
      "         -4.4986e-02,  6.9690e-02,  1.8034e-02,  6.4943e-02, -5.2788e-02,\n",
      "          1.4952e-02,  7.5921e-02, -1.7352e-02, -2.1763e-02, -7.8053e-02,\n",
      "          6.3141e-02,  2.5997e-02, -6.3720e-03, -6.5469e-02, -7.4798e-02,\n",
      "         -7.9742e-02,  8.2475e-02, -4.2604e-02,  1.9424e-02,  1.5992e-02,\n",
      "          6.4784e-02, -2.1470e-03, -2.1254e-02,  1.1253e-02, -6.4680e-02,\n",
      "         -4.9694e-02,  8.1259e-03, -4.4849e-02,  7.1986e-03, -2.9087e-02,\n",
      "          4.8122e-02,  1.0728e-02, -7.5657e-02, -5.1599e-03, -4.0601e-02,\n",
      "         -8.7286e-02, -3.1102e-02, -6.0128e-02, -6.4449e-02,  4.5978e-02,\n",
      "          3.2670e-02,  2.4521e-02, -5.6797e-02,  2.0140e-02,  7.6288e-02,\n",
      "          4.9833e-02,  4.5984e-02,  5.2504e-02, -7.8745e-02,  2.0454e-02,\n",
      "         -5.5656e-02,  7.4411e-02,  5.9370e-02,  8.0931e-02, -5.5604e-02,\n",
      "          9.1125e-02,  8.0322e-02, -1.8972e-02,  2.6521e-03,  6.9167e-02,\n",
      "          6.1412e-02, -4.3952e-02, -1.4217e-02,  3.2643e-02, -6.6850e-02,\n",
      "          8.5213e-02, -8.7090e-02,  3.5398e-02,  3.6040e-02,  4.0841e-02,\n",
      "         -3.1422e-02,  5.7057e-02, -3.8533e-02,  8.3216e-02,  7.1672e-02,\n",
      "          4.7856e-02, -1.6386e-02, -2.1815e-02, -3.0048e-02,  3.7858e-02,\n",
      "         -4.7731e-02, -4.1232e-02, -1.9629e-02,  9.6535e-03,  2.4985e-02,\n",
      "         -5.5127e-02, -6.7492e-02, -8.4300e-02, -1.8194e-02, -4.7798e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([42.3184], device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0441,  0.0258, -0.0282,  ..., -0.0414, -0.0190,  0.0233],\n",
      "        [-0.0542,  0.0434,  0.0127,  ..., -0.0372,  0.0499, -0.0142],\n",
      "        [ 0.0202,  0.0409,  0.0290,  ...,  0.0512,  0.0218,  0.0059],\n",
      "        ...,\n",
      "        [-0.0352,  0.0523,  0.0129,  ...,  0.0277,  0.0386, -0.0418],\n",
      "        [ 0.0217, -0.0363, -0.0308,  ..., -0.0498, -0.0290,  0.0338],\n",
      "        [ 0.0336, -0.0001,  0.0002,  ...,  0.0065,  0.0095, -0.0057]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3728, 0.3288, 0.3352, 0.3486, 0.3349, 0.3598, 0.3332, 0.3743, 0.3607,\n",
      "        0.3608, 0.3493, 0.3413, 0.3472, 0.3705, 0.3606, 0.3844, 0.3442, 0.3316,\n",
      "        0.3745, 0.3782, 0.3648, 0.3632, 0.3656, 0.3679, 0.3539, 0.3629, 0.3428,\n",
      "        0.3522, 0.3557, 0.3754, 0.3544, 0.3728, 0.3568, 0.3300, 0.3560, 0.3341,\n",
      "        0.3647, 0.3604, 0.3673, 0.3803, 0.3544, 0.3508, 0.3773, 0.3519, 0.3517,\n",
      "        0.3645, 0.3624, 0.3699, 0.3602, 0.3785, 0.3485, 0.3520, 0.3345, 0.3788,\n",
      "        0.3731, 0.3589, 0.3618, 0.3501, 0.3594, 0.3596, 0.3496, 0.3605, 0.3691,\n",
      "        0.3501, 0.3267, 0.3712, 0.3835, 0.3744, 0.3506, 0.3795, 0.3409, 0.3757,\n",
      "        0.3437, 0.3454, 0.3302, 0.3537, 0.3520, 0.3517, 0.3707, 0.3527, 0.3789,\n",
      "        0.3704, 0.3710, 0.3437, 0.3550, 0.3713, 0.3618, 0.3343, 0.3483, 0.3752,\n",
      "        0.3485, 0.3693, 0.3498, 0.3647, 0.3279, 0.3624, 0.3378, 0.3687, 0.3791,\n",
      "        0.3296, 0.3351, 0.3510, 0.3342, 0.3466, 0.3335, 0.3703, 0.3547, 0.3648,\n",
      "        0.3517, 0.3506, 0.3423, 0.3676, 0.3654, 0.3798, 0.3392, 0.3672, 0.3650,\n",
      "        0.3950, 0.3457, 0.3324, 0.3859, 0.3287, 0.3571, 0.3581, 0.3673, 0.3471,\n",
      "        0.3709, 0.3465, 0.3691, 0.3753, 0.3707, 0.3629, 0.3920, 0.3475, 0.3501,\n",
      "        0.3756, 0.3313, 0.3799, 0.3551, 0.3773, 0.3537, 0.3471, 0.3676, 0.3467,\n",
      "        0.3789, 0.3533, 0.3645, 0.3492, 0.3568, 0.3622, 0.3652, 0.3698, 0.3545,\n",
      "        0.3713, 0.3772, 0.3670, 0.3486, 0.3634, 0.3763, 0.3669, 0.3579, 0.3427,\n",
      "        0.3748, 0.3726, 0.3763, 0.3467, 0.3324, 0.3689, 0.3724, 0.3849, 0.3888,\n",
      "        0.3451, 0.3499, 0.3646, 0.3448, 0.3650, 0.3552, 0.3735, 0.3179, 0.4017,\n",
      "        0.3712, 0.3554, 0.3713, 0.3552, 0.3710, 0.3481, 0.3732, 0.3643, 0.3488,\n",
      "        0.3635, 0.3876, 0.3503, 0.3697, 0.3527, 0.3707, 0.3786, 0.3592, 0.3582,\n",
      "        0.3565, 0.3744, 0.3586, 0.3655, 0.3645, 0.3417, 0.3423, 0.3657, 0.3538,\n",
      "        0.3385, 0.3791, 0.3782, 0.3546, 0.3443, 0.3629, 0.3695, 0.3781, 0.3442,\n",
      "        0.3537, 0.3439, 0.3828, 0.3509, 0.3638, 0.3675, 0.3444, 0.3420, 0.3199,\n",
      "        0.3568, 0.3560, 0.3610, 0.3655, 0.3635, 0.3824, 0.3613, 0.3562, 0.3397,\n",
      "        0.3508, 0.3664, 0.3726, 0.3446, 0.3394, 0.3548, 0.3568, 0.3578, 0.3640,\n",
      "        0.3753, 0.3338, 0.3564, 0.3407, 0.3824, 0.3714, 0.3630, 0.3771, 0.3614,\n",
      "        0.3687, 0.3610, 0.3367, 0.3853, 0.3651, 0.3626, 0.3480, 0.3670, 0.3690,\n",
      "        0.3509, 0.3174, 0.3289, 0.3684, 0.3723, 0.3622, 0.3540, 0.3470, 0.3662,\n",
      "        0.3703, 0.3812, 0.3588, 0.3569, 0.3665, 0.3510, 0.3430, 0.3498, 0.3598,\n",
      "        0.3424, 0.3458, 0.3444, 0.3512, 0.3420, 0.3542, 0.3507, 0.3348, 0.3439,\n",
      "        0.3674, 0.3276, 0.3654, 0.3794, 0.3912, 0.3569, 0.3489, 0.3308, 0.3587,\n",
      "        0.3507, 0.3561, 0.3618, 0.3588, 0.3530, 0.3718, 0.3502, 0.3640, 0.3716,\n",
      "        0.3090, 0.3499, 0.3634, 0.3604, 0.3542, 0.3844, 0.3656, 0.3375, 0.3653,\n",
      "        0.3889, 0.3376, 0.3450, 0.3868, 0.3681], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0106, -0.0076,  0.0223,  ...,  0.0280, -0.0365, -0.0242],\n",
      "        [-0.0482,  0.0400,  0.0441,  ...,  0.0348, -0.0387,  0.0028],\n",
      "        [ 0.0245,  0.0228, -0.0132,  ...,  0.0440,  0.0207, -0.0142],\n",
      "        ...,\n",
      "        [-0.0328,  0.0476,  0.0040,  ..., -0.0220,  0.0240, -0.0199],\n",
      "        [ 0.0543,  0.0392,  0.0255,  ...,  0.0165,  0.0323, -0.0176],\n",
      "        [ 0.0310,  0.0473,  0.0327,  ...,  0.0034, -0.0040,  0.0276]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3659, 0.3285], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0166,  0.0334, -0.0216,  ...,  0.0451,  0.0140,  0.0470],\n",
      "        [-0.0050, -0.0374,  0.0145,  ...,  0.0178,  0.0414, -0.0164],\n",
      "        [ 0.0582,  0.0066,  0.0150,  ..., -0.0125, -0.0309, -0.0224],\n",
      "        ...,\n",
      "        [-0.0425, -0.0114, -0.0110,  ..., -0.0163, -0.0303, -0.0151],\n",
      "        [ 0.0105,  0.0111,  0.0324,  ...,  0.0573,  0.0321,  0.0549],\n",
      "        [ 0.0162,  0.0118,  0.0065,  ..., -0.0023, -0.0302,  0.0265]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0055, -0.0038, -0.0023,  ..., -0.0030,  0.0067, -0.0031],\n",
      "        [ 0.0037, -0.0042,  0.0023,  ..., -0.0014,  0.0024, -0.0053],\n",
      "        [ 0.0018, -0.0032,  0.0016,  ...,  0.0030,  0.0027, -0.0032],\n",
      "        ...,\n",
      "        [ 0.0068, -0.0050,  0.0014,  ...,  0.0007,  0.0056, -0.0056],\n",
      "        [ 0.0009,  0.0008,  0.0001,  ..., -0.0007,  0.0010, -0.0010],\n",
      "        [ 0.0078, -0.0092,  0.0038,  ..., -0.0084,  0.0090, -0.0072]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5781, 0.9380, 0.8810, 1.0034, 0.8481, 1.7174, 0.8671, 1.3904, 0.7458,\n",
      "        0.6136, 0.8258, 1.1228, 1.2860, 0.9294, 1.2252, 1.8986, 2.3417, 1.6395,\n",
      "        1.3645, 0.8315, 2.1615, 0.5545, 1.0976, 1.0542, 0.8393, 0.7461, 1.4128,\n",
      "        1.6747, 1.0309, 2.0690, 1.1873, 2.2221, 1.3514, 1.6382, 1.6749, 1.6716,\n",
      "        1.5405, 0.9793, 1.3858, 1.1163, 1.3737, 0.7339, 0.9232, 0.7380, 0.9516,\n",
      "        1.7471, 1.5831, 1.0943, 0.5185, 0.8935, 0.7901, 0.7830, 1.0878, 1.2823,\n",
      "        0.8936, 1.1069, 0.6559, 0.8414, 1.0328, 1.2238, 1.3882, 0.4918, 1.3842,\n",
      "        1.1690, 1.1811, 0.8861, 0.9250, 1.6848, 1.8678, 0.6564, 1.6004, 1.1144,\n",
      "        1.7794, 1.4154, 1.4328, 0.7771, 0.4649, 1.7412, 1.6237, 1.4719, 1.1625,\n",
      "        1.2526, 0.9506, 1.2672, 0.6535, 1.1785, 1.5515, 1.6863, 1.6618, 1.1450,\n",
      "        1.5636, 1.3663, 1.6743, 1.8258, 1.0859, 1.4956, 0.5586, 0.5480, 0.7039,\n",
      "        0.8974, 0.6160, 1.2445, 2.0413, 1.5246, 0.6321, 0.5451, 0.5244, 1.4073,\n",
      "        1.0519, 1.1232, 1.2425, 1.6271, 0.5083, 0.4487, 0.6903, 0.5152, 0.6827,\n",
      "        1.0657, 1.0803, 0.9261, 0.4821, 0.5724, 0.6329, 0.6731, 1.2346, 0.5810,\n",
      "        1.3939, 1.2778, 0.8621, 0.6227, 1.5995, 1.5483, 1.0640, 0.9094, 1.0304,\n",
      "        0.7673, 0.3518, 0.8590, 1.4034, 0.9894, 1.2100, 1.2424, 1.0736, 0.7414,\n",
      "        1.8432, 1.2705, 1.5713, 1.6018, 2.3064, 0.4681, 1.6157, 1.8393, 1.5930,\n",
      "        1.2409, 1.2079, 0.5100, 0.4490, 2.5976, 1.4510, 0.9533, 0.7602, 0.8358,\n",
      "        1.1147, 1.1125, 1.3625, 1.5598, 1.6062, 0.7548, 1.1065, 1.0544, 0.8764,\n",
      "        1.3388, 1.8498, 1.4044, 0.7995, 0.7323, 1.3197, 0.9306, 1.2140, 1.9356,\n",
      "        1.4630, 1.3273, 0.9079, 1.6765, 0.7334, 0.8340, 1.0203, 1.9387, 1.3108,\n",
      "        1.2339, 1.4802, 1.2829, 0.5848, 1.1483, 0.8851, 1.1760, 1.2554, 0.6573,\n",
      "        1.0915, 1.7114, 0.4940, 0.9335, 1.2350, 0.9497, 0.7572, 1.0408, 1.7910,\n",
      "        0.9918, 0.4455, 0.6446, 1.4806, 0.9793, 0.8805, 0.7316, 0.6827, 0.8224,\n",
      "        0.5102, 0.5934, 0.3624, 1.1159, 1.1965, 1.3283, 0.7260, 0.6464, 3.4053,\n",
      "        2.2901, 2.3311, 2.2952, 1.4580, 2.1617, 1.5380, 1.7401, 3.1828, 2.2811,\n",
      "        2.9342, 1.6726, 2.8793, 1.8531, 2.1284, 2.2579, 1.1446, 0.8999, 0.5652,\n",
      "        1.4491, 1.9114, 0.8356, 1.1037, 1.2081, 0.7086, 1.1526, 1.4437, 1.2170,\n",
      "        0.4932, 1.9583, 1.0639, 0.8395, 0.5814, 1.1335, 1.4862, 1.2808, 1.6693,\n",
      "        0.5444, 1.7827, 0.8300, 1.0345, 0.9410, 0.5992, 1.5749, 0.7214, 1.8798,\n",
      "        1.8934, 0.8592, 1.3194, 1.5670, 1.6973, 1.4009, 1.2297, 0.7797, 1.7292,\n",
      "        1.2633, 0.9124, 1.0149, 2.0366, 1.9604, 2.2720, 2.4784, 1.1142, 1.1153,\n",
      "        1.6392, 0.7841, 1.6992, 1.9092, 1.2693, 0.7161, 1.6118, 1.0746, 0.8692,\n",
      "        1.3310, 0.5648, 1.0068, 1.1442, 2.0366, 1.8991, 1.1043, 0.5321, 0.5352,\n",
      "        1.4374, 0.4779, 0.4692, 2.1670, 1.5448, 1.1224, 1.5176, 1.9543, 1.1220,\n",
      "        1.5198, 2.3907, 0.5596, 0.8637, 1.0465], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0165, -0.0429, -0.0060,  ...,  0.0507, -0.0398,  0.0252],\n",
      "        [-0.0075,  0.0394, -0.0070,  ...,  0.0241, -0.0053, -0.0419],\n",
      "        [ 0.0168, -0.0095,  0.0494,  ...,  0.0128, -0.0197, -0.0102],\n",
      "        ...,\n",
      "        [-0.0015, -0.0666,  0.0457,  ...,  0.0109, -0.0014, -0.0055],\n",
      "        [-0.0550, -0.0345, -0.0492,  ..., -0.0456,  0.0463, -0.0394],\n",
      "        [ 0.0390,  0.0153, -0.0074,  ...,  0.0418, -0.0385, -0.0390]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0036,  0.0052,  0.0053,  ..., -0.0097, -0.0029,  0.0071],\n",
      "        [ 0.0038, -0.0012, -0.0080,  ...,  0.0045,  0.0063, -0.0082],\n",
      "        [ 0.0050, -0.0035, -0.0012,  ...,  0.0016,  0.0077, -0.0053],\n",
      "        ...,\n",
      "        [ 0.0021,  0.0060,  0.0050,  ..., -0.0056, -0.0040,  0.0050],\n",
      "        [-0.0006,  0.0023,  0.0013,  ...,  0.0034,  0.0032, -0.0018],\n",
      "        [-0.0021, -0.0041,  0.0038,  ..., -0.0028, -0.0028,  0.0004]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8554, 1.4895, 1.3028, 1.4850, 1.3220, 1.5531, 0.8271, 1.3271, 1.1475,\n",
      "        1.3292, 1.5935, 1.8991, 1.3802, 1.2166, 1.2426, 1.8228, 2.2493, 1.5220,\n",
      "        1.4243, 1.9160, 1.6940, 1.8137, 1.1301, 1.0800, 1.4728, 1.1219, 2.2937,\n",
      "        2.0833, 2.0503, 1.5526, 1.1548, 2.2666, 2.3090, 2.3810, 2.4062, 1.5861,\n",
      "        1.5192, 1.2382, 1.6925, 1.1451, 1.2898, 1.9265, 1.5598, 1.8096, 1.3440,\n",
      "        1.7491, 1.8427, 1.1579, 0.9396, 0.8123, 1.3110, 1.8897, 1.6295, 1.5450,\n",
      "        0.9731, 1.2631, 1.0748, 1.5009, 1.6996, 1.2046, 2.1590, 1.0519, 1.3583,\n",
      "        1.6944, 1.7828, 0.8953, 1.7075, 1.9573, 1.9096, 1.7748, 1.6153, 1.0302,\n",
      "        1.2924, 2.1527, 2.0254, 1.7710, 1.3399, 1.3390, 1.7941, 1.8696, 2.0314,\n",
      "        2.3261, 1.5615, 1.7113, 1.5985, 1.2548, 2.0152, 1.9794, 2.3384, 1.5456,\n",
      "        2.3678, 1.8560, 1.2164, 1.7521, 0.9052, 2.0972, 0.8408, 1.0946, 1.2629,\n",
      "        1.0813, 1.2233, 1.6283, 1.9318, 1.8544, 0.9506, 0.9977, 1.1135, 1.5575,\n",
      "        1.4852, 1.6465, 1.5950, 1.6463, 0.5574, 0.8283, 0.8953, 0.8481, 1.3125,\n",
      "        1.7997, 1.1351, 0.7862, 0.6313, 0.7671, 1.1883, 1.6051, 1.8515, 1.0451,\n",
      "        1.3333, 1.8361, 1.4413, 0.8317, 1.2535, 1.3457, 0.6990, 0.6951, 1.0533,\n",
      "        0.9437, 0.4393, 1.1558, 1.2242, 0.8093, 0.7420, 0.5525, 1.0919, 0.6303,\n",
      "        2.1219, 1.7293, 1.9401, 1.9623, 1.4585, 2.6221, 1.5323, 1.8813, 2.3819,\n",
      "        2.2043, 2.3165, 1.9181, 2.1378, 2.0306, 1.5898, 0.9223, 1.6064, 1.8187,\n",
      "        1.8296, 1.6937, 1.9638, 1.5400, 2.1518, 0.7071, 1.4335, 1.4318, 1.6168,\n",
      "        2.2307, 1.7971, 1.7275, 1.0785, 0.7720, 1.2119, 1.5612, 2.1409, 2.1701,\n",
      "        1.7232, 1.5023, 1.2506, 1.8932, 1.8182, 1.6038, 1.4559, 2.0144, 1.7323,\n",
      "        1.5270, 1.2327, 1.4949, 0.7943, 1.5307, 1.1811, 1.3241, 1.7872, 1.0806,\n",
      "        1.1165, 2.1662, 0.9295, 0.8953, 1.4682, 1.7166, 1.4169, 1.5909, 2.0180,\n",
      "        1.3230, 0.9308, 0.8248, 1.6914, 1.1443, 1.0444, 0.6802, 0.8171, 0.8327,\n",
      "        0.8164, 0.9973, 0.4942, 1.2169, 1.2155, 0.7829, 0.6384, 0.8282, 2.9352,\n",
      "        2.7221, 2.8570, 1.7100, 2.3170, 1.3960, 1.4202, 1.6891, 2.2564, 1.7835,\n",
      "        1.8493, 2.2760, 1.4417, 2.2140, 2.0285, 1.8275, 2.1039, 1.6677, 1.4850,\n",
      "        1.6562, 1.5079, 1.3575, 1.3674, 1.3223, 0.9129, 1.8810, 2.2732, 1.9355,\n",
      "        1.7630, 1.9202, 0.9781, 0.9086, 1.4313, 0.8496, 1.8234, 1.6491, 1.6314,\n",
      "        1.2643, 1.4299, 0.7451, 1.0030, 1.6796, 1.2030, 1.6051, 1.5420, 1.6241,\n",
      "        2.1111, 0.9079, 1.8022, 1.8692, 1.1529, 1.5286, 1.7786, 2.7724, 1.5426,\n",
      "        1.3892, 0.9867, 1.4273, 2.7194, 2.1876, 1.9921, 1.5249, 1.2336, 1.0867,\n",
      "        2.2348, 2.0914, 1.8349, 2.1334, 1.3990, 1.1400, 1.7398, 1.1471, 2.0475,\n",
      "        1.8650, 2.2222, 1.8869, 1.6355, 1.9931, 1.9350, 1.2232, 2.1706, 1.8015,\n",
      "        2.1265, 1.5761, 2.5410, 1.8517, 1.9651, 1.5306, 1.7308, 2.9178, 1.7252,\n",
      "        1.7173, 1.5749, 2.1706, 0.9038, 0.9709], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0603, -0.0218, -0.0068,  ..., -0.0167, -0.0375, -0.0312],\n",
      "        [ 0.0415,  0.0047, -0.0518,  ..., -0.0298,  0.0606,  0.0297],\n",
      "        [-0.0475, -0.0578, -0.0411,  ..., -0.0627, -0.0183,  0.0135],\n",
      "        ...,\n",
      "        [-0.0383, -0.0225,  0.0200,  ...,  0.0065,  0.0555, -0.0102],\n",
      "        [-0.0626,  0.0349, -0.0150,  ..., -0.0289,  0.0383,  0.0023],\n",
      "        [ 0.0467,  0.0447, -0.0176,  ..., -0.0467,  0.0163,  0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0038,  0.0050,  0.0051,  ...,  0.0062,  0.0054, -0.0011],\n",
      "        [ 0.0036, -0.0023, -0.0024,  ..., -0.0026, -0.0023, -0.0005],\n",
      "        [ 0.0026, -0.0033, -0.0029,  ..., -0.0032, -0.0038, -0.0040],\n",
      "        ...,\n",
      "        [-0.0052,  0.0046,  0.0034,  ...,  0.0060,  0.0045,  0.0097],\n",
      "        [ 0.0031, -0.0035, -0.0034,  ..., -0.0036, -0.0037, -0.0069],\n",
      "        [-0.0004,  0.0016,  0.0029,  ...,  0.0032,  0.0018, -0.0049]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4779, 1.6100, 1.3984, 1.6248, 1.3495, 1.5395, 1.5952, 1.3381, 1.5166,\n",
      "        1.4928, 1.3234, 1.4069, 1.4897, 1.5043, 1.4669, 1.3926, 0.9284, 1.1032,\n",
      "        1.0268, 1.1331, 0.8843, 1.0847, 1.1989, 0.3960, 1.1988, 0.8615, 1.2154,\n",
      "        0.9848, 1.2416, 0.9289, 1.1681, 1.1767, 0.9173, 1.1089, 1.1159, 1.0196,\n",
      "        1.0651, 1.0959, 1.0802, 1.0411, 0.9911, 0.9620, 0.9963, 1.1976, 1.1100,\n",
      "        1.1809, 1.0585, 0.9647, 1.5709, 1.6095, 1.4278, 1.1241, 1.5002, 1.3877,\n",
      "        1.5135, 1.5872, 0.9992, 1.4182, 1.2469, 1.7639, 1.5388, 1.5035, 1.0324,\n",
      "        1.3794, 0.9311, 1.2885, 1.1548, 1.1513, 1.1763, 1.2538, 1.3836, 1.1470,\n",
      "        1.2840, 1.2896, 1.1526, 1.3172, 1.3033, 1.2057, 1.3094, 1.1875, 0.9729,\n",
      "        0.9964, 1.2613, 1.0507, 1.1919, 1.1811, 1.2482, 1.1844, 1.2927, 1.0171,\n",
      "        1.1856, 1.1153, 1.1109, 1.0839, 1.0697, 1.1313, 1.7171, 1.6534, 1.4589,\n",
      "        1.5465, 1.4902, 1.5129, 1.7512, 1.6650, 1.4577, 1.3656, 1.0594, 1.6626,\n",
      "        1.5866, 1.2692, 1.4491, 1.6939, 1.7323, 1.8150, 1.6443, 1.7916, 2.0522,\n",
      "        1.8221, 1.9812, 2.1412, 1.6793, 1.8741, 1.6607, 1.7580, 1.6132, 1.7925,\n",
      "        1.7478, 2.0980, 0.9980, 1.1497, 1.1992, 1.1056, 1.2188, 1.4863, 1.1973,\n",
      "        1.5491, 1.1248, 1.4478, 1.3296, 1.2136, 1.1094, 1.2158, 1.0938, 1.4256,\n",
      "        1.1505, 1.0338, 1.0874, 1.1198, 1.1033, 1.1023, 1.2018, 1.2265, 1.0976,\n",
      "        1.2046, 1.2175, 1.0980, 1.1139, 1.0126, 1.0803, 1.0511, 1.2350, 1.4687,\n",
      "        1.1730, 1.2832, 1.3386, 1.2571, 1.2993, 1.2599, 1.4957, 1.6620, 1.3422,\n",
      "        1.3501, 1.2536, 1.1321, 1.2732, 1.1794, 1.0206, 1.0279, 0.9040, 1.0476,\n",
      "        0.9771, 1.0714, 1.2965, 1.0549, 0.8686, 0.9931, 1.2096, 0.9947, 1.0988,\n",
      "        0.9439, 1.1584, 1.1346, 1.7626, 1.3895, 0.8676, 1.3186, 1.8746, 1.2668,\n",
      "        1.3170, 1.6817, 1.3688, 1.7848, 1.7198, 1.9518, 1.6939, 1.6945, 1.8290,\n",
      "        1.6854, 1.2821, 1.2600, 1.4028, 1.4748, 1.3751, 1.5552, 1.2916, 1.3938,\n",
      "        1.6010, 1.5252, 1.3271, 1.4299, 1.2276, 1.5736, 1.1577, 1.1832, 0.7396,\n",
      "        0.8216, 0.6791, 0.7921, 0.7226, 0.6878, 0.9175, 0.7889, 0.8877, 0.9555,\n",
      "        0.7431, 0.5829, 0.7252, 0.7660, 0.7083, 0.6740, 1.1863, 1.2566, 1.2494,\n",
      "        1.3626, 1.0394, 1.2673, 1.3064, 1.2073, 1.3201, 1.1610, 1.2064, 1.2719,\n",
      "        1.3808, 1.3588, 1.2123, 1.1850, 1.3220, 1.2601, 1.3602, 1.3423, 1.2562,\n",
      "        1.2878, 1.8088, 1.2438, 1.3187, 1.2057, 1.2843, 1.3196, 1.5538, 1.4199,\n",
      "        1.4046, 1.1221, 0.8885, 0.7986, 0.8883, 1.0481, 0.9815, 0.9581, 1.0058,\n",
      "        0.9692, 0.9327, 0.9217, 0.9465, 1.0906, 0.9138, 0.9838, 0.9365, 0.8729,\n",
      "        1.0533, 1.0072, 1.0009, 1.0309, 1.0453, 0.9003, 0.9753, 0.9765, 0.9818,\n",
      "        1.0682, 0.9236, 1.0460, 1.1479, 1.0297, 1.1383, 0.9606, 1.1639, 1.1019,\n",
      "        1.0513, 1.0305, 1.0950, 1.1817, 1.2169, 1.1741, 1.0757, 1.1868, 1.1692,\n",
      "        1.0849, 1.2722, 1.1229, 1.1976, 1.1887], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0545, -0.0337, -0.0006,  ...,  0.0036,  0.0354, -0.0427],\n",
      "        [ 0.0378, -0.0038,  0.0342,  ..., -0.0359,  0.0218, -0.0248],\n",
      "        [ 0.0131,  0.0238,  0.0322,  ..., -0.0399, -0.0014, -0.0477],\n",
      "        ...,\n",
      "        [-0.0089, -0.0456,  0.0546,  ...,  0.0336,  0.0512, -0.0336],\n",
      "        [ 0.0203,  0.0279, -0.0249,  ...,  0.0307,  0.0158,  0.0435],\n",
      "        [ 0.0509,  0.0428, -0.0524,  ...,  0.0034, -0.0609, -0.0059]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.1060e-03, -5.3223e-03, -3.1869e-03,  ..., -1.6076e-03,\n",
      "          2.2541e-03, -1.1239e-03],\n",
      "        [ 1.1213e-03,  8.0589e-03, -4.0964e-03,  ..., -6.7156e-03,\n",
      "          2.0670e-03,  1.2727e-03],\n",
      "        [ 1.4641e-03, -7.8956e-04,  8.2666e-04,  ...,  2.8086e-03,\n",
      "          2.1048e-03, -1.8843e-03],\n",
      "        ...,\n",
      "        [-4.1816e-06, -1.0448e-03, -3.0301e-03,  ..., -4.8863e-03,\n",
      "         -7.3261e-05,  1.5551e-04],\n",
      "        [-5.3613e-03,  6.0087e-03, -9.3571e-03,  ...,  9.3496e-04,\n",
      "          4.5371e-03,  9.8228e-03],\n",
      "        [-7.5162e-03, -2.5851e-03, -1.0178e-02,  ...,  6.2521e-04,\n",
      "          2.8680e-03,  4.6774e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3694, 1.3738, 1.4931, 1.5246, 1.4587, 1.5674, 1.3958, 1.3981, 1.2997,\n",
      "        1.4425, 1.3419, 1.5056, 1.5154, 1.3621, 1.3523, 1.3793, 1.4228, 1.3484,\n",
      "        1.4879, 1.3437, 1.4281, 1.3931, 1.4144, 1.3607, 1.3923, 1.4372, 1.4509,\n",
      "        1.4127, 1.4234, 1.4600, 1.3764, 1.2963, 1.4405, 1.2836, 1.3525, 1.4007,\n",
      "        1.4296, 1.3834, 1.4197, 1.4677, 1.4187, 1.4209, 1.3534, 1.4408, 1.4367,\n",
      "        1.4387, 1.8069, 1.3609, 1.5684, 1.4238, 1.4312, 1.3743, 1.4542, 1.5509,\n",
      "        1.3842, 1.2346, 1.3879, 1.4166, 1.3756, 1.3284, 1.5091, 1.6862, 1.2550,\n",
      "        1.5185, 1.3910, 1.3772, 1.4435, 1.4343, 1.3784, 1.3009, 1.4150, 1.4720,\n",
      "        1.3614, 1.5179, 1.5116, 1.4022, 1.4251, 1.3074, 1.3848, 1.4713, 1.3976,\n",
      "        1.3709, 1.3161, 1.4260, 1.4509, 1.5628, 1.3649, 1.4714, 1.3788, 1.3999,\n",
      "        1.4937, 1.3992, 1.4624, 1.4310, 1.3647, 1.4272, 1.2946, 1.4476, 1.3954,\n",
      "        1.3514, 1.3782, 1.4314, 1.3799, 1.2816, 1.5175, 1.3295, 1.5250, 1.4928,\n",
      "        1.4140, 1.3590, 1.3633, 1.6162, 1.4741, 1.4632, 1.5049, 1.4036, 1.3122,\n",
      "        1.4437, 1.3368, 1.5368, 1.5859, 1.4174, 1.3675, 1.3373, 1.3371, 1.4508,\n",
      "        1.3572, 1.4376, 1.3630, 1.3356, 1.3813, 1.5798, 1.4362, 1.4764, 1.3665,\n",
      "        1.4632, 1.3893, 1.3104, 1.3893, 1.6474, 1.3463, 1.5402, 1.3588, 1.3351,\n",
      "        1.1600, 1.3870, 1.3828, 1.5050, 1.3817, 1.3653, 1.4466, 1.5438, 1.6024,\n",
      "        1.3778, 1.3914, 1.2998, 1.4242, 1.3735, 1.4726, 1.4041, 1.3371, 1.4282,\n",
      "        1.3884, 1.4869, 1.4784, 1.4955, 1.5022, 1.4486, 1.4080, 1.3753, 1.4493,\n",
      "        1.5945, 1.4618, 1.3897, 1.4042, 1.4507, 1.3302, 1.3611, 1.3710, 1.4315,\n",
      "        1.3985, 1.5221, 1.3612, 1.3577, 1.4538, 1.3161, 1.4275, 1.2425, 1.4397,\n",
      "        1.3750, 1.3864, 1.3347, 1.4759, 0.9672, 1.3848, 1.3276, 1.3876, 1.2980,\n",
      "        1.3707, 1.3417, 1.4505, 1.3728, 1.3249, 1.3522, 1.3947, 1.4502, 1.3726,\n",
      "        1.4161, 1.4259, 1.4616, 1.4099, 1.3208, 1.3011, 1.4137, 1.5015, 1.4098,\n",
      "        1.3838, 1.4110, 1.3760, 1.3657, 1.4070, 1.3759, 1.4607, 1.3234, 1.3384,\n",
      "        1.4046, 1.3952, 1.4793, 1.3796, 1.4520, 1.4375, 1.3419, 1.4633, 1.3398,\n",
      "        1.4339, 1.3939, 1.4739, 1.3765, 1.4066, 1.4328, 1.4427, 1.4648, 1.5406,\n",
      "        1.4248, 1.4258, 1.4018, 1.3899, 1.4343, 1.4927, 1.3253, 1.3417, 1.4282,\n",
      "        1.4682, 1.4170, 1.3161, 1.4492, 1.4587, 1.6884, 1.4272, 1.3768, 1.4181,\n",
      "        1.4419, 1.4592, 1.3457, 1.4811, 1.4150, 1.5156, 1.4070, 1.4241, 1.4893,\n",
      "        1.2891, 1.2392, 1.4667, 1.3542, 1.2699, 1.3782, 1.4602, 1.4511, 1.4227,\n",
      "        1.4116, 1.4625, 1.4498, 1.3409, 1.6356, 1.4530, 1.2701, 1.2909, 1.5203,\n",
      "        1.3734, 1.5000, 1.3994, 1.7564, 1.3872, 1.4227, 1.4776, 1.3389, 1.2745,\n",
      "        1.4032, 1.4147, 1.4191, 1.3865, 1.4274, 1.3841, 1.3627, 1.5235, 1.4203,\n",
      "        1.3782, 1.4266, 1.5668, 1.3918, 1.4841, 1.4920, 1.4332, 1.4520, 1.5170,\n",
      "        1.4441, 1.3857, 1.3807, 1.3913, 1.3717], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0278,  0.0517, -0.0029,  ..., -0.0066,  0.0073, -0.0082],\n",
      "        [-0.0022,  0.0111,  0.0275,  ...,  0.0330,  0.0305, -0.0023],\n",
      "        [ 0.0293, -0.0020,  0.0492,  ...,  0.0388, -0.0326,  0.0079],\n",
      "        ...,\n",
      "        [-0.0323, -0.0679, -0.0361,  ..., -0.0015, -0.0311,  0.0038],\n",
      "        [-0.0308,  0.0577,  0.0179,  ...,  0.0326,  0.0108, -0.0236],\n",
      "        [-0.0212,  0.0578,  0.0249,  ...,  0.0281,  0.0572, -0.0304]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0007, -0.0069, -0.0030,  ..., -0.0034,  0.0058,  0.0043],\n",
      "        [ 0.0066, -0.0067,  0.0037,  ..., -0.0046,  0.0003,  0.0048],\n",
      "        [-0.0012,  0.0009,  0.0028,  ..., -0.0006, -0.0013, -0.0018],\n",
      "        ...,\n",
      "        [-0.0004,  0.0029, -0.0006,  ...,  0.0015, -0.0015, -0.0031],\n",
      "        [ 0.0038, -0.0024,  0.0034,  ..., -0.0016,  0.0012,  0.0035],\n",
      "        [-0.0070,  0.0072,  0.0053,  ...,  0.0091,  0.0006, -0.0085]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3945, 2.1267, 1.3979,  ..., 2.5729, 1.6373, 1.6067], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0103,  0.0048, -0.0084,  ..., -0.0180, -0.0004, -0.0065],\n",
      "        [ 0.0037, -0.0124,  0.0137,  ...,  0.0143,  0.0015, -0.0056],\n",
      "        [ 0.0171, -0.0045,  0.0238,  ...,  0.0182, -0.0278,  0.0178],\n",
      "        ...,\n",
      "        [ 0.0140,  0.0079, -0.0207,  ...,  0.0084,  0.0256, -0.0164],\n",
      "        [ 0.0230,  0.0298,  0.0135,  ...,  0.0192,  0.0226,  0.0237],\n",
      "        [ 0.0083,  0.0182, -0.0188,  ..., -0.0018, -0.0083, -0.0229]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0010, -0.0020, -0.0029,  ...,  0.0034,  0.0016, -0.0002],\n",
      "        [ 0.0034, -0.0001, -0.0017,  ...,  0.0021, -0.0033,  0.0006],\n",
      "        [-0.0024,  0.0011, -0.0050,  ...,  0.0057,  0.0027, -0.0085],\n",
      "        ...,\n",
      "        [ 0.0026,  0.0012, -0.0014,  ...,  0.0034, -0.0024, -0.0009],\n",
      "        [ 0.0007,  0.0008, -0.0051,  ...,  0.0028,  0.0024, -0.0020],\n",
      "        [ 0.0011, -0.0006,  0.0015,  ...,  0.0015, -0.0036, -0.0003]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.8367, 1.9825, 2.0247, 1.9824, 1.8086, 2.0153, 1.9604, 1.9412, 1.9751,\n",
      "        1.9506, 2.0397, 2.0157, 2.0717, 2.1163, 1.8896, 1.9685, 1.9345, 1.9509,\n",
      "        2.0707, 1.9903, 1.9688, 1.9107, 2.0157, 1.8081, 2.0458, 1.8460, 2.1016,\n",
      "        1.9765, 1.8149, 1.8403, 1.8763, 2.8101, 2.0665, 2.0146, 2.0435, 1.8724,\n",
      "        1.8910, 1.9944, 2.1080, 1.9507, 1.9515, 2.0405, 1.9426, 1.9974, 1.9159,\n",
      "        1.9914, 2.7763, 2.0025, 2.0290, 1.9679, 2.0595, 1.9564, 2.0085, 1.9253,\n",
      "        1.9844, 1.9248, 2.0372, 1.9360, 1.9708, 1.8841, 2.0075, 2.3284, 2.8756,\n",
      "        1.9883, 1.9245, 1.8859, 2.1170, 1.9658, 1.9226, 2.0139, 2.0067, 1.9996,\n",
      "        1.9546, 1.9254, 1.9950, 1.8683, 2.0417, 2.0271, 1.9815, 2.0323, 2.0816,\n",
      "        1.9984, 1.9343, 1.9369, 2.0097, 1.9362, 1.9505, 1.8704, 2.4248, 1.9857,\n",
      "        1.9274, 2.0264, 1.8644, 1.9655, 1.9702, 1.9756, 1.9155, 1.9348, 2.0333,\n",
      "        1.9214, 1.9012, 1.9553, 1.9487, 2.0018, 1.9379, 1.8685, 2.0534, 1.8785,\n",
      "        1.9077, 1.9878, 1.8083, 2.0184, 1.9489, 1.8903, 1.8604, 2.0773, 1.9352,\n",
      "        1.9331, 1.9840, 1.8587, 1.9826, 2.1162, 1.8571, 2.0589, 2.2031, 1.8545,\n",
      "        1.8789, 1.9130, 1.9294, 1.9880, 2.1448, 2.0217, 1.9003, 1.8966, 1.8060,\n",
      "        1.8807, 1.9816, 2.0328, 1.9519, 1.9153, 2.0063, 1.8907, 1.9127, 1.9280,\n",
      "        1.8756, 2.0353, 1.9334, 1.9265, 1.8416, 2.0150, 2.1188, 1.8657, 2.0950,\n",
      "        1.9986, 1.9727, 1.9700, 1.9499, 1.9187, 1.9014, 1.9283, 1.9393, 1.9298,\n",
      "        1.8946, 2.0319, 1.9781, 1.9864, 1.9236, 1.9429, 1.9084, 1.9925, 1.9249,\n",
      "        2.0103, 2.0346, 1.9971, 2.0053, 1.9800, 1.9864, 1.8477, 2.0069, 1.9961,\n",
      "        1.9335, 1.9284, 2.0168, 1.9961, 1.9600, 2.0486, 2.0139, 1.8986, 1.8981,\n",
      "        1.9427, 2.0531, 1.9700, 3.8612, 1.9087, 1.9324, 1.8819, 1.9265, 1.9380,\n",
      "        1.8677, 1.8782, 1.9420, 2.0183, 1.8887, 1.9880, 2.0004, 2.0287, 1.8382,\n",
      "        2.1220, 1.8760, 1.9089, 1.9911, 1.9915, 1.9160, 1.9259, 1.9315, 1.9124,\n",
      "        2.0280, 1.8618, 2.3752, 1.9974, 1.9823, 1.9477, 2.0203, 1.8172, 1.9970,\n",
      "        1.9891, 2.0693, 2.0034, 2.1622, 2.0110, 2.0814, 2.1794, 1.9466, 1.9411,\n",
      "        1.9659, 1.9920, 1.9663, 2.0309, 1.9129, 1.8853, 2.1399, 1.9566, 1.9565,\n",
      "        2.0394, 1.9211, 1.9402, 1.9134, 2.0106, 1.9826, 1.9238, 1.9479, 2.0513,\n",
      "        1.9019, 2.0286, 1.9858, 1.9809, 1.9709, 2.0092, 1.9321, 1.8655, 2.0386,\n",
      "        1.9391, 1.9720, 1.9744, 2.1704, 2.0221, 1.9912, 2.0025, 1.9693, 1.9742,\n",
      "        2.0177, 2.0043, 1.9591, 1.9449, 1.8850, 2.0356, 1.9111, 2.1592, 1.9764,\n",
      "        1.9195, 1.9586, 2.0027, 1.8573, 2.0935, 1.9926, 1.9958, 1.9575, 2.0480,\n",
      "        2.0608, 1.9610, 1.9299, 2.2504, 2.0060, 2.0901, 2.2086, 2.1563, 1.9111,\n",
      "        2.0253, 1.9184, 1.9313, 1.9549, 2.0501, 1.9490, 2.0525, 1.9021, 1.9335,\n",
      "        1.8763, 2.1008, 2.0338, 2.1124, 2.0423, 2.1468, 1.9594, 2.0204, 1.9462,\n",
      "        1.9023, 1.9096, 1.9280, 1.8698, 3.4732], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0324,  0.0306,  0.0340,  ...,  0.0257, -0.0132,  0.0191],\n",
      "        [ 0.0495, -0.0115,  0.0548,  ...,  0.0648, -0.0241, -0.0284],\n",
      "        [ 0.0406, -0.0534,  0.0176,  ...,  0.0357,  0.0329,  0.0190],\n",
      "        ...,\n",
      "        [ 0.0394, -0.0087,  0.0125,  ...,  0.0405, -0.0361,  0.0549],\n",
      "        [ 0.0013,  0.0538, -0.0102,  ..., -0.0085, -0.0305, -0.0246],\n",
      "        [-0.0638,  0.0286, -0.0332,  ..., -0.0323,  0.0588, -0.0213]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.2375e-03, -6.0486e-03, -5.8007e-03,  ..., -3.8991e-03,\n",
      "          2.0722e-03,  4.2244e-03],\n",
      "        [ 6.8181e-03, -8.6081e-03, -5.9554e-03,  ..., -6.9465e-03,\n",
      "          7.4997e-03,  7.0984e-03],\n",
      "        [ 3.7353e-03, -5.2006e-05, -1.6028e-03,  ...,  1.6755e-03,\n",
      "         -2.7660e-03, -2.5806e-03],\n",
      "        ...,\n",
      "        [-1.0672e-03,  2.1967e-04,  6.9025e-04,  ...,  2.0556e-03,\n",
      "         -1.7776e-03, -6.9325e-04],\n",
      "        [ 1.2143e-03, -2.5235e-03,  8.5789e-04,  ...,  1.7760e-04,\n",
      "          2.5069e-03,  2.6509e-03],\n",
      "        [-2.2883e-03,  2.4871e-03, -6.1675e-04,  ...,  4.9797e-04,\n",
      "         -2.0335e-03, -3.6368e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3128, 1.7167, 1.7633, 1.6965, 2.5145, 3.0025, 2.8865, 2.6567, 2.2900,\n",
      "        1.9591, 1.9557, 2.6176, 2.1796, 2.3960, 2.8129, 2.6060, 3.4378, 2.5051,\n",
      "        1.9049, 2.3496, 2.5051, 2.6703, 3.1370, 3.6245, 1.8849, 2.0335, 2.8626,\n",
      "        2.0085, 2.3781, 2.5609, 3.1588, 3.2013, 2.4350, 2.0933, 2.8101, 2.7865,\n",
      "        2.9856, 2.3845, 3.4018, 3.9525, 1.9254, 2.1177, 2.2155, 2.5734, 2.9010,\n",
      "        2.8550, 3.3666, 3.0290, 1.3556, 1.9217, 1.0049, 1.2955, 2.2286, 2.3836,\n",
      "        2.7469, 2.6633, 3.4015, 1.9175, 2.7144, 2.7300, 2.5672, 2.4810, 3.0107,\n",
      "        2.9416, 1.9292, 2.0041, 3.3895, 2.7144, 2.4887, 3.1331, 2.7247, 3.2975,\n",
      "        2.3327, 1.2269, 1.0403, 1.5292, 1.9947, 2.4403, 3.1497, 3.0174, 2.3085,\n",
      "        2.3562, 2.9649, 2.9365, 2.8066, 3.2034, 3.3362, 3.4891, 2.3043, 2.4369,\n",
      "        2.4577, 2.6757, 2.7331, 2.6754, 3.3180, 2.9087, 2.4752, 2.3943, 3.3970,\n",
      "        3.8286, 3.7444, 2.3187, 2.3358, 3.7363, 2.6452, 3.0461, 3.8368, 3.6590,\n",
      "        2.9830, 3.8689, 3.2229, 4.1880, 1.3936, 1.2873, 2.6673, 2.4756, 2.4503,\n",
      "        2.5216, 2.7407, 2.6681, 1.8383, 2.5501, 1.9947, 1.9850, 2.2117, 2.5956,\n",
      "        2.7293, 3.1552, 1.9406, 1.9299, 2.1085, 1.8141, 2.3442, 2.9062, 2.7723,\n",
      "        2.8454, 2.0141, 1.9026, 2.4706, 3.4878, 2.5602, 2.4952, 2.8507, 2.8347,\n",
      "        3.2957, 1.9610, 2.5871, 2.1488, 2.3419, 2.2668, 3.0158, 2.7973, 1.0352,\n",
      "        1.5799, 1.8239, 2.1702, 2.2028, 2.9472, 2.9806, 3.2177, 1.9512, 2.3448,\n",
      "        2.0209, 2.2181, 2.3634, 2.9309, 2.8420, 2.8684, 3.0782, 2.0553, 2.6513,\n",
      "        2.3399, 2.6701, 2.5372, 3.0053, 3.3391, 1.4424, 1.4722, 2.4009, 1.6447,\n",
      "        1.9831, 2.7019, 2.4375, 2.7158, 1.2417, 1.5646, 1.4349, 2.3713, 2.0327,\n",
      "        2.1328, 2.5482, 2.7064, 1.2915, 1.7076, 1.7884, 1.4630, 2.0332, 1.9139,\n",
      "        2.8413, 2.6519, 2.2320, 2.0776, 2.1792, 2.6187, 2.1029, 2.2846, 2.8525,\n",
      "        2.8433, 2.6106, 2.3217, 2.3224, 3.2439, 3.4113, 1.9785, 3.0416, 3.1478,\n",
      "        1.8147, 3.1193, 3.6252, 4.3205, 3.5250, 4.1078, 3.2511, 3.7378, 1.3665,\n",
      "        1.4848, 2.5915, 1.3696, 1.7901, 2.1575, 2.4197, 3.5501, 4.4062, 2.1849,\n",
      "        1.3054, 2.3161, 2.1599, 3.0769, 2.5234, 3.0412, 2.1884, 2.4773, 2.2026,\n",
      "        3.2926, 2.0182, 2.6324, 3.0656, 3.0479, 2.2657, 1.9707, 2.0130, 1.3946,\n",
      "        2.7152, 2.5742, 2.9400, 2.9896, 3.0530, 2.1592, 2.2453, 2.0013, 2.1159,\n",
      "        2.3580, 3.3535, 3.1344, 2.5228, 2.4679, 2.2907, 3.0118, 2.7633, 2.2341,\n",
      "        3.4389, 2.4924, 1.9717, 2.4374, 2.9448, 2.3293, 1.9660, 2.5454, 3.3405,\n",
      "        3.3843, 2.0039, 2.6758, 2.9668, 3.0912, 3.1143, 2.6505, 3.1389, 3.0348,\n",
      "        2.2151, 3.0187, 2.9211, 3.2418, 2.1846, 3.2000, 3.3588, 3.4749, 2.8985,\n",
      "        3.4177, 2.8472, 2.3285, 3.2328, 2.4360, 3.8624, 3.3037, 1.4831, 2.1136,\n",
      "        2.3611, 2.0502, 2.0896, 2.4071, 2.4377, 2.8911, 1.6288, 1.4175, 1.4338,\n",
      "        2.7093, 2.0761, 2.5603, 2.6135, 2.7326], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 3.8381e-02, -1.7718e-02,  5.5266e-02,  ...,  1.0145e-02,\n",
      "         -4.0696e-02, -2.0172e-02],\n",
      "        [-3.8408e-05,  8.0096e-03, -4.1493e-02,  ..., -6.9984e-03,\n",
      "          1.0003e-02, -4.1291e-02],\n",
      "        [ 1.3045e-02,  5.2521e-02, -2.7858e-02,  ...,  2.1318e-02,\n",
      "         -2.6862e-02, -5.0582e-02],\n",
      "        ...,\n",
      "        [-5.0041e-02,  4.0664e-02,  2.9821e-02,  ...,  4.1283e-02,\n",
      "          2.3976e-02,  2.7653e-02],\n",
      "        [-2.1177e-02, -7.7259e-03, -1.7641e-02,  ...,  3.2855e-02,\n",
      "          5.1176e-02, -2.0689e-02],\n",
      "        [-2.6843e-02,  7.5449e-03, -7.0814e-04,  ...,  5.0408e-02,\n",
      "          3.6923e-03, -1.0933e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-4.4442e-03, -4.9712e-03, -4.5669e-03,  ...,  4.7218e-03,\n",
      "         -4.3511e-03, -2.1425e-03],\n",
      "        [-4.4785e-04, -1.6852e-03,  1.9685e-03,  ..., -6.5868e-04,\n",
      "         -3.9858e-03, -1.5849e-03],\n",
      "        [ 9.4313e-04, -1.6920e-03, -5.9671e-03,  ...,  3.6441e-03,\n",
      "          3.3477e-03,  2.1385e-03],\n",
      "        ...,\n",
      "        [-3.8133e-03, -2.0856e-03, -2.1883e-03,  ..., -5.2258e-03,\n",
      "         -1.2241e-03, -8.2318e-04],\n",
      "        [-2.0114e-03,  9.1717e-05, -3.1013e-05,  ..., -1.0376e-02,\n",
      "         -3.3012e-03, -4.1579e-03],\n",
      "        [-4.7029e-03, -2.2157e-03, -2.9370e-03,  ..., -6.0484e-03,\n",
      "         -5.1220e-03,  2.5358e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9345, 1.6588, 1.5044, 3.0947, 1.3185, 1.9808, 2.5241, 2.6340, 1.9126,\n",
      "        2.2497, 2.5822, 1.2666, 2.9026, 2.0830, 2.8361, 2.1336, 1.6945, 2.0759,\n",
      "        2.7114, 1.7998, 1.7197, 1.6958, 3.2478, 3.2043, 1.9306, 2.2202, 1.5978,\n",
      "        1.6780, 3.4224, 3.6309, 3.4522, 3.0800, 2.6977, 1.9001, 2.5160, 2.6541,\n",
      "        2.2831, 1.9569, 2.1343, 2.9572, 1.6712, 2.2171, 2.6094, 2.2355, 2.1775,\n",
      "        2.2775, 2.4170, 2.2882, 2.6348, 1.6682, 1.5198, 1.9024, 2.9608, 1.3224,\n",
      "        2.4107, 2.4479, 1.5271, 2.2474, 1.7487, 1.4537, 1.2975, 3.0659, 2.6537,\n",
      "        2.4641, 1.8047, 1.7299, 1.3662, 1.2409, 1.4563, 1.7148, 2.6430, 2.0868,\n",
      "        1.9768, 1.5237, 1.6531, 2.6217, 1.7095, 2.8182, 2.5071, 2.0689, 2.5146,\n",
      "        2.5825, 2.7172, 2.3599, 2.3606, 2.4286, 3.3036, 3.1758, 1.7347, 2.3237,\n",
      "        2.2354, 2.5106, 2.2235, 2.2731, 3.1595, 2.6137, 2.4297, 2.7121, 3.1679,\n",
      "        3.2658, 2.7602, 3.5821, 3.4603, 3.3770, 2.8258, 3.3162, 3.0393, 3.1943,\n",
      "        2.9133, 1.5739, 2.1810, 3.5268, 1.3085, 2.1216, 1.5937, 1.4698, 1.4174,\n",
      "        2.8608, 2.7253, 2.7315, 1.6528, 1.4943, 1.9874, 2.4165, 2.6572, 1.4781,\n",
      "        2.6362, 2.2535, 2.1741, 2.2122, 1.8302, 3.1784, 1.6472, 1.5322, 2.6075,\n",
      "        2.5086, 1.9782, 1.8810, 2.9076, 1.5562, 1.7610, 3.0679, 2.5784, 2.7724,\n",
      "        2.0453, 2.2577, 1.5479, 1.6048, 1.3734, 3.0298, 2.5852, 2.6640, 1.3641,\n",
      "        1.5392, 2.7366, 1.6558, 2.9919, 1.6057, 2.7017, 2.2693, 2.1803, 2.5000,\n",
      "        2.9646, 2.9878, 3.2702, 1.8579, 3.2308, 3.0368, 2.2887, 2.0468, 1.6360,\n",
      "        1.8159, 1.7506, 3.2857, 3.3667, 3.0767, 1.2627, 1.4519, 1.4252, 2.6548,\n",
      "        1.2931, 1.4110, 2.2355, 2.1954, 1.6405, 1.8412, 2.1378, 1.2023, 1.5301,\n",
      "        2.3424, 2.4166, 1.9275, 1.9552, 1.8639, 1.5778, 3.1037, 1.4605, 1.4757,\n",
      "        2.6691, 2.7734, 2.4020, 2.2475, 1.4692, 1.3350, 2.9726, 3.0553, 2.5613,\n",
      "        2.8272, 1.8802, 2.7300, 3.4149, 3.1205, 2.8624, 3.1569, 2.5818, 4.0232,\n",
      "        2.3514, 2.0392, 2.2469, 2.8540, 2.7647, 1.8528, 3.9058, 3.9389, 1.3847,\n",
      "        1.9600, 1.2653, 2.1121, 2.7454, 2.6040, 2.2526, 2.0794, 1.3067, 1.6400,\n",
      "        1.2618, 1.2930, 1.2133, 1.5371, 2.3139, 2.0616, 2.3215, 2.1387, 1.9832,\n",
      "        1.6094, 3.1381, 1.6750, 2.7692, 2.8157, 2.0407, 2.1999, 2.1119, 2.1222,\n",
      "        1.3875, 2.3389, 2.6508, 2.8828, 2.4246, 2.2368, 2.2007, 2.2042, 2.7974,\n",
      "        2.1923, 2.9718, 2.5162, 2.7940, 2.3748, 2.2837, 2.2102, 1.9409, 2.0496,\n",
      "        2.9049, 2.2765, 1.7632, 2.7684, 2.7056, 2.4549, 3.2112, 2.1262, 3.1091,\n",
      "        3.2275, 2.0242, 2.3715, 2.4496, 2.5382, 1.8315, 2.0498, 3.2616, 3.0668,\n",
      "        2.7887, 2.9535, 2.6211, 2.9682, 3.6945, 2.2303, 3.3377, 3.2028, 2.3242,\n",
      "        2.7263, 2.6820, 2.2831, 1.7258, 2.4217, 3.0829, 3.4621, 1.7616, 1.9584,\n",
      "        1.2565, 2.6955, 2.6590, 1.9429, 2.4053, 2.0592, 1.7355, 1.5334, 2.6418,\n",
      "        1.3024, 1.1754, 2.1075, 2.3036, 2.3886], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0502,  0.0447,  0.0482,  ..., -0.0270,  0.0104,  0.0022],\n",
      "        [ 0.0571,  0.0074, -0.0214,  ..., -0.0436,  0.0132,  0.0276],\n",
      "        [-0.0221, -0.0535,  0.0337,  ...,  0.0458, -0.0263, -0.0121],\n",
      "        ...,\n",
      "        [ 0.0392,  0.0196,  0.0411,  ...,  0.0346,  0.0390,  0.0309],\n",
      "        [-0.0388,  0.0433,  0.0418,  ..., -0.0062,  0.0488,  0.0451],\n",
      "        [ 0.0195,  0.0286,  0.0013,  ...,  0.0195, -0.0440, -0.0490]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-3.7443e-03,  4.5884e-03,  3.1502e-03,  ...,  1.9854e-03,\n",
      "          5.4680e-03, -5.0124e-03],\n",
      "        [ 2.8262e-03, -2.6484e-03, -3.3131e-03,  ..., -1.7285e-03,\n",
      "         -2.4657e-03,  5.0748e-03],\n",
      "        [ 1.5864e-03,  1.3040e-03, -5.6292e-04,  ...,  7.9402e-04,\n",
      "          1.9575e-03, -4.5281e-05],\n",
      "        ...,\n",
      "        [ 2.9453e-03, -2.2149e-03, -7.9205e-04,  ..., -1.2036e-03,\n",
      "         -3.2560e-03,  9.7906e-04],\n",
      "        [ 1.4626e-03,  3.8787e-04,  2.4166e-03,  ..., -1.4377e-03,\n",
      "         -1.1843e-03,  5.8946e-04],\n",
      "        [ 4.3381e-03, -2.1391e-03, -2.6510e-03,  ..., -4.0568e-03,\n",
      "          1.4233e-03,  5.3499e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5097, 1.3430, 1.4349, 1.4284, 1.3722, 1.4053, 1.3471, 1.3950, 1.4892,\n",
      "        1.4298, 1.4122, 1.4266, 1.4043, 1.4317, 1.4754, 1.4482, 1.2023, 1.1991,\n",
      "        1.1695, 1.2383, 1.1971, 1.1457, 1.2084, 1.1457, 1.2761, 1.1495, 1.1668,\n",
      "        1.1104, 1.1743, 1.1896, 1.1113, 1.2579, 1.8236, 1.9628, 1.9951, 1.9530,\n",
      "        2.1107, 2.2015, 2.1635, 1.9180, 2.1092, 2.0329, 1.9377, 2.1161, 2.0183,\n",
      "        1.9635, 1.9630, 2.1312, 1.3829, 1.5136, 1.3832, 1.4242, 1.5031, 1.5089,\n",
      "        1.3621, 1.5136, 1.4787, 1.4446, 1.4839, 1.4630, 1.5118, 1.5029, 1.4993,\n",
      "        1.4084, 1.2715, 1.2449, 1.2319, 1.3041, 1.3185, 1.3363, 1.2653, 1.3276,\n",
      "        1.2842, 1.2237, 1.3258, 1.2723, 1.1935, 1.2684, 1.2394, 1.1499, 2.0393,\n",
      "        2.0991, 2.0843, 1.9904, 2.1056, 2.1128, 2.1935, 2.1497, 2.2324, 2.1036,\n",
      "        2.0177, 2.1033, 2.0552, 2.1896, 2.0920, 1.9105, 1.6385, 1.7093, 1.7491,\n",
      "        1.7208, 1.7445, 1.7526, 1.7399, 1.7397, 1.6596, 1.6385, 1.7045, 1.7389,\n",
      "        1.7268, 1.6584, 1.6489, 1.7513, 1.2044, 1.2575, 1.2550, 1.2240, 1.2500,\n",
      "        1.3043, 1.1988, 1.2126, 1.3044, 1.2581, 1.2102, 1.2946, 1.2961, 1.2568,\n",
      "        1.2555, 1.2185, 1.6062, 1.4562, 1.5257, 1.5362, 1.5252, 1.4380, 1.5413,\n",
      "        1.5519, 1.5614, 1.4701, 1.5432, 1.4300, 1.5273, 1.6011, 1.5263, 1.4692,\n",
      "        1.2512, 1.2729, 1.1869, 1.2804, 1.2156, 1.2481, 1.2892, 1.2419, 1.3056,\n",
      "        1.3145, 1.1549, 1.3072, 1.2797, 1.2584, 1.2470, 1.2038, 1.1367, 1.2320,\n",
      "        1.3099, 1.1405, 1.1764, 1.1662, 1.2579, 1.1632, 1.1947, 1.1537, 1.1686,\n",
      "        1.2139, 1.2355, 1.3081, 1.1844, 1.2450, 1.4491, 1.4416, 1.4486, 1.4307,\n",
      "        1.4831, 1.4513, 1.4430, 1.4758, 1.4719, 1.3818, 1.4923, 1.4851, 1.4391,\n",
      "        1.4343, 1.4195, 1.5269, 1.5670, 1.5625, 1.6022, 1.6357, 1.5569, 1.5369,\n",
      "        1.5060, 1.5912, 1.5951, 1.6226, 1.5596, 1.6005, 1.6108, 1.5870, 1.5696,\n",
      "        1.5469, 1.4571, 1.4878, 1.5554, 1.5099, 1.4289, 1.5204, 1.5543, 1.4883,\n",
      "        1.6649, 1.5488, 1.4911, 1.4341, 1.5570, 1.3995, 1.5304, 1.5026, 1.3618,\n",
      "        1.3695, 1.3403, 1.3861, 1.3414, 1.3956, 1.4459, 1.4109, 1.3982, 1.4079,\n",
      "        1.4590, 1.3856, 1.3608, 1.3887, 1.4168, 1.3773, 1.6228, 1.6336, 1.6773,\n",
      "        1.6958, 1.7069, 1.6188, 1.6669, 1.6151, 1.6885, 1.6832, 1.7082, 1.6838,\n",
      "        1.6162, 1.7798, 1.6498, 1.7099, 1.8623, 1.8245, 1.7388, 1.7127, 1.7581,\n",
      "        1.8231, 1.7968, 1.8339, 1.8012, 1.8213, 1.8751, 1.8182, 1.8815, 1.7791,\n",
      "        1.6767, 1.7499, 1.7778, 1.7560, 1.9526, 1.8123, 1.8236, 1.7822, 1.8904,\n",
      "        1.7806, 1.7885, 1.7600, 1.8879, 1.7141, 1.7264, 1.8307, 1.8425, 1.8070,\n",
      "        1.7398, 1.7975, 1.7951, 1.8102, 1.8633, 1.7449, 1.7920, 1.8092, 1.8288,\n",
      "        1.6830, 1.7727, 1.8279, 1.7785, 1.8131, 1.7309, 1.8593, 1.4955, 1.4101,\n",
      "        1.4250, 1.3895, 1.4680, 1.3963, 1.3959, 1.3721, 1.4353, 1.3798, 1.4134,\n",
      "        1.3445, 1.3879, 1.4917, 1.4527, 1.4818], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0347, -0.0398, -0.0504,  ...,  0.0378, -0.0050, -0.0458],\n",
      "        [ 0.0104, -0.0003, -0.0010,  ...,  0.0046,  0.0053, -0.0125],\n",
      "        [ 0.0630, -0.0358, -0.0423,  ...,  0.0171,  0.0257,  0.0347],\n",
      "        ...,\n",
      "        [ 0.0244,  0.0404, -0.0153,  ..., -0.0410,  0.0009, -0.0565],\n",
      "        [ 0.0033, -0.0504,  0.0042,  ..., -0.0501, -0.0139,  0.0237],\n",
      "        [ 0.0164,  0.0502, -0.0391,  ...,  0.0039, -0.0169, -0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0021, -0.0074, -0.0020,  ...,  0.0030, -0.0009, -0.0028],\n",
      "        [-0.0007, -0.0004,  0.0023,  ...,  0.0033,  0.0022, -0.0023],\n",
      "        [-0.0042,  0.0018,  0.0011,  ..., -0.0084,  0.0017, -0.0065],\n",
      "        ...,\n",
      "        [ 0.0006,  0.0027, -0.0002,  ...,  0.0071, -0.0012,  0.0055],\n",
      "        [-0.0005, -0.0024,  0.0022,  ..., -0.0022,  0.0011, -0.0007],\n",
      "        [ 0.0022, -0.0008,  0.0038,  ...,  0.0072,  0.0014,  0.0030]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5146, 1.3393, 1.4426, 1.5557, 1.5718, 1.3733, 1.4325, 1.4003, 1.3945,\n",
      "        1.5407, 1.3973, 1.5344, 1.4174, 1.3917, 1.4170, 1.5175, 1.5428, 1.5218,\n",
      "        1.4068, 1.4063, 1.5467, 1.3909, 1.5462, 1.4135, 1.4474, 1.5204, 1.4797,\n",
      "        1.4419, 1.5445, 1.5157, 1.4708, 1.4883, 1.4377, 1.4065, 1.4668, 1.5842,\n",
      "        1.6137, 1.5030, 1.5556, 1.4885, 1.4375, 1.4898, 1.6755, 1.5632, 1.4270,\n",
      "        1.3748, 1.3079, 1.5154, 1.5317, 1.5483, 1.4406, 1.4235, 1.4342, 1.4799,\n",
      "        1.4536, 1.5178, 1.4196, 1.4799, 1.4735, 1.4185, 1.5042, 1.7787, 1.4386,\n",
      "        1.5763, 1.5090, 1.4639, 1.4173, 1.4406, 1.4719, 1.5705, 1.4603, 1.6191,\n",
      "        1.5973, 1.5235, 1.4323, 1.3347, 1.4642, 1.4446, 1.4205, 1.4730, 1.5164,\n",
      "        1.5605, 1.4444, 1.5344, 1.6039, 1.5236, 1.4343, 1.3539, 1.3628, 1.5904,\n",
      "        1.4621, 1.5348, 1.5429, 1.4265, 1.5013, 1.4819, 1.3039, 1.5371, 1.6197,\n",
      "        1.3964, 1.4717, 1.3754, 1.4159, 1.3238, 1.4349, 1.5396, 1.4702, 1.4075,\n",
      "        1.6341, 1.4004, 1.4241, 1.4881, 1.5552, 1.5278, 1.4336, 1.4656, 1.6048,\n",
      "        1.5460, 1.4433, 1.4669, 1.5175, 1.5755, 1.4449, 1.4773, 1.4909, 1.3819,\n",
      "        1.4197, 1.4367, 1.4761, 1.6523, 1.4645, 1.5197, 1.4612, 1.4878, 1.4383,\n",
      "        1.2693, 1.5132, 1.3333, 1.5628, 1.3831, 1.4486, 1.5187, 1.4620, 1.3976,\n",
      "        1.4231, 1.4129, 1.3829, 1.4102, 1.4894, 1.4880, 1.7324, 1.5301, 1.4653,\n",
      "        1.3493, 1.4394, 1.4992, 1.5784, 1.5317, 1.5502, 1.4892, 1.3797, 1.5103,\n",
      "        1.4868, 1.4635, 1.4838, 1.4892, 1.4781, 1.4378, 1.4115, 1.4788, 1.4621,\n",
      "        1.4635, 1.4993, 1.5291, 1.5583, 1.4330, 1.5015, 1.4797, 1.3890, 1.4503,\n",
      "        1.4674, 1.3830, 1.6427, 1.5537, 1.4434, 1.5005, 1.5315, 1.3354, 1.4023,\n",
      "        1.4159, 1.3908, 1.4817, 2.3216, 1.4178, 1.4912, 1.4213, 1.4496, 1.4229,\n",
      "        1.4591, 1.4768, 1.4653, 1.4842, 1.3969, 1.4844, 1.4236, 1.3916, 1.5698,\n",
      "        1.4484, 1.5143, 1.3621, 1.5066, 1.3881, 1.4587, 1.4724, 1.4272, 1.5121,\n",
      "        1.4615, 1.4347, 1.4629, 1.5175, 1.4722, 1.5453, 1.5110, 1.4563, 1.3819,\n",
      "        1.3444, 1.4635, 1.4456, 1.4984, 1.5384, 1.4983, 1.4735, 1.4712, 1.3980,\n",
      "        1.4337, 1.5138, 1.4815, 1.2873, 1.4521, 1.5386, 1.5827, 1.3492, 1.5013,\n",
      "        1.5157, 1.4206, 1.3571, 1.3468, 1.4528, 1.4506, 1.4984, 1.4084, 1.5289,\n",
      "        1.4995, 1.5615, 1.4160, 1.4942, 1.5797, 1.3959, 1.3374, 1.5093, 1.5139,\n",
      "        1.4334, 1.4442, 1.4931, 1.6145, 1.5741, 1.5195, 1.5568, 1.4647, 1.4887,\n",
      "        1.4422, 1.4872, 1.6833, 1.4451, 1.5125, 1.5063, 1.4762, 1.4105, 1.4351,\n",
      "        1.4512, 1.4769, 1.5252, 1.4100, 1.5381, 1.4194, 1.4596, 1.4251, 1.4488,\n",
      "        1.2973, 1.5294, 1.5494, 1.5704, 1.3762, 1.4945, 1.4948, 1.4923, 1.5730,\n",
      "        1.4887, 1.4274, 1.5022, 1.4502, 1.5342, 1.3884, 1.3440, 1.4612, 1.4590,\n",
      "        1.5775, 1.5075, 1.4595, 1.3478, 1.4739, 1.5301, 1.4428, 1.5427, 1.4122,\n",
      "        1.5607, 1.5039, 1.4342, 1.4186, 2.1280], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 3.1977e-02,  2.4200e-03, -3.3511e-02,  ...,  2.9699e-02,\n",
      "         -2.4382e-02,  5.0666e-02],\n",
      "        [-1.1342e-03, -5.4428e-02,  7.4046e-03,  ...,  2.6496e-02,\n",
      "          3.7439e-02,  1.3650e-02],\n",
      "        [ 3.7817e-02,  3.7896e-03, -5.2374e-02,  ...,  4.4943e-02,\n",
      "         -4.9286e-02,  5.0977e-02],\n",
      "        ...,\n",
      "        [ 3.4583e-02,  5.0053e-02,  2.6828e-02,  ...,  2.4580e-02,\n",
      "          2.8574e-02,  3.0383e-02],\n",
      "        [-3.8812e-02, -4.8664e-02, -5.6153e-02,  ..., -5.8655e-02,\n",
      "          3.3198e-02,  5.0931e-02],\n",
      "        [-2.5911e-02,  1.4335e-02,  2.3774e-03,  ..., -5.9850e-02,\n",
      "          5.2823e-02,  3.3748e-05]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-4.2491e-03,  3.1650e-03, -3.1769e-03,  ...,  1.6990e-03,\n",
      "         -5.4223e-03,  5.5730e-03],\n",
      "        [ 1.6065e-03, -5.3812e-03,  9.1522e-04,  ..., -1.3300e-04,\n",
      "         -4.8788e-04, -9.3326e-04],\n",
      "        [-3.9591e-04, -1.0767e-03, -1.0240e-04,  ..., -9.0686e-04,\n",
      "          9.7769e-04, -4.7589e-04],\n",
      "        ...,\n",
      "        [ 1.0056e-03, -1.7225e-03,  3.6909e-04,  ...,  3.1501e-04,\n",
      "         -1.3695e-03,  7.3785e-07],\n",
      "        [ 7.7969e-03, -6.3764e-03,  8.1455e-03,  ..., -2.8086e-03,\n",
      "          9.7416e-03, -6.2892e-03],\n",
      "        [-1.0563e-03, -2.6528e-03, -2.4988e-03,  ...,  8.3209e-03,\n",
      "         -7.0025e-03,  3.5691e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.2341, 1.1942, 1.2671,  ..., 1.1302, 1.9022, 3.0280], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-2.0395e-02,  1.9459e-02, -3.1802e-05,  ..., -2.0945e-02,\n",
      "          1.4132e-02,  2.6658e-02],\n",
      "        [-1.3582e-02, -9.4787e-03, -2.6835e-02,  ...,  1.4085e-02,\n",
      "          1.5573e-02,  1.2530e-02],\n",
      "        [-3.1180e-02, -1.0200e-02, -4.0045e-03,  ..., -1.9580e-02,\n",
      "          2.2396e-03, -2.1888e-02],\n",
      "        ...,\n",
      "        [-1.0306e-02, -2.9579e-02, -1.2283e-02,  ..., -1.7054e-02,\n",
      "          1.3172e-02,  1.4234e-02],\n",
      "        [-9.2104e-03,  1.9559e-03,  1.2518e-02,  ...,  3.0043e-02,\n",
      "          1.4495e-02,  1.7364e-02],\n",
      "        [-7.5807e-03, -2.0672e-02,  7.7987e-03,  ...,  5.6541e-03,\n",
      "         -6.1396e-03, -2.4854e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 3.2074e-03,  2.6471e-03,  2.8018e-03,  ..., -9.4720e-04,\n",
      "         -2.9568e-03,  2.7372e-03],\n",
      "        [ 6.2279e-03,  7.8534e-03,  4.3019e-03,  ...,  4.0447e-03,\n",
      "         -6.0643e-03,  9.0541e-03],\n",
      "        [-7.0374e-03, -2.2065e-03, -2.3359e-03,  ...,  4.5736e-03,\n",
      "          2.9347e-03,  4.8008e-06],\n",
      "        ...,\n",
      "        [ 7.2389e-03,  2.2039e-03,  4.5882e-03,  ..., -2.4371e-03,\n",
      "         -4.6049e-03,  1.6209e-03],\n",
      "        [ 1.6593e-05, -7.0391e-04, -3.6204e-04,  ...,  5.6737e-04,\n",
      "         -6.0512e-04, -3.1172e-04],\n",
      "        [ 1.3940e-03,  1.2179e-03,  1.7878e-03,  ..., -2.7456e-03,\n",
      "         -2.8438e-04, -3.5645e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.9629, 3.1113, 3.2687, 3.2841, 3.2424, 2.9856, 2.8647, 3.0379, 3.2109,\n",
      "        3.0364, 2.8570, 3.0008, 3.2482, 2.9130, 2.9467, 3.2037, 2.9155, 2.9509,\n",
      "        2.9055, 2.9374, 3.1470, 3.0681, 2.9543, 2.7958, 2.9008, 2.9241, 3.0192,\n",
      "        2.9751, 2.9025, 3.0437, 3.1986, 2.8301, 3.0902, 2.8215, 3.0553, 3.1878,\n",
      "        3.0470, 3.0077, 3.0379, 2.9955, 3.2133, 2.8637, 2.9123, 3.0746, 2.9901,\n",
      "        3.0977, 3.5250, 3.0207, 2.9758, 2.9727, 3.0886, 3.0200, 2.7823, 3.0302,\n",
      "        3.0995, 2.9571, 2.8918, 3.2139, 3.1351, 3.2628, 2.9754, 3.1213, 3.8611,\n",
      "        2.9461, 3.0996, 3.0229, 2.8729, 3.1952, 3.1522, 3.1142, 2.7877, 3.0629,\n",
      "        3.0615, 3.0004, 3.0776, 3.1096, 2.9098, 2.8675, 3.0208, 3.1835, 2.9240,\n",
      "        2.9672, 3.1108, 2.9472, 2.8333, 3.0830, 2.9892, 3.0529, 2.5971, 3.0065,\n",
      "        3.0369, 2.9077, 3.2528, 2.7687, 3.1011, 3.1309, 2.9026, 2.9054, 3.2289,\n",
      "        3.2148, 2.9698, 2.8984, 2.9840, 2.8830, 2.9263, 3.3195, 3.0407, 2.9394,\n",
      "        2.9244, 3.1684, 3.1424, 2.9649, 3.1305, 2.9927, 3.0279, 3.0550, 2.9721,\n",
      "        3.1794, 3.0562, 2.9038, 3.0134, 3.0197, 2.9186, 3.0947, 2.8764, 3.1030,\n",
      "        3.0306, 3.1414, 2.9557, 3.0342, 2.9079, 3.0968, 3.0210, 3.0506, 3.1531,\n",
      "        3.0795, 3.0032, 3.0518, 3.0903, 3.0753, 2.9725, 2.9056, 3.0922, 2.8884,\n",
      "        3.0323, 3.1191, 3.0150, 3.2722, 3.1099, 2.9978, 3.5676, 2.9368, 2.9617,\n",
      "        3.0898, 3.0816, 3.1862, 2.8067, 3.0318, 2.9375, 3.0090, 3.0691, 3.1483,\n",
      "        2.8018, 3.1402, 3.1245, 3.1147, 3.1675, 3.2107, 3.0464, 2.9246, 3.0695,\n",
      "        2.8095, 3.0416, 2.9521, 3.0531, 3.1693, 3.0748, 3.1462, 3.1411, 2.9227,\n",
      "        2.9801, 2.8626, 2.9365, 2.8306, 3.1412, 3.1014, 3.1967, 3.2058, 3.1437,\n",
      "        3.0697, 3.1547, 2.9927, 5.2878, 2.9457, 3.0971, 3.0929, 3.2425, 2.9917,\n",
      "        3.2170, 2.9943, 3.1535, 2.9112, 3.0698, 3.0002, 2.9793, 3.0637, 3.2370,\n",
      "        2.9584, 3.1442, 2.9752, 2.8721, 3.0265, 2.8432, 3.0576, 3.2043, 3.1120,\n",
      "        3.0250, 2.9199, 2.9011, 2.8924, 2.9768, 3.2895, 3.0726, 3.1897, 2.9409,\n",
      "        3.1938, 3.3013, 3.0129, 3.0124, 3.1251, 2.9139, 2.9038, 3.1516, 2.7839,\n",
      "        3.0620, 2.9708, 3.1542, 2.8765, 2.9557, 3.2391, 3.0643, 2.8758, 2.8861,\n",
      "        2.9029, 2.8904, 2.9316, 3.0610, 2.9579, 3.1239, 3.2137, 2.8838, 3.1093,\n",
      "        2.9430, 2.9445, 3.0422, 2.9222, 3.0200, 2.5525, 2.4518, 3.1234, 2.9924,\n",
      "        2.7930, 2.8825, 2.9095, 2.9886, 3.0786, 2.9922, 3.2366, 2.9355, 2.9195,\n",
      "        2.9039, 2.9944, 3.0904, 3.0142, 2.9722, 3.2148, 3.1330, 3.0290, 3.0271,\n",
      "        2.9140, 3.1152, 3.2436, 3.1394, 3.1925, 3.0062, 3.0859, 2.8132, 3.1216,\n",
      "        2.6287, 3.2688, 3.0565, 2.8451, 3.0439, 3.0493, 2.6833, 3.0566, 2.9537,\n",
      "        3.0339, 2.8152, 3.2346, 2.9176, 3.0419, 3.0832, 2.9190, 3.2563, 3.0257,\n",
      "        2.8825, 3.0839, 3.1269, 3.2204, 3.0607, 3.1803, 2.9646, 3.0589, 3.1461,\n",
      "        3.0015, 3.0139, 2.9578, 3.0332, 3.7531], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0514,  0.0644, -0.0023,  ..., -0.0656, -0.0338,  0.0020],\n",
      "        [ 0.0039,  0.0106, -0.0077,  ...,  0.0307, -0.0494,  0.0250],\n",
      "        [-0.0420, -0.0388, -0.0034,  ...,  0.0511,  0.0242,  0.0078],\n",
      "        ...,\n",
      "        [ 0.0125, -0.0484, -0.0043,  ...,  0.0327,  0.0090, -0.0149],\n",
      "        [ 0.0043, -0.0347, -0.0304,  ..., -0.0475, -0.0242,  0.0195],\n",
      "        [-0.0124, -0.0567,  0.0238,  ...,  0.0055, -0.0279,  0.0349]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 6.6234e-04,  2.4244e-04, -7.1877e-04,  ...,  1.3421e-03,\n",
      "         -2.0950e-04, -1.5312e-03],\n",
      "        [ 4.3999e-03, -8.4375e-03, -7.4816e-03,  ..., -7.6144e-03,\n",
      "         -7.2251e-03,  2.7449e-03],\n",
      "        [-6.5706e-03,  4.3528e-03,  3.7905e-03,  ...,  4.4793e-03,\n",
      "          5.3235e-03, -4.8740e-03],\n",
      "        ...,\n",
      "        [ 1.0018e-03,  8.8397e-04,  1.4587e-03,  ..., -6.6085e-04,\n",
      "         -3.9998e-03,  2.8651e-03],\n",
      "        [ 8.3484e-03, -7.3583e-03, -7.4858e-03,  ..., -7.3812e-03,\n",
      "         -9.9738e-03,  1.9345e-04],\n",
      "        [ 2.1382e-03, -5.5185e-04, -3.6558e-04,  ..., -4.5438e-08,\n",
      "         -3.3222e-03,  4.0823e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6259, 2.6448, 2.0958, 1.2535, 2.4728, 4.0938, 4.0526, 4.0992, 1.6258,\n",
      "        1.7497, 2.3947, 2.2293, 1.8266, 3.9750, 3.8750, 3.9439, 1.6959, 1.9979,\n",
      "        2.5530, 2.4951, 2.4278, 2.8202, 2.4930, 2.9177, 2.1337, 2.0422, 1.9701,\n",
      "        1.9504, 1.9859, 2.1118, 2.6536, 3.0690, 2.1065, 1.9654, 1.9618, 1.3704,\n",
      "        1.9453, 2.9885, 2.5306, 2.5642, 2.3069, 1.6214, 1.7933, 2.2796, 2.6156,\n",
      "        1.9647, 2.4344, 2.3949, 1.2604, 1.3960, 2.0679, 1.9270, 1.6271, 2.4842,\n",
      "        2.3869, 2.6711, 1.7383, 2.3028, 1.9812, 1.8843, 1.9665, 1.9692, 2.2384,\n",
      "        2.6585, 2.1707, 2.0701, 1.7506, 2.0054, 2.0979, 2.3141, 2.7769, 3.8694,\n",
      "        2.4040, 2.2303, 2.1069, 1.9248, 1.8067, 2.3454, 2.9916, 2.7818, 1.9235,\n",
      "        2.1177, 2.0782, 1.9915, 2.0832, 2.3717, 2.5764, 2.0368, 1.8287, 1.7052,\n",
      "        2.2787, 2.1603, 2.0027, 2.0652, 2.6542, 2.8203, 2.7372, 2.0065, 2.3083,\n",
      "        2.3599, 2.1233, 2.4636, 2.7321, 2.7597, 1.6161, 2.2078, 1.8067, 2.2852,\n",
      "        2.6021, 3.3134, 2.8236, 3.1154, 3.1515, 1.7640, 2.4415, 2.0309, 2.1898,\n",
      "        2.8643, 2.4908, 2.5464, 1.7584, 2.2825, 1.6628, 2.4433, 2.7709, 2.4062,\n",
      "        2.8387, 2.8643, 1.2768, 1.7807, 1.7668, 2.0582, 2.2302, 2.5649, 2.2339,\n",
      "        2.8436, 2.3597, 1.7387, 2.2621, 1.7375, 1.9688, 2.1868, 2.5075, 3.2108,\n",
      "        2.2480, 1.5018, 1.7395, 2.3256, 2.7111, 2.5332, 2.6048, 2.6330, 1.9130,\n",
      "        1.4660, 1.8394, 1.7454, 2.0669, 2.0643, 2.5261, 2.6287, 1.8798, 2.0322,\n",
      "        2.1631, 2.2538, 2.0370, 1.7303, 2.6093, 2.8533, 1.8381, 2.0996, 2.4403,\n",
      "        2.0894, 2.6042, 2.4530, 2.6752, 1.9694, 2.0996, 2.4523, 1.9681, 2.6130,\n",
      "        3.3700, 3.4090, 2.8985, 3.0804, 2.4239, 2.1245, 2.4147, 2.2680, 2.2166,\n",
      "        2.7273, 2.8039, 3.0270, 1.9476, 1.6725, 1.9983, 1.6099, 2.2199, 3.1472,\n",
      "        2.7641, 2.8974, 2.7100, 2.2616, 2.1390, 2.7214, 2.8540, 2.3419, 2.5234,\n",
      "        2.6493, 2.3530, 2.1834, 2.2417, 2.1519, 1.9506, 2.6565, 2.9000, 2.7407,\n",
      "        2.0685, 2.1710, 2.0232, 2.2788, 2.3586, 2.6972, 2.9006, 2.9329, 1.2785,\n",
      "        1.4851, 2.3056, 1.6090, 2.2985, 1.7362, 2.2437, 2.2156, 1.6392, 1.6877,\n",
      "        1.3589, 2.0732, 1.6058, 2.3598, 2.1174, 2.3228, 2.1815, 1.8422, 1.8374,\n",
      "        1.8594, 1.7784, 1.8728, 2.4055, 2.5763, 1.9009, 1.8154, 2.0800, 2.4598,\n",
      "        2.1458, 2.2524, 2.4478, 2.2968, 1.9254, 2.2979, 2.7346, 2.3033, 2.1113,\n",
      "        2.7294, 2.8546, 3.2577, 2.6098, 2.0600, 1.9122, 2.2110, 2.5735, 3.0833,\n",
      "        3.1168, 3.5773, 2.1863, 2.1498, 2.1903, 2.0515, 1.8504, 2.2372, 3.7190,\n",
      "        3.8738, 2.1954, 2.2579, 2.6607, 2.2690, 2.0660, 2.0176, 3.6219, 3.7149,\n",
      "        2.8977, 1.8916, 1.6525, 2.2093, 2.2946, 2.4028, 2.9222, 2.8893, 1.7570,\n",
      "        1.7454, 2.1601, 1.9222, 1.9643, 2.7509, 2.7547, 3.0108, 2.0338, 1.9684,\n",
      "        2.2788, 2.8431, 2.3848, 2.3197, 3.2392, 2.8080, 2.2208, 2.0070, 2.1177,\n",
      "        1.9178, 2.8423, 2.7809, 2.7976, 2.3335], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0087,  0.0199, -0.0604,  ...,  0.0236, -0.0502, -0.0403],\n",
      "        [ 0.0481, -0.0015,  0.0483,  ...,  0.0463, -0.0127, -0.0547],\n",
      "        [ 0.0628, -0.0287, -0.0529,  ...,  0.0334, -0.0369,  0.0030],\n",
      "        ...,\n",
      "        [-0.0201, -0.0271,  0.0357,  ..., -0.0160,  0.0152, -0.0278],\n",
      "        [ 0.0146, -0.0154,  0.0339,  ..., -0.0244,  0.0317,  0.0263],\n",
      "        [ 0.0242,  0.0303, -0.0041,  ...,  0.0027, -0.0148,  0.0184]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0004,  0.0009, -0.0028,  ...,  0.0045, -0.0036, -0.0031],\n",
      "        [-0.0051,  0.0050, -0.0009,  ..., -0.0006,  0.0027,  0.0032],\n",
      "        [-0.0016,  0.0060, -0.0024,  ..., -0.0006, -0.0010,  0.0011],\n",
      "        ...,\n",
      "        [-0.0072, -0.0014, -0.0025,  ..., -0.0021, -0.0031, -0.0009],\n",
      "        [-0.0084, -0.0085, -0.0070,  ..., -0.0025,  0.0060, -0.0013],\n",
      "        [-0.0081, -0.0015, -0.0031,  ..., -0.0012, -0.0034, -0.0015]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0711, 2.5417, 2.3515, 1.9766, 2.0998, 2.9845, 3.1096, 3.2082, 2.0327,\n",
      "        1.9356, 2.1862, 1.6985, 1.8085, 2.9087, 3.0336, 3.1845, 1.7670, 1.6802,\n",
      "        1.7788, 1.4615, 1.4367, 1.5478, 2.4350, 2.1582, 1.8650, 2.3717, 2.1825,\n",
      "        2.7712, 2.7211, 2.8035, 2.1939, 2.1169, 2.2340, 1.7459, 2.1282, 2.4215,\n",
      "        1.8976, 1.4597, 2.4357, 1.8714, 2.1066, 1.8834, 1.8487, 1.5296, 1.6706,\n",
      "        2.5515, 2.3177, 2.1274, 1.2371, 2.1825, 1.9845, 1.4008, 2.5581, 1.4672,\n",
      "        2.2707, 1.9690, 1.5100, 1.3537, 1.5304, 2.1762, 1.3014, 2.4468, 2.0728,\n",
      "        1.9885, 2.0069, 2.1087, 1.9090, 2.1965, 1.8064, 1.9300, 2.2500, 2.1561,\n",
      "        2.2504, 2.2296, 1.9762, 1.7448, 1.9606, 1.9659, 2.3401, 1.8723, 1.7954,\n",
      "        2.1357, 2.4497, 2.0432, 2.0030, 1.7559, 2.1462, 2.1250, 1.8868, 1.5205,\n",
      "        1.9351, 2.0319, 1.9447, 2.3797, 2.4307, 2.5074, 1.8620, 1.8631, 1.9704,\n",
      "        2.8953, 2.9959, 2.9545, 2.6745, 2.6728, 2.3806, 2.2556, 2.0336, 1.7986,\n",
      "        1.5429, 1.6602, 2.5901, 2.1694, 2.0567, 1.8698, 2.2371, 2.8166, 3.2906,\n",
      "        1.6102, 2.2523, 2.4836, 2.5099, 2.0741, 1.6057, 1.5816, 1.3892, 2.7540,\n",
      "        2.3380, 2.1162, 1.8426, 1.7876, 2.2300, 1.5013, 1.6236, 1.8215, 2.2409,\n",
      "        2.1349, 1.2364, 1.7084, 1.7360, 2.0012, 1.9718, 2.1658, 2.3596, 2.1394,\n",
      "        2.1688, 1.8450, 2.3083, 1.4071, 1.3534, 1.6118, 2.2563, 2.0438, 1.8937,\n",
      "        1.5755, 1.6175, 2.4393, 2.7457, 2.2134, 2.3559, 1.9548, 1.9749, 2.0514,\n",
      "        2.4310, 2.0489, 2.3026, 2.3000, 2.1654, 2.5604, 1.5350, 2.1553, 2.2517,\n",
      "        2.3140, 1.9969, 2.1680, 2.4484, 2.1728, 2.2898, 2.1226, 1.9008, 1.9297,\n",
      "        1.6369, 1.5910, 2.8395, 2.7220, 2.2225, 2.3618, 2.2726, 2.5049, 2.3288,\n",
      "        2.6293, 2.7099, 2.6844, 2.1307, 1.7786, 2.0165, 2.1817, 2.4264, 1.5424,\n",
      "        2.1481, 2.1183, 1.9055, 2.0491, 1.9578, 1.3838, 1.3497, 2.1139, 2.5123,\n",
      "        2.3758, 2.0727, 2.2164, 2.1237, 2.3145, 2.0409, 1.9640, 2.6732, 2.7376,\n",
      "        2.1715, 2.1376, 2.0514, 1.9627, 1.7618, 1.9273, 2.6873, 2.7541, 1.4366,\n",
      "        1.7805, 1.3488, 2.3595, 1.2210, 2.2885, 2.0993, 2.0658, 1.5893, 1.5402,\n",
      "        2.4444, 1.3093, 2.4079, 1.3024, 1.9743, 1.8833, 1.9594, 1.8231, 1.7751,\n",
      "        2.1178, 2.4342, 2.5551, 2.2456, 2.1317, 2.0970, 1.7903, 1.9943, 1.5468,\n",
      "        1.3914, 1.4350, 2.3626, 1.9382, 1.9588, 1.9964, 1.7761, 1.7969, 2.8306,\n",
      "        2.3679, 2.6699, 2.1503, 2.2049, 2.0840, 2.3905, 2.4304, 1.5882, 2.2385,\n",
      "        2.6327, 2.1912, 2.0808, 2.1462, 2.0242, 2.1143, 1.9244, 2.2282, 3.0402,\n",
      "        3.0681, 2.3162, 2.3808, 2.5210, 2.2124, 2.0546, 2.0081, 3.0052, 3.0131,\n",
      "        2.0594, 1.9054, 2.4397, 1.7401, 1.3672, 2.4010, 2.3257, 2.2498, 2.1091,\n",
      "        1.7730, 1.5516, 1.6417, 2.5159, 1.6765, 2.2053, 2.2545, 2.2652, 1.8734,\n",
      "        2.1889, 2.2653, 1.9405, 2.3509, 2.3252, 2.7766, 1.7959, 2.0822, 2.1168,\n",
      "        2.2757, 2.3225, 2.2895, 2.8380, 2.3245], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0120,  0.0258, -0.0410,  ..., -0.0394,  0.0481,  0.0359],\n",
      "        [-0.0309, -0.0428, -0.0043,  ..., -0.0400,  0.0319, -0.0355],\n",
      "        [ 0.0262, -0.0425, -0.0304,  ...,  0.0164,  0.0258,  0.0013],\n",
      "        ...,\n",
      "        [-0.0339, -0.0289,  0.0627,  ..., -0.0018,  0.0263, -0.0215],\n",
      "        [-0.0365,  0.0373, -0.0374,  ...,  0.0050,  0.0199, -0.0406],\n",
      "        [-0.0110,  0.0094,  0.0078,  ..., -0.0486,  0.0003, -0.0202]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-3.7454e-03, -5.9768e-03,  4.5948e-03,  ..., -4.1898e-03,\n",
      "          5.0474e-03, -3.7434e-03],\n",
      "        [ 5.5103e-03,  3.5860e-03, -4.2115e-03,  ...,  5.2168e-03,\n",
      "         -4.6912e-03,  5.0258e-03],\n",
      "        [-3.2045e-03, -3.8088e-03,  3.4061e-03,  ..., -2.7961e-03,\n",
      "          4.0662e-03, -5.0014e-03],\n",
      "        ...,\n",
      "        [ 1.9515e-03,  3.7557e-03, -2.4694e-03,  ...,  5.2798e-03,\n",
      "         -3.7704e-03,  2.3684e-03],\n",
      "        [-1.5683e-03, -1.2196e-03,  3.7600e-03,  ..., -2.0502e-03,\n",
      "          1.0760e-03, -8.0493e-05],\n",
      "        [ 2.2450e-03,  1.8339e-03, -6.9622e-04,  ...,  4.3500e-03,\n",
      "          2.0381e-05,  7.9803e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5361, 1.6520, 1.5965, 1.5537, 1.6424, 1.6282, 1.5040, 1.5537, 1.5358,\n",
      "        1.5709, 1.5092, 1.5689, 1.5187, 1.5025, 1.5701, 1.5305, 1.1631, 1.2366,\n",
      "        1.2085, 1.1116, 1.2383, 1.1824, 1.2504, 1.1840, 1.2354, 1.1923, 1.2146,\n",
      "        1.1790, 1.2109, 1.1785, 1.1581, 1.2238, 1.6583, 1.4834, 1.5942, 1.6216,\n",
      "        1.5916, 1.5525, 1.6634, 1.6037, 1.5955, 1.5653, 1.6118, 1.6437, 1.6189,\n",
      "        1.6416, 1.6316, 1.6076, 1.2700, 1.1565, 1.2517, 1.2156, 1.1790, 1.2284,\n",
      "        1.2190, 1.2477, 1.2596, 1.2968, 1.1824, 1.2409, 1.2154, 1.2187, 1.2620,\n",
      "        1.2162, 1.8286, 1.8188, 1.8201, 1.7569, 1.8898, 1.7808, 1.8323, 1.9415,\n",
      "        1.8729, 1.9171, 1.8331, 1.9754, 1.8598, 1.9508, 1.8614, 1.9546, 1.8179,\n",
      "        1.7909, 1.8474, 1.7529, 1.7362, 1.8923, 1.7598, 1.7408, 1.8563, 1.6892,\n",
      "        1.7757, 1.8159, 1.7730, 1.8578, 1.8290, 1.8313, 1.6612, 1.7968, 1.6312,\n",
      "        1.6937, 1.7759, 1.6795, 1.6962, 1.7044, 1.7353, 1.6510, 1.6520, 1.6710,\n",
      "        1.6607, 1.7250, 1.6267, 1.6505, 1.6167, 1.5579, 1.6074, 1.5507, 1.5637,\n",
      "        1.5628, 1.6472, 1.5893, 1.6093, 1.5805, 1.6172, 1.5121, 1.6209, 1.5710,\n",
      "        1.5641, 1.6467, 1.2302, 1.1850, 1.2367, 1.2243, 1.2220, 1.1754, 1.1725,\n",
      "        1.1569, 1.2070, 1.1525, 1.1712, 1.1747, 1.2011, 1.2017, 1.1576, 1.1971,\n",
      "        1.5959, 1.6152, 1.5889, 1.6010, 1.5259, 1.5960, 1.5983, 1.5503, 1.5923,\n",
      "        1.6154, 1.6248, 1.6272, 1.5955, 1.6399, 1.6239, 1.6505, 1.7571, 1.8338,\n",
      "        1.7469, 1.8565, 1.8225, 1.7835, 1.7684, 1.8387, 1.7088, 1.8157, 1.6973,\n",
      "        1.7924, 1.7167, 1.7928, 1.6838, 1.7741, 1.7431, 1.7512, 1.6572, 1.6325,\n",
      "        1.7794, 1.7257, 1.7670, 1.8330, 1.7301, 1.7698, 1.6898, 1.6220, 1.7247,\n",
      "        1.6836, 1.7366, 1.6589, 1.5289, 1.5663, 1.4671, 1.5099, 1.5151, 1.5570,\n",
      "        1.5509, 1.5551, 1.5523, 1.5708, 1.6613, 1.5779, 1.6104, 1.5908, 1.5121,\n",
      "        1.5097, 1.7950, 1.8311, 1.7404, 1.7729, 1.8105, 1.9073, 1.7040, 1.7237,\n",
      "        1.7053, 1.8175, 1.8593, 1.7235, 1.7682, 1.7722, 1.8222, 1.8093, 1.3487,\n",
      "        1.3512, 1.2816, 1.3633, 1.3445, 1.4531, 1.3669, 1.3723, 1.4560, 1.4979,\n",
      "        1.3783, 1.3551, 1.3920, 1.3156, 1.3461, 1.3737, 1.6730, 1.6579, 1.6884,\n",
      "        1.6271, 1.5659, 1.6420, 1.6359, 1.6854, 1.6142, 1.6508, 1.6812, 1.6305,\n",
      "        1.7038, 1.6549, 1.7001, 1.6696, 1.2548, 1.2773, 1.2898, 1.2240, 1.2460,\n",
      "        1.1969, 1.2456, 1.2067, 1.2647, 1.2329, 1.2563, 1.1634, 1.2822, 1.2598,\n",
      "        1.2674, 1.2181, 1.8124, 1.6605, 1.6830, 1.6834, 1.6814, 1.6513, 1.6211,\n",
      "        1.8370, 1.5601, 1.6464, 1.7165, 1.7708, 1.7381, 1.6042, 1.7415, 1.7767,\n",
      "        1.3181, 1.3037, 1.2549, 1.2821, 1.2592, 1.3097, 1.3483, 1.2181, 1.2676,\n",
      "        1.3337, 1.2812, 1.3135, 1.2879, 1.3035, 1.2488, 1.3128, 1.4770, 1.5412,\n",
      "        1.5717, 1.3982, 1.5602, 1.5190, 1.4286, 1.5137, 1.5222, 1.4667, 1.4968,\n",
      "        1.4413, 1.4314, 1.5331, 1.4572, 1.6479], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0082, -0.0396, -0.0400,  ..., -0.0427,  0.0201, -0.0057],\n",
      "        [-0.0367,  0.0043,  0.0158,  ...,  0.0015, -0.0355,  0.0008],\n",
      "        [-0.0114, -0.0403, -0.0088,  ...,  0.0438, -0.0468, -0.0366],\n",
      "        ...,\n",
      "        [ 0.0050, -0.0515,  0.0182,  ..., -0.0146, -0.0225,  0.0158],\n",
      "        [-0.0291, -0.0272, -0.0362,  ..., -0.0450, -0.0509, -0.0176],\n",
      "        [-0.0224,  0.0479,  0.0463,  ...,  0.0379,  0.0225,  0.0448]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0082, -0.0049,  0.0005,  ...,  0.0028, -0.0008, -0.0068],\n",
      "        [ 0.0038,  0.0036, -0.0042,  ..., -0.0033, -0.0028, -0.0001],\n",
      "        [ 0.0087,  0.0033, -0.0049,  ...,  0.0004, -0.0010,  0.0014],\n",
      "        ...,\n",
      "        [-0.0103,  0.0004,  0.0042,  ...,  0.0049,  0.0036, -0.0058],\n",
      "        [ 0.0051,  0.0067,  0.0055,  ...,  0.0010, -0.0012, -0.0019],\n",
      "        [-0.0003, -0.0111, -0.0042,  ...,  0.0035,  0.0015, -0.0008]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4848, 1.5929, 1.5317, 1.4821, 1.6291, 1.4070, 1.4301, 1.4946, 1.4436,\n",
      "        1.4324, 1.4254, 1.5222, 1.5399, 1.4102, 1.6670, 1.4296, 1.3465, 1.5340,\n",
      "        1.4425, 1.4692, 1.5447, 1.4055, 1.5804, 1.4461, 1.4770, 1.4816, 1.4803,\n",
      "        1.5306, 1.5176, 1.4870, 1.4501, 1.4303, 1.6065, 1.4793, 1.4144, 1.4418,\n",
      "        1.6223, 1.4241, 1.3966, 1.4700, 1.6397, 1.5301, 1.4789, 1.4967, 1.4111,\n",
      "        1.3989, 1.3914, 1.6256, 1.4953, 1.5014, 1.4565, 1.5041, 1.3128, 1.3618,\n",
      "        1.3802, 1.3317, 1.3398, 1.4449, 1.6091, 1.5448, 1.3860, 1.6817, 2.7585,\n",
      "        1.4635, 1.4580, 1.4774, 1.4240, 1.5515, 1.5118, 1.5387, 1.3326, 1.4367,\n",
      "        1.4063, 1.4262, 1.5005, 1.5606, 1.4273, 1.5225, 1.4620, 1.5017, 1.4759,\n",
      "        1.5554, 1.4893, 1.5777, 1.4007, 1.4622, 1.4459, 1.5463, 1.3062, 1.5112,\n",
      "        1.4282, 1.5285, 1.5952, 1.5141, 1.4299, 1.5648, 1.4944, 1.5696, 1.5522,\n",
      "        1.5332, 1.5781, 1.5531, 1.6110, 1.4584, 1.5417, 1.6185, 1.4517, 1.4118,\n",
      "        1.4250, 1.4229, 1.5717, 1.3568, 1.4477, 1.4375, 1.5355, 1.4887, 1.5870,\n",
      "        1.4570, 1.4744, 1.4256, 1.5176, 1.5744, 1.5495, 1.5378, 1.3373, 1.5766,\n",
      "        1.5083, 1.5350, 1.4690, 1.4844, 1.3309, 1.3942, 1.5124, 1.5731, 1.5518,\n",
      "        1.3935, 1.4522, 1.5030, 1.3463, 1.3908, 1.4267, 1.4041, 1.4786, 1.4139,\n",
      "        1.3973, 1.4974, 1.4714, 1.5730, 1.4753, 1.5406, 2.1319, 1.5672, 1.5589,\n",
      "        1.5116, 1.4897, 1.5165, 1.4562, 1.5887, 1.5545, 1.4831, 1.3477, 1.4636,\n",
      "        1.5739, 1.5758, 1.6362, 1.5455, 1.3861, 1.4443, 1.3947, 1.4979, 1.4327,\n",
      "        1.6329, 1.5196, 1.4576, 1.4813, 1.3207, 1.5117, 1.5715, 1.4477, 1.5432,\n",
      "        1.4345, 1.3935, 1.4412, 1.5004, 1.5432, 1.4299, 1.4473, 1.4533, 1.4919,\n",
      "        1.6165, 1.3463, 1.3506, 2.1466, 1.3119, 1.4700, 1.4783, 1.5011, 1.4212,\n",
      "        1.5034, 1.5500, 1.5956, 1.3037, 1.4133, 1.3627, 1.3756, 1.5241, 1.5100,\n",
      "        1.3412, 1.5102, 1.6417, 1.5415, 1.5030, 1.4401, 1.6374, 1.4676, 1.5254,\n",
      "        1.5529, 1.5110, 1.3571, 1.5625, 1.4325, 1.5798, 1.5233, 1.4494, 1.4753,\n",
      "        1.4414, 1.3855, 1.4820, 1.4254, 1.4688, 1.3383, 1.3827, 1.3690, 1.4682,\n",
      "        1.4662, 1.7267, 1.4543, 1.3516, 1.4722, 1.6171, 1.5253, 1.4474, 1.3189,\n",
      "        1.4089, 1.3551, 1.5193, 1.4249, 1.5230, 1.4803, 1.4139, 1.5222, 1.4239,\n",
      "        1.4809, 1.4049, 1.4455, 1.4489, 1.3814, 1.4604, 1.2164, 1.4956, 1.4626,\n",
      "        1.4942, 1.6239, 1.4984, 1.4079, 1.4709, 1.5461, 1.6842, 1.3861, 1.5867,\n",
      "        1.3201, 1.5450, 1.2794, 1.6314, 1.4500, 1.5883, 1.5297, 1.3471, 1.5906,\n",
      "        1.4816, 1.4757, 1.4551, 1.5862, 1.6031, 1.5488, 1.4047, 1.4092, 1.4658,\n",
      "        1.2585, 1.3334, 1.5429, 1.5468, 1.5040, 1.5066, 1.2819, 1.4571, 1.5974,\n",
      "        1.5811, 1.4632, 1.5132, 1.5067, 1.5588, 1.4704, 1.3484, 1.4587, 1.4514,\n",
      "        1.4508, 1.5357, 1.4146, 1.4526, 1.4721, 1.3477, 1.5609, 1.5478, 1.5953,\n",
      "        1.4954, 1.5843, 1.5550, 1.3772, 1.6951], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0371, -0.0187, -0.0606,  ..., -0.0127, -0.0355, -0.0633],\n",
      "        [-0.0507,  0.0054, -0.0359,  ..., -0.0216, -0.0461, -0.0186],\n",
      "        [ 0.0676,  0.0108,  0.0378,  ...,  0.0232, -0.0618, -0.0457],\n",
      "        ...,\n",
      "        [ 0.0060,  0.0097, -0.0132,  ...,  0.0244,  0.0225, -0.0103],\n",
      "        [ 0.0241, -0.0122, -0.0133,  ...,  0.0030,  0.0519, -0.0289],\n",
      "        [ 0.0484,  0.0609,  0.0317,  ...,  0.0457,  0.0034,  0.0320]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0016, -0.0008, -0.0017,  ..., -0.0049, -0.0024,  0.0063],\n",
      "        [-0.0025, -0.0004,  0.0062,  ...,  0.0029, -0.0031,  0.0017],\n",
      "        [ 0.0009, -0.0042, -0.0025,  ..., -0.0022,  0.0012, -0.0039],\n",
      "        ...,\n",
      "        [ 0.0016, -0.0052, -0.0011,  ..., -0.0069,  0.0014,  0.0053],\n",
      "        [ 0.0042,  0.0101, -0.0061,  ..., -0.0034,  0.0001, -0.0087],\n",
      "        [ 0.0065,  0.0052, -0.0038,  ..., -0.0051, -0.0062,  0.0020]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3865, 4.9361, 1.3601,  ..., 5.0171, 1.0016, 1.6570], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0183, -0.0188,  0.0183,  ...,  0.0007, -0.0178,  0.0315],\n",
      "        [ 0.0165,  0.0178,  0.0152,  ..., -0.0100,  0.0268,  0.0189],\n",
      "        [ 0.0103, -0.0097,  0.0041,  ..., -0.0080, -0.0188, -0.0018],\n",
      "        ...,\n",
      "        [ 0.0003, -0.0071, -0.0373,  ..., -0.0251, -0.0209, -0.0128],\n",
      "        [ 0.0206, -0.0041,  0.0282,  ..., -0.0191, -0.0226,  0.0165],\n",
      "        [-0.0189,  0.0078,  0.0081,  ..., -0.0185,  0.0311, -0.0196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 2.3910e-03,  7.9533e-03,  2.9776e-03,  ..., -2.7017e-03,\n",
      "          3.8096e-03,  8.0683e-03],\n",
      "        [-2.9910e-03, -2.4004e-03,  2.4795e-03,  ...,  2.9563e-03,\n",
      "         -3.0261e-03, -1.4810e-03],\n",
      "        [ 3.9762e-03,  7.0255e-03,  8.8544e-04,  ...,  4.4566e-04,\n",
      "          2.8866e-03, -1.2075e-03],\n",
      "        ...,\n",
      "        [-1.6495e-03,  4.1544e-04, -2.8901e-03,  ..., -2.1427e-03,\n",
      "         -8.7514e-04,  7.9458e-04],\n",
      "        [-2.9102e-03,  7.2048e-03,  2.5755e-05,  ...,  3.1281e-03,\n",
      "         -1.3793e-03,  9.0868e-03],\n",
      "        [-3.1454e-03,  6.6309e-04,  2.1899e-03,  ...,  3.3657e-03,\n",
      "         -1.8489e-03,  3.0816e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([3.8158, 3.7313, 3.7682, 3.6701, 3.7975, 3.5698, 3.6008, 3.6832, 3.7082,\n",
      "        3.7549, 3.6215, 3.5841, 3.6771, 3.6812, 3.7028, 3.8599, 3.7017, 3.7598,\n",
      "        3.6324, 3.6409, 3.6792, 3.7072, 3.6478, 3.7090, 3.5855, 3.8392, 3.4470,\n",
      "        3.5967, 3.9736, 3.5752, 3.7546, 4.7796, 3.7559, 3.5594, 3.6562, 3.8220,\n",
      "        3.7366, 3.7439, 3.3988, 3.7585, 3.8099, 3.8845, 3.8646, 3.4257, 3.7159,\n",
      "        3.7573, 3.9585, 3.6225, 3.6361, 3.7539, 4.0764, 3.7528, 3.4824, 3.7196,\n",
      "        4.0250, 3.5405, 3.8170, 3.8632, 3.8169, 3.6763, 3.3963, 3.8738, 8.6778,\n",
      "        3.7122, 3.6975, 3.7292, 3.6464, 3.8172, 3.7671, 3.8032, 3.6443, 3.7244,\n",
      "        3.3606, 3.7398, 3.7408, 3.5663, 3.5351, 3.6582, 3.6904, 3.6378, 3.6274,\n",
      "        3.7782, 3.6739, 3.7752, 3.7254, 3.9704, 3.5985, 3.7786, 3.2509, 3.5739,\n",
      "        3.6843, 3.7200, 4.1238, 3.5838, 3.8689, 3.7186, 3.7678, 3.6121, 3.8845,\n",
      "        3.9948, 3.8388, 3.7570, 3.6157, 3.5547, 3.6635, 3.8142, 3.5971, 3.8189,\n",
      "        3.7421, 3.6108, 3.8272, 3.5700, 3.7223, 3.6822, 3.8673, 4.0349, 3.9045,\n",
      "        3.6312, 3.8683, 3.7471, 3.6769, 3.5589, 3.8133, 3.7354, 3.6952, 3.7778,\n",
      "        3.7396, 3.9400, 3.7692, 3.7045, 3.3548, 3.6971, 3.7446, 4.0565, 3.7292,\n",
      "        3.6895, 3.5430, 3.7651, 3.5551, 3.5552, 3.5628, 3.6489, 3.6166, 3.6374,\n",
      "        3.7188, 3.4260, 3.7236, 3.6986, 3.6842, 3.7623, 3.9959, 3.5157, 3.7502,\n",
      "        3.7290, 3.7794, 3.6833, 3.5231, 3.6049, 3.5953, 3.9065, 3.5011, 3.6356,\n",
      "        3.8786, 3.7241, 3.8796, 4.0845, 3.6611, 3.8714, 3.7228, 3.8306, 3.7802,\n",
      "        3.4792, 3.6635, 3.7139, 3.6157, 3.6125, 3.6931, 3.7806, 3.8247, 3.5663,\n",
      "        3.8301, 3.8019, 3.6024, 3.6817, 3.9709, 3.7633, 3.6927, 3.7736, 3.7811,\n",
      "        3.6660, 3.5456, 3.6788, 7.5583, 3.6284, 3.6089, 3.5906, 3.7605, 3.7496,\n",
      "        3.7791, 3.9879, 3.6310, 3.5872, 3.6986, 3.5824, 3.7951, 3.5375, 3.7431,\n",
      "        3.2953, 3.6533, 3.6388, 3.6473, 3.5792, 3.7322, 3.6154, 3.6513, 3.6535,\n",
      "        3.5300, 3.7108, 3.4484, 3.6711, 3.8312, 3.4773, 3.8541, 3.7395, 3.7205,\n",
      "        3.6981, 3.7493, 3.6465, 3.5442, 3.8430, 3.7342, 3.8806, 3.7956, 3.7699,\n",
      "        3.7441, 3.9130, 3.4696, 3.6633, 3.7718, 3.7863, 3.6731, 3.6169, 3.6459,\n",
      "        3.6360, 3.6612, 3.6223, 3.7287, 3.6742, 3.7596, 3.8168, 3.6771, 3.5887,\n",
      "        3.8351, 3.3646, 3.7137, 3.7768, 3.7240, 3.1596, 3.4910, 3.7217, 3.6838,\n",
      "        3.6257, 3.7355, 3.9317, 4.0093, 3.4769, 3.9194, 4.2747, 3.7573, 3.6643,\n",
      "        3.6271, 3.8929, 3.7547, 3.7304, 3.6696, 3.6613, 3.8137, 3.6857, 3.6909,\n",
      "        3.6320, 3.5048, 3.7336, 3.7303, 3.7113, 3.8997, 3.7612, 3.6027, 3.8530,\n",
      "        3.3916, 3.8341, 3.8373, 3.6149, 3.7661, 3.7623, 3.5556, 3.6281, 3.5701,\n",
      "        3.7055, 3.5714, 3.7255, 3.6896, 3.6174, 3.8991, 3.6555, 3.7181, 3.8792,\n",
      "        3.6074, 3.6263, 3.7477, 3.7154, 3.6327, 3.6159, 3.7676, 3.4983, 3.8433,\n",
      "        3.5141, 3.9003, 3.7757, 3.6147, 3.8470], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0038,  0.0126,  0.0373,  ...,  0.0202,  0.0398, -0.0097],\n",
      "        [-0.0154,  0.0405,  0.0169,  ...,  0.0541, -0.0035, -0.0715],\n",
      "        [ 0.0418,  0.0242, -0.0560,  ..., -0.0181,  0.0464,  0.0305],\n",
      "        ...,\n",
      "        [-0.0586, -0.0533,  0.0426,  ..., -0.0380, -0.0191,  0.0203],\n",
      "        [-0.0008,  0.0102,  0.0215,  ..., -0.0462, -0.0030, -0.0594],\n",
      "        [-0.0249, -0.0604,  0.0649,  ...,  0.0013, -0.0592, -0.0410]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.4693e-04,  8.0212e-04, -6.4261e-04,  ...,  3.3346e-03,\n",
      "         -1.9082e-03, -3.6855e-03],\n",
      "        [ 7.9652e-03,  7.2321e-03,  7.0172e-03,  ..., -9.7533e-03,\n",
      "          7.0298e-03, -5.2521e-03],\n",
      "        [-9.5514e-04, -2.6249e-03,  4.2153e-03,  ...,  6.6700e-04,\n",
      "          3.8882e-05, -7.4155e-04],\n",
      "        ...,\n",
      "        [-5.1466e-03, -2.2295e-03, -1.6142e-03,  ...,  1.5271e-03,\n",
      "         -1.2231e-03,  2.7843e-03],\n",
      "        [ 4.6138e-03,  5.7719e-04,  4.6630e-05,  ..., -2.8891e-03,\n",
      "          4.2212e-03, -3.8590e-03],\n",
      "        [-1.4824e-03,  2.1541e-03, -4.2790e-04,  ..., -6.2129e-04,\n",
      "         -2.9214e-04,  2.5955e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6866, 1.4535, 1.4356, 1.4032, 1.4269, 2.3852, 2.0737, 2.5706, 1.8121,\n",
      "        1.4659, 1.3665, 1.3722, 1.4989, 1.2939, 2.6007, 1.9786, 1.0943, 1.1288,\n",
      "        1.4602, 1.9604, 2.4139, 4.6464, 4.2541, 3.4202, 1.0151, 1.2240, 1.2697,\n",
      "        2.0285, 2.6137, 4.4716, 4.1421, 3.6053, 1.4905, 1.5270, 1.7743, 1.7976,\n",
      "        1.8901, 1.5429, 2.2215, 1.8782, 1.5066, 1.5605, 2.1818, 1.8152, 1.8525,\n",
      "        1.7296, 2.2182, 2.2729, 1.6204, 1.9280, 1.9438, 1.8141, 2.1108, 1.7560,\n",
      "        2.3164, 2.3079, 1.5406, 1.9422, 1.8362, 1.9636, 2.2469, 2.5333, 2.5666,\n",
      "        2.3427, 2.1425, 1.7636, 2.3590, 2.2673, 2.2605, 2.1519, 2.7263, 2.4155,\n",
      "        1.4753, 1.8944, 2.1558, 2.0266, 2.0010, 2.5119, 2.6906, 2.4157, 2.2255,\n",
      "        1.8598, 1.4361, 1.7511, 1.6137, 2.0910, 2.2938, 2.2314, 2.1557, 1.8533,\n",
      "        1.7139, 1.6089, 2.4313, 2.4357, 2.0884, 2.2582, 1.2927, 1.4449, 1.3997,\n",
      "        1.4145, 1.4157, 2.3295, 1.9843, 2.5988, 1.4042, 1.4947, 1.3932, 1.2872,\n",
      "        1.4189, 1.4362, 2.0568, 2.1909, 2.0338, 1.6273, 1.5142, 1.5623, 1.2423,\n",
      "        2.0326, 2.2042, 2.1520, 1.9354, 1.3328, 1.3779, 1.5569, 2.0332, 1.5846,\n",
      "        1.9732, 2.5275, 1.8460, 1.3981, 1.4996, 1.4091, 1.4323, 2.0766, 1.9491,\n",
      "        2.5158, 1.4938, 1.6684, 1.6131, 1.4115, 1.9570, 1.6780, 2.0534, 2.4810,\n",
      "        1.7442, 1.6254, 2.1116, 1.9682, 2.2004, 1.8783, 2.3855, 2.1842, 2.4149,\n",
      "        1.9498, 1.7456, 2.2352, 1.7413, 2.4938, 2.5198, 2.6854, 1.9613, 1.7365,\n",
      "        2.0542, 2.0071, 2.4377, 2.5986, 2.9808, 2.7885, 1.6620, 1.9118, 1.9764,\n",
      "        2.2291, 2.0939, 2.5230, 2.7825, 2.8202, 2.0312, 1.5939, 1.7563, 2.0956,\n",
      "        1.7249, 1.9187, 2.3694, 2.5301, 1.7263, 1.7783, 1.9992, 1.8579, 1.7518,\n",
      "        1.7877, 2.1802, 2.3986, 1.5141, 1.5569, 1.4208, 1.4655, 1.7978, 2.2263,\n",
      "        2.0365, 2.7468, 1.6414, 1.5181, 1.7453, 1.6916, 1.7928, 1.7142, 1.9374,\n",
      "        2.3287, 1.1434, 1.1451, 1.4838, 1.7442, 2.5064, 3.6669, 3.8161, 3.3107,\n",
      "        0.8270, 1.2225, 1.2791, 2.0542, 2.2228, 3.6712, 3.5324, 3.3301, 1.6440,\n",
      "        1.9346, 1.9001, 2.0502, 1.9685, 2.1603, 2.4447, 2.3426, 1.9392, 1.8303,\n",
      "        1.8848, 1.9963, 1.9458, 1.9249, 2.1917, 2.4545, 1.8563, 1.4455, 1.7743,\n",
      "        1.8113, 1.9270, 1.6406, 2.2915, 2.0149, 1.4423, 1.7411, 1.9064, 1.7383,\n",
      "        1.9502, 1.6755, 2.3535, 2.3627, 1.3772, 1.3428, 1.7940, 1.6989, 1.6545,\n",
      "        1.7557, 2.4628, 2.1284, 1.3822, 1.5681, 1.7303, 1.7640, 1.7166, 1.9078,\n",
      "        2.0478, 2.0775, 1.4222, 1.4865, 1.7945, 1.6703, 1.6387, 1.8276, 2.2845,\n",
      "        2.3539, 1.4851, 1.3741, 1.4995, 1.6400, 1.5566, 1.7712, 2.5250, 2.5130,\n",
      "        1.8847, 1.7244, 1.6912, 1.5000, 1.6621, 1.8413, 2.3320, 2.3267, 1.5932,\n",
      "        1.4537, 1.6078, 1.8394, 1.5383, 1.9864, 2.2153, 2.4229, 1.6036, 1.1829,\n",
      "        1.2316, 1.4224, 1.3547, 1.8578, 2.0730, 2.3580, 1.5240, 1.3233, 1.4465,\n",
      "        1.6680, 1.6367, 1.7563, 1.9558, 2.0816], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0253,  0.0143,  0.0272,  ...,  0.0247, -0.0459, -0.0584],\n",
      "        [-0.0129, -0.0200,  0.0364,  ...,  0.0244,  0.0355,  0.0376],\n",
      "        [-0.0481,  0.0084,  0.0460,  ..., -0.0289,  0.0194, -0.0169],\n",
      "        ...,\n",
      "        [-0.0224,  0.0068,  0.0294,  ..., -0.0342,  0.0487, -0.0049],\n",
      "        [-0.0234, -0.0466, -0.0068,  ...,  0.0586,  0.0184,  0.0089],\n",
      "        [-0.0335,  0.0260,  0.0340,  ...,  0.0149,  0.0525, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0006,  0.0002, -0.0018,  ..., -0.0009, -0.0021, -0.0016],\n",
      "        [ 0.0077, -0.0010,  0.0053,  ..., -0.0035,  0.0094,  0.0112],\n",
      "        [ 0.0022, -0.0025,  0.0003,  ..., -0.0027,  0.0030,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0044, -0.0033,  0.0001,  ..., -0.0005, -0.0022,  0.0031],\n",
      "        [ 0.0031, -0.0019, -0.0022,  ...,  0.0007,  0.0007, -0.0003],\n",
      "        [-0.0040,  0.0003,  0.0022,  ...,  0.0019, -0.0011, -0.0010]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6804, 1.4312, 1.7632, 1.6829, 1.6658, 1.3030, 1.8228, 1.7523, 1.8037,\n",
      "        1.7643, 1.6541, 1.7161, 1.6563, 2.2758, 1.6483, 1.7475, 1.1291, 1.4106,\n",
      "        1.3735, 2.0055, 1.8996, 3.2230, 2.9060, 2.8115, 1.1091, 0.9372, 1.4466,\n",
      "        1.9058, 2.2822, 3.0589, 2.9914, 2.7038, 1.4659, 1.5942, 1.9922, 1.8195,\n",
      "        1.9011, 2.2834, 2.0004, 1.9102, 1.4396, 1.4603, 1.8870, 1.8789, 1.8542,\n",
      "        1.6486, 2.1009, 2.1298, 1.6688, 2.0841, 1.8228, 1.7085, 1.7895, 2.2548,\n",
      "        2.3051, 2.0200, 1.4012, 1.6631, 1.9242, 1.9923, 1.8909, 1.5532, 2.1735,\n",
      "        2.0415, 1.8904, 1.7153, 2.0665, 2.0782, 1.8501, 2.0931, 2.4561, 2.1443,\n",
      "        1.7472, 1.9456, 2.2829, 2.0455, 1.9957, 1.9188, 2.5717, 2.2806, 1.7948,\n",
      "        1.8819, 1.7319, 1.7294, 1.9802, 1.9561, 2.0049, 2.0621, 2.2660, 1.8378,\n",
      "        1.9124, 1.8739, 1.3209, 1.4631, 2.1010, 1.9245, 1.4868, 1.4951, 1.7237,\n",
      "        1.8858, 1.4261, 1.2667, 1.8919, 1.5370, 1.3253, 1.7201, 1.7185, 1.5546,\n",
      "        1.7895, 1.9630, 1.8605, 1.6553, 1.8444, 1.6379, 1.7659, 1.6625, 1.9541,\n",
      "        1.4385, 1.9758, 2.0760, 2.0120, 1.7897, 1.7209, 1.8053, 1.3979, 2.2972,\n",
      "        2.1513, 1.7847, 1.8100, 1.5994, 1.6590, 1.7053, 1.8568, 1.3782, 1.9122,\n",
      "        1.8173, 1.4105, 1.6441, 1.8566, 1.7425, 1.3311, 1.9972, 2.0427, 1.8309,\n",
      "        2.0352, 1.7602, 1.9358, 2.0157, 1.7579, 2.1443, 1.9916, 2.0207, 1.5931,\n",
      "        1.7363, 1.8222, 1.9492, 2.0308, 1.8821, 2.2560, 2.2507, 1.9091, 1.8770,\n",
      "        1.8717, 2.0466, 2.0216, 1.9671, 2.8004, 2.6124, 1.6558, 1.7206, 2.1442,\n",
      "        2.0522, 2.0198, 2.3078, 2.5454, 2.3588, 1.7727, 1.5073, 1.9534, 1.9706,\n",
      "        1.9830, 2.0170, 2.4423, 2.4092, 1.8785, 1.8771, 1.7692, 2.0100, 1.7554,\n",
      "        1.7835, 2.0930, 2.2547, 1.7838, 1.6222, 1.7333, 1.8632, 1.5033, 1.2241,\n",
      "        1.9650, 1.8492, 1.9312, 1.7540, 1.5485, 1.3139, 1.3325, 2.1866, 1.9205,\n",
      "        1.9814, 1.1334, 1.2856, 1.5363, 1.9543, 2.2021, 2.7576, 2.7763, 2.5337,\n",
      "        0.8817, 1.1849, 1.1244, 1.7190, 1.8327, 2.8477, 2.5575, 2.6212, 1.7239,\n",
      "        1.8351, 1.9247, 1.8927, 1.6442, 1.8031, 2.2911, 2.2670, 1.7471, 1.9012,\n",
      "        1.8331, 2.0215, 1.8865, 1.7262, 1.9933, 2.2126, 1.7646, 1.5412, 1.7722,\n",
      "        1.8882, 1.8534, 1.9506, 2.1739, 1.9766, 1.3368, 1.6345, 1.8314, 1.7503,\n",
      "        1.9459, 1.8472, 2.2595, 2.3220, 1.2797, 1.5676, 1.8580, 1.8535, 1.7605,\n",
      "        1.8547, 1.9963, 2.1381, 1.5671, 1.4577, 1.7911, 1.8071, 1.9191, 1.7474,\n",
      "        2.4889, 1.9786, 1.2838, 1.3820, 1.7185, 1.7624, 1.7025, 1.7066, 2.1439,\n",
      "        1.8851, 1.0061, 1.3237, 1.6777, 1.9591, 1.7438, 1.7304, 1.9686, 1.9642,\n",
      "        1.7854, 1.7426, 1.7446, 1.7545, 1.7907, 1.7131, 2.2973, 2.0927, 1.6614,\n",
      "        1.6581, 1.8237, 1.8053, 1.6986, 1.5932, 2.0823, 2.2340, 1.4292, 1.5205,\n",
      "        1.4674, 1.3382, 1.8008, 1.2430, 1.4637, 1.5287, 1.7738, 1.4355, 1.4003,\n",
      "        1.2866, 1.1179, 1.2524, 1.6135, 1.5643], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0163, -0.0053, -0.0142,  ...,  0.0509, -0.0263,  0.0457],\n",
      "        [-0.0449, -0.0393,  0.0141,  ...,  0.0313, -0.0147, -0.0411],\n",
      "        [-0.0214,  0.0325,  0.0221,  ...,  0.0056, -0.0150,  0.0251],\n",
      "        ...,\n",
      "        [ 0.0267, -0.0340, -0.0475,  ...,  0.0569,  0.0468, -0.0015],\n",
      "        [-0.0207, -0.0462, -0.0421,  ..., -0.0387,  0.0415, -0.0495],\n",
      "        [ 0.0318,  0.0172,  0.0497,  ...,  0.0441, -0.0271, -0.0429]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 3.5741e-03, -2.7113e-03,  4.7155e-03,  ...,  1.3559e-03,\n",
      "         -3.9017e-03,  3.6749e-03],\n",
      "        [ 5.2727e-03, -5.8218e-03,  6.0455e-03,  ...,  3.1394e-03,\n",
      "         -7.3040e-03,  7.6471e-03],\n",
      "        [ 5.7551e-04, -1.2623e-03,  1.2711e-03,  ...,  4.6952e-03,\n",
      "          1.4855e-03,  6.6021e-04],\n",
      "        ...,\n",
      "        [ 1.4374e-03, -2.7482e-03,  4.7813e-03,  ...,  7.2346e-04,\n",
      "         -2.3233e-03,  2.6733e-03],\n",
      "        [-7.2635e-04, -1.5829e-03, -3.2044e-04,  ..., -1.1612e-03,\n",
      "          9.2989e-05,  2.9626e-03],\n",
      "        [-5.3568e-03,  3.6961e-03, -4.6006e-03,  ..., -3.6229e-03,\n",
      "          5.1761e-03, -7.1440e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0161, 1.8022, 1.7274, 1.8749, 1.9391, 1.7906, 1.7716, 1.8360, 1.7675,\n",
      "        1.8785, 1.7177, 1.8126, 1.7632, 1.8199, 1.9636, 1.8565, 1.5499, 1.4398,\n",
      "        1.5283, 1.4759, 1.4978, 1.4689, 1.4122, 1.4250, 1.4662, 1.4121, 1.4262,\n",
      "        1.4782, 1.5555, 1.4699, 1.4939, 1.4524, 1.7828, 1.8710, 1.8247, 1.7365,\n",
      "        1.8894, 1.7156, 1.8575, 1.6856, 1.8767, 1.9304, 1.7371, 1.7812, 1.6846,\n",
      "        1.7777, 1.7904, 1.8416, 1.7998, 1.7595, 2.2114, 1.9465, 1.8392, 1.8855,\n",
      "        1.8603, 1.8615, 1.9630, 1.9848, 1.7977, 1.8073, 2.0208, 2.1606, 1.8936,\n",
      "        1.8784, 2.1676, 2.0824, 1.7797, 2.3179, 1.7906, 2.2757, 1.7583, 1.9147,\n",
      "        1.9444, 1.9798, 1.9796, 1.9178, 1.8365, 1.9169, 2.0398, 1.9606, 1.8057,\n",
      "        1.9177, 1.7188, 1.7652, 1.7459, 1.7377, 1.7479, 1.7444, 1.8736, 1.7531,\n",
      "        1.7851, 1.6665, 1.7593, 1.7749, 1.7356, 1.7838, 1.7686, 1.7921, 1.8657,\n",
      "        1.8839, 1.8697, 1.8852, 1.7273, 1.9116, 1.9349, 1.8786, 1.9329, 1.8450,\n",
      "        1.9095, 1.9114, 1.8766, 1.8550, 1.8348, 1.8163, 1.7040, 1.8781, 1.8589,\n",
      "        1.8790, 1.7726, 1.8361, 1.7716, 1.8045, 1.8196, 1.7486, 1.8193, 1.7043,\n",
      "        1.7881, 1.8480, 1.8516, 1.8247, 1.8013, 1.7066, 1.8557, 1.7447, 1.7344,\n",
      "        1.9070, 1.7179, 1.8143, 1.7340, 1.7802, 1.7604, 1.8495, 1.9178, 1.7883,\n",
      "        1.6574, 1.5915, 1.7125, 1.6648, 1.7785, 1.6795, 1.7250, 1.6437, 1.6969,\n",
      "        1.7041, 1.7604, 1.7951, 1.7417, 1.7409, 1.6820, 1.7864, 1.8987, 1.8639,\n",
      "        1.8811, 1.8287, 2.0607, 2.0615, 1.8839, 1.8388, 1.9292, 1.8492, 1.9292,\n",
      "        1.9076, 1.9331, 1.8279, 1.8820, 1.9488, 1.8854, 2.0296, 1.9005, 1.9518,\n",
      "        2.0684, 1.8733, 1.9405, 1.9390, 1.9215, 1.9184, 1.9600, 1.9056, 1.9941,\n",
      "        1.9376, 1.9501, 1.9335, 1.3763, 1.3893, 1.3079, 1.4199, 1.2980, 1.3209,\n",
      "        1.4205, 1.3320, 1.3797, 1.3427, 1.5064, 1.4100, 1.4242, 1.4016, 1.3398,\n",
      "        1.3613, 1.5989, 1.5294, 1.4054, 1.5622, 1.5122, 1.5252, 1.5080, 1.5069,\n",
      "        1.4990, 1.3667, 1.4230, 1.5170, 1.4226, 1.4537, 1.5025, 1.4377, 2.1230,\n",
      "        1.9335, 2.0110, 1.8865, 2.1238, 1.9798, 1.8557, 1.8924, 1.8617, 1.9010,\n",
      "        1.7894, 2.1175, 2.1771, 1.9729, 1.9108, 1.8648, 1.7306, 1.8127, 1.7281,\n",
      "        1.7807, 1.7178, 1.6930, 1.7802, 1.7727, 1.7716, 1.7740, 1.8338, 1.6575,\n",
      "        1.6936, 1.7496, 1.7236, 1.8200, 1.8319, 1.7481, 1.8165, 1.7583, 1.7042,\n",
      "        1.6773, 1.7612, 1.7804, 1.7488, 1.7341, 1.7166, 1.8158, 1.8196, 1.8008,\n",
      "        1.6611, 1.7423, 1.8728, 1.9848, 1.8931, 2.0535, 1.8458, 1.8782, 1.9122,\n",
      "        1.8504, 1.7798, 1.9246, 1.8651, 1.8872, 1.9667, 2.0298, 1.9249, 1.9616,\n",
      "        1.9093, 1.8096, 1.8939, 1.8816, 1.9004, 2.0652, 1.8721, 1.9878, 1.8324,\n",
      "        1.9088, 1.9722, 1.8694, 1.9352, 1.9200, 1.8307, 1.9245, 1.7480, 1.7827,\n",
      "        1.8042, 1.6255, 2.0736, 1.7447, 1.6738, 1.8338, 1.7962, 1.7489, 1.9284,\n",
      "        1.6813, 1.7416, 1.7384, 1.7587, 1.8615], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0154,  0.0426, -0.0001,  ...,  0.0434,  0.0244, -0.0471],\n",
      "        [-0.0451,  0.0281, -0.0140,  ...,  0.0099,  0.0142,  0.0184],\n",
      "        [ 0.0307,  0.0253, -0.0134,  ...,  0.0279, -0.0112, -0.0452],\n",
      "        ...,\n",
      "        [ 0.0040,  0.0489, -0.0271,  ..., -0.0009,  0.0229,  0.0407],\n",
      "        [ 0.0432, -0.0079,  0.0491,  ...,  0.0148,  0.0008, -0.0011],\n",
      "        [-0.0367, -0.0128,  0.0069,  ..., -0.0366, -0.0177,  0.0066]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0123, -0.0114,  0.0101,  ..., -0.0078,  0.0082, -0.0106],\n",
      "        [-0.0042,  0.0036, -0.0040,  ...,  0.0063,  0.0028,  0.0018],\n",
      "        [ 0.0120, -0.0118,  0.0113,  ..., -0.0111,  0.0114, -0.0122],\n",
      "        ...,\n",
      "        [-0.0032,  0.0045, -0.0043,  ...,  0.0010,  0.0021,  0.0040],\n",
      "        [-0.0030,  0.0016, -0.0041,  ...,  0.0059, -0.0023,  0.0014],\n",
      "        [-0.0033,  0.0010, -0.0020,  ...,  0.0075, -0.0095,  0.0030]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6067, 1.7699, 1.6560, 1.8599, 1.7666, 1.6438, 1.6780, 1.5704, 1.7809,\n",
      "        1.8147, 1.3654, 1.6053, 1.7102, 1.6984, 1.7449, 1.6612, 1.8372, 1.7883,\n",
      "        1.6593, 1.8357, 1.8280, 1.6550, 1.6989, 1.5917, 1.6470, 1.5314, 1.6057,\n",
      "        1.6994, 1.7371, 1.8625, 1.6628, 1.4332, 1.6512, 1.4492, 1.6932, 1.7548,\n",
      "        1.6830, 1.6827, 1.6157, 1.6629, 1.6877, 1.6801, 1.7755, 1.6177, 1.7568,\n",
      "        1.5836, 1.6130, 1.5391, 1.6769, 1.7641, 1.7590, 1.7493, 1.5144, 1.6580,\n",
      "        1.5925, 1.6764, 1.5464, 1.7122, 1.7850, 1.6377, 1.7118, 1.7548, 5.9821,\n",
      "        1.7399, 1.7180, 1.6979, 1.5838, 1.7083, 1.7231, 1.8718, 1.7131, 1.8221,\n",
      "        1.5683, 1.7577, 1.6334, 1.7730, 1.7694, 1.6753, 1.7407, 1.7506, 1.5321,\n",
      "        1.6635, 1.7031, 1.6360, 1.5938, 1.6999, 1.6793, 1.7242, 1.4769, 1.8228,\n",
      "        1.8342, 1.6127, 1.8996, 1.6430, 1.7105, 1.7355, 1.7319, 1.8196, 1.7682,\n",
      "        1.8609, 1.9063, 1.6737, 1.7495, 1.6110, 1.7744, 1.8212, 1.5185, 1.6607,\n",
      "        1.7373, 1.6537, 1.6521, 1.6603, 1.6081, 1.5335, 1.8363, 1.5659, 1.6734,\n",
      "        1.7160, 1.5988, 1.8329, 1.6133, 1.7006, 1.7031, 1.7503, 1.6301, 1.8080,\n",
      "        1.6783, 1.6586, 1.5052, 1.7797, 1.4911, 1.8386, 1.7241, 1.7651, 1.7322,\n",
      "        1.6439, 1.6981, 1.6700, 1.5783, 1.7564, 1.5460, 1.6707, 1.7079, 1.6443,\n",
      "        1.6642, 1.8754, 1.6883, 1.6562, 1.6839, 1.6716, 1.9617, 1.7405, 1.9378,\n",
      "        1.7586, 1.8434, 1.6798, 1.5599, 1.7578, 1.9374, 1.6920, 1.6453, 1.7206,\n",
      "        1.6488, 1.7073, 1.7643, 1.6638, 1.6981, 1.7280, 1.7446, 1.7407, 1.5810,\n",
      "        1.7129, 1.6820, 1.6188, 1.7261, 1.7141, 1.5377, 1.7092, 1.7332, 1.6165,\n",
      "        1.6118, 1.6578, 1.6701, 1.7405, 1.6051, 1.6381, 1.5640, 1.6335, 1.6093,\n",
      "        1.6607, 1.6684, 1.6096, 2.5948, 1.5883, 1.4722, 1.6665, 1.6685, 1.7380,\n",
      "        1.6928, 1.8710, 1.7706, 1.7826, 1.7055, 1.5664, 1.8264, 1.6567, 1.7281,\n",
      "        1.3422, 1.7041, 1.6470, 1.6636, 1.6241, 1.5935, 1.7692, 1.6675, 1.8140,\n",
      "        1.6524, 1.8194, 1.4087, 1.5654, 1.5074, 1.6502, 1.6205, 1.7246, 1.7305,\n",
      "        1.6457, 1.6432, 1.5981, 1.6136, 1.6771, 1.6307, 1.5995, 1.6797, 1.7202,\n",
      "        1.5964, 1.6475, 1.6016, 1.5724, 1.7314, 1.7864, 1.5593, 1.6812, 1.6746,\n",
      "        1.6612, 1.5591, 1.8070, 1.6184, 1.6325, 1.7607, 1.7201, 1.3846, 1.6983,\n",
      "        1.7002, 1.5873, 1.6327, 1.7560, 1.6852, 1.5430, 1.4908, 1.7404, 1.6070,\n",
      "        1.6588, 1.7403, 1.6010, 1.6464, 1.7412, 1.7486, 2.0602, 1.5565, 1.6575,\n",
      "        1.5842, 1.7861, 1.6169, 1.6384, 1.8137, 1.6449, 1.7261, 1.5746, 1.7424,\n",
      "        1.8182, 1.6933, 1.6690, 1.7048, 1.7407, 1.7524, 1.6962, 1.7962, 1.6997,\n",
      "        1.4578, 1.6589, 1.6571, 1.8017, 1.7023, 1.7277, 1.4619, 1.7935, 1.7512,\n",
      "        1.6527, 1.5480, 1.8012, 1.7821, 1.6826, 1.6192, 1.5547, 1.6325, 1.6594,\n",
      "        1.7075, 1.6707, 1.5957, 1.6843, 1.6882, 1.4928, 1.6182, 1.7025, 1.6963,\n",
      "        1.7257, 1.7910, 1.6139, 1.6906, 1.6451], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0461, -0.0120,  0.0474,  ...,  0.0193, -0.0118, -0.0636],\n",
      "        [-0.0443,  0.0083,  0.0619,  ..., -0.0502,  0.0140,  0.0444],\n",
      "        [-0.0230,  0.0211, -0.0166,  ...,  0.0527,  0.0123, -0.0120],\n",
      "        ...,\n",
      "        [-0.0220, -0.0435, -0.0168,  ...,  0.0472,  0.0468,  0.0169],\n",
      "        [-0.0462, -0.0286,  0.0130,  ..., -0.0121,  0.0548, -0.0009],\n",
      "        [-0.0574, -0.0103,  0.0401,  ...,  0.0489,  0.0484,  0.0370]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0050,  0.0019, -0.0029,  ..., -0.0013,  0.0038,  0.0007],\n",
      "        [-0.0061, -0.0090,  0.0086,  ...,  0.0078, -0.0073, -0.0083],\n",
      "        [-0.0020, -0.0032,  0.0026,  ...,  0.0026, -0.0034, -0.0015],\n",
      "        ...,\n",
      "        [ 0.0058, -0.0093, -0.0004,  ...,  0.0028,  0.0002,  0.0035],\n",
      "        [ 0.0021,  0.0054, -0.0066,  ..., -0.0046,  0.0052,  0.0063],\n",
      "        [-0.0018,  0.0003,  0.0017,  ...,  0.0073, -0.0038, -0.0116]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6494, 1.5362, 4.6484,  ..., 1.6845, 1.6233, 1.7559], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0189, -0.0194,  0.0210,  ..., -0.0159,  0.0165, -0.0124],\n",
      "        [-0.0080, -0.0274,  0.0227,  ...,  0.0148,  0.0231,  0.0274],\n",
      "        [ 0.0025,  0.0241,  0.0253,  ...,  0.0168,  0.0172, -0.0024],\n",
      "        ...,\n",
      "        [-0.0044,  0.0041, -0.0001,  ...,  0.0138,  0.0223, -0.0245],\n",
      "        [ 0.0156,  0.0138, -0.0209,  ...,  0.0122,  0.0178,  0.0079],\n",
      "        [ 0.0034,  0.0055, -0.0067,  ..., -0.0077, -0.0143, -0.0055]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-2.9953e-03, -6.2527e-05,  1.1107e-03,  ..., -7.5459e-04,\n",
      "          1.3639e-03, -4.9051e-03],\n",
      "        [-3.8394e-03,  4.7387e-03,  1.8952e-03,  ..., -5.8504e-03,\n",
      "          1.0111e-03, -1.7702e-03],\n",
      "        [ 7.3764e-03, -4.9954e-03, -6.3064e-03,  ...,  3.3452e-03,\n",
      "         -1.1051e-02,  1.0445e-02],\n",
      "        ...,\n",
      "        [-6.2072e-03,  5.0953e-03,  4.7067e-03,  ..., -5.1311e-03,\n",
      "          6.0972e-03, -6.6934e-03],\n",
      "        [-9.0366e-03,  1.1381e-02,  2.6796e-03,  ..., -1.1472e-02,\n",
      "         -2.2750e-03, -5.0585e-03],\n",
      "        [ 2.7685e-03,  2.9670e-03, -8.5665e-03,  ..., -3.8730e-03,\n",
      "         -1.2601e-02,  1.0875e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 3.5976,  3.7360,  3.5378,  3.7898,  3.9594,  3.5983,  3.6628,  3.5400,\n",
      "         3.6022,  3.7113,  3.4957,  3.8445,  3.7661,  3.5418,  3.8489,  3.6591,\n",
      "         3.5921,  3.5429,  3.5408,  3.6015,  3.7805,  3.9293,  3.6091,  3.6518,\n",
      "         3.4045,  3.6587,  3.5568,  3.6220,  3.6066,  3.6583,  3.6455,  4.5078,\n",
      "         3.7534,  3.5118,  3.6610,  3.6407,  3.6247,  3.4906,  3.5091,  3.8746,\n",
      "         3.5262,  3.6481,  3.6435,  3.5154,  3.9310,  3.5425,  5.2916,  3.5663,\n",
      "         3.5180,  3.9378,  3.7415,  3.6538,  3.4189,  3.6117,  3.6248,  3.5180,\n",
      "         3.8281,  3.8734,  4.0601,  3.5108,  3.5909,  3.8579, 12.3656,  3.7724,\n",
      "         3.8292,  3.7022,  3.7504,  3.5954,  3.8562,  3.8398,  3.7847,  3.7253,\n",
      "         3.4419,  3.9103,  3.7243,  3.5396,  3.5458,  3.5104,  3.6525,  3.4334,\n",
      "         3.6326,  3.5663,  3.7357,  3.7287,  3.6249,  3.5138,  3.6284,  3.7345,\n",
      "         3.3809,  3.7917,  3.8419,  3.7875,  4.4636,  3.7077,  3.7527,  3.7823,\n",
      "         3.6071,  3.8138,  3.7372,  3.8631,  3.7323,  3.7734,  3.7551,  3.5779,\n",
      "         3.5206,  3.8311,  3.6972,  3.6886,  3.7207,  3.6479,  3.7766,  3.8418,\n",
      "         3.7247,  3.6253,  3.7463,  3.4999,  3.7271,  3.7041,  3.5897,  3.7149,\n",
      "         3.7159,  3.6865,  3.7167,  3.8160,  3.7739,  3.6141,  3.6051,  3.6237,\n",
      "         3.6461,  3.4501,  3.4464,  3.6636,  3.8883,  3.7588,  3.6346,  3.6509,\n",
      "         3.6183,  3.7988,  3.6438,  3.6923,  3.5463,  3.5713,  3.5339,  3.7541,\n",
      "         3.7545,  3.7779,  3.7424,  3.6678,  3.7875,  3.7656,  5.2095,  3.6403,\n",
      "         3.6686,  3.7556,  3.9532,  3.6412,  3.7637,  3.9007,  4.0419,  3.6412,\n",
      "         3.7110,  3.8426,  3.5605,  3.6596,  3.7992,  3.9965,  3.5933,  3.6127,\n",
      "         3.5586,  3.6031,  3.4925,  3.5676,  3.5813,  3.8951,  3.6085,  3.4729,\n",
      "         3.6698,  3.6955,  3.8538,  3.8335,  3.6015,  3.5941,  3.6627,  3.5499,\n",
      "         3.7062,  3.7600,  3.5706,  3.7742,  3.7683,  3.7502,  3.6105,  3.3881,\n",
      "         6.3370,  3.6086,  3.6119,  3.5217,  3.6886,  3.7014,  3.7991,  3.8542,\n",
      "         3.5944,  3.6578,  3.8276,  3.6659,  3.7852,  3.7640,  3.5160,  3.2752,\n",
      "         3.6554,  3.5671,  3.7976,  3.5740,  3.5443,  4.2013,  3.4503,  3.6715,\n",
      "         3.6261,  3.7691,  3.3511,  3.5190,  3.4848,  3.6057,  3.9777,  3.6860,\n",
      "         3.5795,  3.7487,  3.5583,  3.7159,  3.6211,  3.4697,  3.5819,  3.4728,\n",
      "         3.6096,  3.7175,  3.7572,  3.8422,  3.7659,  3.4552,  3.7396,  3.8678,\n",
      "         3.5848,  3.6464,  3.5588,  3.5829,  3.3919,  3.5224,  3.6081,  3.5707,\n",
      "         3.7100,  3.8178,  3.6800,  3.7189,  3.9074,  3.4905,  3.6575,  3.8061,\n",
      "         3.5542,  3.3272,  3.2465,  4.0030,  3.6698,  3.7004,  3.6644,  3.6196,\n",
      "         3.8238,  3.7267,  3.9044,  5.4848,  3.5531,  3.5837,  3.6451,  3.5835,\n",
      "         3.6126,  3.8498,  3.6785,  3.7912,  3.7197,  3.4155,  3.8045,  3.6927,\n",
      "         3.5125,  3.6619,  3.6027,  3.6457,  3.8838,  3.8669,  3.7754,  3.6329,\n",
      "         3.5814,  3.9324,  3.6206,  3.8077,  3.6216,  3.9810,  3.4774,  3.5564,\n",
      "         3.6287,  3.6436,  3.4836,  3.6020,  3.5215,  3.6710,  3.6446,  3.5734,\n",
      "         3.6726,  3.4544,  3.8892,  3.5207,  3.5835,  3.8195,  3.5105,  3.5990,\n",
      "         3.5810,  3.6289,  3.5408,  4.0882,  3.7676,  3.6439,  3.4556,  3.8922],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0135,  0.0424, -0.0434,  ...,  0.0078, -0.0312, -0.0619],\n",
      "        [-0.0021,  0.0653,  0.0104,  ...,  0.0330,  0.0644, -0.0371],\n",
      "        [-0.0606,  0.0187,  0.0409,  ...,  0.0266,  0.0493,  0.0315],\n",
      "        ...,\n",
      "        [ 0.0357,  0.0367,  0.0152,  ..., -0.0191,  0.0175, -0.0395],\n",
      "        [ 0.0396,  0.0169, -0.0022,  ..., -0.0315, -0.0198,  0.0568],\n",
      "        [ 0.0336,  0.0181,  0.0132,  ...,  0.0501,  0.0505,  0.0106]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0046, -0.0035, -0.0027,  ..., -0.0025,  0.0027, -0.0040],\n",
      "        [ 0.0049,  0.0051,  0.0022,  ...,  0.0050, -0.0039,  0.0052],\n",
      "        [-0.0118, -0.0127, -0.0125,  ..., -0.0137,  0.0120, -0.0119],\n",
      "        ...,\n",
      "        [-0.0090, -0.0084, -0.0090,  ..., -0.0083,  0.0083, -0.0090],\n",
      "        [ 0.0025,  0.0023,  0.0038,  ...,  0.0019, -0.0019,  0.0023],\n",
      "        [-0.0024, -0.0028, -0.0024,  ..., -0.0029,  0.0030, -0.0026]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6348, 1.7485, 1.7441, 1.8784, 1.9193, 1.6815, 2.1911, 2.5716, 1.5435,\n",
      "        1.7007, 1.9425, 2.0766, 1.9870, 1.6495, 2.4465, 2.1084, 1.5200, 1.3482,\n",
      "        1.7116, 1.6745, 2.5337, 3.4355, 3.6093, 4.1405, 1.0337, 1.4878, 1.6841,\n",
      "        1.9774, 2.3194, 3.5033, 3.6231, 3.8132, 1.3664, 1.3272, 0.9892, 1.4215,\n",
      "        1.8972, 1.9519, 1.6488, 1.8324, 1.1167, 0.7405, 1.3873, 1.2659, 1.2942,\n",
      "        1.4150, 1.6955, 1.8444, 1.5372, 1.6607, 2.0455, 1.9642, 2.2106, 3.5222,\n",
      "        3.3108, 3.5562, 1.3119, 1.7406, 1.9970, 1.8835, 2.2779, 3.4241, 3.2379,\n",
      "        3.3023, 1.6210, 2.2510, 1.6740, 1.3790, 1.7910, 1.7038, 1.9758, 2.0580,\n",
      "        1.9593, 1.9060, 1.7654, 1.7078, 1.4127, 1.8840, 2.0781, 2.1377, 1.3950,\n",
      "        1.1507, 1.3715, 1.5774, 2.2100, 2.0480, 1.8693, 2.0894, 1.5782, 1.5021,\n",
      "        1.2352, 1.2238, 1.3727, 1.6779, 1.9687, 1.9622, 1.7224, 1.7369, 1.8037,\n",
      "        1.7381, 1.6258, 1.6137, 2.3609, 2.2448, 2.0277, 2.0644, 1.9983, 1.5561,\n",
      "        2.0500, 1.4772, 2.4700, 2.1946, 1.6972, 1.5279, 1.4996, 1.0007, 1.4131,\n",
      "        2.1093, 1.8505, 1.8880, 1.6378, 1.2767, 0.9763, 1.7373, 1.4331, 1.5580,\n",
      "        1.9077, 1.8776, 1.4314, 1.5123, 1.3562, 1.3907, 1.2473, 1.8220, 1.7876,\n",
      "        1.3416, 1.7543, 1.6803, 1.6448, 1.2968, 1.6418, 1.3187, 1.9996, 1.8824,\n",
      "        1.0553, 1.1066, 1.0869, 1.6316, 1.4018, 1.5368, 1.7902, 2.3063, 1.5978,\n",
      "        1.1235, 1.5286, 1.4178, 2.1046, 1.9436, 1.8166, 1.9713, 1.8405, 1.3885,\n",
      "        1.0251, 1.2811, 1.7968, 1.5372, 1.7390, 1.9294, 1.2116, 1.3925, 1.4864,\n",
      "        1.5155, 1.4455, 1.9970, 1.9122, 1.9596, 2.1116, 2.3144, 2.2226, 1.9695,\n",
      "        1.9763, 1.7551, 2.3736, 2.3438, 1.7828, 1.8129, 1.9283, 1.8660, 1.6520,\n",
      "        2.6523, 2.3788, 2.3304, 1.8451, 1.2312, 1.6007, 1.2752, 1.9333, 2.0460,\n",
      "        1.7668, 1.9884, 0.9242, 1.3307, 1.0767, 1.7733, 1.4600, 1.4835, 1.7377,\n",
      "        2.0053, 1.9922, 1.7327, 1.5370, 1.5914, 1.7153, 1.8226, 1.9421, 2.2930,\n",
      "        1.7646, 1.9689, 2.0118, 1.8214, 1.6440, 2.1985, 1.5298, 2.2571, 1.0361,\n",
      "        1.2027, 1.6451, 1.9621, 1.8464, 1.4222, 1.8697, 1.8262, 1.6636, 1.0378,\n",
      "        1.1289, 1.2386, 1.3746, 2.0036, 1.7975, 2.2204, 1.9865, 1.7288, 1.8447,\n",
      "        1.8676, 1.7972, 2.3645, 2.9853, 3.0236, 2.0928, 2.0163, 2.0867, 1.9176,\n",
      "        2.4754, 2.3593, 3.0383, 2.9665, 1.5857, 1.3302, 1.1668, 1.0864, 2.1682,\n",
      "        1.5798, 1.9756, 1.8797, 1.7757, 1.0232, 1.5320, 2.0983, 1.5376, 1.9978,\n",
      "        1.7844, 1.9186, 1.6474, 1.3460, 1.5266, 1.2276, 1.2759, 1.8991, 1.8910,\n",
      "        1.9454, 1.5390, 1.5245, 1.3588, 1.7236, 1.8549, 1.4798, 1.7622, 1.9153,\n",
      "        1.6264, 1.2861, 1.2297, 1.7473, 1.9741, 2.2727, 2.0528, 2.0204, 1.9139,\n",
      "        1.3157, 1.3228, 1.2711, 1.6706, 1.4671, 1.9639, 1.9478, 2.1536, 1.4946,\n",
      "        1.5920, 1.9513, 2.2613, 2.1678, 1.8906, 2.2268, 1.0673, 1.4796, 1.5074,\n",
      "        1.6355, 1.5709, 1.8097, 1.9928, 2.2592], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0023,  0.0377, -0.0504,  ...,  0.0362,  0.0350, -0.0504],\n",
      "        [ 0.0432,  0.0058,  0.0286,  ..., -0.0389, -0.0293,  0.0560],\n",
      "        [-0.0298,  0.0079, -0.0498,  ..., -0.0071, -0.0513,  0.0339],\n",
      "        ...,\n",
      "        [ 0.0174, -0.0153, -0.0025,  ..., -0.0066,  0.0032, -0.0461],\n",
      "        [-0.0419,  0.0102, -0.0116,  ...,  0.0187, -0.0101,  0.0053],\n",
      "        [ 0.0198,  0.0321,  0.0097,  ..., -0.0465, -0.0009,  0.0127]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-2.3678e-03, -3.9073e-03,  2.6426e-03,  ...,  9.5157e-03,\n",
      "          3.1465e-03,  7.6476e-03],\n",
      "        [ 9.7352e-04,  3.4912e-03,  2.9025e-03,  ...,  4.1142e-03,\n",
      "         -3.2349e-03, -1.4141e-04],\n",
      "        [ 4.7579e-03, -1.6168e-03, -1.3717e-03,  ..., -3.4920e-05,\n",
      "         -3.3671e-03, -4.7715e-03],\n",
      "        ...,\n",
      "        [-1.1807e-03,  3.0391e-03, -2.9165e-03,  ..., -1.8179e-03,\n",
      "         -3.4077e-03, -2.6698e-03],\n",
      "        [ 9.8870e-04, -4.1213e-03,  7.2358e-03,  ...,  4.6975e-03,\n",
      "          1.2274e-03,  2.1552e-04],\n",
      "        [ 2.7167e-03,  6.3984e-04, -4.0990e-03,  ..., -2.7141e-03,\n",
      "         -2.5699e-04,  4.3238e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5248, 1.6969, 1.9300, 2.0780, 1.8557, 1.7200, 2.2564, 2.1166, 1.6859,\n",
      "        1.6531, 1.8003, 1.7827, 2.0011, 2.2509, 2.0929, 2.0249, 1.4860, 1.3873,\n",
      "        1.6291, 1.8155, 2.2114, 2.5398, 2.6794, 2.5859, 0.8701, 1.4076, 1.6213,\n",
      "        1.7871, 1.9953, 2.4672, 2.6816, 2.2711, 1.5720, 1.3269, 1.4261, 1.2356,\n",
      "        1.0406, 1.2160, 1.5984, 1.7007, 1.5298, 1.5276, 1.6033, 1.4439, 1.9558,\n",
      "        1.8688, 1.6009, 1.5879, 1.4692, 1.6311, 1.9245, 2.0942, 2.0131, 2.5620,\n",
      "        2.7471, 2.8154, 1.4131, 1.7923, 2.0880, 1.7832, 2.0701, 2.4187, 2.6879,\n",
      "        2.6706, 1.7512, 2.0009, 1.7602, 1.5796, 1.5327, 2.0475, 1.9119, 1.9517,\n",
      "        1.7806, 1.9668, 1.8971, 2.0257, 1.7396, 1.4749, 1.9398, 1.9337, 1.8648,\n",
      "        1.4891, 1.6788, 1.3127, 1.1520, 1.2650, 1.8257, 1.7495, 1.6367, 1.7513,\n",
      "        1.5214, 1.7794, 2.2905, 1.9059, 1.7989, 1.8241, 1.7644, 1.5965, 1.7372,\n",
      "        1.8811, 2.1193, 2.0623, 2.1553, 2.0969, 1.8941, 2.0253, 2.0268, 1.8571,\n",
      "        1.6004, 1.7170, 2.0613, 2.0707, 1.6883, 1.7678, 1.5849, 1.9960, 2.1371,\n",
      "        1.2861, 1.7535, 1.8081, 1.7039, 1.5724, 1.8165, 1.3141, 1.2614, 2.0772,\n",
      "        1.7899, 1.7503, 1.5776, 1.6572, 1.7417, 1.7573, 1.7340, 1.3663, 1.7200,\n",
      "        1.7733, 1.5880, 1.7488, 1.6522, 1.5535, 1.5751, 1.8392, 1.8899, 1.8075,\n",
      "        1.4537, 1.4226, 1.7148, 1.4060, 2.1160, 1.8809, 1.7170, 1.5331, 1.7658,\n",
      "        1.6150, 1.4931, 1.3662, 1.1466, 1.2184, 1.6889, 1.7071, 1.6172, 1.4422,\n",
      "        1.5340, 1.5365, 1.2007, 2.1916, 1.6963, 1.6808, 2.0316, 1.8477, 1.5878,\n",
      "        1.5000, 1.8041, 1.2366, 1.7345, 2.0781, 1.9515, 2.0760, 2.0545, 1.8941,\n",
      "        1.4770, 2.2770, 2.2497, 2.2749, 1.8926, 1.9692, 1.9583, 1.9671, 2.2293,\n",
      "        1.3695, 2.1558, 2.1326, 1.7616, 1.5407, 1.4575, 1.9232, 1.2550, 1.1391,\n",
      "        1.6294, 1.7766, 1.6613, 1.7075, 1.7566, 1.2060, 1.7689, 2.1063, 1.5860,\n",
      "        1.7426, 1.6912, 1.8620, 1.7221, 1.6250, 1.4534, 1.9344, 1.9837, 2.0388,\n",
      "        1.9908, 1.8123, 1.7783, 1.5741, 1.6726, 1.4740, 1.6792, 1.9929, 1.2138,\n",
      "        1.5586, 1.6068, 1.1627, 1.1031, 2.0391, 1.7117, 1.6945, 2.1260, 1.4162,\n",
      "        1.4712, 1.8075, 1.9807, 1.2479, 1.7543, 1.8665, 2.1674, 1.8288, 2.0418,\n",
      "        1.7672, 1.8957, 1.7603, 2.2885, 2.4793, 1.8983, 1.9636, 1.8856, 2.0277,\n",
      "        1.7702, 1.7413, 2.4582, 2.2674, 1.7227, 1.6568, 1.8372, 1.9920, 1.1421,\n",
      "        1.9701, 1.6926, 1.8123, 1.8644, 1.6163, 1.5333, 1.0827, 1.9817, 1.1856,\n",
      "        1.7471, 1.8722, 1.7058, 1.6042, 1.7688, 1.7182, 2.2315, 1.1736, 1.7738,\n",
      "        1.8545, 1.7253, 1.8120, 1.5412, 1.2777, 1.1578, 2.0789, 1.7848, 1.7859,\n",
      "        1.9572, 1.6886, 1.6115, 1.3648, 1.2227, 1.1345, 1.9316, 1.8009, 1.7087,\n",
      "        1.6437, 1.8029, 1.5954, 1.6612, 2.1101, 1.7497, 1.8790, 1.8306, 1.6801,\n",
      "        1.6224, 1.4403, 1.1995, 1.2860, 1.8729, 1.8225, 1.5203, 1.5027, 1.6571,\n",
      "        1.6825, 2.0762, 2.1725, 1.7911, 1.7940], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0364,  0.0178,  0.0517,  ...,  0.0301, -0.0496,  0.0462],\n",
      "        [ 0.0035, -0.0651,  0.0270,  ..., -0.0078, -0.0372, -0.0139],\n",
      "        [-0.0432,  0.0012, -0.0050,  ..., -0.0216, -0.0491,  0.0185],\n",
      "        ...,\n",
      "        [ 0.0369, -0.0178, -0.0600,  ...,  0.0495, -0.0261,  0.0309],\n",
      "        [ 0.0335,  0.0367,  0.0340,  ...,  0.0394, -0.0083,  0.0487],\n",
      "        [-0.0148,  0.0206,  0.0197,  ...,  0.0226, -0.0312,  0.0078]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0008,  0.0014,  0.0016,  ...,  0.0025, -0.0028,  0.0024],\n",
      "        [ 0.0042,  0.0049, -0.0053,  ..., -0.0029,  0.0028,  0.0024],\n",
      "        [ 0.0023, -0.0012, -0.0011,  ..., -0.0038,  0.0038, -0.0010],\n",
      "        ...,\n",
      "        [ 0.0001,  0.0039,  0.0018,  ..., -0.0006, -0.0006,  0.0035],\n",
      "        [ 0.0055,  0.0021, -0.0072,  ..., -0.0056,  0.0059, -0.0060],\n",
      "        [-0.0033, -0.0043,  0.0025,  ...,  0.0022, -0.0017, -0.0060]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0696, 2.1807, 2.1065, 2.1958, 2.1135, 2.0783, 2.2112, 2.2300, 2.1959,\n",
      "        2.2279, 2.1346, 2.0434, 2.2094, 2.3115, 2.1043, 2.0587, 1.9067, 2.0417,\n",
      "        1.9636, 1.9911, 1.9747, 2.0465, 2.0055, 1.9824, 2.0128, 1.9292, 1.9484,\n",
      "        1.9847, 1.9163, 1.9473, 2.0085, 1.9181, 2.3030, 2.0883, 2.2100, 2.0310,\n",
      "        2.2363, 2.1920, 2.2728, 2.1092, 2.1580, 2.1569, 2.1721, 2.3172, 2.2232,\n",
      "        2.2552, 2.2040, 2.2027, 2.1354, 2.0645, 2.0550, 2.0355, 2.1160, 2.1079,\n",
      "        2.0499, 2.1132, 2.0743, 2.0974, 2.0079, 2.1281, 2.1347, 2.2028, 2.1003,\n",
      "        2.0977, 2.0317, 2.1646, 2.0417, 2.1556, 2.2484, 2.1288, 2.0644, 2.0992,\n",
      "        2.0650, 2.1310, 2.1301, 2.0946, 2.0980, 2.0707, 2.0719, 2.0675, 2.1611,\n",
      "        2.1538, 2.1392, 2.1849, 1.9882, 2.1218, 2.0301, 1.9777, 2.2314, 2.0814,\n",
      "        2.0540, 1.9104, 2.1400, 1.9891, 2.2391, 2.0446, 2.2730, 2.0638, 1.9594,\n",
      "        2.0815, 2.1663, 2.0572, 2.0371, 2.0704, 2.1194, 2.1148, 2.0681, 2.1246,\n",
      "        2.1186, 2.0459, 2.1693, 2.1053, 2.1687, 2.1280, 2.1807, 2.1610, 2.1533,\n",
      "        2.1254, 2.0282, 2.1199, 2.2643, 2.1823, 2.1935, 2.1671, 2.0966, 2.1346,\n",
      "        2.0643, 2.1796, 2.2782, 2.3399, 2.3914, 2.2097, 2.4193, 2.3461, 2.2927,\n",
      "        2.3408, 2.2189, 2.4045, 2.1219, 2.3718, 2.2477, 2.4042, 2.2219, 2.4019,\n",
      "        1.8942, 2.0987, 2.1667, 2.2668, 1.9131, 2.0797, 2.1173, 2.0540, 1.9436,\n",
      "        2.1339, 2.0746, 2.0727, 2.1336, 2.1610, 1.9860, 2.1798, 1.9567, 2.0754,\n",
      "        2.1375, 2.1912, 2.0627, 1.9729, 2.0681, 2.0497, 2.0977, 2.0747, 2.0693,\n",
      "        2.0245, 2.1868, 1.9966, 2.0554, 2.1477, 2.0819, 2.1532, 2.1791, 2.1989,\n",
      "        2.2931, 2.0934, 2.1830, 2.2258, 2.0962, 2.3168, 2.1158, 2.1167, 2.3021,\n",
      "        2.1195, 2.0848, 2.1692, 1.9732, 1.9520, 2.0253, 2.0746, 2.0774, 2.1360,\n",
      "        2.0311, 2.0065, 2.0959, 1.9673, 2.1754, 2.0273, 2.0988, 2.0076, 2.0271,\n",
      "        1.9857, 2.1824, 2.0036, 2.1324, 2.0706, 2.1571, 2.1784, 2.0548, 2.1253,\n",
      "        2.1659, 2.3208, 2.2957, 2.1823, 2.1455, 2.1377, 2.1991, 2.2160, 1.9602,\n",
      "        1.9724, 2.0566, 1.9254, 1.9314, 2.0328, 2.0096, 1.8538, 2.0436, 1.9935,\n",
      "        2.0570, 1.9511, 1.9176, 2.0457, 2.0471, 1.9959, 2.2067, 2.1717, 2.0965,\n",
      "        2.3079, 2.1298, 2.3069, 2.0869, 1.7297, 2.1269, 2.3356, 2.0652, 2.1927,\n",
      "        2.1972, 2.1352, 2.2607, 2.0995, 2.1630, 2.1346, 2.1188, 2.0484, 2.0038,\n",
      "        2.1339, 2.0756, 2.0399, 2.0041, 2.1417, 1.9594, 2.0772, 2.1603, 2.0148,\n",
      "        2.0907, 2.1284, 1.9781, 2.0282, 2.0896, 2.0210, 2.0939, 2.1374, 2.2037,\n",
      "        2.1142, 2.1467, 2.0926, 2.0740, 2.0804, 2.0916, 1.9320, 2.0053, 2.1324,\n",
      "        2.0730, 2.0842, 2.1020, 2.1775, 2.0847, 2.1515, 2.1758, 2.0418, 2.0777,\n",
      "        2.0231, 2.0998, 2.0292, 2.0206, 2.1384, 1.9169, 2.0616, 1.9343, 1.7637,\n",
      "        1.8359, 1.7974, 1.7777, 1.9541, 1.7785, 1.8256, 1.8053, 1.8327, 1.8875,\n",
      "        1.7931, 1.8369, 1.8727, 1.7863, 1.7869], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0444, -0.0138, -0.0229,  ..., -0.0070, -0.0292, -0.0547],\n",
      "        [-0.0628, -0.0303,  0.0224,  ...,  0.0031,  0.0415, -0.0039],\n",
      "        [ 0.0377, -0.0438,  0.0088,  ..., -0.0282,  0.0041,  0.0514],\n",
      "        ...,\n",
      "        [-0.0511,  0.0556, -0.0408,  ..., -0.0250, -0.0276, -0.0217],\n",
      "        [-0.0342,  0.0398,  0.0174,  ...,  0.0215,  0.0513, -0.0278],\n",
      "        [-0.0305,  0.0486, -0.0413,  ..., -0.0218,  0.0148,  0.0287]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0111,  0.0068,  0.0064,  ...,  0.0059,  0.0007,  0.0109],\n",
      "        [-0.0062,  0.0046, -0.0023,  ...,  0.0022, -0.0015,  0.0030],\n",
      "        [ 0.0102, -0.0095, -0.0009,  ..., -0.0057, -0.0070, -0.0086],\n",
      "        ...,\n",
      "        [-0.0115,  0.0111,  0.0070,  ...,  0.0104,  0.0046,  0.0169],\n",
      "        [-0.0124,  0.0087,  0.0058,  ...,  0.0133,  0.0027,  0.0134],\n",
      "        [ 0.0085, -0.0108, -0.0037,  ..., -0.0062, -0.0082, -0.0044]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9910, 1.9678, 1.9714, 2.1399, 2.1748, 1.9518, 2.0721, 2.0033, 2.0480,\n",
      "        1.9534, 1.8214, 1.9682, 1.9505, 1.7904, 2.1472, 2.1183, 1.9318, 1.9939,\n",
      "        1.8293, 1.9072, 2.1904, 1.9509, 1.9367, 2.0215, 1.8431, 1.9274, 1.8422,\n",
      "        2.0359, 2.2037, 2.0878, 1.9446, 1.6371, 2.1568, 1.8754, 1.9394, 1.8236,\n",
      "        1.8982, 1.8831, 1.8780, 2.1606, 1.9340, 2.0292, 2.0377, 2.0180, 2.0313,\n",
      "        2.0328, 2.9434, 1.8859, 1.9603, 1.9885, 2.0114, 1.8991, 1.9453, 1.8406,\n",
      "        1.9973, 1.9073, 2.1576, 2.1456, 2.3487, 2.0480, 1.8883, 1.9527, 4.8693,\n",
      "        1.8897, 2.0770, 2.0958, 2.1584, 2.0278, 1.8852, 2.0182, 1.9498, 1.9305,\n",
      "        1.8704, 2.2102, 1.9347, 2.1394, 1.9662, 1.9761, 1.9228, 1.7231, 1.8254,\n",
      "        1.9627, 1.8855, 2.1791, 1.8795, 1.8556, 1.7319, 2.0035, 1.7186, 2.0494,\n",
      "        2.1910, 2.0023, 2.2508, 1.9689, 1.9496, 2.0870, 1.9467, 2.1473, 2.1265,\n",
      "        2.1674, 2.0542, 2.2035, 1.9578, 1.9917, 1.9515, 2.0286, 2.0887, 2.0709,\n",
      "        1.8339, 1.7860, 2.1959, 2.1295, 2.0691, 1.9892, 2.1714, 1.9347, 2.0630,\n",
      "        1.8899, 1.9106, 2.2127, 2.0671, 1.9717, 2.1802, 2.0686, 1.8043, 2.1066,\n",
      "        1.8940, 1.9807, 1.9419, 1.9073, 1.8274, 2.1131, 2.1408, 2.0090, 2.2155,\n",
      "        1.9438, 1.8632, 1.9828, 1.9204, 1.9972, 1.9703, 1.8956, 2.3231, 2.1033,\n",
      "        1.9506, 1.9759, 1.9556, 2.0701, 1.9664, 1.8696, 2.1643, 2.0431, 1.9806,\n",
      "        1.9267, 2.1157, 1.9094, 1.8363, 1.9718, 2.2111, 1.9235, 2.0220, 2.0105,\n",
      "        2.1080, 1.9400, 2.0342, 2.1327, 2.1111, 2.0471, 2.0602, 1.9668, 1.9441,\n",
      "        1.7381, 2.0799, 2.0207, 1.8291, 1.8255, 1.8058, 2.0405, 2.3215, 1.9585,\n",
      "        1.8138, 1.9589, 2.0469, 1.9945, 2.0201, 2.0137, 1.9684, 1.9827, 2.0205,\n",
      "        1.9523, 2.0081, 1.8248, 2.7504, 2.0921, 1.9557, 2.0232, 1.9274, 1.8779,\n",
      "        2.0299, 1.9759, 1.9596, 2.0280, 1.9575, 2.1265, 1.9855, 2.0641, 2.0255,\n",
      "        1.7452, 1.9795, 1.9333, 1.8696, 2.0914, 2.0789, 2.1366, 2.2151, 2.0389,\n",
      "        1.8923, 2.0893, 1.6688, 1.8916, 1.8713, 2.0122, 1.9430, 2.1559, 2.1631,\n",
      "        2.0065, 2.0921, 2.0130, 1.9427, 2.0630, 1.8973, 1.7636, 2.0427, 1.8874,\n",
      "        2.0193, 2.0516, 1.8734, 1.8857, 2.0900, 2.0107, 1.9029, 1.8573, 2.0500,\n",
      "        2.0563, 2.0060, 1.8272, 2.0299, 2.0234, 1.9416, 1.9699, 1.9441, 1.9139,\n",
      "        2.0732, 1.7845, 1.9141, 1.9637, 1.9771, 1.7644, 1.7747, 2.0562, 2.2161,\n",
      "        2.1428, 2.0726, 1.9708, 1.9739, 2.0234, 2.0146, 2.5427, 2.0171, 1.9102,\n",
      "        1.8925, 2.1843, 1.8635, 1.9875, 1.9876, 1.9370, 2.1967, 1.8480, 1.9797,\n",
      "        2.0853, 1.9341, 1.9174, 2.0452, 2.0035, 2.0348, 2.0213, 2.0468, 1.8559,\n",
      "        1.9319, 1.8821, 1.8863, 2.1537, 1.9668, 2.1264, 1.7500, 2.0457, 2.1030,\n",
      "        1.9519, 1.8370, 1.8982, 2.0755, 2.0211, 2.1989, 1.9701, 2.0959, 1.9575,\n",
      "        2.0600, 1.7289, 1.8528, 2.1118, 2.0634, 1.8134, 1.8018, 2.2020, 2.1229,\n",
      "        1.9537, 2.0295, 1.9681, 2.1916, 1.9376], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0467,  0.0265,  0.0527,  ...,  0.0100, -0.0264, -0.0595],\n",
      "        [ 0.0089, -0.0039, -0.0545,  ..., -0.0589,  0.0122, -0.0209],\n",
      "        [ 0.0536,  0.0056,  0.0014,  ...,  0.0177,  0.0194, -0.0481],\n",
      "        ...,\n",
      "        [-0.0101, -0.0192, -0.0115,  ..., -0.0303, -0.0382, -0.0307],\n",
      "        [-0.0657, -0.0552, -0.0253,  ..., -0.0627, -0.0288,  0.0294],\n",
      "        [ 0.0194,  0.0431, -0.0124,  ..., -0.0324, -0.0064, -0.0539]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 4.5971e-03, -5.8318e-03,  5.0624e-03,  ..., -5.5364e-03,\n",
      "         -1.8127e-03,  4.8652e-05],\n",
      "        [-7.3843e-03,  9.2028e-03, -8.7769e-03,  ..., -5.8791e-04,\n",
      "          7.4964e-03, -3.8891e-03],\n",
      "        [ 7.3282e-03, -2.6515e-03,  7.5294e-03,  ...,  3.5233e-03,\n",
      "         -2.0419e-03,  5.6808e-03],\n",
      "        ...,\n",
      "        [-4.2162e-03,  5.3287e-03, -4.8179e-03,  ..., -5.8856e-03,\n",
      "          6.1406e-03, -3.7750e-03],\n",
      "        [-4.9584e-04,  1.7298e-03, -3.1631e-03,  ...,  4.3568e-03,\n",
      "         -1.1933e-03,  2.1367e-03],\n",
      "        [ 2.6544e-03, -2.4774e-03, -1.4487e-03,  ..., -5.3529e-04,\n",
      "          1.8188e-03, -1.9472e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.1098, 1.8829, 2.1273,  ..., 2.6319, 1.8483, 1.9411], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-1.0102e-02,  1.0290e-03, -1.4399e-02,  ...,  1.4780e-02,\n",
      "          2.5445e-02,  2.8290e-02],\n",
      "        [ 5.8334e-03,  1.3839e-02,  9.3028e-03,  ...,  2.6106e-02,\n",
      "         -1.6159e-02,  1.8881e-02],\n",
      "        [-9.6156e-03,  1.0949e-02, -3.9223e-04,  ..., -6.7829e-05,\n",
      "         -7.5074e-03,  9.9973e-03],\n",
      "        ...,\n",
      "        [ 3.1605e-02, -1.1553e-02, -6.6808e-03,  ..., -1.0985e-02,\n",
      "          2.2619e-02,  2.1081e-02],\n",
      "        [-2.6867e-03, -2.2056e-02,  2.0758e-03,  ...,  2.2232e-02,\n",
      "         -1.0372e-02,  1.7846e-02],\n",
      "        [-1.0191e-02,  3.9653e-03,  1.1917e-02,  ...,  3.6425e-03,\n",
      "          3.1473e-02, -1.8562e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0051, -0.0034,  0.0042,  ..., -0.0125, -0.0020, -0.0089],\n",
      "        [-0.0028, -0.0071,  0.0057,  ..., -0.0041, -0.0007, -0.0040],\n",
      "        [ 0.0015,  0.0107, -0.0069,  ..., -0.0025,  0.0038, -0.0008],\n",
      "        ...,\n",
      "        [-0.0066, -0.0047,  0.0111,  ..., -0.0119, -0.0040, -0.0114],\n",
      "        [-0.0029, -0.0075,  0.0095,  ..., -0.0033, -0.0023, -0.0050],\n",
      "        [ 0.0037,  0.0064,  0.0005,  ..., -0.0030,  0.0033, -0.0040]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([4.3327, 4.4909, 4.1940, 4.4683, 4.8575, 4.3163, 4.4355, 4.2069, 4.5102,\n",
      "        4.4133, 4.1952, 4.3010, 4.4507, 4.0291, 4.5870, 4.5173, 4.1678, 4.3001,\n",
      "        4.0644, 4.4301, 4.6044, 4.6814, 4.1738, 4.3260, 4.1871, 4.2258, 4.1796,\n",
      "        4.3733, 4.5095, 4.6348, 4.2067, 3.8946, 4.7018, 4.2840, 4.3037, 4.2379,\n",
      "        3.9596, 4.2216, 4.2704, 4.8983, 4.0888, 4.3943, 4.3322, 4.2811, 4.6533,\n",
      "        4.3478, 5.5452, 4.1375, 4.4145, 4.5973, 4.6362, 4.4655, 4.1079, 3.9907,\n",
      "        4.3122, 4.3599, 4.5010, 4.3693, 5.4306, 4.3612, 4.5545, 4.3705, 9.4375,\n",
      "        4.3393, 4.5400, 4.3205, 4.4694, 4.0606, 4.6931, 4.5913, 4.0639, 4.1263,\n",
      "        4.0358, 4.6711, 4.3021, 4.2984, 4.2109, 4.3864, 4.5197, 3.8677, 4.2102,\n",
      "        4.0796, 4.2709, 4.8578, 4.3412, 4.2258, 4.0893, 4.4469, 3.9411, 4.4095,\n",
      "        4.5379, 4.0694, 5.1647, 4.4479, 4.4059, 4.7317, 4.3114, 4.4109, 4.3497,\n",
      "        4.8161, 4.5438, 4.6340, 4.3794, 4.4616, 4.4385, 4.3624, 4.1676, 4.3764,\n",
      "        4.3646, 4.1124, 4.6640, 4.3357, 4.2135, 4.2439, 4.2637, 4.0909, 4.6060,\n",
      "        4.2912, 4.6100, 4.7463, 4.4038, 4.2260, 4.4066, 4.3913, 4.2984, 4.7992,\n",
      "        4.3375, 4.2966, 4.3539, 3.8741, 4.1725, 4.6103, 4.4781, 4.5779, 4.6601,\n",
      "        4.3776, 4.1785, 4.5123, 4.5295, 4.7885, 3.9747, 4.1483, 4.5169, 4.3869,\n",
      "        4.6334, 4.6296, 4.5752, 4.3547, 4.2195, 4.3032, 5.0286, 4.3485, 4.3430,\n",
      "        4.2776, 4.4364, 4.5042, 4.3063, 4.4833, 4.9396, 4.5872, 4.3046, 4.5937,\n",
      "        4.4760, 4.2263, 4.2797, 4.4395, 4.2276, 4.4103, 4.3753, 4.4317, 4.1098,\n",
      "        4.3305, 4.3220, 4.4140, 4.4551, 4.1077, 4.1526, 4.4343, 4.7265, 4.2260,\n",
      "        4.5390, 4.0642, 4.6507, 4.3564, 4.3428, 4.6942, 4.3396, 4.5334, 4.3403,\n",
      "        4.3250, 4.2997, 4.0719, 6.6736, 4.3124, 4.3631, 4.2064, 4.5280, 4.2067,\n",
      "        4.6001, 4.6773, 4.4208, 4.4508, 4.6530, 4.2642, 4.5742, 4.5510, 4.1981,\n",
      "        3.8292, 4.4923, 4.1387, 4.4683, 4.4385, 4.4915, 4.4745, 4.7146, 4.4347,\n",
      "        4.2393, 4.6422, 3.8013, 4.1709, 4.3147, 4.1816, 4.4078, 4.8044, 4.7841,\n",
      "        4.4106, 4.5205, 4.2955, 4.2481, 4.3083, 4.2103, 3.9722, 4.4496, 4.3494,\n",
      "        4.1774, 4.6274, 4.3085, 4.1323, 4.3462, 4.4141, 4.0943, 4.3207, 4.1111,\n",
      "        4.1816, 4.2131, 4.1899, 5.0022, 4.1995, 4.6317, 4.4165, 4.3832, 4.5354,\n",
      "        4.3144, 4.1243, 4.3294, 4.4147, 4.5320, 4.0017, 3.9876, 4.6786, 4.2454,\n",
      "        4.5735, 4.6466, 4.3267, 4.2555, 4.5245, 4.7093, 6.7654, 4.1038, 4.0441,\n",
      "        4.4830, 4.0792, 4.3539, 4.1975, 4.2253, 4.3798, 4.5797, 4.3232, 4.4043,\n",
      "        4.3926, 4.0718, 4.3069, 4.3378, 4.6597, 4.3234, 4.2891, 4.5910, 4.2055,\n",
      "        4.0540, 4.3897, 4.6018, 4.6044, 4.1594, 4.6673, 4.1128, 4.3839, 4.4620,\n",
      "        4.1262, 4.2025, 4.2242, 4.2953, 4.1840, 4.4190, 4.3553, 4.5890, 4.3375,\n",
      "        4.7693, 4.0729, 4.1665, 4.3929, 4.2128, 4.2386, 3.9836, 4.3875, 4.3014,\n",
      "        4.2549, 4.4263, 4.3838, 4.5815, 4.3093], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0026, -0.0410,  0.0437,  ..., -0.0367,  0.0423,  0.0220],\n",
      "        [-0.0091,  0.0152, -0.0102,  ..., -0.0369, -0.0119,  0.0256],\n",
      "        [ 0.0450, -0.0049, -0.0494,  ..., -0.0456,  0.0128, -0.0572],\n",
      "        ...,\n",
      "        [-0.0433,  0.0241,  0.0538,  ..., -0.0150, -0.0374, -0.0238],\n",
      "        [-0.0450, -0.0336, -0.0471,  ..., -0.0268, -0.0457,  0.0069],\n",
      "        [-0.0146, -0.0489, -0.0329,  ..., -0.0167, -0.0622, -0.0425]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.3166e-03,  5.0676e-03, -5.7677e-03,  ...,  3.9988e-03,\n",
      "          5.2315e-03,  5.5987e-03],\n",
      "        [-2.8023e-03,  3.4918e-03, -4.6500e-03,  ...,  3.6435e-03,\n",
      "          3.2914e-03,  3.2718e-03],\n",
      "        [-2.5509e-04, -3.4761e-04,  1.3424e-03,  ..., -5.3658e-04,\n",
      "         -2.1674e-04, -9.8043e-06],\n",
      "        ...,\n",
      "        [ 3.9001e-03, -3.0704e-03,  5.9446e-04,  ...,  1.0119e-03,\n",
      "         -3.5980e-03, -4.4675e-03],\n",
      "        [-5.0348e-03,  4.6361e-03, -3.8521e-03,  ...,  3.2551e-03,\n",
      "          5.0664e-03,  5.3570e-03],\n",
      "        [-1.5197e-02,  1.4685e-02, -1.4391e-02,  ...,  1.2686e-02,\n",
      "          1.6213e-02,  1.4634e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5265, 1.3324, 1.3689, 1.2995, 1.5735, 1.7071, 1.6212, 1.6120, 1.5898,\n",
      "        1.4222, 1.1337, 1.3573, 1.4217, 1.3098, 1.6174, 1.5716, 1.4989, 1.3492,\n",
      "        1.0197, 1.3321, 1.7010, 1.4908, 1.8944, 1.9725, 1.0787, 1.0306, 1.3189,\n",
      "        1.1710, 1.1133, 1.9583, 2.0737, 1.8611, 1.0690, 1.1824, 1.2035, 1.6862,\n",
      "        2.1559, 1.5540, 1.8093, 2.5764, 2.0210, 1.5057, 1.6647, 1.4747, 1.4900,\n",
      "        2.1916, 1.9490, 1.9759, 1.6563, 1.7462, 1.7008, 1.5554, 1.4432, 1.5151,\n",
      "        1.8748, 1.9653, 1.6983, 1.7630, 1.5794, 1.7488, 1.7274, 1.5498, 1.8760,\n",
      "        1.9387, 1.1766, 1.6737, 1.4370, 1.9061, 2.0111, 2.0941, 2.2809, 2.0190,\n",
      "        1.2038, 1.2515, 1.5233, 2.0857, 2.0637, 1.6352, 2.2573, 2.1126, 1.7945,\n",
      "        1.2918, 1.6477, 1.0776, 1.8532, 1.7197, 1.3039, 1.4448, 1.6276, 1.7324,\n",
      "        1.4029, 1.7898, 1.1494, 1.1832, 1.2899, 1.3190, 1.1456, 0.9929, 1.0770,\n",
      "        1.1336, 1.4889, 1.4805, 1.4488, 1.5948, 1.1531, 0.9392, 1.2801, 1.3822,\n",
      "        1.2552, 1.5468, 1.4131, 1.4545, 1.5744, 1.6683, 1.9299, 1.7000, 1.8068,\n",
      "        1.6668, 2.2359, 2.3302, 1.5050, 1.6965, 1.7655, 1.8280, 1.5923, 1.9902,\n",
      "        2.2296, 2.3328, 2.0245, 1.7006, 1.6827, 1.5104, 1.7025, 2.2766, 1.9600,\n",
      "        1.9491, 1.6452, 1.7240, 1.6484, 1.3705, 1.6925, 1.4040, 1.8722, 1.9076,\n",
      "        1.9137, 1.2770, 1.6994, 1.2259, 1.2840, 1.4069, 1.6102, 2.1303, 1.2916,\n",
      "        1.2657, 1.1697, 1.8375, 1.9697, 2.0082, 1.7283, 1.7960, 1.4645, 1.4096,\n",
      "        1.5981, 1.7462, 1.5397, 2.3310, 1.7917, 1.8301, 1.6467, 1.4389, 1.6925,\n",
      "        1.7068, 1.9012, 1.4902, 1.6655, 1.9764, 0.7665, 1.3515, 1.5146, 1.4589,\n",
      "        2.0799, 2.0059, 2.3837, 1.9873, 1.3952, 1.1174, 1.2263, 1.6602, 1.9130,\n",
      "        2.0583, 2.3340, 1.9327, 1.6634, 1.2018, 1.6214, 1.8843, 1.8616, 1.8717,\n",
      "        1.6476, 1.9406, 1.2414, 1.4286, 1.0681, 1.2087, 1.2406, 1.3823, 1.6545,\n",
      "        1.6103, 0.9783, 1.3328, 1.2983, 1.2685, 1.2400, 1.2178, 1.7640, 1.7807,\n",
      "        1.2134, 1.2078, 1.2933, 1.3845, 1.1728, 1.7541, 1.9146, 1.8005, 1.4416,\n",
      "        1.3111, 1.7256, 1.7165, 1.2708, 1.3154, 1.6928, 1.6258, 1.6566, 1.4701,\n",
      "        1.1005, 1.0981, 1.6584, 1.9279, 1.5644, 1.8778, 0.6992, 1.5611, 1.4555,\n",
      "        1.6614, 2.1852, 1.7560, 2.1350, 2.2531, 1.3528, 1.1981, 1.4604, 1.5393,\n",
      "        2.2140, 1.7782, 2.2694, 2.3880, 1.9309, 1.9870, 1.9327, 1.6294, 1.3920,\n",
      "        2.1232, 1.8101, 2.0197, 1.8513, 1.8187, 1.8103, 1.3278, 1.7532, 1.5293,\n",
      "        2.0376, 1.8869, 1.8168, 1.9073, 1.8941, 1.8600, 1.4880, 2.1560, 2.1041,\n",
      "        2.0864, 1.6799, 1.7381, 1.9813, 1.9808, 1.7435, 1.5781, 2.2144, 2.0110,\n",
      "        1.3041, 1.3178, 1.3986, 1.2880, 1.3492, 1.1408, 2.0058, 2.0529, 1.2400,\n",
      "        1.2703, 1.2580, 1.3790, 1.2624, 1.5755, 1.6114, 1.3895, 0.7874, 0.7958,\n",
      "        0.8374, 1.2124, 1.9426, 2.0874, 2.0292, 2.0121, 1.7655, 1.4057, 1.5212,\n",
      "        1.5406, 1.7697, 2.0665, 1.9743, 1.6746], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0347, -0.0423, -0.0258,  ..., -0.0657,  0.0014,  0.0271],\n",
      "        [-0.0643,  0.0379,  0.0118,  ...,  0.0305, -0.0554,  0.0244],\n",
      "        [ 0.0508, -0.0299,  0.0286,  ...,  0.0210, -0.0325, -0.0393],\n",
      "        ...,\n",
      "        [-0.0184, -0.0016, -0.0102,  ...,  0.0054,  0.0390,  0.0671],\n",
      "        [-0.0093,  0.0439,  0.0026,  ..., -0.0485, -0.0169, -0.0304],\n",
      "        [-0.0591, -0.0499,  0.0286,  ...,  0.0055, -0.0273, -0.0287]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0037, -0.0017,  0.0016,  ..., -0.0042, -0.0049, -0.0003],\n",
      "        [-0.0002, -0.0051,  0.0079,  ..., -0.0046,  0.0005,  0.0040],\n",
      "        [-0.0030, -0.0027,  0.0028,  ...,  0.0061,  0.0009,  0.0057],\n",
      "        ...,\n",
      "        [-0.0037, -0.0026, -0.0095,  ...,  0.0018,  0.0064, -0.0029],\n",
      "        [ 0.0033,  0.0046, -0.0011,  ..., -0.0059,  0.0062,  0.0025],\n",
      "        [ 0.0004,  0.0021,  0.0017,  ..., -0.0062,  0.0037,  0.0054]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3953, 1.2998, 1.4072, 1.4471, 1.2198, 1.1670, 1.5664, 1.5766, 1.4625,\n",
      "        1.4151, 1.3091, 1.3260, 1.4233, 1.8668, 1.5525, 1.5303, 1.1090, 1.4880,\n",
      "        1.0690, 1.2169, 1.3879, 1.3762, 1.5779, 1.4395, 1.2997, 1.0515, 1.5206,\n",
      "        1.2519, 1.1930, 1.3428, 1.7649, 1.6447, 1.5372, 1.4974, 1.5528, 1.3111,\n",
      "        1.2839, 2.0826, 1.7389, 1.8667, 1.4802, 1.4551, 1.4466, 1.7082, 2.0762,\n",
      "        1.3187, 1.8709, 1.8503, 1.6597, 1.7417, 1.7667, 1.4924, 1.5294, 1.4579,\n",
      "        1.7347, 1.8592, 1.6089, 1.8425, 1.7802, 1.7150, 1.4416, 2.0535, 1.7407,\n",
      "        1.9196, 1.2075, 1.5793, 1.5886, 1.7249, 1.5829, 1.3234, 1.8499, 1.7469,\n",
      "        1.2193, 1.2632, 1.6503, 1.7554, 1.4002, 1.6286, 1.6825, 1.8959, 1.8031,\n",
      "        1.4687, 1.6004, 1.5681, 0.9525, 1.0076, 1.3887, 1.4723, 1.5915, 1.6909,\n",
      "        1.4557, 1.2672, 1.5797, 1.3793, 1.3537, 1.3026, 1.0240, 1.2511, 1.2413,\n",
      "        1.3110, 1.0515, 1.0968, 1.3886, 1.4798, 1.1990, 1.1339, 1.3467, 1.0751,\n",
      "        1.1365, 1.2960, 1.3645, 1.4486, 1.4687, 1.6008, 1.9481, 1.7904, 1.7267,\n",
      "        1.8165, 1.9756, 1.7755, 1.5554, 1.7214, 1.8006, 1.8200, 1.7564, 1.6168,\n",
      "        2.0090, 1.8947, 1.9248, 1.7432, 1.8082, 1.6189, 1.2532, 1.1339, 1.8320,\n",
      "        1.8355, 1.7159, 1.8488, 1.7775, 1.5051, 1.4316, 2.3212, 1.7515, 1.7668,\n",
      "        1.8127, 1.4628, 1.3220, 1.8488, 1.8289, 1.9450, 1.5445, 1.7126, 1.4698,\n",
      "        1.4849, 1.5741, 1.1291, 1.0466, 1.1875, 1.7056, 1.6829, 1.5914, 1.5402,\n",
      "        1.4376, 1.3913, 1.6653, 1.1775, 1.6638, 1.7157, 1.5202, 1.6217, 1.4831,\n",
      "        1.4298, 1.1874, 1.9739, 1.7098, 1.7016, 1.5646, 1.0716, 1.3189, 1.4838,\n",
      "        1.6201, 1.8881, 1.9530, 1.8458, 0.8330, 1.5035, 1.3468, 1.3635, 1.7231,\n",
      "        1.8101, 1.9349, 1.8161, 1.7774, 1.4228, 1.3876, 1.1344, 1.0342, 1.1615,\n",
      "        1.6010, 1.7568, 1.4021, 1.5135, 1.5322, 1.5917, 1.9731, 1.8395, 1.6599,\n",
      "        1.5306, 0.8837, 1.0029, 1.0177, 1.3058, 1.1598, 1.2976, 1.4818, 1.4519,\n",
      "        1.0486, 1.1622, 1.3925, 1.1925, 1.1933, 1.2311, 1.7628, 1.3449, 1.5613,\n",
      "        1.4257, 1.6214, 1.0633, 1.8665, 1.9175, 1.6338, 1.6008, 1.7837, 1.5965,\n",
      "        1.3213, 1.7144, 0.9982, 1.0735, 1.5423, 1.6406, 1.4991, 1.5464, 1.5563,\n",
      "        1.6674, 1.6314, 1.7613, 1.9291, 1.9325, 1.1202, 0.9583, 1.1522, 1.2522,\n",
      "        1.8542, 1.7202, 2.0628, 1.9539, 1.9156, 1.9204, 1.9722, 1.7088, 1.9501,\n",
      "        1.2412, 1.6260, 1.7605, 1.7786, 1.8901, 1.8180, 1.5899, 1.4690, 2.1823,\n",
      "        1.9101, 1.7091, 1.7699, 1.8960, 1.9271, 1.9072, 1.6147, 1.4681, 1.9591,\n",
      "        1.8644, 1.6738, 1.7083, 1.8649, 1.8634, 1.5794, 2.1354, 2.0589, 1.9833,\n",
      "        0.9893, 0.9289, 1.1905, 1.1317, 1.3231, 1.2617, 1.6775, 1.4619, 0.8571,\n",
      "        1.0238, 1.2674, 1.3417, 1.0942, 1.1837, 1.4742, 1.4561, 1.1716, 1.5166,\n",
      "        1.6333, 1.6197, 1.6861, 1.8008, 1.8720, 1.8612, 1.4647, 1.1382, 1.0343,\n",
      "        1.5113, 1.7270, 1.8620, 1.8737, 1.7501], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0237, -0.0194, -0.0219,  ..., -0.0465,  0.0062,  0.0127],\n",
      "        [ 0.0288,  0.0027,  0.0013,  ...,  0.0448,  0.0470, -0.0217],\n",
      "        [-0.0381,  0.0056,  0.0324,  ..., -0.0365,  0.0023, -0.0408],\n",
      "        ...,\n",
      "        [ 0.0430,  0.0016,  0.0001,  ...,  0.0190, -0.0203,  0.0193],\n",
      "        [ 0.0650,  0.0287, -0.0014,  ...,  0.0601,  0.0358, -0.0333],\n",
      "        [ 0.0111, -0.0459,  0.0528,  ...,  0.0455, -0.0179,  0.0247]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0007, -0.0014,  0.0024,  ...,  0.0067,  0.0031, -0.0017],\n",
      "        [-0.0006,  0.0009, -0.0041,  ..., -0.0069, -0.0038,  0.0029],\n",
      "        [-0.0014, -0.0033,  0.0047,  ...,  0.0080,  0.0052, -0.0041],\n",
      "        ...,\n",
      "        [ 0.0142,  0.0078, -0.0061,  ..., -0.0063, -0.0061,  0.0084],\n",
      "        [-0.0105,  0.0007, -0.0030,  ..., -0.0019, -0.0030,  0.0001],\n",
      "        [-0.0078, -0.0088,  0.0078,  ...,  0.0081,  0.0081, -0.0090]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0680, 2.0826, 2.0639, 2.0709, 2.3414, 2.2444, 2.0853, 1.9715, 2.1057,\n",
      "        2.1573, 2.1745, 2.0319, 2.1065, 2.1629, 2.0347, 2.1535, 2.2866, 2.2327,\n",
      "        2.2252, 2.3665, 2.2510, 2.1187, 2.3059, 2.4061, 2.3101, 2.2786, 2.1831,\n",
      "        2.1112, 2.2788, 2.2086, 2.2829, 2.2417, 2.0484, 1.9376, 2.1289, 2.0773,\n",
      "        1.9755, 2.0230, 2.0882, 2.0756, 2.0650, 2.0589, 2.0094, 2.0050, 2.0481,\n",
      "        2.0205, 2.0255, 1.9679, 2.2099, 2.2587, 2.2453, 2.3264, 1.8872, 2.1628,\n",
      "        2.2231, 2.0432, 2.0277, 2.1760, 2.2642, 2.2367, 2.2943, 2.0121, 2.3081,\n",
      "        2.2176, 2.2474, 2.3384, 2.0977, 2.1437, 2.4344, 2.1976, 2.2314, 2.1894,\n",
      "        2.2316, 2.2463, 2.2367, 2.1483, 2.2294, 2.1316, 2.1633, 2.2511, 2.1120,\n",
      "        2.1289, 2.1956, 2.1327, 2.2940, 2.1236, 2.1091, 2.1099, 2.2904, 2.1051,\n",
      "        2.2136, 2.1886, 2.1212, 2.1962, 2.0738, 2.0900, 2.3285, 2.0198, 2.2357,\n",
      "        2.0900, 2.1867, 2.1181, 2.0592, 2.1092, 2.1646, 2.1500, 2.2448, 2.2064,\n",
      "        2.3937, 2.1897, 2.1455, 2.1793, 2.1646, 2.2261, 2.0851, 2.1370, 2.0360,\n",
      "        2.1878, 2.2241, 2.2668, 2.5244, 2.1112, 2.1935, 2.2409, 2.0121, 2.1929,\n",
      "        2.2422, 2.2959, 2.1798, 2.0777, 2.2134, 2.2196, 2.2550, 2.2969, 2.0745,\n",
      "        2.2211, 2.1387, 2.2462, 2.1161, 2.3124, 2.2460, 2.2258, 2.1345, 2.1354,\n",
      "        2.0647, 2.1109, 1.8166, 1.9498, 2.0408, 2.1041, 2.1311, 1.9584, 2.2634,\n",
      "        2.1748, 2.0616, 2.1080, 2.0079, 2.2072, 2.0718, 2.0182, 2.1646, 2.0787,\n",
      "        2.0457, 2.0999, 2.0773, 2.1238, 2.1327, 2.0829, 2.0637, 1.9996, 2.1268,\n",
      "        2.1214, 2.0677, 1.9543, 2.0266, 2.1334, 1.9786, 1.8309, 2.0162, 1.9613,\n",
      "        1.8466, 1.9004, 1.9444, 1.9403, 2.0134, 1.9973, 1.9415, 1.9090, 1.9233,\n",
      "        1.9703, 1.9475, 1.8567, 2.0770, 1.9205, 2.0180, 2.2050, 2.1755, 2.0095,\n",
      "        2.0327, 1.9762, 2.1818, 2.2161, 2.0028, 2.1183, 2.1888, 2.0140, 1.8596,\n",
      "        2.1890, 2.5965, 2.5603, 2.5316, 2.5141, 2.5450, 2.9648, 2.5332, 2.2830,\n",
      "        2.1173, 2.3586, 2.2819, 2.2944, 2.2336, 2.4692, 2.3699, 2.3398, 2.1362,\n",
      "        2.1519, 2.2364, 2.1789, 2.1098, 1.9435, 2.1917, 2.1468, 2.2548, 2.0839,\n",
      "        2.1485, 2.1648, 2.0827, 2.1741, 2.1049, 1.9263, 2.0111, 2.0299, 1.8815,\n",
      "        1.9430, 1.9708, 1.9524, 1.8967, 1.9906, 1.9293, 1.9253, 1.9782, 1.8945,\n",
      "        1.9062, 1.8671, 1.9866, 1.9659, 2.2743, 2.2042, 1.9957, 2.3166, 2.2138,\n",
      "        2.1036, 2.0764, 2.2111, 2.2096, 2.2267, 2.1120, 2.1934, 2.2345, 2.1137,\n",
      "        2.1912, 2.1019, 2.2079, 2.1708, 2.1810, 2.2875, 2.1493, 2.2065, 2.2082,\n",
      "        2.0866, 2.2539, 2.1805, 2.0944, 2.0390, 2.0166, 2.3085, 2.1237, 2.2125,\n",
      "        3.5404, 2.7026, 2.6264, 3.2147, 2.3730, 2.8218, 2.5166, 2.7006, 3.5796,\n",
      "        2.5268, 2.7217, 2.6830, 3.0144, 2.7977, 2.8033, 2.7487, 2.0906, 1.9811,\n",
      "        1.8619, 1.8801, 1.9179, 1.9049, 1.9257, 2.1308, 1.9666, 1.9702, 1.9246,\n",
      "        1.9101, 2.0878, 1.9580, 1.9533, 1.9519], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0449,  0.0224,  0.0242,  ..., -0.0014,  0.0179,  0.0307],\n",
      "        [-0.0406,  0.0223,  0.0389,  ...,  0.0478, -0.0361,  0.0369],\n",
      "        [-0.0289, -0.0421,  0.0258,  ...,  0.0315, -0.0207,  0.0504],\n",
      "        ...,\n",
      "        [-0.0464, -0.0079, -0.0513,  ...,  0.0246, -0.0122, -0.0049],\n",
      "        [ 0.0287, -0.0005,  0.0576,  ...,  0.0207,  0.0039,  0.0390],\n",
      "        [-0.0007,  0.0316,  0.0026,  ..., -0.0256, -0.0102, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0049,  0.0151,  0.0119,  ...,  0.0098, -0.0094, -0.0123],\n",
      "        [ 0.0017,  0.0149,  0.0087,  ...,  0.0056, -0.0071, -0.0089],\n",
      "        [ 0.0023, -0.0187, -0.0131,  ..., -0.0103,  0.0129,  0.0138],\n",
      "        ...,\n",
      "        [-0.0048,  0.0126,  0.0101,  ...,  0.0056, -0.0111, -0.0131],\n",
      "        [-0.0026,  0.0177,  0.0120,  ...,  0.0095, -0.0139, -0.0146],\n",
      "        [-0.0017, -0.0084, -0.0040,  ..., -0.0043,  0.0029,  0.0042]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2008, 2.0806, 2.1911, 2.2206, 2.5566, 2.2739, 2.1239, 2.3172, 2.3037,\n",
      "        2.1669, 2.1134, 2.4567, 2.5237, 2.0472, 2.3805, 2.1877, 2.2567, 2.1531,\n",
      "        2.3547, 2.0873, 2.4661, 2.4318, 2.0420, 2.1788, 2.1585, 1.9930, 2.2276,\n",
      "        2.1653, 2.4616, 2.2236, 2.0815, 2.1023, 2.4750, 2.1866, 2.1114, 2.0804,\n",
      "        2.0957, 2.2530, 2.3435, 2.1537, 2.0933, 2.3613, 2.3397, 2.1941, 2.3112,\n",
      "        2.1446, 3.1406, 1.9660, 2.2683, 2.5391, 2.2171, 2.2159, 2.0842, 2.3518,\n",
      "        2.0688, 2.1888, 2.2027, 2.3848, 2.6123, 2.3779, 2.3010, 2.0799, 6.8990,\n",
      "        2.1834, 2.2097, 2.1614, 2.4031, 2.6326, 2.3359, 2.1337, 2.2289, 2.1619,\n",
      "        2.0123, 2.3045, 2.1543, 2.3683, 2.2172, 2.4435, 1.9154, 1.9709, 2.0234,\n",
      "        2.1097, 2.3135, 2.2587, 2.0063, 2.1482, 2.4971, 2.4197, 1.9584, 2.2158,\n",
      "        2.1723, 2.0359, 2.4265, 2.2456, 2.2285, 2.3196, 2.1247, 2.3072, 2.1770,\n",
      "        2.3060, 2.3609, 2.2502, 2.4469, 2.4054, 2.3111, 2.3794, 2.1277, 2.2928,\n",
      "        1.9953, 2.3179, 2.2642, 2.2975, 2.2122, 2.1546, 2.4567, 2.1346, 2.2377,\n",
      "        2.3270, 2.4682, 2.5024, 2.4318, 2.2549, 2.2810, 2.2309, 1.9996, 2.5418,\n",
      "        2.1018, 2.1589, 2.2556, 2.3595, 2.1439, 2.0180, 2.1734, 2.1880, 2.4041,\n",
      "        2.1349, 2.2238, 2.4532, 2.1711, 2.3379, 2.1139, 2.0811, 2.2391, 2.3687,\n",
      "        2.3572, 2.3010, 2.1383, 2.3433, 2.1663, 2.2835, 2.4318, 2.2615, 2.3745,\n",
      "        2.1308, 2.2438, 2.3193, 2.0793, 2.3537, 2.2765, 2.1442, 2.0838, 2.2005,\n",
      "        2.3260, 2.3832, 2.3423, 2.3561, 2.2874, 2.1261, 2.3056, 2.1637, 2.2839,\n",
      "        2.3181, 2.2647, 2.1982, 2.2351, 2.0763, 2.2794, 2.2648, 2.1535, 1.9758,\n",
      "        2.0349, 2.1062, 2.1874, 2.1916, 2.3061, 2.1743, 2.3035, 2.2387, 1.9164,\n",
      "        2.2190, 2.1433, 2.0785, 3.3515, 2.1767, 2.2271, 2.2683, 2.4374, 2.2492,\n",
      "        2.3722, 2.3072, 2.2966, 2.3387, 2.2814, 2.2693, 2.2586, 2.3518, 1.9538,\n",
      "        2.2860, 2.6067, 2.1882, 2.1689, 2.3590, 2.4005, 2.3129, 2.3184, 2.1600,\n",
      "        2.1797, 2.0316, 2.0126, 2.1414, 2.0766, 2.0950, 2.3983, 2.4505, 2.3889,\n",
      "        2.2329, 2.3492, 2.1217, 1.9453, 2.3978, 2.1283, 2.1136, 2.0349, 2.3132,\n",
      "        2.0207, 2.1516, 2.2641, 2.2492, 2.0763, 2.2623, 2.1175, 2.1062, 2.3328,\n",
      "        2.0923, 2.2698, 2.0231, 2.3634, 1.9943, 2.0494, 2.4195, 2.2960, 2.1646,\n",
      "        2.0537, 1.9762, 2.2517, 2.2194, 2.3416, 2.0354, 2.3255, 2.2751, 2.0917,\n",
      "        2.1763, 2.2283, 2.1622, 2.1599, 2.2058, 2.3060, 2.8016, 2.3398, 2.0960,\n",
      "        1.9475, 2.1986, 2.0597, 2.4592, 2.4453, 2.3426, 2.1437, 2.1094, 2.2862,\n",
      "        2.1952, 2.2256, 2.1096, 2.2669, 2.5722, 2.1716, 2.2627, 2.2825, 2.3095,\n",
      "        2.1295, 2.1514, 2.1708, 2.2602, 2.2375, 2.3248, 2.0606, 2.0518, 2.1970,\n",
      "        2.0767, 2.0354, 2.3083, 2.2190, 2.1611, 2.1926, 2.1983, 2.3634, 2.2343,\n",
      "        2.3191, 2.0264, 2.0050, 2.0278, 2.3614, 1.9927, 1.9204, 2.3286, 2.3150,\n",
      "        2.1454, 2.3719, 2.1238, 2.2217, 1.9818], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0435,  0.0514, -0.0201,  ...,  0.0085, -0.0393,  0.0178],\n",
      "        [ 0.0361,  0.0090,  0.0167,  ..., -0.0557, -0.0050, -0.0325],\n",
      "        [-0.0340, -0.0424, -0.0558,  ..., -0.0141, -0.0441,  0.0212],\n",
      "        ...,\n",
      "        [-0.0235, -0.0275,  0.0250,  ...,  0.0463, -0.0266, -0.0298],\n",
      "        [ 0.0297, -0.0276,  0.0343,  ...,  0.0578, -0.0352, -0.0168],\n",
      "        [-0.0295, -0.0276, -0.0276,  ..., -0.0407,  0.0069,  0.0029]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.5470e-03,  8.4642e-04,  2.3562e-04,  ..., -5.9440e-04,\n",
      "         -9.7817e-04,  1.2437e-03],\n",
      "        [-1.1145e-03, -3.5599e-03,  3.7021e-03,  ..., -1.8834e-03,\n",
      "          2.5922e-03, -3.6784e-03],\n",
      "        [ 7.3287e-05, -1.2913e-02,  1.1437e-02,  ..., -1.1363e-02,\n",
      "          1.1929e-02, -1.2172e-02],\n",
      "        ...,\n",
      "        [ 1.2817e-02, -5.3651e-04,  1.6701e-03,  ..., -2.5347e-03,\n",
      "          1.4590e-04, -9.0750e-04],\n",
      "        [-4.9046e-04,  8.8708e-04, -1.4852e-03,  ...,  8.8481e-04,\n",
      "         -1.5710e-03,  1.2161e-03],\n",
      "        [ 4.9861e-04,  4.7108e-04, -8.0568e-04,  ...,  3.5134e-04,\n",
      "         -1.4134e-03,  1.1154e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.7301, 1.9095, 1.6064,  ..., 2.2147, 2.0477, 2.0304], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0018, -0.0058,  0.0032,  ...,  0.0043,  0.0051,  0.0182],\n",
      "        [-0.0199,  0.0197,  0.0315,  ..., -0.0169,  0.0169, -0.0179],\n",
      "        [ 0.0192, -0.0014, -0.0005,  ..., -0.0169, -0.0172,  0.0218],\n",
      "        ...,\n",
      "        [ 0.0131, -0.0301,  0.0201,  ..., -0.0203, -0.0251, -0.0189],\n",
      "        [-0.0305, -0.0070,  0.0034,  ...,  0.0230, -0.0216, -0.0098],\n",
      "        [ 0.0114,  0.0244,  0.0234,  ...,  0.0030, -0.0083, -0.0182]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0119,  0.0074, -0.0128,  ..., -0.0138,  0.0115,  0.0126],\n",
      "        [-0.0033, -0.0060,  0.0056,  ...,  0.0033, -0.0043, -0.0069],\n",
      "        [-0.0151, -0.0102,  0.0142,  ...,  0.0140, -0.0134, -0.0123],\n",
      "        ...,\n",
      "        [ 0.0129,  0.0065, -0.0107,  ..., -0.0128,  0.0109,  0.0092],\n",
      "        [ 0.0140,  0.0078, -0.0122,  ..., -0.0133,  0.0123,  0.0102],\n",
      "        [ 0.0133,  0.0099, -0.0126,  ..., -0.0123,  0.0119,  0.0108]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 5.2847,  4.9901,  4.9747,  3.5785,  5.1544,  5.9390,  6.1613,  6.2372,\n",
      "         5.7769,  5.4244,  5.9335,  5.5870,  5.7335,  5.2753,  5.4144,  5.9534,\n",
      "         5.3166,  5.9170,  5.2847,  5.7524,  5.4382,  6.0932,  4.7062,  6.1284,\n",
      "         5.5737,  5.8806,  4.9708,  5.4902,  5.4248,  5.5807,  5.5977,  4.1791,\n",
      "         6.1437,  4.6424,  5.7128,  5.7457,  4.5842,  6.0232,  5.8581,  5.1589,\n",
      "         5.6095,  5.8787,  5.5784,  5.7273,  6.0542,  5.7617,  5.3945,  5.3897,\n",
      "         5.3359,  5.9677,  5.2603,  6.1021,  5.5475,  5.8427,  5.4215,  6.0504,\n",
      "         5.3030,  6.2925,  5.2465,  4.7841,  6.3816,  5.5406, 12.0068,  5.7785,\n",
      "         6.0463,  5.5301,  5.6352,  5.9145,  5.7116,  5.7560,  5.4141,  5.8320,\n",
      "         5.1126,  5.4194,  5.4772,  6.1769,  5.6844,  6.3540,  4.9360,  5.6246,\n",
      "         5.1228,  5.0692,  5.6642,  5.0287,  5.7222,  5.8819,  5.9271,  5.6511,\n",
      "         5.4801,  5.3620,  5.7743,  4.9976,  5.7335,  5.9270,  5.2482,  6.6251,\n",
      "         6.0812,  5.4850,  4.1839,  5.4442,  5.7174,  6.0582,  5.9522,  5.5258,\n",
      "         5.8181,  5.8472,  5.4402,  5.9473,  5.6697,  5.4505,  6.3702,  5.6826,\n",
      "         5.6289,  6.0797,  5.6788,  5.8009,  5.4098,  5.2988,  6.0820,  5.5807,\n",
      "         5.9295,  5.5565,  5.4195,  4.4460,  4.0813,  5.3234,  6.0171,  5.4650,\n",
      "         5.6640,  5.4247,  5.8153,  5.0344,  3.9591,  5.2820,  6.1628,  5.2880,\n",
      "         5.7397,  6.2072,  5.4549,  4.8830,  5.1220,  4.9680,  5.8989,  6.0235,\n",
      "         6.6266,  6.0982,  6.2702,  6.2406,  6.1062,  5.9211,  5.8673,  5.8843,\n",
      "         5.3498,  5.8888,  5.4150,  6.1073,  5.0367,  5.9294,  6.0709,  5.8521,\n",
      "         5.6042,  5.5555,  5.4137,  6.1256,  5.6536,  6.1268,  6.0143,  5.6645,\n",
      "         5.3756,  5.9837,  4.8985,  5.9212,  5.5199,  5.6226,  5.5071,  5.0357,\n",
      "         5.6188,  6.2272,  5.2471,  5.0380,  5.2105,  5.0915,  5.5644,  6.0616,\n",
      "         5.7358,  5.7779,  5.7921,  6.2276,  5.4138,  5.6156,  5.2859,  5.5884,\n",
      "         4.9274,  5.7657,  5.5406,  5.9294,  6.2224,  5.9035,  5.9175,  5.6051,\n",
      "         5.6195,  6.1064,  6.1103,  5.9105,  5.8404,  6.0119,  5.6656,  3.5037,\n",
      "         6.0038,  4.9160,  5.7686,  6.0326,  5.9401,  5.6781,  5.9284,  6.2340,\n",
      "         5.6110,  5.3373,  4.8332,  5.2650,  5.7597,  5.6384,  5.5400,  5.6963,\n",
      "         6.5207,  4.6758,  5.9252,  5.5005,  5.8730,  5.8748,  5.9115,  5.2763,\n",
      "         5.4315,  5.1025,  5.6619,  5.7911,  5.5091,  6.1072,  5.5233,  5.8943,\n",
      "         5.0694,  5.9096,  5.8319,  5.5669,  6.0198,  5.6807,  5.1116,  5.7761,\n",
      "         4.9091,  5.8865,  5.5879,  5.4919,  5.2468,  5.3131,  5.7755,  5.0918,\n",
      "         5.4334,  5.5053,  5.1742,  5.3725,  5.7343,  5.6979,  5.8674,  5.6941,\n",
      "         5.9509,  5.7632,  5.8247,  5.3862,  5.3742,  5.6580,  3.9241,  5.7879,\n",
      "         5.3000,  5.9071,  5.7405,  5.7559,  6.3465,  5.4692,  5.6798,  5.9038,\n",
      "         5.4906,  5.9311,  6.0472,  6.2952,  5.2179,  5.5242,  6.0947,  5.9163,\n",
      "         3.8367,  5.4336,  5.8984,  6.0058,  6.0188,  5.6067,  5.6794,  6.2198,\n",
      "         5.4101,  4.7764,  5.7400,  5.6781,  5.7375,  5.6784,  5.4158,  5.8558,\n",
      "         6.3216,  5.9057,  6.2229,  5.0039,  4.9036,  6.0066,  5.6960,  5.0667,\n",
      "         5.0232,  5.7984,  5.2101,  5.4329,  5.9919,  5.6719,  5.5024,  3.4013],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 5.2048e-02,  5.7168e-02,  3.0671e-03,  7.6395e-03,  4.9073e-02,\n",
      "         -3.1210e-02,  6.1013e-02, -2.0448e-02, -4.3302e-02, -9.0638e-02,\n",
      "         -3.1305e-02,  8.1594e-03, -1.9025e-02,  6.1880e-02,  3.3327e-02,\n",
      "         -3.2257e-02, -5.1502e-02, -6.3603e-02,  4.7971e-02,  1.8586e-02,\n",
      "         -1.0727e-02, -9.0833e-02, -2.3802e-02, -4.8193e-02, -2.7362e-02,\n",
      "          4.1894e-02,  2.4672e-02,  3.4746e-02, -2.1268e-03,  6.4929e-02,\n",
      "         -4.7017e-02, -5.3023e-02,  7.5545e-02, -8.5475e-03, -1.8681e-02,\n",
      "          1.9075e-02, -3.6381e-02, -3.6849e-02,  6.7679e-02, -6.5267e-03,\n",
      "          5.4711e-02, -9.1222e-02, -8.6994e-03,  6.5640e-02,  7.8718e-02,\n",
      "         -1.8303e-02, -3.3819e-02, -5.8391e-02,  6.2718e-02, -7.2402e-02,\n",
      "         -2.5109e-02,  8.3493e-03,  6.9597e-02,  1.1976e-02, -2.4708e-02,\n",
      "         -3.9660e-02,  9.0227e-03,  4.8732e-03, -5.2985e-02,  5.2058e-02,\n",
      "         -4.4509e-02, -6.5427e-03, -7.9234e-02, -2.9316e-02, -4.5654e-02,\n",
      "          7.0541e-02, -2.9225e-02, -4.6966e-02,  1.9730e-02,  4.3822e-02,\n",
      "         -7.7667e-02,  3.8662e-02, -3.4669e-02,  2.9455e-02,  4.8630e-02,\n",
      "         -1.8747e-03, -4.0188e-02,  8.0200e-02,  3.1501e-02,  2.1120e-03,\n",
      "         -3.8209e-02, -1.7818e-02, -2.3472e-02,  2.2498e-02, -2.3097e-02,\n",
      "          2.0090e-03, -1.2906e-02, -9.4848e-03, -9.5199e-04, -3.1765e-02,\n",
      "          6.8914e-02, -2.4542e-02,  5.6995e-02,  7.3282e-02,  8.2201e-02,\n",
      "          1.6637e-02,  4.4703e-03,  6.5569e-03,  8.0486e-02,  8.2753e-02,\n",
      "          4.9726e-02,  4.7019e-03,  2.7874e-02, -2.8243e-02,  2.3701e-02,\n",
      "         -2.1732e-02, -8.0983e-02, -2.4880e-02,  1.1293e-02,  4.1167e-02,\n",
      "          1.5900e-02, -2.6957e-02, -2.3409e-03, -2.8293e-02,  6.9298e-02,\n",
      "          8.6540e-02,  2.7610e-02, -5.4482e-02,  7.8792e-02, -1.0626e-02],\n",
      "        [-4.3524e-02,  1.8156e-02,  8.7717e-02, -8.0032e-03, -8.4494e-02,\n",
      "          3.1344e-03, -6.1531e-02,  2.6168e-02, -4.7147e-02,  8.0173e-02,\n",
      "         -6.0429e-02, -1.4185e-02,  4.8383e-02,  9.0423e-02, -7.8544e-02,\n",
      "         -5.4365e-02, -6.4657e-02, -3.7951e-02, -6.7491e-02,  2.3218e-02,\n",
      "          3.1010e-02, -7.9554e-02,  8.9294e-02, -3.6038e-02, -6.0452e-02,\n",
      "         -8.4869e-02, -5.4067e-02,  1.2882e-02, -8.3364e-03,  3.1729e-02,\n",
      "          5.0105e-02, -1.4880e-02,  7.0933e-02,  7.4586e-02,  4.5191e-02,\n",
      "          2.1829e-02, -4.9204e-02, -3.6967e-02, -4.4102e-02,  1.2695e-03,\n",
      "          5.5328e-02, -2.6898e-02,  2.8673e-02,  7.6428e-02, -4.9325e-02,\n",
      "         -3.9762e-02, -5.5717e-02, -5.3646e-02,  5.1280e-02,  6.5921e-02,\n",
      "         -7.3198e-02,  4.0161e-02, -7.5979e-02,  4.1211e-03, -3.6509e-02,\n",
      "         -2.1323e-02,  1.8490e-03, -2.8367e-02, -6.8964e-02,  8.1847e-02,\n",
      "         -7.5619e-03, -5.9387e-02,  2.3774e-02, -1.0436e-03,  4.6711e-02,\n",
      "          3.2849e-02,  3.5498e-02, -6.8003e-02, -4.6913e-02,  6.8553e-02,\n",
      "          8.9541e-02,  7.6555e-02,  7.4681e-02,  5.8823e-02,  4.2131e-02,\n",
      "         -3.8928e-02, -6.3926e-02,  4.8324e-02, -3.6324e-02,  4.3068e-02,\n",
      "         -6.4895e-02, -5.1385e-02,  2.1957e-02,  2.9131e-02, -1.3269e-03,\n",
      "         -4.4508e-02,  8.5692e-02,  5.5540e-02,  6.6900e-02,  4.8650e-02,\n",
      "         -7.7568e-02, -6.1602e-02,  2.7745e-02,  2.5610e-02, -6.8598e-02,\n",
      "         -8.6124e-02, -8.1652e-02, -1.2628e-02, -6.9427e-02,  1.2360e-02,\n",
      "          7.5987e-02, -8.5291e-02, -2.0682e-02, -5.1821e-03, -2.8576e-02,\n",
      "         -3.9674e-02,  6.3789e-02,  6.9545e-02,  6.2593e-02,  8.1183e-03,\n",
      "          9.0493e-02,  8.0044e-02, -4.6054e-02, -1.4563e-02, -4.1635e-02,\n",
      "         -7.2867e-02,  3.0071e-02,  8.3747e-02, -2.4794e-02,  5.5450e-02],\n",
      "        [ 2.5866e-02,  7.9405e-02, -1.3064e-02,  6.1466e-02, -4.9401e-02,\n",
      "          5.4963e-02,  7.8275e-02, -2.8372e-02, -8.8238e-02,  1.7308e-02,\n",
      "          7.1522e-02, -2.3194e-03, -3.3775e-03, -2.5514e-02,  4.9239e-02,\n",
      "         -1.2804e-02,  1.4174e-02,  9.0242e-02,  1.7368e-03, -1.6282e-02,\n",
      "         -4.2620e-04,  6.4290e-02,  5.5246e-02,  5.0454e-02,  5.6458e-02,\n",
      "         -8.4477e-02, -4.5525e-03,  2.8170e-02, -5.2229e-02, -2.4550e-02,\n",
      "         -5.0321e-02, -2.2997e-02,  6.1522e-02, -1.9320e-02, -7.3430e-02,\n",
      "         -5.7278e-02,  5.0821e-02, -1.2768e-02,  7.8118e-02, -3.1079e-02,\n",
      "          7.7682e-02, -1.6414e-02,  6.5233e-02, -2.3002e-02, -6.5459e-02,\n",
      "         -8.0094e-02, -4.3684e-02,  7.1106e-02, -6.8804e-02, -7.2889e-02,\n",
      "          6.7741e-02,  8.4391e-02,  7.6311e-02,  7.9899e-02,  8.8938e-02,\n",
      "          8.3535e-02, -8.2436e-02, -3.5915e-03,  3.6392e-02,  2.2132e-02,\n",
      "          3.1260e-02,  6.6301e-02,  8.7614e-02,  2.0461e-02,  6.1568e-02,\n",
      "         -4.9511e-02,  3.3442e-02,  7.8538e-02,  2.1029e-02,  4.0005e-02,\n",
      "         -2.1998e-02, -9.1218e-02,  5.1963e-02, -4.5896e-02,  2.2432e-02,\n",
      "         -1.4790e-02,  4.9881e-02,  8.4726e-02, -1.6578e-02, -2.9716e-02,\n",
      "          8.3143e-02, -7.5306e-02,  2.5014e-02, -8.9941e-02, -3.9233e-02,\n",
      "          2.3438e-02, -7.7145e-04,  4.2188e-03, -8.8095e-02, -6.7135e-02,\n",
      "          4.9220e-02, -1.2141e-02, -6.6777e-02, -1.5566e-02, -2.3173e-02,\n",
      "         -5.1371e-02, -2.5879e-02, -8.1535e-02, -4.1013e-02, -4.7851e-02,\n",
      "         -6.6397e-02,  8.7765e-02,  8.3044e-02, -4.6970e-02, -2.6849e-02,\n",
      "          7.5462e-02,  7.9259e-03, -1.6544e-02, -6.8735e-02, -4.4598e-02,\n",
      "          4.9845e-04, -3.7067e-02, -8.1111e-02,  5.1616e-02,  6.2984e-02,\n",
      "          4.7545e-02,  3.6439e-02,  1.2973e-02, -5.8141e-02, -4.1015e-02],\n",
      "        [-7.0770e-02, -2.3912e-02, -5.4811e-02, -6.8518e-02,  5.3705e-02,\n",
      "         -1.7017e-02, -3.7094e-02, -3.2247e-02, -7.7896e-02,  8.7733e-02,\n",
      "         -7.5835e-02,  1.9084e-02, -1.7063e-02,  5.7932e-02,  7.0905e-02,\n",
      "          3.0538e-02,  7.1833e-02, -1.8382e-02, -8.6879e-02, -2.4391e-03,\n",
      "         -1.8961e-04,  6.5857e-02, -1.3579e-02, -8.7461e-02,  7.2151e-03,\n",
      "         -4.0687e-02, -4.0339e-02, -8.6915e-03,  6.8748e-02,  3.1219e-02,\n",
      "          6.6404e-02, -4.3203e-02, -9.0433e-02, -1.6474e-02,  9.1238e-02,\n",
      "          7.1505e-03, -4.5745e-02,  1.5214e-02,  7.8110e-02, -9.1125e-02,\n",
      "         -5.9617e-02,  3.8887e-02, -7.7777e-02,  3.2791e-02, -8.0137e-02,\n",
      "         -3.3836e-02,  5.9641e-03, -5.0180e-02,  6.7538e-02,  7.5382e-02,\n",
      "         -2.5732e-02, -5.5171e-02,  6.5320e-02,  4.2669e-02,  7.9218e-02,\n",
      "         -5.5700e-02, -8.2238e-02,  8.0124e-03,  3.1590e-03, -2.5704e-02,\n",
      "         -5.6741e-03,  4.9395e-04,  1.2604e-02,  3.7754e-02, -5.2252e-02,\n",
      "         -6.3988e-05, -1.9274e-02,  6.4993e-02,  3.3794e-02, -3.0669e-02,\n",
      "          8.3265e-02, -8.6009e-03, -2.9982e-02,  2.9370e-02,  5.1132e-02,\n",
      "         -2.7723e-03, -6.1257e-02,  5.1481e-03,  7.6030e-02, -1.1829e-02,\n",
      "          9.0769e-02, -3.5690e-02, -8.0731e-02,  3.9631e-02, -4.0781e-02,\n",
      "         -6.1122e-02, -2.0530e-02,  2.9503e-02, -8.8690e-02,  7.4762e-02,\n",
      "          4.4173e-02, -5.6355e-02, -8.4142e-02,  4.5109e-02,  5.6040e-02,\n",
      "          2.7298e-02, -4.0730e-02, -3.6586e-02, -6.3828e-02,  7.0806e-03,\n",
      "          5.3247e-02, -2.2195e-02,  3.6842e-02,  8.9839e-02, -4.8552e-03,\n",
      "         -8.5568e-02, -5.2697e-04, -7.2547e-02,  6.8513e-02,  8.0152e-02,\n",
      "         -4.0386e-02,  3.2646e-02, -6.3814e-02, -2.5210e-02,  5.9936e-02,\n",
      "         -7.8991e-02, -8.7769e-02,  3.5507e-02,  5.9511e-02, -8.5726e-02],\n",
      "        [ 4.8205e-02,  4.3737e-02,  5.5488e-02,  5.3117e-02, -6.8854e-02,\n",
      "         -1.3530e-02,  6.8259e-02, -2.2976e-02,  5.1076e-02,  4.4870e-02,\n",
      "         -5.5534e-02,  8.6940e-02,  7.3180e-02,  4.7798e-02,  7.6495e-02,\n",
      "         -6.4625e-02,  4.9923e-02,  4.1502e-02, -5.2620e-02, -1.0773e-03,\n",
      "          1.4424e-02, -1.8933e-02, -7.4799e-02,  2.4285e-02, -4.0002e-03,\n",
      "         -7.5474e-02, -7.4636e-03, -4.3678e-02, -2.8748e-02,  5.2301e-02,\n",
      "         -3.6687e-02,  8.5370e-02,  5.5577e-03, -1.4829e-02, -4.3796e-02,\n",
      "          7.7980e-02,  3.8031e-03, -5.7689e-02, -2.8371e-02, -1.1608e-02,\n",
      "          4.0987e-03, -6.7840e-02, -3.0961e-02, -6.1162e-02,  2.4371e-02,\n",
      "          6.4245e-02, -2.8524e-03, -3.7693e-03,  6.6016e-02, -7.4197e-02,\n",
      "         -6.9313e-02, -6.9040e-02,  6.8146e-02,  2.4434e-02,  3.0487e-02,\n",
      "          8.0080e-03, -2.9285e-03,  8.8121e-02, -6.8492e-02, -7.0580e-02,\n",
      "          4.6320e-02,  1.0411e-02, -3.9706e-02, -6.7674e-02, -3.0895e-02,\n",
      "          8.5238e-02,  8.3126e-02, -2.1042e-02, -5.0605e-02,  5.7306e-02,\n",
      "         -8.0013e-02,  3.9645e-02, -8.2224e-02,  6.4132e-02, -4.4297e-02,\n",
      "         -6.4583e-02,  6.2014e-03, -3.9227e-03,  2.5545e-02, -9.0680e-02,\n",
      "         -6.4731e-02, -8.2919e-02, -3.7788e-02,  9.1263e-02,  6.1929e-02,\n",
      "         -6.1832e-02,  1.5195e-02,  2.7057e-02,  1.3705e-02,  1.9044e-02,\n",
      "         -1.6152e-02, -5.4144e-02, -4.3002e-02, -6.1121e-02, -6.3252e-02,\n",
      "          6.1997e-02, -2.0995e-02,  1.8157e-03, -7.9598e-02,  6.1480e-02,\n",
      "         -3.8141e-03,  6.0415e-02, -2.9940e-02, -2.6105e-02, -2.5436e-02,\n",
      "          1.5003e-02,  7.9838e-02, -3.8747e-02,  8.9901e-02, -3.8925e-02,\n",
      "          1.9897e-02, -3.8338e-02,  4.5365e-02, -3.5450e-02,  4.2659e-02,\n",
      "         -3.1645e-02,  9.0453e-02, -7.4355e-02,  8.6922e-02, -3.2312e-02],\n",
      "        [ 7.8611e-02,  5.0023e-02,  3.1569e-02,  7.4060e-02,  3.4692e-02,\n",
      "         -3.8813e-02, -1.3483e-03,  6.5831e-02,  8.4281e-02, -6.6874e-02,\n",
      "         -2.8215e-02, -8.0277e-02, -4.5427e-02, -4.2700e-02,  4.4488e-02,\n",
      "         -8.3317e-02, -6.9992e-02, -3.6561e-02, -8.5886e-03,  5.8279e-04,\n",
      "          2.4995e-02,  5.7487e-02, -5.3198e-02,  9.0176e-03,  8.1387e-04,\n",
      "         -7.6916e-02,  8.6139e-02, -8.7993e-02,  8.3792e-02,  5.5021e-02,\n",
      "         -7.8043e-02, -1.0653e-02, -6.6272e-02,  7.9602e-02,  3.5707e-02,\n",
      "         -4.4018e-03,  2.7479e-02,  1.6917e-02, -5.4429e-02, -6.8105e-02,\n",
      "         -2.8116e-02,  3.8720e-02, -8.8406e-02,  2.8788e-02,  7.8700e-02,\n",
      "          3.1168e-02,  5.0301e-02,  7.2557e-02,  7.7166e-02,  5.1948e-02,\n",
      "         -1.3104e-02,  1.7543e-03, -4.1497e-02, -6.6288e-02,  2.4006e-02,\n",
      "         -6.7753e-02, -4.6466e-02,  7.9295e-02,  4.0833e-02,  6.2728e-02,\n",
      "         -5.4178e-02,  2.7313e-02, -7.6762e-02,  7.8231e-02, -4.7464e-02,\n",
      "         -3.3610e-02,  3.9732e-02,  8.1485e-02, -4.1542e-02,  2.6831e-02,\n",
      "         -2.6617e-02, -4.0696e-02,  6.0360e-02, -5.9014e-02,  3.0745e-02,\n",
      "         -8.7112e-03,  4.4217e-02,  8.5085e-02,  5.0352e-02,  8.7947e-02,\n",
      "         -7.5486e-02, -2.8877e-02,  7.8583e-02, -8.3780e-02,  9.0747e-03,\n",
      "         -7.3395e-04, -3.2927e-02,  1.5885e-02, -2.2105e-02,  7.7039e-02,\n",
      "         -4.3619e-02, -1.2271e-02, -4.4721e-02,  4.5569e-02,  6.4266e-02,\n",
      "         -3.8143e-02, -9.6399e-03, -5.2199e-02, -8.3583e-02, -6.2214e-02,\n",
      "          5.2032e-02, -2.3657e-02, -4.2494e-02,  2.2489e-02,  6.0503e-02,\n",
      "          8.6333e-02, -4.2516e-02, -8.4919e-02,  2.6706e-02, -6.3692e-02,\n",
      "          8.8765e-02,  1.0036e-02, -5.0520e-02, -5.6726e-02, -2.9083e-02,\n",
      "         -5.2542e-02,  8.8274e-02, -2.4867e-03, -8.9578e-02, -7.2576e-02],\n",
      "        [ 8.9572e-02, -2.3469e-02,  6.2593e-02, -9.8068e-03,  4.4818e-02,\n",
      "         -1.7806e-02,  7.7258e-02, -8.7955e-02,  2.0717e-02,  3.1078e-02,\n",
      "          5.9258e-02,  7.0024e-02,  1.4849e-02, -5.8861e-02, -1.1326e-02,\n",
      "         -6.5491e-02,  2.6469e-02, -2.5098e-02,  5.2014e-02,  1.3741e-02,\n",
      "         -1.3696e-02,  1.1992e-02, -5.7642e-02, -4.6052e-02, -3.6499e-02,\n",
      "         -3.3648e-02, -6.9920e-02, -3.0771e-02,  7.1880e-02,  8.9643e-02,\n",
      "          7.9920e-03,  1.2432e-02, -8.8859e-02, -2.9080e-02,  8.1511e-02,\n",
      "          3.3576e-02,  2.4451e-02, -8.1077e-02, -4.2202e-02,  6.4616e-02,\n",
      "          3.9724e-02, -5.9294e-02,  1.7537e-02,  2.1365e-02, -3.0884e-02,\n",
      "         -7.9861e-02,  5.2485e-02, -7.4168e-02, -8.7185e-02, -7.2453e-02,\n",
      "         -6.0077e-02, -6.7730e-02, -5.2165e-03, -6.1453e-02, -4.5829e-03,\n",
      "         -8.4484e-02, -3.0384e-02,  2.6585e-02,  8.9533e-02,  7.0750e-02,\n",
      "         -8.6600e-02, -5.7153e-02,  7.1428e-02, -5.2724e-02,  4.8475e-02,\n",
      "         -2.1627e-02, -6.5967e-02, -4.3553e-02, -1.8233e-02, -6.9964e-02,\n",
      "         -7.8359e-02, -6.5171e-02, -6.6544e-03, -4.6473e-02, -3.8572e-02,\n",
      "         -3.3438e-02,  7.8866e-02,  6.8237e-02,  7.6397e-02, -8.9299e-03,\n",
      "          2.1678e-02, -4.4581e-02,  8.8017e-02, -1.1254e-02, -1.0109e-02,\n",
      "          6.7245e-02,  8.8134e-02,  6.3138e-02,  2.1344e-02,  6.6490e-02,\n",
      "         -2.1411e-02, -4.8203e-03, -8.1019e-02,  8.3602e-03,  7.8788e-02,\n",
      "          6.3118e-02,  4.8445e-02,  2.6266e-02,  6.5160e-02,  3.6574e-02,\n",
      "         -4.3626e-02,  3.6461e-02, -7.2867e-02,  8.9647e-02, -4.8377e-02,\n",
      "         -2.7007e-02,  6.1859e-02, -7.2727e-02,  7.8305e-02, -1.4361e-02,\n",
      "          5.1413e-02,  7.4190e-02, -6.3165e-02, -8.4459e-02,  5.3581e-02,\n",
      "         -2.6601e-02, -8.2763e-02,  4.4337e-02,  6.2857e-02, -6.2388e-03],\n",
      "        [ 7.1211e-02,  6.6658e-02, -5.9428e-02, -4.2631e-02,  6.9394e-02,\n",
      "         -7.0819e-03, -3.5744e-02, -3.8549e-02, -6.2739e-02,  2.1319e-02,\n",
      "          5.5233e-02,  6.3317e-02, -3.8381e-02, -2.7255e-02, -1.4849e-02,\n",
      "          4.3682e-02, -4.0378e-02, -6.4587e-02,  6.8455e-02,  1.8486e-02,\n",
      "          6.8346e-02, -4.1347e-02, -1.5260e-02,  7.0921e-02, -8.5917e-02,\n",
      "         -6.5156e-02,  8.3500e-02, -2.0661e-02,  4.1561e-03,  1.4112e-02,\n",
      "         -4.4986e-02,  6.9690e-02,  1.8034e-02,  6.4943e-02, -5.2788e-02,\n",
      "          1.4952e-02,  7.5921e-02, -1.7352e-02, -2.1763e-02, -7.8053e-02,\n",
      "          6.3141e-02,  2.5997e-02, -6.3720e-03, -6.5469e-02, -7.4798e-02,\n",
      "         -7.9742e-02,  8.2475e-02, -4.2604e-02,  1.9424e-02,  1.5992e-02,\n",
      "          6.4784e-02, -2.1470e-03, -2.1254e-02,  1.1253e-02, -6.4680e-02,\n",
      "         -4.9694e-02,  8.1259e-03, -4.4849e-02,  7.1986e-03, -2.9087e-02,\n",
      "          4.8122e-02,  1.0728e-02, -7.5657e-02, -5.1599e-03, -4.0601e-02,\n",
      "         -8.7286e-02, -3.1102e-02, -6.0128e-02, -6.4449e-02,  4.5978e-02,\n",
      "          3.2670e-02,  2.4521e-02, -5.6797e-02,  2.0140e-02,  7.6288e-02,\n",
      "          4.9833e-02,  4.5984e-02,  5.2504e-02, -7.8745e-02,  2.0454e-02,\n",
      "         -5.5656e-02,  7.4411e-02,  5.9370e-02,  8.0931e-02, -5.5604e-02,\n",
      "          9.1125e-02,  8.0322e-02, -1.8972e-02,  2.6521e-03,  6.9167e-02,\n",
      "          6.1412e-02, -4.3952e-02, -1.4217e-02,  3.2643e-02, -6.6850e-02,\n",
      "          8.5213e-02, -8.7090e-02,  3.5398e-02,  3.6040e-02,  4.0841e-02,\n",
      "         -3.1422e-02,  5.7057e-02, -3.8533e-02,  8.3216e-02,  7.1672e-02,\n",
      "          4.7856e-02, -1.6386e-02, -2.1815e-02, -3.0048e-02,  3.7858e-02,\n",
      "         -4.7731e-02, -4.1232e-02, -1.9629e-02,  9.6535e-03,  2.4985e-02,\n",
      "         -5.5127e-02, -6.7492e-02, -8.4300e-02, -1.8194e-02, -4.7798e-02]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([42.3184], device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0548,  0.0282, -0.0169,  ..., -0.0434, -0.0319,  0.0282],\n",
      "        [-0.0655,  0.0459,  0.0242,  ..., -0.0394,  0.0366, -0.0089],\n",
      "        [ 0.0330,  0.0394,  0.0182,  ...,  0.0546,  0.0354, -0.0007],\n",
      "        ...,\n",
      "        [-0.0502,  0.0542,  0.0257,  ...,  0.0239,  0.0232, -0.0338],\n",
      "        [ 0.0362, -0.0389, -0.0439,  ..., -0.0466, -0.0140,  0.0267],\n",
      "        [ 0.0478, -0.0016, -0.0115,  ...,  0.0103,  0.0243, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0027, -0.0035,  0.0128,  ..., -0.0133,  0.0103,  0.0145],\n",
      "        [-0.0027, -0.0034,  0.0127,  ..., -0.0131,  0.0102,  0.0145],\n",
      "        [ 0.0027,  0.0034, -0.0126,  ...,  0.0129, -0.0101, -0.0143],\n",
      "        ...,\n",
      "        [-0.0032, -0.0039,  0.0131,  ..., -0.0134,  0.0105,  0.0147],\n",
      "        [-0.0030, -0.0037,  0.0127,  ..., -0.0130,  0.0103,  0.0143],\n",
      "        [ 0.0028,  0.0035, -0.0129,  ...,  0.0133, -0.0104, -0.0147]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3803, 0.3318, 0.3452, 0.3615, 0.3373, 0.3577, 0.3373, 0.3805, 0.3621,\n",
      "        0.3603, 0.3572, 0.3430, 0.3581, 0.3786, 0.3633, 0.3857, 0.3504, 0.3230,\n",
      "        0.3761, 0.3774, 0.3676, 0.3619, 0.3687, 0.3731, 0.3543, 0.3634, 0.3328,\n",
      "        0.3571, 0.3590, 0.3856, 0.3540, 0.3799, 0.3625, 0.3315, 0.3609, 0.3447,\n",
      "        0.3710, 0.3695, 0.3693, 0.3818, 0.3526, 0.3609, 0.3802, 0.3486, 0.3625,\n",
      "        0.3655, 0.3755, 0.3760, 0.3639, 0.3869, 0.3520, 0.3575, 0.3392, 0.3767,\n",
      "        0.3755, 0.3570, 0.3618, 0.3531, 0.3586, 0.3641, 0.3500, 0.3619, 0.3691,\n",
      "        0.3595, 0.3343, 0.3816, 0.3828, 0.3807, 0.3598, 0.3819, 0.3383, 0.3747,\n",
      "        0.3506, 0.3486, 0.3290, 0.3559, 0.3578, 0.3548, 0.3752, 0.3581, 0.3803,\n",
      "        0.3754, 0.3773, 0.3481, 0.3594, 0.3687, 0.3576, 0.3355, 0.3544, 0.3745,\n",
      "        0.3524, 0.3658, 0.3514, 0.3662, 0.3242, 0.3681, 0.3436, 0.3738, 0.3814,\n",
      "        0.3328, 0.3345, 0.3529, 0.3390, 0.3515, 0.3385, 0.3667, 0.3543, 0.3683,\n",
      "        0.3586, 0.3455, 0.3463, 0.3759, 0.3642, 0.3794, 0.3373, 0.3810, 0.3651,\n",
      "        0.4018, 0.3570, 0.3263, 0.3883, 0.3280, 0.3500, 0.3606, 0.3789, 0.3508,\n",
      "        0.3781, 0.3475, 0.3645, 0.3749, 0.3680, 0.3583, 0.3935, 0.3502, 0.3527,\n",
      "        0.3824, 0.3365, 0.3861, 0.3648, 0.3735, 0.3542, 0.3467, 0.3715, 0.3502,\n",
      "        0.3822, 0.3605, 0.3618, 0.3485, 0.3634, 0.3631, 0.3693, 0.3724, 0.3666,\n",
      "        0.3724, 0.3846, 0.3774, 0.3519, 0.3611, 0.3803, 0.3732, 0.3594, 0.3471,\n",
      "        0.3800, 0.3720, 0.3855, 0.3525, 0.3328, 0.3691, 0.3728, 0.3891, 0.3892,\n",
      "        0.3588, 0.3533, 0.3703, 0.3462, 0.3637, 0.3583, 0.3763, 0.3192, 0.4071,\n",
      "        0.3721, 0.3632, 0.3733, 0.3629, 0.3659, 0.3577, 0.3713, 0.3661, 0.3534,\n",
      "        0.3648, 0.3858, 0.3532, 0.3749, 0.3589, 0.3746, 0.3776, 0.3695, 0.3614,\n",
      "        0.3570, 0.3679, 0.3677, 0.3678, 0.3691, 0.3450, 0.3378, 0.3783, 0.3629,\n",
      "        0.3403, 0.3880, 0.3822, 0.3474, 0.3444, 0.3710, 0.3793, 0.3839, 0.3475,\n",
      "        0.3687, 0.3371, 0.3904, 0.3534, 0.3653, 0.3794, 0.3502, 0.3535, 0.3268,\n",
      "        0.3644, 0.3592, 0.3632, 0.3729, 0.3728, 0.3893, 0.3707, 0.3594, 0.3498,\n",
      "        0.3519, 0.3692, 0.3775, 0.3508, 0.3286, 0.3557, 0.3589, 0.3699, 0.3618,\n",
      "        0.3878, 0.3324, 0.3638, 0.3457, 0.3817, 0.3725, 0.3637, 0.3882, 0.3675,\n",
      "        0.3753, 0.3649, 0.3436, 0.3959, 0.3703, 0.3563, 0.3510, 0.3733, 0.3822,\n",
      "        0.3541, 0.3181, 0.3365, 0.3677, 0.3754, 0.3607, 0.3566, 0.3514, 0.3751,\n",
      "        0.3744, 0.3823, 0.3648, 0.3593, 0.3662, 0.3545, 0.3339, 0.3507, 0.3629,\n",
      "        0.3433, 0.3379, 0.3444, 0.3571, 0.3521, 0.3537, 0.3587, 0.3339, 0.3512,\n",
      "        0.3673, 0.3220, 0.3680, 0.3768, 0.3922, 0.3558, 0.3507, 0.3254, 0.3685,\n",
      "        0.3579, 0.3618, 0.3646, 0.3619, 0.3610, 0.3849, 0.3541, 0.3649, 0.3700,\n",
      "        0.3163, 0.3581, 0.3613, 0.3662, 0.3565, 0.3895, 0.3759, 0.3367, 0.3652,\n",
      "        0.3893, 0.3388, 0.3491, 0.3845, 0.3697], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0206, -0.0040,  0.0069,  ...,  0.0368, -0.0414, -0.0287],\n",
      "        [-0.0383,  0.0434,  0.0294,  ...,  0.0427, -0.0431, -0.0010],\n",
      "        [ 0.0346,  0.0252, -0.0294,  ...,  0.0538,  0.0147, -0.0206],\n",
      "        ...,\n",
      "        [-0.0435,  0.0449,  0.0213,  ..., -0.0329,  0.0307, -0.0136],\n",
      "        [ 0.0648,  0.0421,  0.0091,  ...,  0.0263,  0.0266, -0.0235],\n",
      "        [ 0.0212,  0.0440,  0.0476,  ..., -0.0048,  0.0007,  0.0319]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0123,  0.0105,  0.0042,  0.0092, -0.0042, -0.0125,  0.0078, -0.0071],\n",
      "        [-0.0123, -0.0105, -0.0042, -0.0092,  0.0042,  0.0125, -0.0078,  0.0071]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3797, 0.3424], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in mod.model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0165,  0.0334, -0.0204,  ...,  0.0451,  0.0146,  0.0468],\n",
      "        [-0.0049, -0.0374,  0.0132,  ...,  0.0175,  0.0412, -0.0162],\n",
      "        [ 0.0576,  0.0062,  0.0150,  ..., -0.0121, -0.0313, -0.0231],\n",
      "        ...,\n",
      "        [-0.0432, -0.0117, -0.0119,  ..., -0.0162, -0.0306, -0.0155],\n",
      "        [ 0.0100,  0.0110,  0.0332,  ...,  0.0574,  0.0319,  0.0545],\n",
      "        [ 0.0162,  0.0118,  0.0055,  ..., -0.0026, -0.0303,  0.0268]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0053, -0.0035, -0.0028,  ..., -0.0029,  0.0065, -0.0030],\n",
      "        [ 0.0045, -0.0048,  0.0025,  ..., -0.0022,  0.0029, -0.0058],\n",
      "        [ 0.0019, -0.0031,  0.0016,  ...,  0.0032,  0.0028, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0069, -0.0051,  0.0014,  ...,  0.0009,  0.0055, -0.0057],\n",
      "        [ 0.0010,  0.0009, -0.0001,  ..., -0.0010,  0.0010, -0.0009],\n",
      "        [ 0.0079, -0.0094,  0.0040,  ..., -0.0086,  0.0090, -0.0073]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5781, 0.9372, 0.8809, 1.0033, 0.8477, 1.7176, 0.8676, 1.3902, 0.7466,\n",
      "        0.6137, 0.8261, 1.1227, 1.2862, 0.9293, 1.2248, 1.8992, 2.3413, 1.6393,\n",
      "        1.3645, 0.8317, 2.1616, 0.5547, 1.0968, 1.0541, 0.8388, 0.7461, 1.4121,\n",
      "        1.6743, 1.0309, 2.0691, 1.1874, 2.2225, 1.3512, 1.6381, 1.6750, 1.6713,\n",
      "        1.5406, 0.9796, 1.3858, 1.1167, 1.3741, 0.7335, 0.9230, 0.7372, 0.9518,\n",
      "        1.7471, 1.5831, 1.0939, 0.5177, 0.8925, 0.7904, 0.7818, 1.0882, 1.2832,\n",
      "        0.8931, 1.1064, 0.6561, 0.8410, 1.0319, 1.2230, 1.3876, 0.4918, 1.3844,\n",
      "        1.1689, 1.1810, 0.8859, 0.9240, 1.6844, 1.8682, 0.6566, 1.6003, 1.1143,\n",
      "        1.7788, 1.4153, 1.4323, 0.7771, 0.4644, 1.7416, 1.6233, 1.4719, 1.1619,\n",
      "        1.2528, 0.9507, 1.2672, 0.6533, 1.1783, 1.5514, 1.6861, 1.6612, 1.1453,\n",
      "        1.5638, 1.3663, 1.6741, 1.8254, 1.0866, 1.4957, 0.5591, 0.5481, 0.7044,\n",
      "        0.8973, 0.6166, 1.2448, 2.0408, 1.5247, 0.6321, 0.5450, 0.5237, 1.4074,\n",
      "        1.0516, 1.1237, 1.2419, 1.6270, 0.5085, 0.4480, 0.6902, 0.5150, 0.6831,\n",
      "        1.0655, 1.0800, 0.9255, 0.4819, 0.5731, 0.6329, 0.6735, 1.2346, 0.5808,\n",
      "        1.3939, 1.2774, 0.8629, 0.6237, 1.5994, 1.5482, 1.0643, 0.9092, 1.0304,\n",
      "        0.7668, 0.3517, 0.8597, 1.4035, 0.9897, 1.2101, 1.2424, 1.0726, 0.7405,\n",
      "        1.8432, 1.2710, 1.5719, 1.6019, 2.3057, 0.4677, 1.6149, 1.8393, 1.5934,\n",
      "        1.2412, 1.2087, 0.5098, 0.4487, 2.5967, 1.4509, 0.9533, 0.7603, 0.8364,\n",
      "        1.1154, 1.1128, 1.3627, 1.5599, 1.6057, 0.7549, 1.1065, 1.0552, 0.8764,\n",
      "        1.3390, 1.8499, 1.4040, 0.7992, 0.7317, 1.3197, 0.9295, 1.2131, 1.9355,\n",
      "        1.4630, 1.3273, 0.9082, 1.6768, 0.7343, 0.8331, 1.0196, 1.9387, 1.3097,\n",
      "        1.2337, 1.4808, 1.2830, 0.5842, 1.1478, 0.8853, 1.1761, 1.2557, 0.6568,\n",
      "        1.0915, 1.7117, 0.4936, 0.9333, 1.2350, 0.9502, 0.7568, 1.0404, 1.7910,\n",
      "        0.9933, 0.4456, 0.6441, 1.4811, 0.9798, 0.8798, 0.7317, 0.6829, 0.8222,\n",
      "        0.5102, 0.5937, 0.3629, 1.1157, 1.1968, 1.3278, 0.7267, 0.6472, 3.4048,\n",
      "        2.2900, 2.3311, 2.2945, 1.4579, 2.1614, 1.5380, 1.7398, 3.1827, 2.2808,\n",
      "        2.9339, 1.6725, 2.8790, 1.8528, 2.1287, 2.2582, 1.1441, 0.9010, 0.5648,\n",
      "        1.4487, 1.9112, 0.8346, 1.1037, 1.2079, 0.7097, 1.1520, 1.4437, 1.2171,\n",
      "        0.4941, 1.9583, 1.0643, 0.8396, 0.5810, 1.1346, 1.4868, 1.2811, 1.6695,\n",
      "        0.5453, 1.7830, 0.8295, 1.0355, 0.9410, 0.5985, 1.5751, 0.7213, 1.8798,\n",
      "        1.8930, 0.8579, 1.3189, 1.5664, 1.6966, 1.4005, 1.2296, 0.7792, 1.7292,\n",
      "        1.2636, 0.9128, 1.0138, 2.0365, 1.9598, 2.2714, 2.4792, 1.1147, 1.1151,\n",
      "        1.6393, 0.7843, 1.6993, 1.9090, 1.2698, 0.7159, 1.6118, 1.0744, 0.8689,\n",
      "        1.3316, 0.5652, 1.0069, 1.1440, 2.0358, 1.8987, 1.1043, 0.5318, 0.5352,\n",
      "        1.4377, 0.4776, 0.4693, 2.1665, 1.5444, 1.1223, 1.5175, 1.9545, 1.1222,\n",
      "        1.5198, 2.3906, 0.5602, 0.8637, 1.0463], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0165, -0.0443, -0.0062,  ...,  0.0508, -0.0401,  0.0251],\n",
      "        [-0.0078,  0.0398, -0.0064,  ...,  0.0239, -0.0056, -0.0421],\n",
      "        [ 0.0171, -0.0079,  0.0491,  ...,  0.0132, -0.0199, -0.0104],\n",
      "        ...,\n",
      "        [-0.0023, -0.0676,  0.0467,  ...,  0.0102, -0.0015, -0.0049],\n",
      "        [-0.0561, -0.0355, -0.0491,  ..., -0.0451,  0.0463, -0.0394],\n",
      "        [ 0.0393,  0.0165, -0.0071,  ...,  0.0419, -0.0392, -0.0389]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.0704e-03,  6.0018e-03,  6.6971e-03,  ..., -1.1011e-02,\n",
      "         -4.1556e-03,  8.2221e-03],\n",
      "        [ 3.6563e-03, -1.2084e-03, -8.0422e-03,  ...,  4.3302e-03,\n",
      "          7.1306e-03, -8.9303e-03],\n",
      "        [ 5.9162e-03, -4.1235e-03, -1.2817e-03,  ...,  2.0689e-03,\n",
      "          8.2159e-03, -5.7432e-03],\n",
      "        ...,\n",
      "        [ 2.1906e-03,  5.7527e-03,  5.1682e-03,  ..., -5.8070e-03,\n",
      "         -3.8293e-03,  5.0486e-03],\n",
      "        [-5.6859e-04,  2.4162e-03,  1.2826e-03,  ...,  3.7534e-03,\n",
      "          3.2043e-03, -2.1172e-03],\n",
      "        [-2.6695e-03, -4.3248e-03,  3.6102e-03,  ..., -3.3616e-03,\n",
      "         -2.6608e-03, -3.1646e-05]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8562, 1.4884, 1.3027, 1.4847, 1.3219, 1.5533, 0.8277, 1.3269, 1.1479,\n",
      "        1.3291, 1.5935, 1.8990, 1.3801, 1.2164, 1.2423, 1.8232, 2.2486, 1.5217,\n",
      "        1.4243, 1.9165, 1.6938, 1.8138, 1.1293, 1.0799, 1.4728, 1.1220, 2.2932,\n",
      "        2.0826, 2.0504, 1.5527, 1.1548, 2.2669, 2.3088, 2.3812, 2.4061, 1.5857,\n",
      "        1.5192, 1.2384, 1.6925, 1.1454, 1.2906, 1.9257, 1.5601, 1.8094, 1.3440,\n",
      "        1.7490, 1.8426, 1.1575, 0.9391, 0.8119, 1.3102, 1.8890, 1.6294, 1.5461,\n",
      "        0.9728, 1.2629, 1.0748, 1.5004, 1.6988, 1.2041, 2.1587, 1.0513, 1.3584,\n",
      "        1.6943, 1.7824, 0.8956, 1.7069, 1.9571, 1.9105, 1.7749, 1.6152, 1.0300,\n",
      "        1.2924, 2.1525, 2.0250, 1.7705, 1.3397, 1.3393, 1.7937, 1.8697, 2.0307,\n",
      "        2.3262, 1.5618, 1.7114, 1.5982, 1.2545, 2.0150, 1.9792, 2.3378, 1.5460,\n",
      "        2.3679, 1.8560, 1.2163, 1.7519, 0.9059, 2.0972, 0.8416, 1.0954, 1.2628,\n",
      "        1.0814, 1.2229, 1.6286, 1.9311, 1.8545, 0.9502, 0.9968, 1.1143, 1.5576,\n",
      "        1.4853, 1.6469, 1.5948, 1.6463, 0.5573, 0.8292, 0.8956, 0.8480, 1.3123,\n",
      "        1.7994, 1.1347, 0.7854, 0.6315, 0.7666, 1.1879, 1.6057, 1.8517, 1.0453,\n",
      "        1.3332, 1.8357, 1.4420, 0.8323, 1.2533, 1.3455, 0.6991, 0.6949, 1.0533,\n",
      "        0.9432, 0.4396, 1.1569, 1.2242, 0.8096, 0.7423, 0.5525, 1.0910, 0.6294,\n",
      "        2.1230, 1.7292, 1.9409, 1.9630, 1.4585, 2.6214, 1.5315, 1.8813, 2.3809,\n",
      "        2.2048, 2.3169, 1.9177, 2.1372, 2.0299, 1.5897, 0.9222, 1.6061, 1.8197,\n",
      "        1.8303, 1.6938, 1.9638, 1.5399, 2.1513, 0.7072, 1.4340, 1.4315, 1.6164,\n",
      "        2.2310, 1.7971, 1.7271, 1.0786, 0.7714, 1.2121, 1.5603, 2.1402, 2.1698,\n",
      "        1.7229, 1.5031, 1.2509, 1.8936, 1.8184, 1.6031, 1.4551, 2.0146, 1.7322,\n",
      "        1.5267, 1.2333, 1.4950, 0.7937, 1.5306, 1.1809, 1.3249, 1.7870, 1.0798,\n",
      "        1.1167, 2.1665, 0.9292, 0.8947, 1.4685, 1.7166, 1.4174, 1.5906, 2.0177,\n",
      "        1.3240, 0.9308, 0.8247, 1.6919, 1.1438, 1.0444, 0.6807, 0.8172, 0.8324,\n",
      "        0.8163, 0.9968, 0.4942, 1.2175, 1.2153, 0.7825, 0.6389, 0.8285, 2.9348,\n",
      "        2.7219, 2.8567, 1.7094, 2.3167, 1.3959, 1.4203, 1.6888, 2.2562, 1.7832,\n",
      "        1.8490, 2.2756, 1.4417, 2.2137, 2.0288, 1.8278, 2.1035, 1.6682, 1.4848,\n",
      "        1.6561, 1.5076, 1.3575, 1.3674, 1.3221, 0.9124, 1.8806, 2.2732, 1.9356,\n",
      "        1.7632, 1.9201, 0.9784, 0.9087, 1.4321, 0.8496, 1.8241, 1.6485, 1.6313,\n",
      "        1.2649, 1.4303, 0.7446, 1.0029, 1.6803, 1.2023, 1.6057, 1.5424, 1.6240,\n",
      "        2.1106, 0.9067, 1.8017, 1.8683, 1.1523, 1.5284, 1.7782, 2.7731, 1.5426,\n",
      "        1.3896, 0.9869, 1.4265, 2.7190, 2.1871, 1.9915, 1.5247, 1.2344, 1.0865,\n",
      "        2.2350, 2.0913, 1.8349, 2.1330, 1.3986, 1.1405, 1.7397, 1.1469, 2.0471,\n",
      "        1.8656, 2.2223, 1.8872, 1.6362, 1.9923, 1.9347, 1.2233, 2.1704, 1.8013,\n",
      "        2.1269, 1.5759, 2.5413, 1.8513, 1.9647, 1.5304, 1.7308, 2.9181, 1.7253,\n",
      "        1.7173, 1.5746, 2.1705, 0.9038, 0.9707], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0604, -0.0216, -0.0072,  ..., -0.0167, -0.0378, -0.0315],\n",
      "        [ 0.0412,  0.0046, -0.0516,  ..., -0.0298,  0.0610,  0.0299],\n",
      "        [-0.0478, -0.0579, -0.0410,  ..., -0.0627, -0.0180,  0.0137],\n",
      "        ...,\n",
      "        [-0.0388, -0.0226,  0.0202,  ...,  0.0068,  0.0558, -0.0102],\n",
      "        [-0.0631,  0.0348, -0.0150,  ..., -0.0286,  0.0386,  0.0022],\n",
      "        [ 0.0469,  0.0447, -0.0168,  ..., -0.0463,  0.0157,  0.0109]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-3.8704e-03,  5.1367e-03,  5.2745e-03,  ...,  6.3268e-03,\n",
      "          5.5271e-03, -1.4753e-03],\n",
      "        [ 3.4987e-03, -2.2207e-03, -2.3228e-03,  ..., -2.5262e-03,\n",
      "         -2.2629e-03, -7.3282e-05],\n",
      "        [ 2.4868e-03, -3.2472e-03, -2.8333e-03,  ..., -3.2425e-03,\n",
      "         -3.7695e-03, -4.5113e-03],\n",
      "        ...,\n",
      "        [-5.1780e-03,  4.6521e-03,  3.5113e-03,  ...,  6.0224e-03,\n",
      "          4.6935e-03,  9.5624e-03],\n",
      "        [ 2.9812e-03, -3.3011e-03, -3.2452e-03,  ..., -3.0776e-03,\n",
      "         -3.2992e-03, -6.5655e-03],\n",
      "        [-1.7428e-04,  1.4428e-03,  2.6045e-03,  ...,  2.9375e-03,\n",
      "          1.4179e-03, -4.6577e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4780, 1.6108, 1.3982, 1.6252, 1.3500, 1.5400, 1.5952, 1.3383, 1.5163,\n",
      "        1.4929, 1.3239, 1.4070, 1.4894, 1.5046, 1.4668, 1.3923, 0.9275, 1.1038,\n",
      "        1.0281, 1.1335, 0.8842, 1.0843, 1.1988, 0.3961, 1.1992, 0.8608, 1.2151,\n",
      "        0.9856, 1.2417, 0.9286, 1.1677, 1.1772, 0.9175, 1.1091, 1.1163, 1.0201,\n",
      "        1.0649, 1.0961, 1.0805, 1.0401, 0.9907, 0.9631, 0.9961, 1.1976, 1.1096,\n",
      "        1.1807, 1.0590, 0.9646, 1.5708, 1.6094, 1.4278, 1.1238, 1.4999, 1.3880,\n",
      "        1.5130, 1.5872, 0.9999, 1.4184, 1.2471, 1.7635, 1.5394, 1.5037, 1.0313,\n",
      "        1.3806, 0.9310, 1.2895, 1.1545, 1.1520, 1.1760, 1.2543, 1.3837, 1.1468,\n",
      "        1.2825, 1.2902, 1.1524, 1.3172, 1.3036, 1.2052, 1.3096, 1.1870, 0.9731,\n",
      "        0.9958, 1.2621, 1.0509, 1.1910, 1.1812, 1.2481, 1.1847, 1.2931, 1.0173,\n",
      "        1.1854, 1.1154, 1.1099, 1.0842, 1.0695, 1.1317, 1.7177, 1.6532, 1.4591,\n",
      "        1.5465, 1.4904, 1.5124, 1.7511, 1.6645, 1.4575, 1.3655, 1.0590, 1.6619,\n",
      "        1.5855, 1.2693, 1.4485, 1.6938, 1.7327, 1.8132, 1.6442, 1.7913, 2.0512,\n",
      "        1.8217, 1.9818, 2.1410, 1.6798, 1.8747, 1.6601, 1.7587, 1.6135, 1.7930,\n",
      "        1.7483, 2.0972, 0.9979, 1.1503, 1.2001, 1.1058, 1.2194, 1.4867, 1.1972,\n",
      "        1.5486, 1.1252, 1.4479, 1.3301, 1.2140, 1.1101, 1.2164, 1.0939, 1.4256,\n",
      "        1.1501, 1.0336, 1.0882, 1.1199, 1.1030, 1.1024, 1.2014, 1.2261, 1.0974,\n",
      "        1.2042, 1.2186, 1.0978, 1.1150, 1.0125, 1.0808, 1.0509, 1.2360, 1.4687,\n",
      "        1.1721, 1.2829, 1.3397, 1.2569, 1.2998, 1.2600, 1.4954, 1.6611, 1.3423,\n",
      "        1.3498, 1.2534, 1.1317, 1.2729, 1.1790, 1.0203, 1.0282, 0.9040, 1.0477,\n",
      "        0.9774, 1.0713, 1.2966, 1.0557, 0.8679, 0.9935, 1.2097, 0.9946, 1.0994,\n",
      "        0.9434, 1.1573, 1.1345, 1.7631, 1.3893, 0.8681, 1.3174, 1.8748, 1.2667,\n",
      "        1.3171, 1.6820, 1.3684, 1.7846, 1.7203, 1.9524, 1.6945, 1.6949, 1.8300,\n",
      "        1.6862, 1.2826, 1.2606, 1.4032, 1.4754, 1.3755, 1.5547, 1.2915, 1.3935,\n",
      "        1.6004, 1.5261, 1.3266, 1.4307, 1.2269, 1.5733, 1.1570, 1.1838, 0.7396,\n",
      "        0.8214, 0.6792, 0.7919, 0.7220, 0.6879, 0.9169, 0.7888, 0.8879, 0.9555,\n",
      "        0.7431, 0.5834, 0.7252, 0.7654, 0.7087, 0.6744, 1.1871, 1.2561, 1.2494,\n",
      "        1.3623, 1.0393, 1.2673, 1.3066, 1.2073, 1.3197, 1.1599, 1.2065, 1.2715,\n",
      "        1.3805, 1.3587, 1.2121, 1.1862, 1.3221, 1.2601, 1.3603, 1.3424, 1.2555,\n",
      "        1.2875, 1.8086, 1.2441, 1.3181, 1.2054, 1.2843, 1.3196, 1.5537, 1.4206,\n",
      "        1.4053, 1.1218, 0.8883, 0.7981, 0.8878, 1.0484, 0.9816, 0.9583, 1.0059,\n",
      "        0.9688, 0.9325, 0.9224, 0.9470, 1.0909, 0.9131, 0.9833, 0.9367, 0.8725,\n",
      "        1.0532, 1.0072, 1.0006, 1.0311, 1.0455, 0.9003, 0.9752, 0.9772, 0.9819,\n",
      "        1.0679, 0.9240, 1.0460, 1.1475, 1.0296, 1.1379, 0.9610, 1.1654, 1.1018,\n",
      "        1.0517, 1.0309, 1.0954, 1.1818, 1.2164, 1.1743, 1.0756, 1.1872, 1.1692,\n",
      "        1.0835, 1.2720, 1.1234, 1.1976, 1.1878], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0547, -0.0341, -0.0013,  ...,  0.0037,  0.0356, -0.0429],\n",
      "        [ 0.0383, -0.0037,  0.0344,  ..., -0.0358,  0.0211, -0.0244],\n",
      "        [ 0.0128,  0.0237,  0.0315,  ..., -0.0397, -0.0010, -0.0481],\n",
      "        ...,\n",
      "        [-0.0092, -0.0461,  0.0540,  ...,  0.0337,  0.0516, -0.0336],\n",
      "        [ 0.0206,  0.0284, -0.0249,  ...,  0.0306,  0.0157,  0.0435],\n",
      "        [ 0.0513,  0.0430, -0.0518,  ...,  0.0034, -0.0613, -0.0056]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0053, -0.0054, -0.0034,  ..., -0.0018,  0.0022, -0.0013],\n",
      "        [ 0.0012,  0.0085, -0.0044,  ..., -0.0069,  0.0019,  0.0015],\n",
      "        [ 0.0013, -0.0014,  0.0013,  ...,  0.0035,  0.0028, -0.0017],\n",
      "        ...,\n",
      "        [-0.0003, -0.0006, -0.0038,  ..., -0.0047, -0.0003,  0.0006],\n",
      "        [-0.0051,  0.0063, -0.0094,  ...,  0.0004,  0.0044,  0.0100],\n",
      "        [-0.0076, -0.0021, -0.0108,  ...,  0.0002,  0.0024,  0.0047]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3693, 1.3727, 1.4928, 1.5238, 1.4587, 1.5678, 1.3965, 1.3984, 1.2999,\n",
      "        1.4422, 1.3422, 1.5060, 1.5142, 1.3618, 1.3520, 1.3791, 1.4222, 1.3489,\n",
      "        1.4884, 1.3439, 1.4284, 1.3918, 1.4144, 1.3608, 1.3927, 1.4368, 1.4507,\n",
      "        1.4120, 1.4238, 1.4602, 1.3775, 1.2959, 1.4418, 1.2838, 1.3527, 1.4002,\n",
      "        1.4288, 1.3833, 1.4195, 1.4678, 1.4184, 1.4216, 1.3538, 1.4405, 1.4364,\n",
      "        1.4387, 1.8065, 1.3609, 1.5689, 1.4240, 1.4307, 1.3740, 1.4553, 1.5503,\n",
      "        1.3851, 1.2347, 1.3881, 1.4166, 1.3762, 1.3280, 1.5093, 1.6860, 1.2540,\n",
      "        1.5188, 1.3909, 1.3765, 1.4444, 1.4344, 1.3785, 1.3008, 1.4154, 1.4720,\n",
      "        1.3620, 1.5189, 1.5118, 1.4027, 1.4252, 1.3065, 1.3849, 1.4717, 1.3981,\n",
      "        1.3712, 1.3154, 1.4254, 1.4516, 1.5631, 1.3648, 1.4719, 1.3792, 1.4000,\n",
      "        1.4940, 1.3992, 1.4623, 1.4317, 1.3651, 1.4280, 1.2941, 1.4475, 1.3953,\n",
      "        1.3520, 1.3779, 1.4314, 1.3806, 1.2821, 1.5183, 1.3299, 1.5249, 1.4923,\n",
      "        1.4138, 1.3594, 1.3637, 1.6161, 1.4735, 1.4628, 1.5049, 1.4030, 1.3122,\n",
      "        1.4433, 1.3368, 1.5374, 1.5857, 1.4170, 1.3670, 1.3374, 1.3371, 1.4505,\n",
      "        1.3576, 1.4379, 1.3631, 1.3362, 1.3812, 1.5802, 1.4366, 1.4766, 1.3673,\n",
      "        1.4628, 1.3891, 1.3098, 1.3899, 1.6490, 1.3459, 1.5399, 1.3592, 1.3353,\n",
      "        1.1601, 1.3863, 1.3833, 1.5057, 1.3813, 1.3663, 1.4469, 1.5443, 1.6018,\n",
      "        1.3784, 1.3914, 1.3002, 1.4242, 1.3742, 1.4720, 1.4043, 1.3359, 1.4279,\n",
      "        1.3887, 1.4868, 1.4784, 1.4958, 1.5025, 1.4489, 1.4078, 1.3756, 1.4479,\n",
      "        1.5951, 1.4621, 1.3898, 1.4045, 1.4506, 1.3304, 1.3616, 1.3701, 1.4323,\n",
      "        1.3984, 1.5215, 1.3616, 1.3573, 1.4541, 1.3159, 1.4270, 1.2429, 1.4397,\n",
      "        1.3750, 1.3870, 1.3348, 1.4754, 0.9675, 1.3853, 1.3266, 1.3873, 1.2985,\n",
      "        1.3716, 1.3408, 1.4509, 1.3735, 1.3250, 1.3521, 1.3948, 1.4501, 1.3732,\n",
      "        1.4163, 1.4263, 1.4618, 1.4106, 1.3210, 1.3010, 1.4134, 1.5012, 1.4097,\n",
      "        1.3851, 1.4110, 1.3754, 1.3660, 1.4073, 1.3749, 1.4599, 1.3237, 1.3390,\n",
      "        1.4037, 1.3953, 1.4789, 1.3787, 1.4518, 1.4373, 1.3417, 1.4627, 1.3392,\n",
      "        1.4334, 1.3937, 1.4734, 1.3757, 1.4065, 1.4321, 1.4429, 1.4656, 1.5410,\n",
      "        1.4245, 1.4255, 1.4013, 1.3907, 1.4346, 1.4922, 1.3253, 1.3423, 1.4285,\n",
      "        1.4685, 1.4175, 1.3168, 1.4481, 1.4598, 1.6882, 1.4263, 1.3777, 1.4180,\n",
      "        1.4411, 1.4594, 1.3457, 1.4814, 1.4150, 1.5158, 1.4069, 1.4245, 1.4898,\n",
      "        1.2895, 1.2389, 1.4660, 1.3547, 1.2691, 1.3782, 1.4596, 1.4506, 1.4229,\n",
      "        1.4110, 1.4626, 1.4503, 1.3405, 1.6353, 1.4537, 1.2704, 1.2913, 1.5206,\n",
      "        1.3730, 1.5006, 1.3993, 1.7568, 1.3867, 1.4226, 1.4776, 1.3382, 1.2745,\n",
      "        1.4039, 1.4153, 1.4187, 1.3860, 1.4273, 1.3839, 1.3642, 1.5231, 1.4203,\n",
      "        1.3778, 1.4265, 1.5658, 1.3919, 1.4841, 1.4917, 1.4336, 1.4523, 1.5174,\n",
      "        1.4439, 1.3852, 1.3806, 1.3919, 1.3717], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0264,  0.0543, -0.0028,  ..., -0.0047,  0.0070, -0.0090],\n",
      "        [-0.0009,  0.0099,  0.0268,  ...,  0.0311,  0.0296, -0.0009],\n",
      "        [ 0.0311, -0.0033,  0.0498,  ...,  0.0375, -0.0336,  0.0095],\n",
      "        ...,\n",
      "        [-0.0312, -0.0693, -0.0366,  ..., -0.0024, -0.0316,  0.0048],\n",
      "        [-0.0310,  0.0578,  0.0175,  ...,  0.0330,  0.0111, -0.0241],\n",
      "        [-0.0221,  0.0584,  0.0256,  ...,  0.0294,  0.0583, -0.0320]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0011, -0.0076, -0.0032,  ..., -0.0038,  0.0064,  0.0048],\n",
      "        [ 0.0076, -0.0080,  0.0036,  ..., -0.0053,  0.0005,  0.0058],\n",
      "        [-0.0015,  0.0010,  0.0030,  ..., -0.0004, -0.0010, -0.0021],\n",
      "        ...,\n",
      "        [-0.0011,  0.0040, -0.0003,  ...,  0.0023, -0.0012, -0.0042],\n",
      "        [ 0.0040, -0.0024,  0.0034,  ..., -0.0015,  0.0009,  0.0036],\n",
      "        [-0.0068,  0.0070,  0.0050,  ...,  0.0090,  0.0010, -0.0084]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3946, 2.1271, 1.3981,  ..., 2.5727, 1.6371, 1.6072],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0109,  0.0041, -0.0091,  ..., -0.0188, -0.0009, -0.0073],\n",
      "        [ 0.0038, -0.0126,  0.0136,  ...,  0.0139,  0.0013, -0.0057],\n",
      "        [ 0.0175, -0.0043,  0.0236,  ...,  0.0184, -0.0275,  0.0179],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0079, -0.0207,  ...,  0.0081,  0.0253, -0.0167],\n",
      "        [ 0.0232,  0.0301,  0.0138,  ...,  0.0195,  0.0228,  0.0241],\n",
      "        [ 0.0081,  0.0181, -0.0194,  ..., -0.0014, -0.0084, -0.0230]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0009, -0.0019, -0.0031,  ...,  0.0035,  0.0015, -0.0003],\n",
      "        [ 0.0034, -0.0001, -0.0017,  ...,  0.0021, -0.0034,  0.0006],\n",
      "        [-0.0028,  0.0008, -0.0043,  ...,  0.0051,  0.0031, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0010, -0.0013,  ...,  0.0035, -0.0023, -0.0009],\n",
      "        [ 0.0019,  0.0019, -0.0061,  ...,  0.0038,  0.0013, -0.0029],\n",
      "        [ 0.0014, -0.0005,  0.0011,  ...,  0.0019, -0.0039, -0.0006]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.8371, 1.9826, 2.0237, 1.9819, 1.8091, 2.0150, 1.9608, 1.9405, 1.9741,\n",
      "        1.9505, 2.0388, 2.0150, 2.0721, 2.1165, 1.8905, 1.9687, 1.9349, 1.9508,\n",
      "        2.0705, 1.9902, 1.9687, 1.9116, 2.0157, 1.8077, 2.0457, 1.8459, 2.1015,\n",
      "        1.9758, 1.8150, 1.8410, 1.8759, 2.8103, 2.0663, 2.0147, 2.0445, 1.8726,\n",
      "        1.8909, 1.9940, 2.1080, 1.9511, 1.9529, 2.0401, 1.9420, 1.9972, 1.9156,\n",
      "        1.9914, 2.7756, 2.0026, 2.0288, 1.9676, 2.0590, 1.9568, 2.0084, 1.9251,\n",
      "        1.9852, 1.9247, 2.0373, 1.9354, 1.9698, 1.8840, 2.0070, 2.3287, 2.8747,\n",
      "        1.9883, 1.9243, 1.8857, 2.1168, 1.9656, 1.9229, 2.0137, 2.0070, 1.9988,\n",
      "        1.9541, 1.9249, 1.9944, 1.8679, 2.0414, 2.0267, 1.9817, 2.0314, 2.0817,\n",
      "        1.9975, 1.9345, 1.9355, 2.0097, 1.9369, 1.9504, 1.8691, 2.4246, 1.9853,\n",
      "        1.9272, 2.0267, 1.8649, 1.9649, 1.9700, 1.9757, 1.9156, 1.9348, 2.0330,\n",
      "        1.9207, 1.9021, 1.9554, 1.9485, 2.0019, 1.9372, 1.8687, 2.0532, 1.8787,\n",
      "        1.9086, 1.9872, 1.8079, 2.0175, 1.9483, 1.8909, 1.8595, 2.0778, 1.9356,\n",
      "        1.9330, 1.9844, 1.8582, 1.9824, 2.1164, 1.8573, 2.0590, 2.2029, 1.8545,\n",
      "        1.8786, 1.9130, 1.9295, 1.9873, 2.1451, 2.0217, 1.9004, 1.8963, 1.8056,\n",
      "        1.8804, 1.9814, 2.0328, 1.9518, 1.9149, 2.0056, 1.8909, 1.9127, 1.9278,\n",
      "        1.8757, 2.0353, 1.9326, 1.9259, 1.8412, 2.0143, 2.1199, 1.8653, 2.0949,\n",
      "        1.9992, 1.9730, 1.9702, 1.9496, 1.9186, 1.9013, 1.9285, 1.9398, 1.9300,\n",
      "        1.8944, 2.0322, 1.9783, 1.9857, 1.9232, 1.9431, 1.9079, 1.9928, 1.9249,\n",
      "        2.0109, 2.0350, 1.9970, 2.0050, 1.9790, 1.9869, 1.8482, 2.0077, 1.9966,\n",
      "        1.9332, 1.9277, 2.0169, 1.9966, 1.9601, 2.0484, 2.0139, 1.8990, 1.8976,\n",
      "        1.9427, 2.0532, 1.9699, 3.8607, 1.9088, 1.9314, 1.8814, 1.9257, 1.9383,\n",
      "        1.8685, 1.8785, 1.9409, 2.0177, 1.8885, 1.9869, 2.0006, 2.0288, 1.8374,\n",
      "        2.1219, 1.8761, 1.9090, 1.9907, 1.9911, 1.9158, 1.9254, 1.9319, 1.9119,\n",
      "        2.0283, 1.8619, 2.3745, 1.9967, 1.9819, 1.9481, 2.0201, 1.8172, 1.9971,\n",
      "        1.9888, 2.0693, 2.0039, 2.1622, 2.0101, 2.0805, 2.1791, 1.9472, 1.9406,\n",
      "        1.9665, 1.9918, 1.9665, 2.0316, 1.9135, 1.8858, 2.1398, 1.9567, 1.9561,\n",
      "        2.0392, 1.9217, 1.9400, 1.9134, 2.0102, 1.9831, 1.9237, 1.9482, 2.0512,\n",
      "        1.9016, 2.0288, 1.9860, 1.9812, 1.9713, 2.0094, 1.9320, 1.8652, 2.0388,\n",
      "        1.9386, 1.9723, 1.9744, 2.1703, 2.0228, 1.9906, 2.0024, 1.9691, 1.9744,\n",
      "        2.0169, 2.0044, 1.9585, 1.9455, 1.8848, 2.0357, 1.9122, 2.1596, 1.9767,\n",
      "        1.9199, 1.9592, 2.0026, 1.8574, 2.0931, 1.9923, 1.9953, 1.9577, 2.0482,\n",
      "        2.0598, 1.9599, 1.9298, 2.2504, 2.0060, 2.0904, 2.2088, 2.1562, 1.9106,\n",
      "        2.0249, 1.9183, 1.9320, 1.9556, 2.0502, 1.9483, 2.0520, 1.9014, 1.9343,\n",
      "        1.8770, 2.1013, 2.0339, 2.1124, 2.0417, 2.1470, 1.9591, 2.0202, 1.9465,\n",
      "        1.9024, 1.9100, 1.9277, 1.8693, 3.4729], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0312,  0.0311,  0.0338,  ...,  0.0250, -0.0127,  0.0178],\n",
      "        [ 0.0502, -0.0123,  0.0555,  ...,  0.0651, -0.0244, -0.0275],\n",
      "        [ 0.0420, -0.0539,  0.0179,  ...,  0.0362,  0.0323,  0.0204],\n",
      "        ...,\n",
      "        [ 0.0413, -0.0090,  0.0127,  ...,  0.0414, -0.0376,  0.0566],\n",
      "        [ 0.0001,  0.0543, -0.0106,  ..., -0.0090, -0.0301, -0.0261],\n",
      "        [-0.0647,  0.0291, -0.0339,  ..., -0.0329,  0.0588, -0.0225]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0057, -0.0066, -0.0064,  ..., -0.0042,  0.0025,  0.0047],\n",
      "        [ 0.0074, -0.0090, -0.0066,  ..., -0.0076,  0.0084,  0.0079],\n",
      "        [ 0.0049, -0.0009, -0.0026,  ...,  0.0009, -0.0026, -0.0021],\n",
      "        ...,\n",
      "        [-0.0016,  0.0006,  0.0012,  ...,  0.0025, -0.0022, -0.0012],\n",
      "        [ 0.0009, -0.0023,  0.0011,  ...,  0.0004,  0.0024,  0.0024],\n",
      "        [-0.0021,  0.0023, -0.0008,  ...,  0.0003, -0.0019, -0.0034]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3132, 1.7173, 1.7645, 1.6955, 2.5149, 3.0024, 2.8858, 2.6566, 2.2908,\n",
      "        1.9590, 1.9553, 2.6178, 2.1796, 2.3956, 2.8127, 2.6064, 3.4379, 2.5055,\n",
      "        1.9048, 2.3492, 2.5050, 2.6702, 3.1366, 3.6244, 1.8848, 2.0338, 2.8627,\n",
      "        2.0081, 2.3779, 2.5610, 3.1587, 3.2007, 2.4346, 2.0927, 2.8108, 2.7870,\n",
      "        2.9855, 2.3839, 3.4012, 3.9526, 1.9249, 2.1179, 2.2162, 2.5730, 2.9014,\n",
      "        2.8549, 3.3663, 3.0290, 1.3544, 1.9220, 1.0060, 1.2956, 2.2280, 2.3828,\n",
      "        2.7470, 2.6614, 3.4022, 1.9158, 2.7143, 2.7296, 2.5671, 2.4820, 3.0107,\n",
      "        2.9411, 1.9280, 2.0037, 3.3894, 2.7150, 2.4891, 3.1325, 2.7251, 3.2981,\n",
      "        2.3328, 1.2271, 1.0401, 1.5293, 1.9954, 2.4406, 3.1489, 3.0182, 2.3075,\n",
      "        2.3558, 2.9646, 2.9360, 2.8055, 3.2030, 3.3353, 3.4883, 2.3039, 2.4368,\n",
      "        2.4580, 2.6757, 2.7323, 2.6756, 3.3172, 2.9087, 2.4738, 2.3946, 3.3965,\n",
      "        3.8280, 3.7442, 2.3188, 2.3358, 3.7362, 2.6445, 3.0459, 3.8366, 3.6589,\n",
      "        2.9826, 3.8681, 3.2236, 4.1882, 1.3945, 1.2871, 2.6666, 2.4751, 2.4496,\n",
      "        2.5214, 2.7407, 2.6677, 1.8375, 2.5501, 1.9952, 1.9858, 2.2120, 2.5948,\n",
      "        2.7291, 3.1545, 1.9409, 1.9300, 2.1086, 1.8136, 2.3446, 2.9067, 2.7732,\n",
      "        2.8460, 2.0127, 1.9024, 2.4710, 3.4882, 2.5607, 2.4954, 2.8509, 2.8352,\n",
      "        3.2951, 1.9606, 2.5871, 2.1491, 2.3415, 2.2666, 3.0158, 2.7967, 1.0348,\n",
      "        1.5804, 1.8233, 2.1706, 2.2025, 2.9473, 2.9805, 3.2179, 1.9507, 2.3447,\n",
      "        2.0218, 2.2180, 2.3631, 2.9310, 2.8429, 2.8691, 3.0780, 2.0552, 2.6510,\n",
      "        2.3397, 2.6705, 2.5368, 3.0057, 3.3387, 1.4418, 1.4719, 2.4009, 1.6449,\n",
      "        1.9829, 2.7007, 2.4371, 2.7150, 1.2406, 1.5638, 1.4347, 2.3707, 2.0326,\n",
      "        2.1333, 2.5478, 2.7054, 1.2910, 1.7076, 1.7877, 1.4626, 2.0330, 1.9137,\n",
      "        2.8414, 2.6522, 2.2318, 2.0767, 2.1806, 2.6180, 2.1028, 2.2843, 2.8529,\n",
      "        2.8424, 2.6110, 2.3215, 2.3224, 3.2448, 3.4110, 1.9783, 3.0416, 3.1481,\n",
      "        1.8149, 3.1194, 3.6259, 4.3204, 3.5246, 4.1075, 3.2507, 3.7375, 1.3679,\n",
      "        1.4852, 2.5917, 1.3688, 1.7904, 2.1564, 2.4202, 3.5510, 4.4056, 2.1850,\n",
      "        1.3047, 2.3150, 2.1587, 3.0764, 2.5239, 3.0416, 2.1880, 2.4773, 2.2022,\n",
      "        3.2929, 2.0185, 2.6319, 3.0664, 3.0471, 2.2664, 1.9714, 2.0134, 1.3950,\n",
      "        2.7146, 2.5750, 2.9396, 2.9898, 3.0534, 2.1583, 2.2454, 2.0017, 2.1158,\n",
      "        2.3572, 3.3527, 3.1344, 2.5235, 2.4681, 2.2910, 3.0120, 2.7627, 2.2339,\n",
      "        3.4374, 2.4917, 1.9716, 2.4371, 2.9450, 2.3286, 1.9656, 2.5456, 3.3401,\n",
      "        3.3845, 2.0047, 2.6756, 2.9667, 3.0906, 3.1146, 2.6507, 3.1391, 3.0352,\n",
      "        2.2151, 3.0187, 2.9209, 3.2418, 2.1848, 3.1998, 3.3591, 3.4748, 2.8977,\n",
      "        3.4169, 2.8477, 2.3281, 3.2323, 2.4361, 3.8610, 3.3031, 1.4827, 2.1134,\n",
      "        2.3612, 2.0493, 2.0895, 2.4076, 2.4373, 2.8912, 1.6286, 1.4178, 1.4336,\n",
      "        2.7094, 2.0763, 2.5606, 2.6138, 2.7323], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0386, -0.0182,  0.0560,  ...,  0.0105, -0.0409, -0.0200],\n",
      "        [ 0.0004,  0.0074, -0.0412,  ..., -0.0063,  0.0097, -0.0424],\n",
      "        [ 0.0132,  0.0523, -0.0277,  ...,  0.0211, -0.0270, -0.0519],\n",
      "        ...,\n",
      "        [-0.0504,  0.0401,  0.0298,  ...,  0.0416,  0.0243,  0.0287],\n",
      "        [-0.0214, -0.0085, -0.0175,  ...,  0.0326,  0.0522, -0.0226],\n",
      "        [-0.0264,  0.0073, -0.0006,  ...,  0.0516,  0.0036, -0.0104]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0049, -0.0056, -0.0058,  ...,  0.0056, -0.0056, -0.0018],\n",
      "        [-0.0006, -0.0013,  0.0014,  ..., -0.0004, -0.0047, -0.0017],\n",
      "        [ 0.0010, -0.0028, -0.0056,  ...,  0.0039,  0.0040,  0.0023],\n",
      "        ...,\n",
      "        [-0.0038, -0.0018, -0.0020,  ..., -0.0056, -0.0008, -0.0010],\n",
      "        [-0.0018,  0.0004, -0.0001,  ..., -0.0106, -0.0027, -0.0045],\n",
      "        [-0.0047, -0.0021, -0.0030,  ..., -0.0067, -0.0048, -0.0002]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9355, 1.6595, 1.5052, 3.0938, 1.3189, 1.9806, 2.5234, 2.6337, 1.9127,\n",
      "        2.2493, 2.5819, 1.2669, 2.9026, 2.0828, 2.8359, 2.1341, 1.6952, 2.0768,\n",
      "        2.7115, 1.7996, 1.7195, 1.6958, 3.2474, 3.2042, 1.9306, 2.2202, 1.5979,\n",
      "        1.6775, 3.4225, 3.6305, 3.4521, 3.0795, 2.6968, 1.9000, 2.5169, 2.6545,\n",
      "        2.2836, 1.9568, 2.1335, 2.9574, 1.6712, 2.2165, 2.6095, 2.2353, 2.1769,\n",
      "        2.2767, 2.4171, 2.2881, 2.6350, 1.6676, 1.5193, 1.9029, 2.9607, 1.3216,\n",
      "        2.4109, 2.4460, 1.5262, 2.2485, 1.7489, 1.4531, 1.2975, 3.0666, 2.6537,\n",
      "        2.4638, 1.8051, 1.7297, 1.3659, 1.2415, 1.4568, 1.7143, 2.6433, 2.0876,\n",
      "        1.9756, 1.5239, 1.6540, 2.6221, 1.7102, 2.8184, 2.5065, 2.0695, 2.5138,\n",
      "        2.5822, 2.7172, 2.3594, 2.3596, 2.4280, 3.3028, 3.1754, 1.7344, 2.3234,\n",
      "        2.2353, 2.5107, 2.2225, 2.2735, 3.1587, 2.6137, 2.4295, 2.7121, 3.1676,\n",
      "        3.2651, 2.7599, 3.5813, 3.4607, 3.3769, 2.8245, 3.3161, 3.0389, 3.1942,\n",
      "        2.9130, 1.5739, 2.1815, 3.5271, 1.3093, 2.1218, 1.5939, 1.4697, 1.4169,\n",
      "        2.8606, 2.7257, 2.7311, 1.6525, 1.4942, 1.9866, 2.4160, 2.6571, 1.4780,\n",
      "        2.6359, 2.2529, 2.1745, 2.2122, 1.8305, 3.1778, 1.6469, 1.5326, 2.6082,\n",
      "        2.5091, 1.9769, 1.8809, 2.9077, 1.5565, 1.7613, 3.0680, 2.5788, 2.7729,\n",
      "        2.0450, 2.2570, 1.5476, 1.6049, 1.3730, 3.0297, 2.5852, 2.6634, 1.3636,\n",
      "        1.5394, 2.7367, 1.6563, 2.9917, 1.6059, 2.7016, 2.2696, 2.1796, 2.5006,\n",
      "        2.9639, 2.9879, 3.2698, 1.8579, 3.2318, 3.0376, 2.2887, 2.0467, 1.6365,\n",
      "        1.8156, 1.7510, 3.2853, 3.3668, 3.0763, 1.2632, 1.4512, 1.4253, 2.6549,\n",
      "        1.2931, 1.4109, 2.2352, 2.1941, 1.6392, 1.8403, 2.1379, 1.2020, 1.5305,\n",
      "        2.3420, 2.4163, 1.9267, 1.9542, 1.8629, 1.5791, 3.1029, 1.4603, 1.4755,\n",
      "        2.6691, 2.7737, 2.4022, 2.2475, 1.4692, 1.3345, 2.9720, 3.0549, 2.5617,\n",
      "        2.8263, 1.8803, 2.7296, 3.4154, 3.1212, 2.8622, 3.1566, 2.5818, 4.0234,\n",
      "        2.3520, 2.0394, 2.2469, 2.8539, 2.7644, 1.8528, 3.9054, 3.9386, 1.3859,\n",
      "        1.9597, 1.2661, 2.1112, 2.7458, 2.6032, 2.2531, 2.0803, 1.3054, 1.6406,\n",
      "        1.2607, 1.2919, 1.2119, 1.5367, 2.3146, 2.0619, 2.3220, 2.1385, 1.9829,\n",
      "        1.6096, 3.1381, 1.6745, 2.7701, 2.8149, 2.0405, 2.2006, 2.1121, 2.1225,\n",
      "        1.3869, 2.3394, 2.6503, 2.8830, 2.4248, 2.2364, 2.2009, 2.2043, 2.7972,\n",
      "        2.1918, 2.9710, 2.5163, 2.7948, 2.3751, 2.2837, 2.2109, 1.9405, 2.0493,\n",
      "        2.9035, 2.2758, 1.7640, 2.7674, 2.7056, 2.4542, 3.2111, 2.1260, 3.1088,\n",
      "        3.2277, 2.0244, 2.3725, 2.4495, 2.5379, 1.8316, 2.0504, 3.2617, 3.0671,\n",
      "        2.7885, 2.9536, 2.6211, 2.9679, 3.6945, 2.2301, 3.3379, 3.2026, 2.3238,\n",
      "        2.7256, 2.6821, 2.2830, 1.7252, 2.4220, 3.0817, 3.4615, 1.7612, 1.9585,\n",
      "        1.2566, 2.6949, 2.6589, 1.9439, 2.4050, 2.0593, 1.7354, 1.5327, 2.6415,\n",
      "        1.3026, 1.1757, 2.1075, 2.3039, 2.3884], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0507,  0.0457,  0.0476,  ..., -0.0267,  0.0105,  0.0019],\n",
      "        [ 0.0579,  0.0067, -0.0207,  ..., -0.0439,  0.0129,  0.0281],\n",
      "        [-0.0216, -0.0541,  0.0336,  ...,  0.0467, -0.0267, -0.0114],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0185,  0.0421,  ...,  0.0340,  0.0391,  0.0311],\n",
      "        [-0.0388,  0.0430,  0.0416,  ..., -0.0061,  0.0486,  0.0457],\n",
      "        [ 0.0194,  0.0295,  0.0012,  ...,  0.0194, -0.0439, -0.0493]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0038,  0.0048,  0.0031,  ...,  0.0020,  0.0058, -0.0047],\n",
      "        [ 0.0032, -0.0028, -0.0036,  ..., -0.0021, -0.0026,  0.0053],\n",
      "        [ 0.0019,  0.0009, -0.0009,  ...,  0.0005,  0.0016,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0027, -0.0012,  ..., -0.0018, -0.0031,  0.0014],\n",
      "        [ 0.0015,  0.0003,  0.0024,  ..., -0.0016, -0.0013,  0.0005],\n",
      "        [ 0.0040, -0.0017, -0.0019,  ..., -0.0038,  0.0024,  0.0050]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5098, 1.3428, 1.4363, 1.4276, 1.3722, 1.4052, 1.3479, 1.3964, 1.4899,\n",
      "        1.4307, 1.4120, 1.4264, 1.4037, 1.4314, 1.4766, 1.4485, 1.2031, 1.1994,\n",
      "        1.1693, 1.2382, 1.1978, 1.1459, 1.2096, 1.1454, 1.2759, 1.1506, 1.1661,\n",
      "        1.1109, 1.1748, 1.1890, 1.1115, 1.2569, 1.8241, 1.9628, 1.9948, 1.9538,\n",
      "        2.1109, 2.2007, 2.1631, 1.9180, 2.1089, 2.0322, 1.9374, 2.1167, 2.0180,\n",
      "        1.9640, 1.9635, 2.1306, 1.3834, 1.5126, 1.3832, 1.4238, 1.5017, 1.5089,\n",
      "        1.3615, 1.5125, 1.4777, 1.4442, 1.4838, 1.4635, 1.5115, 1.5044, 1.4996,\n",
      "        1.4078, 1.2711, 1.2440, 1.2319, 1.3049, 1.3188, 1.3367, 1.2652, 1.3275,\n",
      "        1.2845, 1.2245, 1.3251, 1.2717, 1.1936, 1.2671, 1.2399, 1.1487, 2.0390,\n",
      "        2.0992, 2.0837, 1.9899, 2.1059, 2.1122, 2.1942, 2.1492, 2.2327, 2.1034,\n",
      "        2.0172, 2.1029, 2.0555, 2.1902, 2.0911, 1.9105, 1.6390, 1.7087, 1.7492,\n",
      "        1.7212, 1.7444, 1.7523, 1.7401, 1.7398, 1.6598, 1.6378, 1.7039, 1.7395,\n",
      "        1.7269, 1.6589, 1.6489, 1.7513, 1.2045, 1.2574, 1.2548, 1.2245, 1.2503,\n",
      "        1.3046, 1.1973, 1.2135, 1.3039, 1.2583, 1.2107, 1.2950, 1.2960, 1.2563,\n",
      "        1.2556, 1.2187, 1.6072, 1.4564, 1.5249, 1.5364, 1.5250, 1.4378, 1.5416,\n",
      "        1.5510, 1.5620, 1.4695, 1.5437, 1.4292, 1.5273, 1.6015, 1.5265, 1.4693,\n",
      "        1.2517, 1.2717, 1.1867, 1.2802, 1.2162, 1.2481, 1.2892, 1.2417, 1.3061,\n",
      "        1.3133, 1.1549, 1.3082, 1.2792, 1.2586, 1.2474, 1.2034, 1.1361, 1.2317,\n",
      "        1.3096, 1.1404, 1.1772, 1.1664, 1.2576, 1.1629, 1.1952, 1.1532, 1.1685,\n",
      "        1.2142, 1.2361, 1.3082, 1.1838, 1.2444, 1.4493, 1.4412, 1.4472, 1.4305,\n",
      "        1.4824, 1.4516, 1.4420, 1.4755, 1.4712, 1.3809, 1.4918, 1.4853, 1.4380,\n",
      "        1.4342, 1.4192, 1.5268, 1.5672, 1.5629, 1.6012, 1.6366, 1.5565, 1.5357,\n",
      "        1.5070, 1.5921, 1.5954, 1.6214, 1.5608, 1.6003, 1.6107, 1.5868, 1.5698,\n",
      "        1.5474, 1.4573, 1.4885, 1.5561, 1.5100, 1.4294, 1.5211, 1.5540, 1.4878,\n",
      "        1.6650, 1.5486, 1.4909, 1.4340, 1.5570, 1.3993, 1.5305, 1.5030, 1.3616,\n",
      "        1.3686, 1.3409, 1.3860, 1.3412, 1.3960, 1.4454, 1.4104, 1.3981, 1.4073,\n",
      "        1.4592, 1.3857, 1.3598, 1.3883, 1.4162, 1.3778, 1.6234, 1.6344, 1.6778,\n",
      "        1.6965, 1.7070, 1.6187, 1.6665, 1.6156, 1.6883, 1.6832, 1.7084, 1.6827,\n",
      "        1.6151, 1.7802, 1.6497, 1.7090, 1.8619, 1.8245, 1.7387, 1.7123, 1.7584,\n",
      "        1.8234, 1.7963, 1.8343, 1.8013, 1.8223, 1.8751, 1.8188, 1.8823, 1.7792,\n",
      "        1.6770, 1.7495, 1.7778, 1.7558, 1.9529, 1.8123, 1.8237, 1.7828, 1.8904,\n",
      "        1.7808, 1.7883, 1.7604, 1.8873, 1.7135, 1.7277, 1.8307, 1.8431, 1.8072,\n",
      "        1.7395, 1.7973, 1.7953, 1.8094, 1.8626, 1.7438, 1.7917, 1.8098, 1.8288,\n",
      "        1.6828, 1.7729, 1.8276, 1.7777, 1.8126, 1.7317, 1.8590, 1.4966, 1.4099,\n",
      "        1.4256, 1.3889, 1.4682, 1.3970, 1.3954, 1.3717, 1.4345, 1.3800, 1.4136,\n",
      "        1.3444, 1.3876, 1.4923, 1.4534, 1.4815], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0351, -0.0406, -0.0502,  ...,  0.0378, -0.0046, -0.0455],\n",
      "        [ 0.0107, -0.0003, -0.0013,  ...,  0.0049,  0.0055, -0.0124],\n",
      "        [ 0.0632, -0.0368, -0.0417,  ...,  0.0168,  0.0259,  0.0353],\n",
      "        ...,\n",
      "        [ 0.0250,  0.0406, -0.0145,  ..., -0.0409,  0.0007, -0.0567],\n",
      "        [ 0.0037, -0.0499,  0.0040,  ..., -0.0499, -0.0140,  0.0235],\n",
      "        [ 0.0162,  0.0497, -0.0386,  ...,  0.0039, -0.0165, -0.0024]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0016, -0.0087, -0.0014,  ...,  0.0040, -0.0021, -0.0026],\n",
      "        [-0.0005, -0.0003,  0.0016,  ...,  0.0034,  0.0022, -0.0024],\n",
      "        [-0.0046,  0.0020,  0.0004,  ..., -0.0088,  0.0010, -0.0063],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0024,  0.0012,  ...,  0.0075, -0.0014,  0.0061],\n",
      "        [-0.0007, -0.0025,  0.0022,  ..., -0.0024,  0.0018, -0.0004],\n",
      "        [ 0.0018, -0.0008,  0.0033,  ...,  0.0083,  0.0028,  0.0022]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5151, 1.3401, 1.4406, 1.5555, 1.5715, 1.3727, 1.4330, 1.4005, 1.3943,\n",
      "        1.5413, 1.3969, 1.5340, 1.4176, 1.3922, 1.4163, 1.5175, 1.5430, 1.5220,\n",
      "        1.4075, 1.4062, 1.5464, 1.3906, 1.5464, 1.4136, 1.4475, 1.5200, 1.4801,\n",
      "        1.4423, 1.5452, 1.5151, 1.4707, 1.4880, 1.4380, 1.4062, 1.4664, 1.5837,\n",
      "        1.6137, 1.5035, 1.5553, 1.4884, 1.4375, 1.4902, 1.6757, 1.5633, 1.4270,\n",
      "        1.3748, 1.3079, 1.5158, 1.5316, 1.5482, 1.4403, 1.4235, 1.4339, 1.4801,\n",
      "        1.4532, 1.5172, 1.4194, 1.4796, 1.4729, 1.4188, 1.5039, 1.7786, 1.4402,\n",
      "        1.5766, 1.5081, 1.4633, 1.4175, 1.4406, 1.4725, 1.5703, 1.4597, 1.6191,\n",
      "        1.5974, 1.5227, 1.4323, 1.3342, 1.4648, 1.4439, 1.4202, 1.4725, 1.5170,\n",
      "        1.5607, 1.4448, 1.5346, 1.6038, 1.5237, 1.4342, 1.3530, 1.3634, 1.5913,\n",
      "        1.4620, 1.5344, 1.5434, 1.4272, 1.5004, 1.4811, 1.3042, 1.5367, 1.6194,\n",
      "        1.3964, 1.4712, 1.3765, 1.4164, 1.3238, 1.4351, 1.5392, 1.4704, 1.4078,\n",
      "        1.6345, 1.4005, 1.4246, 1.4880, 1.5552, 1.5272, 1.4343, 1.4652, 1.6053,\n",
      "        1.5465, 1.4444, 1.4666, 1.5178, 1.5750, 1.4457, 1.4763, 1.4908, 1.3823,\n",
      "        1.4198, 1.4375, 1.4752, 1.6520, 1.4647, 1.5199, 1.4610, 1.4879, 1.4385,\n",
      "        1.2688, 1.5131, 1.3334, 1.5633, 1.3824, 1.4484, 1.5189, 1.4605, 1.3965,\n",
      "        1.4227, 1.4134, 1.3834, 1.4107, 1.4890, 1.4877, 1.7318, 1.5302, 1.4646,\n",
      "        1.3499, 1.4393, 1.4989, 1.5787, 1.5313, 1.5501, 1.4894, 1.3801, 1.5105,\n",
      "        1.4861, 1.4635, 1.4833, 1.4895, 1.4796, 1.4381, 1.4112, 1.4790, 1.4623,\n",
      "        1.4632, 1.4982, 1.5290, 1.5579, 1.4324, 1.5014, 1.4799, 1.3883, 1.4502,\n",
      "        1.4668, 1.3827, 1.6434, 1.5531, 1.4438, 1.5007, 1.5309, 1.3341, 1.4030,\n",
      "        1.4162, 1.3900, 1.4824, 2.3219, 1.4178, 1.4911, 1.4204, 1.4497, 1.4230,\n",
      "        1.4588, 1.4771, 1.4652, 1.4840, 1.3967, 1.4846, 1.4229, 1.3916, 1.5696,\n",
      "        1.4475, 1.5143, 1.3630, 1.5070, 1.3878, 1.4586, 1.4723, 1.4275, 1.5121,\n",
      "        1.4608, 1.4348, 1.4623, 1.5180, 1.4721, 1.5448, 1.5110, 1.4563, 1.3817,\n",
      "        1.3434, 1.4634, 1.4455, 1.4989, 1.5383, 1.4984, 1.4742, 1.4706, 1.3982,\n",
      "        1.4342, 1.5140, 1.4813, 1.2873, 1.4518, 1.5383, 1.5827, 1.3492, 1.5013,\n",
      "        1.5160, 1.4210, 1.3570, 1.3462, 1.4532, 1.4510, 1.4979, 1.4083, 1.5287,\n",
      "        1.4999, 1.5614, 1.4159, 1.4939, 1.5797, 1.3966, 1.3387, 1.5087, 1.5131,\n",
      "        1.4336, 1.4443, 1.4929, 1.6143, 1.5734, 1.5189, 1.5558, 1.4644, 1.4888,\n",
      "        1.4422, 1.4876, 1.6829, 1.4446, 1.5126, 1.5055, 1.4766, 1.4103, 1.4352,\n",
      "        1.4509, 1.4766, 1.5255, 1.4107, 1.5383, 1.4188, 1.4595, 1.4247, 1.4495,\n",
      "        1.2975, 1.5295, 1.5493, 1.5705, 1.3766, 1.4951, 1.4948, 1.4913, 1.5726,\n",
      "        1.4885, 1.4277, 1.5017, 1.4506, 1.5342, 1.3889, 1.3446, 1.4614, 1.4581,\n",
      "        1.5769, 1.5077, 1.4598, 1.3480, 1.4739, 1.5295, 1.4425, 1.5424, 1.4128,\n",
      "        1.5599, 1.5041, 1.4341, 1.4178, 2.1279], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0333,  0.0021, -0.0338,  ...,  0.0312, -0.0253,  0.0519],\n",
      "        [-0.0022, -0.0543,  0.0069,  ...,  0.0256,  0.0386,  0.0128],\n",
      "        [ 0.0387,  0.0035, -0.0518,  ...,  0.0463, -0.0503,  0.0518],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0500,  0.0269,  ...,  0.0240,  0.0292,  0.0293],\n",
      "        [-0.0390, -0.0484, -0.0562,  ..., -0.0591,  0.0333,  0.0508],\n",
      "        [-0.0269,  0.0145,  0.0028,  ..., -0.0614,  0.0538, -0.0010]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-4.8878e-03,  3.3231e-03, -3.9977e-03,  ...,  2.0056e-03,\n",
      "         -5.9200e-03,  6.1798e-03],\n",
      "        [ 1.6736e-03, -5.6377e-03,  1.1252e-03,  ..., -1.7838e-04,\n",
      "         -6.3642e-04, -1.2113e-03],\n",
      "        [-1.6400e-04, -1.3610e-03,  9.0726e-05,  ..., -1.1768e-03,\n",
      "          9.8020e-04, -4.7609e-04],\n",
      "        ...,\n",
      "        [ 6.8320e-04, -1.5922e-03,  2.9774e-04,  ...,  6.0998e-04,\n",
      "         -2.0076e-03,  1.3815e-04],\n",
      "        [ 7.8869e-03, -6.5845e-03,  7.9197e-03,  ..., -2.3341e-03,\n",
      "          9.9070e-03, -5.9489e-03],\n",
      "        [-4.9188e-04, -3.1961e-03, -1.9554e-03,  ...,  7.9817e-03,\n",
      "         -6.4822e-03,  3.2266e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.2354, 1.1939, 1.2675,  ..., 1.1296, 1.9021, 3.0284],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0207,  0.0195,  0.0001,  ..., -0.0212,  0.0141,  0.0262],\n",
      "        [-0.0140, -0.0095, -0.0265,  ...,  0.0138,  0.0154,  0.0125],\n",
      "        [-0.0316, -0.0104, -0.0038,  ..., -0.0200,  0.0019, -0.0221],\n",
      "        ...,\n",
      "        [-0.0109, -0.0302, -0.0118,  ..., -0.0173,  0.0127,  0.0147],\n",
      "        [-0.0089,  0.0020,  0.0121,  ...,  0.0302,  0.0146,  0.0173],\n",
      "        [-0.0076, -0.0206,  0.0084,  ...,  0.0058, -0.0062, -0.0244]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 3.4213e-03,  2.7873e-03,  2.8891e-03,  ..., -1.0825e-03,\n",
      "         -3.0777e-03,  2.8495e-03],\n",
      "        [ 7.1240e-03,  8.2690e-03,  4.5269e-03,  ...,  3.8422e-03,\n",
      "         -6.4782e-03,  9.6047e-03],\n",
      "        [-7.3098e-03, -2.4560e-03, -2.7313e-03,  ...,  4.2428e-03,\n",
      "          3.3066e-03, -4.2974e-04],\n",
      "        ...,\n",
      "        [ 8.0359e-03,  3.0304e-03,  5.4079e-03,  ..., -1.7025e-03,\n",
      "         -5.3814e-03,  2.3666e-03],\n",
      "        [ 4.7467e-04, -4.0395e-04, -1.8284e-04,  ...,  3.5112e-04,\n",
      "         -8.9206e-04,  3.0616e-05],\n",
      "        [ 1.3643e-03,  1.1466e-03,  1.6153e-03,  ..., -2.7570e-03,\n",
      "         -2.6088e-04, -2.5826e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.9627, 3.1115, 3.2687, 3.2839, 3.2422, 2.9855, 2.8641, 3.0383, 3.2100,\n",
      "        3.0361, 2.8575, 3.0005, 3.2483, 2.9130, 2.9464, 3.2039, 2.9153, 2.9506,\n",
      "        2.9051, 2.9377, 3.1469, 3.0680, 2.9553, 2.7964, 2.9013, 2.9233, 3.0191,\n",
      "        2.9756, 2.9021, 3.0429, 3.1984, 2.8299, 3.0898, 2.8210, 3.0550, 3.1875,\n",
      "        3.0454, 3.0078, 3.0385, 2.9954, 3.2129, 2.8643, 2.9120, 3.0750, 2.9905,\n",
      "        3.0976, 3.5245, 3.0204, 2.9755, 2.9728, 3.0884, 3.0205, 2.7820, 3.0296,\n",
      "        3.0998, 2.9577, 2.8916, 3.2129, 3.1356, 3.2631, 2.9748, 3.1223, 3.8604,\n",
      "        2.9460, 3.0999, 3.0238, 2.8726, 3.1957, 3.1532, 3.1138, 2.7881, 3.0630,\n",
      "        3.0615, 3.0013, 3.0765, 3.1101, 2.9101, 2.8679, 3.0198, 3.1840, 2.9237,\n",
      "        2.9667, 3.1111, 2.9478, 2.8327, 3.0835, 2.9885, 3.0522, 2.5975, 3.0055,\n",
      "        3.0371, 2.9081, 3.2536, 2.7695, 3.1025, 3.1314, 2.9029, 2.9060, 3.2290,\n",
      "        3.2153, 2.9696, 2.8985, 2.9836, 2.8837, 2.9264, 3.3201, 3.0397, 2.9391,\n",
      "        2.9243, 3.1682, 3.1422, 2.9643, 3.1307, 2.9923, 3.0266, 3.0551, 2.9717,\n",
      "        3.1797, 3.0568, 2.9039, 3.0134, 3.0197, 2.9179, 3.0952, 2.8765, 3.1029,\n",
      "        3.0313, 3.1412, 2.9556, 3.0338, 2.9078, 3.0962, 3.0211, 3.0498, 3.1526,\n",
      "        3.0790, 3.0031, 3.0518, 3.0896, 3.0753, 2.9725, 2.9050, 3.0922, 2.8889,\n",
      "        3.0322, 3.1181, 3.0155, 3.2714, 3.1099, 2.9977, 3.5673, 2.9367, 2.9617,\n",
      "        3.0898, 3.0814, 3.1866, 2.8066, 3.0315, 2.9378, 3.0083, 3.0686, 3.1476,\n",
      "        2.8016, 3.1394, 3.1248, 3.1146, 3.1667, 3.2098, 3.0455, 2.9242, 3.0705,\n",
      "        2.8096, 3.0418, 2.9524, 3.0527, 3.1693, 3.0748, 3.1457, 3.1407, 2.9228,\n",
      "        2.9801, 2.8620, 2.9364, 2.8313, 3.1410, 3.1007, 3.1966, 3.2055, 3.1437,\n",
      "        3.0682, 3.1554, 2.9922, 5.2875, 2.9458, 3.0970, 3.0926, 3.2426, 2.9912,\n",
      "        3.2172, 2.9936, 3.1528, 2.9115, 3.0701, 3.0004, 2.9791, 3.0638, 3.2364,\n",
      "        2.9578, 3.1449, 2.9747, 2.8723, 3.0261, 2.8431, 3.0581, 3.2044, 3.1121,\n",
      "        3.0251, 2.9204, 2.9007, 2.8924, 2.9766, 3.2887, 3.0727, 3.1894, 2.9404,\n",
      "        3.1939, 3.3012, 3.0138, 3.0119, 3.1254, 2.9134, 2.9042, 3.1518, 2.7848,\n",
      "        3.0618, 2.9701, 3.1545, 2.8757, 2.9555, 3.2382, 3.0639, 2.8759, 2.8863,\n",
      "        2.9031, 2.8911, 2.9305, 3.0610, 2.9579, 3.1239, 3.2133, 2.8835, 3.1089,\n",
      "        2.9427, 2.9445, 3.0422, 2.9225, 3.0208, 2.5521, 2.4516, 3.1231, 2.9925,\n",
      "        2.7937, 2.8829, 2.9095, 2.9891, 3.0775, 2.9922, 3.2363, 2.9352, 2.9199,\n",
      "        2.9033, 2.9941, 3.0903, 3.0141, 2.9721, 3.2147, 3.1333, 3.0290, 3.0268,\n",
      "        2.9138, 3.1151, 3.2431, 3.1393, 3.1929, 3.0055, 3.0859, 2.8129, 3.1223,\n",
      "        2.6289, 3.2687, 3.0555, 2.8451, 3.0440, 3.0484, 2.6835, 3.0561, 2.9532,\n",
      "        3.0337, 2.8151, 3.2350, 2.9175, 3.0411, 3.0818, 2.9187, 3.2556, 3.0251,\n",
      "        2.8826, 3.0841, 3.1265, 3.2198, 3.0610, 3.1799, 2.9638, 3.0580, 3.1450,\n",
      "        3.0016, 3.0138, 2.9580, 3.0331, 3.7531], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0526,  0.0637, -0.0029,  ..., -0.0663, -0.0336,  0.0017],\n",
      "        [ 0.0050,  0.0108, -0.0069,  ...,  0.0311, -0.0496,  0.0253],\n",
      "        [-0.0410, -0.0384, -0.0025,  ...,  0.0511,  0.0242,  0.0082],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0485, -0.0035,  ...,  0.0334,  0.0087, -0.0146],\n",
      "        [ 0.0055, -0.0348, -0.0292,  ..., -0.0479, -0.0244,  0.0198],\n",
      "        [-0.0116, -0.0561,  0.0248,  ...,  0.0044, -0.0274,  0.0347]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 4.1183e-04,  5.1898e-04, -5.2915e-04,  ...,  1.6480e-03,\n",
      "         -4.9027e-06, -1.7321e-03],\n",
      "        [ 5.4938e-03, -9.6367e-03, -8.5315e-03,  ..., -8.7425e-03,\n",
      "         -8.3730e-03,  3.4823e-03],\n",
      "        [-5.3388e-03,  3.2801e-03,  2.6825e-03,  ...,  3.3941e-03,\n",
      "          3.8866e-03, -4.9495e-03],\n",
      "        ...,\n",
      "        [ 1.0240e-03,  8.4190e-04,  1.5552e-03,  ..., -7.2852e-04,\n",
      "         -4.1230e-03,  3.0812e-03],\n",
      "        [ 8.8475e-03, -7.8993e-03, -8.0972e-03,  ..., -7.8761e-03,\n",
      "         -1.0454e-02,  3.0546e-04],\n",
      "        [ 2.6974e-03, -1.3961e-03, -1.1606e-03,  ..., -7.7809e-04,\n",
      "         -4.1103e-03,  6.0352e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6251, 2.6459, 2.0950, 1.2526, 2.4734, 4.0939, 4.0528, 4.0984, 1.6258,\n",
      "        1.7498, 2.3940, 2.2293, 1.8260, 3.9749, 3.8749, 3.9437, 1.6965, 1.9970,\n",
      "        2.5529, 2.4945, 2.4270, 2.8197, 2.4923, 2.9178, 2.1323, 2.0425, 1.9697,\n",
      "        1.9504, 1.9860, 2.1116, 2.6534, 3.0697, 2.1063, 1.9648, 1.9621, 1.3702,\n",
      "        1.9452, 2.9877, 2.5310, 2.5641, 2.3064, 1.6216, 1.7931, 2.2795, 2.6160,\n",
      "        1.9646, 2.4352, 2.3954, 1.2609, 1.3968, 2.0674, 1.9271, 1.6272, 2.4845,\n",
      "        2.3867, 2.6704, 1.7382, 2.3025, 1.9814, 1.8835, 1.9664, 1.9693, 2.2375,\n",
      "        2.6577, 2.1699, 2.0699, 1.7503, 2.0047, 2.0979, 2.3143, 2.7781, 3.8691,\n",
      "        2.4042, 2.2303, 2.1068, 1.9244, 1.8057, 2.3460, 2.9915, 2.7822, 1.9239,\n",
      "        2.1179, 2.0796, 1.9913, 2.0829, 2.3717, 2.5766, 2.0374, 1.8274, 1.7050,\n",
      "        2.2797, 2.1600, 2.0022, 2.0650, 2.6553, 2.8205, 2.7373, 2.0058, 2.3082,\n",
      "        2.3597, 2.1232, 2.4632, 2.7316, 2.7595, 1.6165, 2.2072, 1.8075, 2.2850,\n",
      "        2.6018, 3.3129, 2.8237, 3.1153, 3.1512, 1.7643, 2.4407, 2.0310, 2.1901,\n",
      "        2.8640, 2.4919, 2.5470, 1.7588, 2.2823, 1.6629, 2.4428, 2.7711, 2.4074,\n",
      "        2.8385, 2.8638, 1.2774, 1.7800, 1.7666, 2.0585, 2.2304, 2.5650, 2.2350,\n",
      "        2.8436, 2.3583, 1.7385, 2.2623, 1.7376, 1.9687, 2.1867, 2.5075, 3.2102,\n",
      "        2.2480, 1.5015, 1.7398, 2.3254, 2.7109, 2.5342, 2.6040, 2.6346, 1.9131,\n",
      "        1.4668, 1.8390, 1.7457, 2.0670, 2.0639, 2.5260, 2.6283, 1.8806, 2.0316,\n",
      "        2.1635, 2.2540, 2.0373, 1.7298, 2.6085, 2.8535, 1.8382, 2.1000, 2.4409,\n",
      "        2.0900, 2.6051, 2.4532, 2.6759, 1.9697, 2.0989, 2.4524, 1.9675, 2.6125,\n",
      "        3.3689, 3.4088, 2.8976, 3.0807, 2.4242, 2.1250, 2.4137, 2.2678, 2.2166,\n",
      "        2.7274, 2.8038, 3.0275, 1.9476, 1.6724, 1.9972, 1.6096, 2.2213, 3.1477,\n",
      "        2.7641, 2.8963, 2.7089, 2.2615, 2.1392, 2.7213, 2.8540, 2.3423, 2.5243,\n",
      "        2.6484, 2.3531, 2.1841, 2.2417, 2.1521, 1.9504, 2.6564, 2.9010, 2.7402,\n",
      "        2.0689, 2.1705, 2.0229, 2.2793, 2.3591, 2.6975, 2.9014, 2.9326, 1.2779,\n",
      "        1.4844, 2.3054, 1.6093, 2.2985, 1.7351, 2.2441, 2.2157, 1.6390, 1.6877,\n",
      "        1.3590, 2.0729, 1.6051, 2.3609, 2.1178, 2.3231, 2.1813, 1.8422, 1.8371,\n",
      "        1.8594, 1.7772, 1.8731, 2.4048, 2.5751, 1.9013, 1.8157, 2.0794, 2.4594,\n",
      "        2.1461, 2.2528, 2.4476, 2.2963, 1.9243, 2.2980, 2.7347, 2.3037, 2.1110,\n",
      "        2.7301, 2.8535, 3.2572, 2.6108, 2.0600, 1.9121, 2.2110, 2.5729, 3.0840,\n",
      "        3.1170, 3.5771, 2.1859, 2.1502, 2.1911, 2.0514, 1.8502, 2.2366, 3.7195,\n",
      "        3.8732, 2.1959, 2.2586, 2.6611, 2.2685, 2.0664, 2.0182, 3.6214, 3.7151,\n",
      "        2.8974, 1.8913, 1.6529, 2.2098, 2.2944, 2.4037, 2.9225, 2.8887, 1.7557,\n",
      "        1.7449, 2.1590, 1.9214, 1.9640, 2.7504, 2.7541, 3.0107, 2.0338, 1.9682,\n",
      "        2.2781, 2.8427, 2.3850, 2.3192, 3.2396, 2.8082, 2.2207, 2.0076, 2.1172,\n",
      "        1.9178, 2.8429, 2.7808, 2.7981, 2.3343], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0079,  0.0196, -0.0602,  ...,  0.0236, -0.0495, -0.0393],\n",
      "        [ 0.0473, -0.0012,  0.0489,  ...,  0.0465, -0.0126, -0.0549],\n",
      "        [ 0.0642, -0.0291, -0.0519,  ...,  0.0335, -0.0361,  0.0045],\n",
      "        ...,\n",
      "        [-0.0202, -0.0266,  0.0355,  ..., -0.0162,  0.0148, -0.0285],\n",
      "        [ 0.0163, -0.0151,  0.0338,  ..., -0.0243,  0.0319,  0.0265],\n",
      "        [ 0.0240,  0.0292, -0.0046,  ...,  0.0022, -0.0145,  0.0181]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0002,  0.0013, -0.0027,  ...,  0.0044, -0.0039, -0.0034],\n",
      "        [-0.0064,  0.0049, -0.0016,  ...,  0.0002,  0.0025,  0.0026],\n",
      "        [-0.0016,  0.0066, -0.0018,  ..., -0.0013, -0.0003,  0.0018],\n",
      "        ...,\n",
      "        [-0.0078, -0.0015, -0.0034,  ..., -0.0022, -0.0031, -0.0009],\n",
      "        [-0.0092, -0.0089, -0.0081,  ..., -0.0029,  0.0062, -0.0019],\n",
      "        [-0.0089, -0.0018, -0.0041,  ..., -0.0012, -0.0035, -0.0018]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0705, 2.5428, 2.3508, 1.9760, 2.0991, 2.9845, 3.1099, 3.2076, 2.0326,\n",
      "        1.9360, 2.1854, 1.6982, 1.8092, 2.9088, 3.0336, 3.1842, 1.7676, 1.6798,\n",
      "        1.7784, 1.4614, 1.4361, 1.5470, 2.4343, 2.1582, 1.8641, 2.3713, 2.1831,\n",
      "        2.7708, 2.7214, 2.8038, 2.1937, 2.1177, 2.2337, 1.7458, 2.1283, 2.4211,\n",
      "        1.8971, 1.4590, 2.4361, 1.8712, 2.1058, 1.8827, 1.8491, 1.5297, 1.6714,\n",
      "        2.5513, 2.3186, 2.1279, 1.2376, 2.1820, 1.9844, 1.4009, 2.5583, 1.4676,\n",
      "        2.2705, 1.9684, 1.5102, 1.3544, 1.5301, 2.1755, 1.3011, 2.4472, 2.0719,\n",
      "        1.9877, 2.0071, 2.1089, 1.9090, 2.1962, 1.8062, 1.9300, 2.2513, 2.1558,\n",
      "        2.2494, 2.2295, 1.9751, 1.7442, 1.9602, 1.9665, 2.3400, 1.8728, 1.7940,\n",
      "        2.1360, 2.4510, 2.0427, 2.0027, 1.7558, 2.1464, 2.1257, 1.8868, 1.5204,\n",
      "        1.9364, 2.0315, 1.9444, 2.3798, 2.4317, 2.5076, 1.8621, 1.8623, 1.9703,\n",
      "        2.8948, 2.9952, 2.9549, 2.6740, 2.6723, 2.3808, 2.2558, 2.0334, 1.7987,\n",
      "        1.5428, 1.6599, 2.5902, 2.1694, 2.0567, 1.8703, 2.2366, 2.8172, 3.2906,\n",
      "        1.6099, 2.2535, 2.4842, 2.5101, 2.0728, 1.6054, 1.5813, 1.3896, 2.7549,\n",
      "        2.3377, 2.1157, 1.8416, 1.7870, 2.2301, 1.5017, 1.6241, 1.8215, 2.2420,\n",
      "        2.1349, 1.2369, 1.7084, 1.7365, 2.0014, 1.9713, 2.1656, 2.3596, 2.1385,\n",
      "        2.1685, 1.8456, 2.3087, 1.4073, 1.3531, 1.6128, 2.2556, 2.0454, 1.8942,\n",
      "        1.5752, 1.6171, 2.4392, 2.7460, 2.2130, 2.3558, 1.9544, 1.9751, 2.0513,\n",
      "        2.4317, 2.0491, 2.3031, 2.3000, 2.1649, 2.5608, 1.5359, 2.1554, 2.2521,\n",
      "        2.3143, 1.9976, 2.1674, 2.4492, 2.1731, 2.2896, 2.1228, 1.9004, 1.9294,\n",
      "        1.6358, 1.5909, 2.8386, 2.7222, 2.2226, 2.3623, 2.2717, 2.5050, 2.3296,\n",
      "        2.6293, 2.7097, 2.6849, 2.1296, 1.7786, 2.0160, 2.1816, 2.4280, 1.5430,\n",
      "        2.1482, 2.1172, 1.9056, 2.0488, 1.9572, 1.3843, 1.3501, 2.1146, 2.5132,\n",
      "        2.3750, 2.0737, 2.2170, 2.1236, 2.3145, 2.0413, 1.9640, 2.6743, 2.7371,\n",
      "        2.1716, 2.1373, 2.0513, 1.9630, 1.7619, 1.9278, 2.6878, 2.7538, 1.4357,\n",
      "        1.7803, 1.3488, 2.3593, 1.2206, 2.2874, 2.0997, 2.0660, 1.5893, 1.5398,\n",
      "        2.4441, 1.3093, 2.4074, 1.3033, 1.9746, 1.8836, 1.9593, 1.8228, 1.7753,\n",
      "        2.1170, 2.4332, 2.5551, 2.2448, 2.1305, 2.0972, 1.7915, 1.9935, 1.5466,\n",
      "        1.3920, 1.4355, 2.3625, 1.9377, 1.9587, 1.9965, 1.7765, 1.7974, 2.8305,\n",
      "        2.3684, 2.6687, 2.1497, 2.2049, 2.0840, 2.3904, 2.4306, 1.5878, 2.2392,\n",
      "        2.6330, 2.1911, 2.0811, 2.1465, 2.0247, 2.1139, 1.9245, 2.2276, 3.0404,\n",
      "        3.0677, 2.3161, 2.3813, 2.5214, 2.2125, 2.0549, 2.0084, 3.0050, 3.0132,\n",
      "        2.0597, 1.9055, 2.4398, 1.7407, 1.3676, 2.4017, 2.3252, 2.2493, 2.1084,\n",
      "        1.7722, 1.5507, 1.6412, 2.5152, 1.6760, 2.2049, 2.2541, 2.2650, 1.8736,\n",
      "        2.1891, 2.2645, 1.9403, 2.3514, 2.3254, 2.7769, 1.7965, 2.0821, 2.1158,\n",
      "        2.2760, 2.3232, 2.2892, 2.8385, 2.3254], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0126,  0.0255, -0.0413,  ..., -0.0395,  0.0487,  0.0353],\n",
      "        [-0.0305, -0.0427, -0.0048,  ..., -0.0401,  0.0325, -0.0366],\n",
      "        [ 0.0259, -0.0421, -0.0302,  ...,  0.0171,  0.0252,  0.0018],\n",
      "        ...,\n",
      "        [-0.0334, -0.0290,  0.0626,  ..., -0.0026,  0.0268, -0.0216],\n",
      "        [-0.0372,  0.0375, -0.0370,  ...,  0.0055,  0.0195, -0.0399],\n",
      "        [-0.0106,  0.0094,  0.0074,  ..., -0.0492,  0.0004, -0.0210]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0038, -0.0063,  0.0048,  ..., -0.0043,  0.0051, -0.0039],\n",
      "        [ 0.0050,  0.0030, -0.0036,  ...,  0.0047, -0.0042,  0.0046],\n",
      "        [-0.0035, -0.0041,  0.0038,  ..., -0.0031,  0.0043, -0.0053],\n",
      "        ...,\n",
      "        [ 0.0019,  0.0037, -0.0025,  ...,  0.0055, -0.0038,  0.0025],\n",
      "        [-0.0024, -0.0021,  0.0046,  ..., -0.0032,  0.0017, -0.0010],\n",
      "        [ 0.0027,  0.0024, -0.0012,  ...,  0.0050, -0.0002,  0.0088]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5365, 1.6515, 1.5967, 1.5537, 1.6421, 1.6285, 1.5041, 1.5537, 1.5362,\n",
      "        1.5709, 1.5089, 1.5683, 1.5184, 1.5014, 1.5707, 1.5312, 1.1628, 1.2364,\n",
      "        1.2089, 1.1110, 1.2377, 1.1826, 1.2507, 1.1834, 1.2354, 1.1921, 1.2145,\n",
      "        1.1792, 1.2113, 1.1790, 1.1572, 1.2247, 1.6577, 1.4829, 1.5938, 1.6214,\n",
      "        1.5915, 1.5520, 1.6633, 1.6032, 1.5963, 1.5659, 1.6108, 1.6440, 1.6196,\n",
      "        1.6412, 1.6302, 1.6070, 1.2701, 1.1565, 1.2511, 1.2151, 1.1801, 1.2283,\n",
      "        1.2193, 1.2472, 1.2597, 1.2975, 1.1828, 1.2415, 1.2150, 1.2186, 1.2618,\n",
      "        1.2166, 1.8286, 1.8192, 1.8198, 1.7568, 1.8907, 1.7810, 1.8324, 1.9419,\n",
      "        1.8722, 1.9157, 1.8333, 1.9758, 1.8602, 1.9514, 1.8608, 1.9549, 1.8185,\n",
      "        1.7905, 1.8470, 1.7529, 1.7361, 1.8922, 1.7595, 1.7414, 1.8564, 1.6888,\n",
      "        1.7761, 1.8164, 1.7732, 1.8574, 1.8286, 1.8306, 1.6610, 1.7971, 1.6306,\n",
      "        1.6942, 1.7758, 1.6793, 1.6948, 1.7042, 1.7354, 1.6514, 1.6520, 1.6711,\n",
      "        1.6607, 1.7248, 1.6270, 1.6508, 1.6171, 1.5577, 1.6077, 1.5498, 1.5634,\n",
      "        1.5629, 1.6464, 1.5889, 1.6094, 1.5799, 1.6177, 1.5118, 1.6200, 1.5716,\n",
      "        1.5640, 1.6459, 1.2302, 1.1850, 1.2367, 1.2238, 1.2226, 1.1751, 1.1717,\n",
      "        1.1568, 1.2080, 1.1526, 1.1714, 1.1738, 1.2004, 1.2005, 1.1585, 1.1970,\n",
      "        1.5958, 1.6151, 1.5888, 1.6019, 1.5255, 1.5965, 1.5973, 1.5506, 1.5928,\n",
      "        1.6158, 1.6245, 1.6272, 1.5965, 1.6401, 1.6243, 1.6506, 1.7572, 1.8343,\n",
      "        1.7464, 1.8561, 1.8222, 1.7837, 1.7687, 1.8379, 1.7084, 1.8166, 1.6981,\n",
      "        1.7919, 1.7170, 1.7927, 1.6823, 1.7745, 1.7433, 1.7506, 1.6572, 1.6326,\n",
      "        1.7790, 1.7256, 1.7681, 1.8328, 1.7304, 1.7689, 1.6888, 1.6225, 1.7242,\n",
      "        1.6816, 1.7363, 1.6585, 1.5288, 1.5655, 1.4681, 1.5090, 1.5145, 1.5574,\n",
      "        1.5514, 1.5550, 1.5526, 1.5707, 1.6610, 1.5776, 1.6089, 1.5914, 1.5118,\n",
      "        1.5099, 1.7942, 1.8313, 1.7402, 1.7738, 1.8102, 1.9072, 1.7051, 1.7233,\n",
      "        1.7040, 1.8166, 1.8600, 1.7234, 1.7673, 1.7723, 1.8241, 1.8085, 1.3474,\n",
      "        1.3518, 1.2818, 1.3629, 1.3453, 1.4532, 1.3673, 1.3719, 1.4564, 1.4978,\n",
      "        1.3781, 1.3548, 1.3925, 1.3155, 1.3461, 1.3736, 1.6737, 1.6571, 1.6887,\n",
      "        1.6277, 1.5657, 1.6417, 1.6357, 1.6849, 1.6142, 1.6503, 1.6814, 1.6300,\n",
      "        1.7039, 1.6548, 1.6993, 1.6701, 1.2539, 1.2764, 1.2900, 1.2242, 1.2461,\n",
      "        1.1963, 1.2445, 1.2062, 1.2651, 1.2331, 1.2561, 1.1629, 1.2826, 1.2599,\n",
      "        1.2665, 1.2178, 1.8126, 1.6607, 1.6833, 1.6833, 1.6807, 1.6514, 1.6210,\n",
      "        1.8369, 1.5600, 1.6461, 1.7167, 1.7704, 1.7386, 1.6039, 1.7404, 1.7772,\n",
      "        1.3174, 1.3024, 1.2545, 1.2825, 1.2582, 1.3100, 1.3466, 1.2184, 1.2667,\n",
      "        1.3335, 1.2801, 1.3133, 1.2878, 1.3026, 1.2481, 1.3123, 1.4768, 1.5412,\n",
      "        1.5726, 1.3971, 1.5619, 1.5192, 1.4278, 1.5137, 1.5211, 1.4671, 1.4959,\n",
      "        1.4411, 1.4316, 1.5327, 1.4572, 1.6477], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0084, -0.0394, -0.0403,  ..., -0.0429,  0.0201, -0.0060],\n",
      "        [-0.0365,  0.0047,  0.0154,  ...,  0.0012, -0.0356,  0.0005],\n",
      "        [-0.0115, -0.0406, -0.0085,  ...,  0.0435, -0.0467, -0.0364],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0521,  0.0188,  ..., -0.0150, -0.0226,  0.0161],\n",
      "        [-0.0299, -0.0272, -0.0358,  ..., -0.0450, -0.0510, -0.0172],\n",
      "        [-0.0225,  0.0482,  0.0461,  ...,  0.0377,  0.0225,  0.0445]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-8.7675e-03, -5.1151e-03,  9.7319e-04,  ...,  2.7422e-03,\n",
      "         -5.0855e-04, -6.2907e-03],\n",
      "        [ 3.6475e-03,  4.5030e-03, -3.6968e-03,  ..., -3.7198e-03,\n",
      "         -2.0764e-03,  9.4974e-05],\n",
      "        [ 8.6947e-03,  4.0728e-03, -4.6027e-03,  ...,  7.1768e-04,\n",
      "         -7.5554e-04,  1.3800e-03],\n",
      "        ...,\n",
      "        [-1.0729e-02, -4.1713e-04,  3.6773e-03,  ...,  5.8027e-03,\n",
      "          4.1815e-03, -6.3043e-03],\n",
      "        [ 5.4869e-03,  7.8676e-03,  6.1966e-03,  ...,  7.7516e-04,\n",
      "         -3.3381e-04, -1.4271e-03],\n",
      "        [-7.1521e-04, -1.1882e-02, -4.6201e-03,  ...,  3.1847e-03,\n",
      "          8.9954e-04, -9.6641e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4847, 1.5929, 1.5313, 1.4825, 1.6294, 1.4067, 1.4296, 1.4947, 1.4432,\n",
      "        1.4327, 1.4255, 1.5232, 1.5402, 1.4102, 1.6674, 1.4297, 1.3469, 1.5343,\n",
      "        1.4429, 1.4683, 1.5447, 1.4051, 1.5802, 1.4457, 1.4765, 1.4807, 1.4803,\n",
      "        1.5303, 1.5174, 1.4868, 1.4502, 1.4296, 1.6065, 1.4787, 1.4153, 1.4420,\n",
      "        1.6229, 1.4237, 1.3961, 1.4692, 1.6398, 1.5301, 1.4804, 1.4961, 1.4105,\n",
      "        1.3994, 1.3918, 1.6254, 1.4942, 1.5017, 1.4562, 1.5038, 1.3135, 1.3621,\n",
      "        1.3804, 1.3310, 1.3383, 1.4443, 1.6097, 1.5453, 1.3860, 1.6812, 2.7586,\n",
      "        1.4634, 1.4569, 1.4774, 1.4244, 1.5519, 1.5115, 1.5387, 1.3334, 1.4361,\n",
      "        1.4067, 1.4260, 1.5004, 1.5598, 1.4265, 1.5222, 1.4611, 1.5024, 1.4757,\n",
      "        1.5560, 1.4889, 1.5779, 1.4003, 1.4627, 1.4467, 1.5466, 1.3058, 1.5106,\n",
      "        1.4288, 1.5281, 1.5950, 1.5137, 1.4293, 1.5646, 1.4940, 1.5696, 1.5519,\n",
      "        1.5333, 1.5781, 1.5537, 1.6112, 1.4588, 1.5417, 1.6178, 1.4521, 1.4122,\n",
      "        1.4249, 1.4224, 1.5717, 1.3567, 1.4474, 1.4381, 1.5354, 1.4890, 1.5866,\n",
      "        1.4564, 1.4747, 1.4263, 1.5180, 1.5736, 1.5495, 1.5371, 1.3367, 1.5771,\n",
      "        1.5077, 1.5349, 1.4689, 1.4845, 1.3318, 1.3942, 1.5118, 1.5730, 1.5526,\n",
      "        1.3925, 1.4515, 1.5033, 1.3460, 1.3906, 1.4260, 1.4044, 1.4792, 1.4145,\n",
      "        1.3966, 1.4975, 1.4712, 1.5728, 1.4750, 1.5404, 2.1312, 1.5662, 1.5578,\n",
      "        1.5114, 1.4889, 1.5163, 1.4562, 1.5872, 1.5550, 1.4841, 1.3470, 1.4638,\n",
      "        1.5737, 1.5756, 1.6357, 1.5455, 1.3866, 1.4440, 1.3946, 1.4988, 1.4320,\n",
      "        1.6333, 1.5200, 1.4571, 1.4814, 1.3203, 1.5121, 1.5712, 1.4472, 1.5434,\n",
      "        1.4350, 1.3939, 1.4408, 1.5003, 1.5428, 1.4292, 1.4469, 1.4532, 1.4918,\n",
      "        1.6166, 1.3459, 1.3503, 2.1469, 1.3113, 1.4698, 1.4783, 1.5003, 1.4214,\n",
      "        1.5030, 1.5503, 1.5963, 1.3037, 1.4132, 1.3628, 1.3755, 1.5242, 1.5102,\n",
      "        1.3413, 1.5110, 1.6419, 1.5411, 1.5027, 1.4399, 1.6380, 1.4678, 1.5260,\n",
      "        1.5531, 1.5105, 1.3565, 1.5618, 1.4329, 1.5806, 1.5240, 1.4493, 1.4755,\n",
      "        1.4407, 1.3851, 1.4817, 1.4257, 1.4689, 1.3388, 1.3836, 1.3691, 1.4675,\n",
      "        1.4649, 1.7272, 1.4546, 1.3515, 1.4730, 1.6176, 1.5258, 1.4473, 1.3189,\n",
      "        1.4092, 1.3544, 1.5197, 1.4250, 1.5236, 1.4793, 1.4133, 1.5223, 1.4242,\n",
      "        1.4813, 1.4046, 1.4455, 1.4489, 1.3817, 1.4607, 1.2166, 1.4952, 1.4622,\n",
      "        1.4940, 1.6238, 1.4989, 1.4074, 1.4712, 1.5460, 1.6845, 1.3857, 1.5860,\n",
      "        1.3208, 1.5446, 1.2793, 1.6310, 1.4503, 1.5875, 1.5296, 1.3466, 1.5903,\n",
      "        1.4810, 1.4760, 1.4536, 1.5861, 1.6028, 1.5472, 1.4058, 1.4097, 1.4652,\n",
      "        1.2587, 1.3328, 1.5430, 1.5461, 1.5039, 1.5061, 1.2816, 1.4576, 1.5984,\n",
      "        1.5808, 1.4635, 1.5130, 1.5072, 1.5583, 1.4701, 1.3484, 1.4578, 1.4514,\n",
      "        1.4510, 1.5357, 1.4142, 1.4526, 1.4724, 1.3488, 1.5598, 1.5471, 1.5955,\n",
      "        1.4956, 1.5843, 1.5546, 1.3769, 1.6946], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0380, -0.0195, -0.0616,  ..., -0.0118, -0.0346, -0.0639],\n",
      "        [-0.0510,  0.0050, -0.0363,  ..., -0.0216, -0.0460, -0.0192],\n",
      "        [ 0.0685,  0.0114,  0.0390,  ...,  0.0223, -0.0623, -0.0453],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0098, -0.0132,  ...,  0.0242,  0.0224, -0.0100],\n",
      "        [ 0.0232, -0.0126, -0.0143,  ...,  0.0039,  0.0524, -0.0293],\n",
      "        [ 0.0487,  0.0613,  0.0325,  ...,  0.0447,  0.0033,  0.0323]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.1816e-03, -1.5096e-03, -1.2411e-03,  ..., -5.4148e-03,\n",
      "         -2.9381e-03,  6.3840e-03],\n",
      "        [-2.7670e-03, -1.2711e-05,  6.4100e-03,  ...,  4.1453e-03,\n",
      "         -3.1416e-03,  1.3537e-03],\n",
      "        [ 1.0963e-03, -4.2313e-03, -2.4132e-03,  ..., -3.1423e-03,\n",
      "          1.1254e-03, -3.5998e-03],\n",
      "        ...,\n",
      "        [ 1.3117e-03, -5.5392e-03, -8.0036e-04,  ..., -6.8896e-03,\n",
      "          1.4623e-03,  5.2303e-03],\n",
      "        [ 4.1048e-03,  1.0363e-02, -5.9255e-03,  ..., -3.4513e-03,\n",
      "          1.2009e-04, -8.7777e-03],\n",
      "        [ 6.4115e-03,  5.1365e-03, -3.8479e-03,  ..., -5.1208e-03,\n",
      "         -6.1386e-03,  1.9917e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3860, 4.9359, 1.3603,  ..., 5.0172, 1.0020, 1.6571],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0189, -0.0192,  0.0186,  ...,  0.0009, -0.0175,  0.0314],\n",
      "        [ 0.0158,  0.0179,  0.0156,  ..., -0.0105,  0.0267,  0.0187],\n",
      "        [ 0.0109, -0.0102,  0.0042,  ..., -0.0081, -0.0184, -0.0021],\n",
      "        ...,\n",
      "        [-0.0005, -0.0067, -0.0376,  ..., -0.0254, -0.0210, -0.0129],\n",
      "        [ 0.0212, -0.0044,  0.0285,  ..., -0.0189, -0.0224,  0.0165],\n",
      "        [-0.0181,  0.0074,  0.0082,  ..., -0.0187,  0.0315, -0.0198]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0033,  0.0076,  0.0036,  ..., -0.0035,  0.0047,  0.0084],\n",
      "        [-0.0036, -0.0029,  0.0023,  ...,  0.0035, -0.0036, -0.0022],\n",
      "        [ 0.0041,  0.0073,  0.0009,  ...,  0.0003,  0.0030, -0.0012],\n",
      "        ...,\n",
      "        [-0.0018,  0.0006, -0.0028,  ..., -0.0019, -0.0010,  0.0006],\n",
      "        [-0.0027,  0.0071,  0.0003,  ...,  0.0030, -0.0012,  0.0098],\n",
      "        [-0.0035,  0.0008,  0.0020,  ...,  0.0037, -0.0023,  0.0027]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([3.8149, 3.7315, 3.7687, 3.6700, 3.7981, 3.5698, 3.6005, 3.6823, 3.7088,\n",
      "        3.7539, 3.6213, 3.5834, 3.6762, 3.6809, 3.7029, 3.8593, 3.7013, 3.7596,\n",
      "        3.6325, 3.6418, 3.6793, 3.7078, 3.6466, 3.7083, 3.5864, 3.8389, 3.4463,\n",
      "        3.5969, 3.9735, 3.5738, 3.7543, 4.7797, 3.7552, 3.5598, 3.6554, 3.8208,\n",
      "        3.7364, 3.7439, 3.3990, 3.7586, 3.8101, 3.8845, 3.8639, 3.4256, 3.7160,\n",
      "        3.7569, 3.9576, 3.6226, 3.6368, 3.7530, 4.0760, 3.7530, 3.4820, 3.7196,\n",
      "        4.0250, 3.5411, 3.8174, 3.8629, 3.8166, 3.6762, 3.3964, 3.8738, 8.6770,\n",
      "        3.7124, 3.6986, 3.7288, 3.6463, 3.8160, 3.7670, 3.8035, 3.6441, 3.7238,\n",
      "        3.3602, 3.7404, 3.7404, 3.5672, 3.5347, 3.6581, 3.6898, 3.6379, 3.6280,\n",
      "        3.7780, 3.6740, 3.7749, 3.7256, 3.9697, 3.5983, 3.7784, 3.2506, 3.5741,\n",
      "        3.6843, 3.7197, 4.1230, 3.5840, 3.8690, 3.7187, 3.7679, 3.6121, 3.8842,\n",
      "        3.9946, 3.8396, 3.7563, 3.6157, 3.5551, 3.6632, 3.8133, 3.5974, 3.8192,\n",
      "        3.7414, 3.6111, 3.8265, 3.5699, 3.7218, 3.6816, 3.8667, 4.0355, 3.9039,\n",
      "        3.6302, 3.8673, 3.7473, 3.6772, 3.5590, 3.8138, 3.7352, 3.6948, 3.7770,\n",
      "        3.7383, 3.9396, 3.7687, 3.7051, 3.3543, 3.6969, 3.7449, 4.0564, 3.7283,\n",
      "        3.6897, 3.5430, 3.7640, 3.5548, 3.5545, 3.5626, 3.6484, 3.6171, 3.6371,\n",
      "        3.7190, 3.4253, 3.7235, 3.6980, 3.6843, 3.7616, 3.9960, 3.5164, 3.7495,\n",
      "        3.7289, 3.7786, 3.6836, 3.5225, 3.6051, 3.5948, 3.9058, 3.5008, 3.6366,\n",
      "        3.8777, 3.7252, 3.8791, 4.0843, 3.6611, 3.8704, 3.7233, 3.8303, 3.7804,\n",
      "        3.4797, 3.6630, 3.7143, 3.6164, 3.6129, 3.6935, 3.7812, 3.8237, 3.5658,\n",
      "        3.8297, 3.8019, 3.6010, 3.6819, 3.9706, 3.7639, 3.6924, 3.7734, 3.7815,\n",
      "        3.6659, 3.5451, 3.6783, 7.5579, 3.6284, 3.6100, 3.5907, 3.7603, 3.7497,\n",
      "        3.7785, 3.9873, 3.6308, 3.5869, 3.6986, 3.5823, 3.7949, 3.5366, 3.7427,\n",
      "        3.2953, 3.6525, 3.6382, 3.6468, 3.5785, 3.7321, 3.6147, 3.6521, 3.6534,\n",
      "        3.5297, 3.7112, 3.4478, 3.6716, 3.8303, 3.4771, 3.8540, 3.7395, 3.7196,\n",
      "        3.6983, 3.7498, 3.6468, 3.5441, 3.8425, 3.7331, 3.8806, 3.7959, 3.7692,\n",
      "        3.7438, 3.9123, 3.4694, 3.6633, 3.7713, 3.7864, 3.6730, 3.6161, 3.6453,\n",
      "        3.6352, 3.6619, 3.6223, 3.7292, 3.6740, 3.7596, 3.8164, 3.6765, 3.5884,\n",
      "        3.8351, 3.3653, 3.7132, 3.7771, 3.7238, 3.1600, 3.4906, 3.7218, 3.6837,\n",
      "        3.6254, 3.7353, 3.9306, 4.0093, 3.4772, 3.9187, 4.2752, 3.7574, 3.6649,\n",
      "        3.6273, 3.8925, 3.7543, 3.7307, 3.6691, 3.6610, 3.8134, 3.6856, 3.6903,\n",
      "        3.6316, 3.5049, 3.7339, 3.7304, 3.7111, 3.8996, 3.7621, 3.6024, 3.8520,\n",
      "        3.3911, 3.8338, 3.8376, 3.6155, 3.7657, 3.7620, 3.5554, 3.6280, 3.5693,\n",
      "        3.7054, 3.5710, 3.7255, 3.6887, 3.6178, 3.8995, 3.6548, 3.7173, 3.8786,\n",
      "        3.6073, 3.6268, 3.7480, 3.7155, 3.6325, 3.6161, 3.7669, 3.4983, 3.8438,\n",
      "        3.5142, 3.8993, 3.7748, 3.6151, 3.8467], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0033,  0.0132,  0.0365,  ...,  0.0202,  0.0412, -0.0107],\n",
      "        [-0.0153,  0.0409,  0.0161,  ...,  0.0545, -0.0022, -0.0723],\n",
      "        [ 0.0422,  0.0250, -0.0572,  ..., -0.0181,  0.0479,  0.0297],\n",
      "        ...,\n",
      "        [-0.0588, -0.0539,  0.0434,  ..., -0.0382, -0.0204,  0.0213],\n",
      "        [-0.0007,  0.0106,  0.0208,  ..., -0.0460, -0.0020, -0.0602],\n",
      "        [-0.0256, -0.0612,  0.0660,  ...,  0.0010, -0.0607, -0.0399]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 7.1777e-04,  1.0107e-03, -2.5687e-04,  ...,  3.1009e-03,\n",
      "         -1.5622e-03, -3.9114e-03],\n",
      "        [ 8.6661e-03,  7.9020e-03,  7.3792e-03,  ..., -1.0475e-02,\n",
      "          7.6158e-03, -5.7935e-03],\n",
      "        [-1.4525e-04, -1.8623e-03,  4.7877e-03,  ..., -1.3071e-04,\n",
      "          8.3135e-04, -1.5976e-03],\n",
      "        ...,\n",
      "        [-5.5887e-03, -2.5704e-03, -2.3791e-03,  ...,  1.8088e-03,\n",
      "         -1.6268e-03,  3.3067e-03],\n",
      "        [ 4.6318e-03,  5.0659e-04, -2.0670e-04,  ..., -2.8731e-03,\n",
      "          4.0944e-03, -3.8617e-03],\n",
      "        [-9.4495e-04,  2.8302e-03,  3.6313e-05,  ..., -1.3287e-03,\n",
      "          4.0950e-04,  2.1146e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6871, 1.4526, 1.4351, 1.4024, 1.4268, 2.3852, 2.0736, 2.5694, 1.8121,\n",
      "        1.4658, 1.3666, 1.3725, 1.4985, 1.2941, 2.6005, 1.9778, 1.0946, 1.1285,\n",
      "        1.4597, 1.9606, 2.4150, 4.6460, 4.2533, 3.4192, 1.0164, 1.2230, 1.2697,\n",
      "        2.0283, 2.6129, 4.4712, 4.1415, 3.6047, 1.4896, 1.5277, 1.7731, 1.7966,\n",
      "        1.8904, 1.5432, 2.2209, 1.8780, 1.5062, 1.5607, 2.1811, 1.8150, 1.8523,\n",
      "        1.7297, 2.2179, 2.2731, 1.6199, 1.9287, 1.9438, 1.8144, 2.1108, 1.7567,\n",
      "        2.3158, 2.3083, 1.5412, 1.9419, 1.8362, 1.9638, 2.2470, 2.5333, 2.5669,\n",
      "        2.3425, 2.1427, 1.7635, 2.3592, 2.2677, 2.2606, 2.1520, 2.7258, 2.4155,\n",
      "        1.4766, 1.8940, 2.1557, 2.0275, 2.0003, 2.5118, 2.6909, 2.4162, 2.2252,\n",
      "        1.8601, 1.4370, 1.7513, 1.6127, 2.0905, 2.2929, 2.2310, 2.1556, 1.8539,\n",
      "        1.7142, 1.6086, 2.4307, 2.4353, 2.0882, 2.2586, 1.2930, 1.4454, 1.4001,\n",
      "        1.4142, 1.4159, 2.3297, 1.9839, 2.5975, 1.4049, 1.4948, 1.3935, 1.2872,\n",
      "        1.4183, 1.4360, 2.0558, 2.1910, 2.0336, 1.6276, 1.5143, 1.5623, 1.2417,\n",
      "        2.0317, 2.2051, 2.1520, 1.9363, 1.3326, 1.3782, 1.5574, 2.0338, 1.5844,\n",
      "        1.9743, 2.5284, 1.8455, 1.3974, 1.4992, 1.4091, 1.4330, 2.0765, 1.9490,\n",
      "        2.5149, 1.4942, 1.6683, 1.6125, 1.4124, 1.9578, 1.6786, 2.0531, 2.4802,\n",
      "        1.7447, 1.6264, 2.1102, 1.9689, 2.2007, 1.8780, 2.3857, 2.1843, 2.4149,\n",
      "        1.9493, 1.7460, 2.2348, 1.7408, 2.4936, 2.5201, 2.6852, 1.9608, 1.7359,\n",
      "        2.0534, 2.0076, 2.4374, 2.5983, 2.9817, 2.7885, 1.6619, 1.9112, 1.9765,\n",
      "        2.2281, 2.0940, 2.5230, 2.7824, 2.8213, 2.0322, 1.5943, 1.7566, 2.0962,\n",
      "        1.7254, 1.9171, 2.3698, 2.5297, 1.7274, 1.7787, 1.9995, 1.8577, 1.7525,\n",
      "        1.7882, 2.1802, 2.3978, 1.5140, 1.5569, 1.4216, 1.4660, 1.7978, 2.2260,\n",
      "        2.0362, 2.7466, 1.6412, 1.5185, 1.7459, 1.6913, 1.7925, 1.7132, 1.9373,\n",
      "        2.3290, 1.1439, 1.1450, 1.4831, 1.7441, 2.5057, 3.6659, 3.8161, 3.3104,\n",
      "        0.8287, 1.2223, 1.2778, 2.0544, 2.2225, 3.6710, 3.5317, 3.3299, 1.6439,\n",
      "        1.9347, 1.8999, 2.0499, 1.9679, 2.1601, 2.4444, 2.3433, 1.9394, 1.8300,\n",
      "        1.8855, 1.9967, 1.9451, 1.9240, 2.1914, 2.4545, 1.8550, 1.4453, 1.7742,\n",
      "        1.8116, 1.9268, 1.6398, 2.2911, 2.0152, 1.4429, 1.7404, 1.9054, 1.7377,\n",
      "        1.9494, 1.6748, 2.3530, 2.3627, 1.3776, 1.3428, 1.7945, 1.6995, 1.6546,\n",
      "        1.7550, 2.4635, 2.1288, 1.3825, 1.5683, 1.7315, 1.7646, 1.7170, 1.9074,\n",
      "        2.0470, 2.0770, 1.4225, 1.4862, 1.7946, 1.6706, 1.6385, 1.8275, 2.2847,\n",
      "        2.3543, 1.4846, 1.3737, 1.5001, 1.6398, 1.5561, 1.7707, 2.5246, 2.5135,\n",
      "        1.8847, 1.7240, 1.6909, 1.4999, 1.6617, 1.8414, 2.3312, 2.3273, 1.5929,\n",
      "        1.4534, 1.6078, 1.8392, 1.5384, 1.9866, 2.2148, 2.4222, 1.6033, 1.1833,\n",
      "        1.2311, 1.4226, 1.3535, 1.8578, 2.0727, 2.3582, 1.5245, 1.3240, 1.4471,\n",
      "        1.6675, 1.6368, 1.7552, 1.9553, 2.0807], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0259,  0.0148,  0.0269,  ...,  0.0243, -0.0443, -0.0591],\n",
      "        [-0.0123, -0.0204,  0.0366,  ...,  0.0250,  0.0338,  0.0382],\n",
      "        [-0.0486,  0.0084,  0.0461,  ..., -0.0285,  0.0198, -0.0169],\n",
      "        ...,\n",
      "        [-0.0225,  0.0065,  0.0294,  ..., -0.0341,  0.0481, -0.0048],\n",
      "        [-0.0236, -0.0467, -0.0067,  ...,  0.0590,  0.0187,  0.0084],\n",
      "        [-0.0341,  0.0264,  0.0343,  ...,  0.0150,  0.0527, -0.0121]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.5796e-06,  6.9217e-04, -1.5964e-03,  ..., -1.5364e-04,\n",
      "         -2.9192e-03, -2.7409e-03],\n",
      "        [ 7.5962e-03, -1.2124e-03,  5.4978e-03,  ..., -3.1802e-03,\n",
      "          9.6218e-03,  1.1493e-02],\n",
      "        [ 2.6537e-03, -2.9250e-03,  4.7768e-04,  ..., -3.2449e-03,\n",
      "          3.6641e-03,  1.8413e-03],\n",
      "        ...,\n",
      "        [ 5.0311e-03, -3.6959e-03, -3.8882e-04,  ..., -1.2281e-03,\n",
      "         -2.0591e-03,  3.4481e-03],\n",
      "        [ 4.2291e-03, -2.8448e-03, -2.5524e-03,  ...,  5.2200e-04,\n",
      "          1.0510e-03,  3.9964e-04],\n",
      "        [-4.8445e-03,  7.5215e-04,  2.8268e-03,  ...,  2.8266e-03,\n",
      "         -1.2314e-03, -1.4022e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6807, 1.4307, 1.7636, 1.6830, 1.6657, 1.3030, 1.8228, 1.7511, 1.8039,\n",
      "        1.7640, 1.6534, 1.7153, 1.6557, 2.2761, 1.6481, 1.7467, 1.1294, 1.4099,\n",
      "        1.3730, 2.0052, 1.8995, 3.2229, 2.9052, 2.8106, 1.1104, 0.9371, 1.4466,\n",
      "        1.9059, 2.2824, 3.0584, 2.9910, 2.7032, 1.4649, 1.5942, 1.9909, 1.8187,\n",
      "        1.9011, 2.2834, 1.9998, 1.9099, 1.4398, 1.4613, 1.8863, 1.8782, 1.8544,\n",
      "        1.6487, 2.1006, 2.1299, 1.6681, 2.0846, 1.8235, 1.7084, 1.7897, 2.2552,\n",
      "        2.3046, 2.0202, 1.4018, 1.6632, 1.9240, 1.9929, 1.8908, 1.5534, 2.1737,\n",
      "        2.0413, 1.8911, 1.7152, 2.0667, 2.0785, 1.8501, 2.0928, 2.4556, 2.1443,\n",
      "        1.7476, 1.9452, 2.2823, 2.0459, 1.9958, 1.9189, 2.5719, 2.2809, 1.7947,\n",
      "        1.8825, 1.7319, 1.7296, 1.9794, 1.9553, 2.0040, 2.0618, 2.2655, 1.8380,\n",
      "        1.9133, 1.8738, 1.3205, 1.4627, 2.1007, 1.9249, 1.4870, 1.4953, 1.7241,\n",
      "        1.8854, 1.4263, 1.2668, 1.8915, 1.5358, 1.3256, 1.7200, 1.7187, 1.5546,\n",
      "        1.7890, 1.9629, 1.8596, 1.6556, 1.8440, 1.6380, 1.7662, 1.6628, 1.9546,\n",
      "        1.4378, 1.9767, 2.0761, 2.0126, 1.7900, 1.7213, 1.8054, 1.3985, 2.2964,\n",
      "        2.1524, 1.7854, 1.8095, 1.5997, 1.6582, 1.7066, 1.8571, 1.3782, 1.9123,\n",
      "        1.8164, 1.4106, 1.6430, 1.8560, 1.7432, 1.3322, 1.9971, 2.0423, 1.8300,\n",
      "        2.0353, 1.7606, 1.9358, 2.0164, 1.7584, 2.1436, 1.9919, 2.0209, 1.5934,\n",
      "        1.7357, 1.8210, 1.9488, 2.0299, 1.8821, 2.2560, 2.2505, 1.9087, 1.8767,\n",
      "        1.8714, 2.0468, 2.0214, 1.9669, 2.8013, 2.6124, 1.6555, 1.7199, 2.1441,\n",
      "        2.0517, 2.0197, 2.3078, 2.5453, 2.3597, 1.7734, 1.5072, 1.9539, 1.9709,\n",
      "        1.9837, 2.0163, 2.4425, 2.4088, 1.8796, 1.8776, 1.7694, 2.0101, 1.7557,\n",
      "        1.7838, 2.0930, 2.2539, 1.7841, 1.6222, 1.7333, 1.8630, 1.5031, 1.2235,\n",
      "        1.9648, 1.8491, 1.9308, 1.7543, 1.5493, 1.3137, 1.3323, 2.1866, 1.9203,\n",
      "        1.9818, 1.1345, 1.2856, 1.5352, 1.9543, 2.2014, 2.7564, 2.7764, 2.5335,\n",
      "        0.8825, 1.1847, 1.1243, 1.7192, 1.8325, 2.8481, 2.5568, 2.6210, 1.7238,\n",
      "        1.8357, 1.9251, 1.8922, 1.6434, 1.8027, 2.2908, 2.2678, 1.7471, 1.9001,\n",
      "        1.8337, 2.0221, 1.8860, 1.7256, 1.9931, 2.2125, 1.7633, 1.5414, 1.7713,\n",
      "        1.8878, 1.8525, 1.9498, 2.1735, 1.9772, 1.3362, 1.6338, 1.8312, 1.7498,\n",
      "        1.9454, 1.8467, 2.2588, 2.3220, 1.2796, 1.5677, 1.8584, 1.8536, 1.7605,\n",
      "        1.8537, 1.9970, 2.1384, 1.5676, 1.4576, 1.7917, 1.8079, 1.9201, 1.7471,\n",
      "        2.4884, 1.9781, 1.2839, 1.3820, 1.7189, 1.7621, 1.7019, 1.7062, 2.1437,\n",
      "        1.8855, 1.0064, 1.3232, 1.6782, 1.9594, 1.7435, 1.7301, 1.9685, 1.9647,\n",
      "        1.7858, 1.7421, 1.7448, 1.7540, 1.7910, 1.7134, 2.2966, 2.0933, 1.6607,\n",
      "        1.6581, 1.8232, 1.8057, 1.6983, 1.5933, 2.0818, 2.2333, 1.4301, 1.5214,\n",
      "        1.4677, 1.3381, 1.8000, 1.2431, 1.4636, 1.5289, 1.7736, 1.4357, 1.4008,\n",
      "        1.2863, 1.1181, 1.2514, 1.6129, 1.5635], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0169, -0.0057, -0.0141,  ...,  0.0508, -0.0265,  0.0463],\n",
      "        [-0.0454, -0.0390,  0.0142,  ...,  0.0315, -0.0147, -0.0418],\n",
      "        [-0.0209,  0.0322,  0.0220,  ...,  0.0055, -0.0152,  0.0257],\n",
      "        ...,\n",
      "        [ 0.0269, -0.0349, -0.0469,  ...,  0.0569,  0.0462, -0.0007],\n",
      "        [-0.0212, -0.0460, -0.0420,  ..., -0.0385,  0.0417, -0.0500],\n",
      "        [ 0.0321,  0.0170,  0.0496,  ...,  0.0440, -0.0270, -0.0421]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0032, -0.0022,  0.0043,  ...,  0.0016, -0.0033,  0.0032],\n",
      "        [ 0.0052, -0.0057,  0.0060,  ...,  0.0032, -0.0073,  0.0075],\n",
      "        [ 0.0006, -0.0014,  0.0014,  ...,  0.0043,  0.0013,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0027,  0.0049,  ...,  0.0003, -0.0023,  0.0027],\n",
      "        [-0.0006, -0.0019, -0.0001,  ..., -0.0019, -0.0003,  0.0032],\n",
      "        [-0.0059,  0.0039, -0.0048,  ..., -0.0038,  0.0055, -0.0074]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0157, 1.8019, 1.7274, 1.8741, 1.9388, 1.7895, 1.7706, 1.8351, 1.7671,\n",
      "        1.8792, 1.7175, 1.8124, 1.7629, 1.8196, 1.9635, 1.8564, 1.5505, 1.4399,\n",
      "        1.5271, 1.4752, 1.4976, 1.4683, 1.4122, 1.4246, 1.4647, 1.4120, 1.4259,\n",
      "        1.4778, 1.5551, 1.4697, 1.4932, 1.4528, 1.7829, 1.8704, 1.8244, 1.7359,\n",
      "        1.8889, 1.7156, 1.8566, 1.6853, 1.8763, 1.9312, 1.7365, 1.7814, 1.6848,\n",
      "        1.7775, 1.7902, 1.8418, 1.8000, 1.7595, 2.2120, 1.9459, 1.8395, 1.8849,\n",
      "        1.8604, 1.8619, 1.9627, 1.9837, 1.7978, 1.8078, 2.0202, 2.1606, 1.8939,\n",
      "        1.8782, 2.1685, 2.0821, 1.7798, 2.3172, 1.7910, 2.2750, 1.7583, 1.9139,\n",
      "        1.9447, 1.9798, 1.9792, 1.9179, 1.8361, 1.9155, 2.0399, 1.9603, 1.8064,\n",
      "        1.9186, 1.7186, 1.7648, 1.7463, 1.7378, 1.7486, 1.7444, 1.8732, 1.7531,\n",
      "        1.7841, 1.6663, 1.7606, 1.7739, 1.7353, 1.7846, 1.7696, 1.7919, 1.8656,\n",
      "        1.8842, 1.8700, 1.8850, 1.7273, 1.9108, 1.9347, 1.8787, 1.9328, 1.8454,\n",
      "        1.9100, 1.9108, 1.8761, 1.8545, 1.8353, 1.8158, 1.7044, 1.8782, 1.8593,\n",
      "        1.8788, 1.7721, 1.8355, 1.7718, 1.8029, 1.8199, 1.7489, 1.8203, 1.7040,\n",
      "        1.7886, 1.8478, 1.8516, 1.8242, 1.8020, 1.7064, 1.8558, 1.7449, 1.7347,\n",
      "        1.9073, 1.7177, 1.8140, 1.7344, 1.7803, 1.7619, 1.8497, 1.9177, 1.7890,\n",
      "        1.6578, 1.5925, 1.7129, 1.6639, 1.7793, 1.6799, 1.7249, 1.6437, 1.6969,\n",
      "        1.7042, 1.7601, 1.7942, 1.7408, 1.7412, 1.6820, 1.7866, 1.8983, 1.8643,\n",
      "        1.8802, 1.8284, 2.0615, 2.0613, 1.8835, 1.8383, 1.9284, 1.8492, 1.9284,\n",
      "        1.9081, 1.9335, 1.8283, 1.8829, 1.9498, 1.8861, 2.0301, 1.9009, 1.9513,\n",
      "        2.0681, 1.8736, 1.9404, 1.9392, 1.9203, 1.9185, 1.9599, 1.9055, 1.9942,\n",
      "        1.9380, 1.9500, 1.9337, 1.3754, 1.3890, 1.3077, 1.4203, 1.2986, 1.3203,\n",
      "        1.4207, 1.3317, 1.3794, 1.3424, 1.5064, 1.4101, 1.4242, 1.4025, 1.3395,\n",
      "        1.3613, 1.5976, 1.5292, 1.4052, 1.5621, 1.5117, 1.5260, 1.5065, 1.5066,\n",
      "        1.4992, 1.3663, 1.4227, 1.5165, 1.4228, 1.4534, 1.5027, 1.4374, 2.1221,\n",
      "        1.9334, 2.0105, 1.8866, 2.1236, 1.9795, 1.8556, 1.8910, 1.8606, 1.9011,\n",
      "        1.7899, 2.1173, 2.1776, 1.9726, 1.9106, 1.8646, 1.7307, 1.8132, 1.7280,\n",
      "        1.7809, 1.7179, 1.6925, 1.7793, 1.7728, 1.7715, 1.7731, 1.8334, 1.6572,\n",
      "        1.6935, 1.7497, 1.7228, 1.8197, 1.8315, 1.7464, 1.8162, 1.7579, 1.7040,\n",
      "        1.6778, 1.7605, 1.7791, 1.7488, 1.7348, 1.7164, 1.8160, 1.8196, 1.8014,\n",
      "        1.6610, 1.7425, 1.8720, 1.9847, 1.8933, 2.0537, 1.8459, 1.8784, 1.9121,\n",
      "        1.8506, 1.7790, 1.9241, 1.8651, 1.8870, 1.9662, 2.0304, 1.9246, 1.9625,\n",
      "        1.9097, 1.8095, 1.8948, 1.8818, 1.9004, 2.0653, 1.8721, 1.9872, 1.8330,\n",
      "        1.9091, 1.9717, 1.8692, 1.9356, 1.9202, 1.8306, 1.9235, 1.7483, 1.7828,\n",
      "        1.8043, 1.6252, 2.0728, 1.7454, 1.6731, 1.8337, 1.7958, 1.7485, 1.9289,\n",
      "        1.6809, 1.7414, 1.7384, 1.7585, 1.8604], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0159,  0.0425, -0.0004,  ...,  0.0434,  0.0239, -0.0472],\n",
      "        [-0.0457,  0.0283, -0.0137,  ...,  0.0103,  0.0145,  0.0185],\n",
      "        [ 0.0314,  0.0251, -0.0137,  ...,  0.0279, -0.0114, -0.0452],\n",
      "        ...,\n",
      "        [ 0.0036,  0.0490, -0.0268,  ..., -0.0008,  0.0233,  0.0407],\n",
      "        [ 0.0436, -0.0079,  0.0488,  ...,  0.0146,  0.0006, -0.0011],\n",
      "        [-0.0373, -0.0127,  0.0070,  ..., -0.0364, -0.0175,  0.0066]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0132, -0.0117,  0.0111,  ..., -0.0087,  0.0089, -0.0112],\n",
      "        [-0.0043,  0.0034, -0.0036,  ...,  0.0065,  0.0031,  0.0009],\n",
      "        [ 0.0119, -0.0123,  0.0112,  ..., -0.0110,  0.0116, -0.0129],\n",
      "        ...,\n",
      "        [-0.0038,  0.0044, -0.0047,  ...,  0.0013,  0.0019,  0.0044],\n",
      "        [-0.0033,  0.0013, -0.0044,  ...,  0.0062, -0.0023,  0.0010],\n",
      "        [-0.0040,  0.0009, -0.0017,  ...,  0.0082, -0.0099,  0.0033]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6067, 1.7704, 1.6566, 1.8597, 1.7667, 1.6433, 1.6774, 1.5701, 1.7817,\n",
      "        1.8153, 1.3653, 1.6064, 1.7092, 1.6978, 1.7449, 1.6615, 1.8366, 1.7884,\n",
      "        1.6595, 1.8355, 1.8281, 1.6543, 1.6978, 1.5922, 1.6456, 1.5314, 1.6054,\n",
      "        1.6990, 1.7374, 1.8622, 1.6635, 1.4337, 1.6517, 1.4495, 1.6933, 1.7534,\n",
      "        1.6826, 1.6821, 1.6162, 1.6630, 1.6874, 1.6798, 1.7752, 1.6176, 1.7568,\n",
      "        1.5831, 1.6128, 1.5385, 1.6769, 1.7641, 1.7594, 1.7489, 1.5140, 1.6572,\n",
      "        1.5922, 1.6767, 1.5473, 1.7119, 1.7849, 1.6380, 1.7101, 1.7547, 5.9812,\n",
      "        1.7400, 1.7186, 1.6974, 1.5837, 1.7088, 1.7222, 1.8712, 1.7129, 1.8223,\n",
      "        1.5689, 1.7573, 1.6328, 1.7721, 1.7698, 1.6757, 1.7404, 1.7506, 1.5328,\n",
      "        1.6649, 1.7036, 1.6356, 1.5937, 1.7000, 1.6787, 1.7230, 1.4770, 1.8228,\n",
      "        1.8348, 1.6120, 1.8995, 1.6431, 1.7100, 1.7357, 1.7318, 1.8198, 1.7678,\n",
      "        1.8610, 1.9062, 1.6743, 1.7490, 1.6111, 1.7737, 1.8217, 1.5183, 1.6614,\n",
      "        1.7369, 1.6534, 1.6528, 1.6588, 1.6085, 1.5338, 1.8365, 1.5659, 1.6737,\n",
      "        1.7157, 1.5990, 1.8325, 1.6130, 1.7011, 1.7031, 1.7500, 1.6300, 1.8079,\n",
      "        1.6797, 1.6583, 1.5062, 1.7787, 1.4906, 1.8386, 1.7237, 1.7641, 1.7320,\n",
      "        1.6439, 1.6969, 1.6698, 1.5787, 1.7563, 1.5464, 1.6715, 1.7082, 1.6443,\n",
      "        1.6638, 1.8756, 1.6881, 1.6553, 1.6846, 1.6713, 1.9620, 1.7405, 1.9372,\n",
      "        1.7599, 1.8437, 1.6799, 1.5606, 1.7587, 1.9374, 1.6921, 1.6447, 1.7213,\n",
      "        1.6494, 1.7068, 1.7641, 1.6637, 1.6982, 1.7280, 1.7438, 1.7407, 1.5813,\n",
      "        1.7129, 1.6833, 1.6193, 1.7266, 1.7137, 1.5375, 1.7093, 1.7323, 1.6162,\n",
      "        1.6115, 1.6578, 1.6698, 1.7406, 1.6051, 1.6396, 1.5645, 1.6327, 1.6098,\n",
      "        1.6609, 1.6683, 1.6097, 2.5952, 1.5879, 1.4720, 1.6662, 1.6685, 1.7379,\n",
      "        1.6936, 1.8702, 1.7704, 1.7825, 1.7057, 1.5661, 1.8266, 1.6566, 1.7289,\n",
      "        1.3421, 1.7034, 1.6460, 1.6635, 1.6247, 1.5934, 1.7699, 1.6676, 1.8142,\n",
      "        1.6528, 1.8195, 1.4090, 1.5664, 1.5072, 1.6499, 1.6215, 1.7249, 1.7308,\n",
      "        1.6453, 1.6428, 1.5970, 1.6127, 1.6770, 1.6305, 1.5997, 1.6801, 1.7204,\n",
      "        1.5961, 1.6472, 1.6017, 1.5721, 1.7311, 1.7864, 1.5595, 1.6808, 1.6743,\n",
      "        1.6613, 1.5589, 1.8072, 1.6187, 1.6327, 1.7599, 1.7200, 1.3839, 1.6988,\n",
      "        1.6993, 1.5867, 1.6323, 1.7548, 1.6845, 1.5426, 1.4912, 1.7405, 1.6063,\n",
      "        1.6585, 1.7396, 1.6008, 1.6472, 1.7417, 1.7493, 2.0602, 1.5561, 1.6574,\n",
      "        1.5844, 1.7866, 1.6164, 1.6381, 1.8137, 1.6449, 1.7252, 1.5740, 1.7416,\n",
      "        1.8183, 1.6935, 1.6699, 1.7051, 1.7405, 1.7519, 1.6943, 1.7957, 1.6995,\n",
      "        1.4572, 1.6589, 1.6567, 1.8010, 1.7020, 1.7282, 1.4622, 1.7940, 1.7505,\n",
      "        1.6543, 1.5468, 1.8006, 1.7825, 1.6824, 1.6195, 1.5545, 1.6330, 1.6590,\n",
      "        1.7078, 1.6709, 1.5965, 1.6844, 1.6884, 1.4929, 1.6187, 1.7026, 1.6952,\n",
      "        1.7263, 1.7908, 1.6135, 1.6903, 1.6452], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0472, -0.0120,  0.0471,  ...,  0.0190, -0.0100, -0.0648],\n",
      "        [-0.0455,  0.0083,  0.0616,  ..., -0.0499,  0.0150,  0.0438],\n",
      "        [-0.0222,  0.0209, -0.0164,  ...,  0.0532,  0.0111, -0.0110],\n",
      "        ...,\n",
      "        [-0.0215, -0.0431, -0.0165,  ...,  0.0482,  0.0448,  0.0186],\n",
      "        [-0.0470, -0.0289,  0.0128,  ..., -0.0127,  0.0568, -0.0021],\n",
      "        [-0.0586, -0.0109,  0.0397,  ...,  0.0491,  0.0495,  0.0369]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 6.0570e-03,  3.0275e-03, -3.6607e-03,  ..., -2.5852e-03,\n",
      "          4.8957e-03,  1.4018e-03],\n",
      "        [-7.3015e-03, -1.0378e-02,  9.9925e-03,  ...,  9.0027e-03,\n",
      "         -8.6360e-03, -9.7374e-03],\n",
      "        [-1.6266e-03, -2.6649e-03,  2.2445e-03,  ...,  2.0780e-03,\n",
      "         -2.9271e-03, -1.1321e-03],\n",
      "        ...,\n",
      "        [ 6.6193e-03, -8.9959e-03, -1.2115e-03,  ...,  2.0549e-03,\n",
      "          9.2343e-04,  4.1864e-03],\n",
      "        [ 2.0050e-03,  5.2059e-03, -6.5143e-03,  ..., -4.5720e-03,\n",
      "          5.1839e-03,  6.4788e-03],\n",
      "        [-1.9403e-03,  1.4193e-05,  1.7447e-03,  ...,  7.7181e-03,\n",
      "         -3.9025e-03, -1.2568e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6496, 1.5361, 4.6478,  ..., 1.6842, 1.6236, 1.7560],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0188, -0.0192,  0.0209,  ..., -0.0158,  0.0169, -0.0118],\n",
      "        [-0.0078, -0.0274,  0.0226,  ...,  0.0147,  0.0227,  0.0271],\n",
      "        [ 0.0026,  0.0237,  0.0254,  ...,  0.0171,  0.0172, -0.0031],\n",
      "        ...,\n",
      "        [-0.0045,  0.0039, -0.0002,  ...,  0.0137,  0.0229, -0.0244],\n",
      "        [ 0.0156,  0.0131, -0.0208,  ...,  0.0125,  0.0179,  0.0068],\n",
      "        [ 0.0033,  0.0057, -0.0069,  ..., -0.0079, -0.0142, -0.0046]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0040,  0.0008,  0.0010,  ..., -0.0017,  0.0010, -0.0056],\n",
      "        [-0.0035,  0.0043,  0.0021,  ..., -0.0053,  0.0013, -0.0018],\n",
      "        [ 0.0073, -0.0046, -0.0065,  ...,  0.0027, -0.0114,  0.0109],\n",
      "        ...,\n",
      "        [-0.0066,  0.0060,  0.0041,  ..., -0.0058,  0.0056, -0.0062],\n",
      "        [-0.0091,  0.0114,  0.0024,  ..., -0.0120, -0.0028, -0.0048],\n",
      "        [ 0.0022,  0.0029, -0.0090,  ..., -0.0045, -0.0136,  0.0111]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 3.5978,  3.7346,  3.5381,  3.7899,  3.9584,  3.5981,  3.6625,  3.5402,\n",
      "         3.6029,  3.7119,  3.4955,  3.8437,  3.7663,  3.5412,  3.8481,  3.6584,\n",
      "         3.5921,  3.5433,  3.5409,  3.6027,  3.7802,  3.9287,  3.6090,  3.6506,\n",
      "         3.4038,  3.6587,  3.5564,  3.6223,  3.6069,  3.6590,  3.6454,  4.5075,\n",
      "         3.7538,  3.5116,  3.6616,  3.6413,  3.6242,  3.4911,  3.5085,  3.8746,\n",
      "         3.5258,  3.6481,  3.6436,  3.5151,  3.9298,  3.5416,  5.2902,  3.5661,\n",
      "         3.5179,  3.9366,  3.7410,  3.6541,  3.4195,  3.6123,  3.6246,  3.5183,\n",
      "         3.8264,  3.8732,  4.0598,  3.5105,  3.5912,  3.8581, 12.3648,  3.7725,\n",
      "         3.8288,  3.7026,  3.7497,  3.5961,  3.8571,  3.8401,  3.7850,  3.7244,\n",
      "         3.4411,  3.9100,  3.7238,  3.5400,  3.5454,  3.5094,  3.6529,  3.4337,\n",
      "         3.6327,  3.5656,  3.7361,  3.7288,  3.6244,  3.5133,  3.6288,  3.7351,\n",
      "         3.3809,  3.7915,  3.8422,  3.7880,  4.4630,  3.7082,  3.7531,  3.7820,\n",
      "         3.6065,  3.8141,  3.7373,  3.8634,  3.7326,  3.7730,  3.7556,  3.5779,\n",
      "         3.5191,  3.8314,  3.6974,  3.6882,  3.7212,  3.6480,  3.7760,  3.8429,\n",
      "         3.7247,  3.6251,  3.7463,  3.4994,  3.7268,  3.7043,  3.5900,  3.7145,\n",
      "         3.7160,  3.6867,  3.7164,  3.8160,  3.7742,  3.6146,  3.6047,  3.6242,\n",
      "         3.6459,  3.4500,  3.4470,  3.6631,  3.8881,  3.7588,  3.6348,  3.6504,\n",
      "         3.6191,  3.7989,  3.6424,  3.6928,  3.5461,  3.5723,  3.5347,  3.7543,\n",
      "         3.7549,  3.7775,  3.7423,  3.6673,  3.7877,  3.7641,  5.2090,  3.6406,\n",
      "         3.6682,  3.7550,  3.9528,  3.6413,  3.7637,  3.9006,  4.0412,  3.6411,\n",
      "         3.7118,  3.8429,  3.5608,  3.6601,  3.7993,  3.9963,  3.5924,  3.6127,\n",
      "         3.5586,  3.6035,  3.4920,  3.5676,  3.5810,  3.8952,  3.6070,  3.4722,\n",
      "         3.6692,  3.6947,  3.8538,  3.8329,  3.6004,  3.5945,  3.6628,  3.5497,\n",
      "         3.7059,  3.7592,  3.5712,  3.7742,  3.7684,  3.7493,  3.6094,  3.3878,\n",
      "         6.3363,  3.6086,  3.6106,  3.5219,  3.6887,  3.7009,  3.7988,  3.8544,\n",
      "         3.5942,  3.6564,  3.8282,  3.6658,  3.7849,  3.7637,  3.5155,  3.2757,\n",
      "         3.6566,  3.5658,  3.7974,  3.5733,  3.5439,  4.2014,  3.4494,  3.6704,\n",
      "         3.6253,  3.7687,  3.3508,  3.5175,  3.4852,  3.6059,  3.9774,  3.6852,\n",
      "         3.5789,  3.7485,  3.5577,  3.7161,  3.6207,  3.4699,  3.5819,  3.4720,\n",
      "         3.6089,  3.7174,  3.7577,  3.8413,  3.7654,  3.4543,  3.7390,  3.8679,\n",
      "         3.5843,  3.6458,  3.5589,  3.5821,  3.3914,  3.5220,  3.6077,  3.5711,\n",
      "         3.7104,  3.8168,  3.6794,  3.7184,  3.9078,  3.4899,  3.6578,  3.8066,\n",
      "         3.5552,  3.3275,  3.2465,  4.0034,  3.6701,  3.7006,  3.6641,  3.6192,\n",
      "         3.8232,  3.7264,  3.9042,  5.4846,  3.5536,  3.5843,  3.6456,  3.5834,\n",
      "         3.6121,  3.8499,  3.6785,  3.7908,  3.7195,  3.4159,  3.8028,  3.6927,\n",
      "         3.5121,  3.6617,  3.6017,  3.6452,  3.8839,  3.8682,  3.7742,  3.6330,\n",
      "         3.5820,  3.9323,  3.6208,  3.8067,  3.6217,  3.9814,  3.4779,  3.5567,\n",
      "         3.6281,  3.6431,  3.4831,  3.6019,  3.5212,  3.6700,  3.6451,  3.5738,\n",
      "         3.6730,  3.4535,  3.8890,  3.5203,  3.5831,  3.8185,  3.5114,  3.5989,\n",
      "         3.5809,  3.6285,  3.5411,  4.0875,  3.7671,  3.6426,  3.4556,  3.8920],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0119,  0.0440, -0.0439,  ...,  0.0091, -0.0309, -0.0630],\n",
      "        [-0.0040,  0.0667,  0.0104,  ...,  0.0342,  0.0648, -0.0382],\n",
      "        [-0.0625,  0.0204,  0.0409,  ...,  0.0281,  0.0505,  0.0298],\n",
      "        ...,\n",
      "        [ 0.0340,  0.0382,  0.0151,  ..., -0.0180,  0.0185, -0.0409],\n",
      "        [ 0.0413,  0.0156, -0.0023,  ..., -0.0326, -0.0210,  0.0582],\n",
      "        [ 0.0317,  0.0195,  0.0134,  ...,  0.0512,  0.0514,  0.0094]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0043, -0.0031, -0.0023,  ..., -0.0022,  0.0023, -0.0037],\n",
      "        [ 0.0051,  0.0051,  0.0021,  ...,  0.0051, -0.0038,  0.0052],\n",
      "        [-0.0120, -0.0129, -0.0129,  ..., -0.0140,  0.0123, -0.0121],\n",
      "        ...,\n",
      "        [-0.0099, -0.0092, -0.0098,  ..., -0.0091,  0.0091, -0.0098],\n",
      "        [ 0.0030,  0.0028,  0.0043,  ...,  0.0023, -0.0025,  0.0028],\n",
      "        [-0.0032, -0.0038, -0.0032,  ..., -0.0040,  0.0040, -0.0035]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6347, 1.7477, 1.7440, 1.8784, 1.9181, 1.6816, 2.1900, 2.5722, 1.5431,\n",
      "        1.7020, 1.9422, 2.0768, 1.9874, 1.6498, 2.4465, 2.1087, 1.5202, 1.3482,\n",
      "        1.7116, 1.6758, 2.5338, 3.4355, 3.6089, 4.1409, 1.0344, 1.4874, 1.6849,\n",
      "        1.9782, 2.3204, 3.5023, 3.6239, 3.8123, 1.3655, 1.3264, 0.9898, 1.4213,\n",
      "        1.8973, 1.9518, 1.6483, 1.8330, 1.1164, 0.7407, 1.3872, 1.2660, 1.2936,\n",
      "        1.4150, 1.6957, 1.8440, 1.5375, 1.6607, 2.0458, 1.9637, 2.2102, 3.5229,\n",
      "        3.3109, 3.5561, 1.3116, 1.7409, 1.9969, 1.8843, 2.2777, 3.4235, 3.2384,\n",
      "        3.3016, 1.6208, 2.2510, 1.6738, 1.3802, 1.7906, 1.7044, 1.9756, 2.0585,\n",
      "        1.9599, 1.9064, 1.7661, 1.7074, 1.4144, 1.8839, 2.0782, 2.1382, 1.3944,\n",
      "        1.1502, 1.3702, 1.5767, 2.2107, 2.0483, 1.8678, 2.0895, 1.5775, 1.5028,\n",
      "        1.2353, 1.2250, 1.3722, 1.6780, 1.9683, 1.9624, 1.7231, 1.7372, 1.8042,\n",
      "        1.7375, 1.6263, 1.6131, 2.3609, 2.2448, 2.0275, 2.0645, 1.9987, 1.5553,\n",
      "        2.0495, 1.4765, 2.4698, 2.1948, 1.6977, 1.5273, 1.4997, 1.0005, 1.4125,\n",
      "        2.1094, 1.8512, 1.8882, 1.6380, 1.2772, 0.9755, 1.7383, 1.4337, 1.5577,\n",
      "        1.9070, 1.8767, 1.4304, 1.5129, 1.3566, 1.3904, 1.2466, 1.8215, 1.7879,\n",
      "        1.3408, 1.7547, 1.6808, 1.6451, 1.2966, 1.6422, 1.3191, 1.9997, 1.8823,\n",
      "        1.0547, 1.1075, 1.0863, 1.6316, 1.4015, 1.5369, 1.7903, 2.3059, 1.5980,\n",
      "        1.1232, 1.5287, 1.4182, 2.1047, 1.9431, 1.8165, 1.9708, 1.8390, 1.3895,\n",
      "        1.0250, 1.2817, 1.7975, 1.5367, 1.7394, 1.9296, 1.2120, 1.3922, 1.4866,\n",
      "        1.5159, 1.4446, 1.9964, 1.9117, 1.9593, 2.1112, 2.3143, 2.2221, 1.9694,\n",
      "        1.9762, 1.7550, 2.3741, 2.3431, 1.7834, 1.8120, 1.9284, 1.8660, 1.6517,\n",
      "        2.6516, 2.3782, 2.3300, 1.8451, 1.2319, 1.6000, 1.2749, 1.9332, 2.0454,\n",
      "        1.7667, 1.9873, 0.9248, 1.3314, 1.0776, 1.7732, 1.4615, 1.4835, 1.7375,\n",
      "        2.0057, 1.9932, 1.7338, 1.5357, 1.5910, 1.7158, 1.8224, 1.9420, 2.2921,\n",
      "        1.7647, 1.9700, 2.0115, 1.8213, 1.6438, 2.1969, 1.5287, 2.2566, 1.0362,\n",
      "        1.2024, 1.6454, 1.9614, 1.8456, 1.4230, 1.8704, 1.8272, 1.6628, 1.0373,\n",
      "        1.1278, 1.2387, 1.3742, 2.0040, 1.7978, 2.2198, 1.9867, 1.7284, 1.8440,\n",
      "        1.8674, 1.7964, 2.3654, 2.9857, 3.0239, 2.0915, 2.0161, 2.0863, 1.9179,\n",
      "        2.4762, 2.3602, 3.0395, 2.9668, 1.5852, 1.3298, 1.1664, 1.0862, 2.1685,\n",
      "        1.5791, 1.9750, 1.8797, 1.7756, 1.0227, 1.5319, 2.0977, 1.5381, 1.9972,\n",
      "        1.7844, 1.9191, 1.6474, 1.3457, 1.5272, 1.2283, 1.2762, 1.8996, 1.8916,\n",
      "        1.9457, 1.5386, 1.5242, 1.3582, 1.7246, 1.8551, 1.4814, 1.7626, 1.9146,\n",
      "        1.6269, 1.2864, 1.2301, 1.7471, 1.9743, 2.2713, 2.0534, 2.0212, 1.9128,\n",
      "        1.3160, 1.3228, 1.2712, 1.6712, 1.4660, 1.9638, 1.9478, 2.1536, 1.4944,\n",
      "        1.5921, 1.9516, 2.2610, 2.1663, 1.8905, 2.2260, 1.0674, 1.4787, 1.5075,\n",
      "        1.6366, 1.5711, 1.8105, 1.9937, 2.2593], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0025,  0.0387, -0.0509,  ...,  0.0368,  0.0354, -0.0507],\n",
      "        [ 0.0435,  0.0055,  0.0279,  ..., -0.0392, -0.0292,  0.0567],\n",
      "        [-0.0303,  0.0089, -0.0509,  ..., -0.0074, -0.0504,  0.0337],\n",
      "        ...,\n",
      "        [ 0.0179, -0.0153, -0.0026,  ..., -0.0056,  0.0040, -0.0458],\n",
      "        [-0.0420,  0.0106, -0.0107,  ...,  0.0197, -0.0091,  0.0046],\n",
      "        [ 0.0196,  0.0323,  0.0104,  ..., -0.0458, -0.0011,  0.0123]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0019, -0.0043,  0.0016,  ...,  0.0109,  0.0039,  0.0089],\n",
      "        [ 0.0011,  0.0038,  0.0030,  ...,  0.0043, -0.0035, -0.0002],\n",
      "        [ 0.0049, -0.0019, -0.0025,  ..., -0.0007, -0.0039, -0.0051],\n",
      "        ...,\n",
      "        [-0.0013,  0.0034, -0.0042,  ..., -0.0021, -0.0040, -0.0028],\n",
      "        [ 0.0014, -0.0047,  0.0082,  ...,  0.0045,  0.0014,  0.0006],\n",
      "        [ 0.0024,  0.0011, -0.0042,  ..., -0.0029, -0.0003,  0.0046]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5245, 1.6975, 1.9299, 2.0781, 1.8549, 1.7202, 2.2554, 2.1172, 1.6854,\n",
      "        1.6531, 1.7998, 1.7827, 2.0010, 2.2508, 2.0930, 2.0252, 1.4865, 1.3871,\n",
      "        1.6291, 1.8161, 2.2119, 2.5397, 2.6790, 2.5864, 0.8706, 1.4072, 1.6221,\n",
      "        1.7883, 1.9958, 2.4665, 2.6825, 2.2701, 1.5708, 1.3263, 1.4264, 1.2355,\n",
      "        1.0406, 1.2159, 1.5979, 1.7013, 1.5297, 1.5274, 1.6032, 1.4442, 1.9554,\n",
      "        1.8687, 1.6010, 1.5876, 1.4694, 1.6311, 1.9243, 2.0949, 2.0128, 2.5624,\n",
      "        2.7472, 2.8153, 1.4129, 1.7926, 2.0883, 1.7827, 2.0699, 2.4182, 2.6884,\n",
      "        2.6699, 1.7503, 2.0008, 1.7601, 1.5793, 1.5342, 2.0474, 1.9118, 1.9521,\n",
      "        1.7818, 1.9673, 1.8984, 2.0256, 1.7396, 1.4750, 1.9399, 1.9343, 1.8638,\n",
      "        1.4892, 1.6782, 1.3121, 1.1532, 1.2651, 1.8245, 1.7497, 1.6372, 1.7515,\n",
      "        1.5207, 1.7802, 2.2907, 1.9060, 1.7985, 1.8242, 1.7645, 1.5966, 1.7376,\n",
      "        1.8801, 2.1194, 2.0621, 2.1553, 2.0969, 1.8941, 2.0255, 2.0274, 1.8570,\n",
      "        1.5999, 1.7161, 2.0610, 2.0709, 1.6884, 1.7673, 1.5846, 1.9950, 2.1360,\n",
      "        1.2862, 1.7542, 1.8084, 1.7043, 1.5723, 1.8168, 1.3150, 1.2621, 2.0770,\n",
      "        1.7892, 1.7494, 1.5783, 1.6578, 1.7423, 1.7573, 1.7332, 1.3659, 1.7203,\n",
      "        1.7726, 1.5868, 1.7491, 1.6516, 1.5526, 1.5757, 1.8394, 1.8900, 1.8075,\n",
      "        1.4539, 1.4231, 1.7140, 1.4055, 2.1165, 1.8807, 1.7170, 1.5327, 1.7660,\n",
      "        1.6162, 1.4933, 1.3670, 1.1465, 1.2178, 1.6887, 1.7065, 1.6174, 1.4434,\n",
      "        1.5341, 1.5369, 1.2011, 2.1911, 1.6967, 1.6811, 2.0300, 1.8479, 1.5880,\n",
      "        1.5005, 1.8036, 1.2361, 1.7341, 2.0779, 1.9518, 2.0751, 2.0547, 1.8949,\n",
      "        1.4770, 2.2767, 2.2502, 2.2742, 1.8926, 1.9692, 1.9572, 1.9665, 2.2287,\n",
      "        1.3688, 2.1553, 2.1323, 1.7613, 1.5412, 1.4584, 1.9233, 1.2550, 1.1386,\n",
      "        1.6293, 1.7756, 1.6617, 1.7082, 1.7561, 1.2056, 1.7694, 2.1064, 1.5859,\n",
      "        1.7430, 1.6906, 1.8633, 1.7222, 1.6242, 1.4546, 1.9341, 1.9836, 2.0377,\n",
      "        1.9926, 1.8129, 1.7769, 1.5742, 1.6722, 1.4728, 1.6782, 1.9924, 1.2131,\n",
      "        1.5588, 1.6058, 1.1622, 1.1025, 2.0396, 1.7124, 1.6954, 2.1257, 1.4154,\n",
      "        1.4717, 1.8066, 1.9798, 1.2484, 1.7546, 1.8658, 2.1673, 1.8277, 2.0408,\n",
      "        1.7670, 1.8952, 1.7611, 2.2889, 2.4796, 1.8973, 1.9640, 1.8850, 2.0279,\n",
      "        1.7709, 1.7424, 2.4595, 2.2678, 1.7221, 1.6565, 1.8368, 1.9912, 1.1424,\n",
      "        1.9695, 1.6921, 1.8123, 1.8643, 1.6158, 1.5331, 1.0821, 1.9824, 1.1849,\n",
      "        1.7472, 1.8726, 1.7063, 1.6032, 1.7690, 1.7191, 2.2318, 1.1745, 1.7745,\n",
      "        1.8548, 1.7244, 1.8118, 1.5406, 1.2787, 1.1582, 2.0800, 1.7851, 1.7852,\n",
      "        1.9567, 1.6886, 1.6113, 1.3648, 1.2227, 1.1333, 1.9322, 1.8017, 1.7081,\n",
      "        1.6443, 1.8035, 1.5951, 1.6620, 2.1089, 1.7495, 1.8790, 1.8306, 1.6801,\n",
      "        1.6227, 1.4411, 1.1991, 1.2847, 1.8727, 1.8219, 1.5202, 1.5023, 1.6571,\n",
      "        1.6829, 2.0761, 2.1727, 1.7920, 1.7935], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0365,  0.0170,  0.0517,  ...,  0.0296, -0.0499,  0.0468],\n",
      "        [ 0.0041, -0.0661,  0.0270,  ..., -0.0088, -0.0373, -0.0141],\n",
      "        [-0.0431,  0.0020, -0.0050,  ..., -0.0213, -0.0488,  0.0181],\n",
      "        ...,\n",
      "        [ 0.0371, -0.0167, -0.0602,  ...,  0.0499, -0.0259,  0.0305],\n",
      "        [ 0.0332,  0.0362,  0.0341,  ...,  0.0393, -0.0085,  0.0491],\n",
      "        [-0.0145,  0.0207,  0.0197,  ...,  0.0223, -0.0313,  0.0073]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0009,  0.0014,  0.0013,  ...,  0.0022, -0.0026,  0.0018],\n",
      "        [ 0.0043,  0.0050, -0.0057,  ..., -0.0031,  0.0030,  0.0020],\n",
      "        [ 0.0022, -0.0014, -0.0009,  ..., -0.0037,  0.0037, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0007,  0.0049,  0.0013,  ..., -0.0015, -0.0001,  0.0031],\n",
      "        [ 0.0053,  0.0018, -0.0070,  ..., -0.0055,  0.0056, -0.0062],\n",
      "        [-0.0025, -0.0031,  0.0016,  ...,  0.0012, -0.0009, -0.0074]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0690, 2.1805, 2.1064, 2.1968, 2.1148, 2.0781, 2.2111, 2.2303, 2.1953,\n",
      "        2.2271, 2.1345, 2.0434, 2.2094, 2.3120, 2.1038, 2.0597, 1.9077, 2.0418,\n",
      "        1.9640, 1.9909, 1.9751, 2.0465, 2.0049, 1.9823, 2.0129, 1.9297, 1.9487,\n",
      "        1.9861, 1.9161, 1.9476, 2.0084, 1.9176, 2.3021, 2.0881, 2.2089, 2.0301,\n",
      "        2.2356, 2.1912, 2.2734, 2.1094, 2.1587, 2.1568, 2.1715, 2.3178, 2.2220,\n",
      "        2.2550, 2.2032, 2.2023, 2.1346, 2.0642, 2.0549, 2.0358, 2.1155, 2.1078,\n",
      "        2.0502, 2.1140, 2.0746, 2.0977, 2.0083, 2.1275, 2.1354, 2.2017, 2.1002,\n",
      "        2.0974, 2.0317, 2.1638, 2.0417, 2.1552, 2.2488, 2.1295, 2.0652, 2.0989,\n",
      "        2.0650, 2.1309, 2.1299, 2.0950, 2.0977, 2.0721, 2.0711, 2.0670, 2.1596,\n",
      "        2.1523, 2.1397, 2.1851, 1.9884, 2.1221, 2.0306, 1.9780, 2.2299, 2.0810,\n",
      "        2.0530, 1.9095, 2.1388, 1.9889, 2.2374, 2.0446, 2.2720, 2.0637, 1.9591,\n",
      "        2.0815, 2.1667, 2.0565, 2.0377, 2.0708, 2.1183, 2.1146, 2.0687, 2.1250,\n",
      "        2.1187, 2.0457, 2.1694, 2.1048, 2.1691, 2.1279, 2.1808, 2.1612, 2.1523,\n",
      "        2.1251, 2.0289, 2.1196, 2.2645, 2.1825, 2.1937, 2.1677, 2.0967, 2.1340,\n",
      "        2.0640, 2.1805, 2.2778, 2.3396, 2.3912, 2.2091, 2.4193, 2.3456, 2.2938,\n",
      "        2.3410, 2.2188, 2.4043, 2.1221, 2.3714, 2.2484, 2.4039, 2.2215, 2.4013,\n",
      "        1.8934, 2.0990, 2.1666, 2.2665, 1.9122, 2.0803, 2.1170, 2.0539, 1.9435,\n",
      "        2.1336, 2.0752, 2.0727, 2.1337, 2.1609, 1.9861, 2.1800, 1.9576, 2.0756,\n",
      "        2.1382, 2.1915, 2.0630, 1.9736, 2.0681, 2.0489, 2.0983, 2.0747, 2.0699,\n",
      "        2.0243, 2.1848, 1.9970, 2.0555, 2.1470, 2.0819, 2.1529, 2.1790, 2.1989,\n",
      "        2.2926, 2.0924, 2.1828, 2.2258, 2.0958, 2.3159, 2.1153, 2.1176, 2.3028,\n",
      "        2.1191, 2.0850, 2.1687, 1.9733, 1.9516, 2.0247, 2.0744, 2.0775, 2.1368,\n",
      "        2.0309, 2.0059, 2.0959, 1.9675, 2.1743, 2.0268, 2.0992, 2.0069, 2.0272,\n",
      "        1.9856, 2.1822, 2.0036, 2.1323, 2.0709, 2.1564, 2.1784, 2.0548, 2.1251,\n",
      "        2.1659, 2.3199, 2.2956, 2.1816, 2.1452, 2.1379, 2.1985, 2.2161, 1.9607,\n",
      "        1.9714, 2.0560, 1.9254, 1.9312, 2.0330, 2.0099, 1.8535, 2.0440, 1.9928,\n",
      "        2.0556, 1.9512, 1.9183, 2.0444, 2.0478, 1.9960, 2.2065, 2.1725, 2.0962,\n",
      "        2.3075, 2.1297, 2.3074, 2.0871, 1.7292, 2.1272, 2.3348, 2.0649, 2.1916,\n",
      "        2.1972, 2.1352, 2.2613, 2.0995, 2.1636, 2.1344, 2.1178, 2.0480, 2.0030,\n",
      "        2.1338, 2.0747, 2.0401, 2.0046, 2.1411, 1.9584, 2.0768, 2.1596, 2.0150,\n",
      "        2.0904, 2.1282, 1.9783, 2.0292, 2.0898, 2.0220, 2.0934, 2.1385, 2.2032,\n",
      "        2.1138, 2.1465, 2.0921, 2.0736, 2.0808, 2.0916, 1.9313, 2.0050, 2.1312,\n",
      "        2.0721, 2.0843, 2.1021, 2.1772, 2.0855, 2.1520, 2.1755, 2.0425, 2.0776,\n",
      "        2.0226, 2.1000, 2.0280, 2.0207, 2.1385, 1.9169, 2.0623, 1.9336, 1.7639,\n",
      "        1.8360, 1.7983, 1.7785, 1.9543, 1.7779, 1.8262, 1.8061, 1.8331, 1.8870,\n",
      "        1.7921, 1.8361, 1.8723, 1.7859, 1.7871], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0445, -0.0139, -0.0226,  ..., -0.0067, -0.0299, -0.0543],\n",
      "        [-0.0629, -0.0302,  0.0221,  ...,  0.0028,  0.0421, -0.0043],\n",
      "        [ 0.0376, -0.0438,  0.0084,  ..., -0.0286,  0.0046,  0.0512],\n",
      "        ...,\n",
      "        [-0.0509,  0.0557, -0.0410,  ..., -0.0253, -0.0268, -0.0222],\n",
      "        [-0.0347,  0.0400,  0.0168,  ...,  0.0212,  0.0518, -0.0284],\n",
      "        [-0.0307,  0.0488, -0.0416,  ..., -0.0221,  0.0155,  0.0282]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0116,  0.0071,  0.0063,  ...,  0.0070,  0.0011,  0.0122],\n",
      "        [-0.0066,  0.0046, -0.0026,  ...,  0.0021, -0.0016,  0.0035],\n",
      "        [ 0.0106, -0.0094, -0.0008,  ..., -0.0066, -0.0075, -0.0097],\n",
      "        ...,\n",
      "        [-0.0118,  0.0112,  0.0068,  ...,  0.0114,  0.0050,  0.0182],\n",
      "        [-0.0126,  0.0079,  0.0052,  ...,  0.0136,  0.0023,  0.0139],\n",
      "        [ 0.0081, -0.0109, -0.0025,  ..., -0.0060, -0.0083, -0.0041]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9915, 1.9685, 1.9719, 2.1403, 2.1753, 1.9527, 2.0714, 2.0038, 2.0477,\n",
      "        1.9532, 1.8231, 1.9680, 1.9493, 1.7907, 2.1472, 2.1172, 1.9321, 1.9931,\n",
      "        1.8297, 1.9081, 2.1902, 1.9498, 1.9356, 2.0226, 1.8427, 1.9276, 1.8430,\n",
      "        2.0361, 2.2028, 2.0867, 1.9442, 1.6365, 2.1565, 1.8766, 1.9391, 1.8225,\n",
      "        1.8976, 1.8823, 1.8782, 2.1606, 1.9334, 2.0300, 2.0379, 2.0179, 2.0318,\n",
      "        2.0337, 2.9432, 1.8862, 1.9594, 1.9888, 2.0109, 1.8992, 1.9445, 1.8399,\n",
      "        1.9966, 1.9082, 2.1574, 2.1459, 2.3493, 2.0480, 1.8882, 1.9530, 4.8682,\n",
      "        1.8900, 2.0767, 2.0952, 2.1589, 2.0277, 1.8864, 2.0190, 1.9502, 1.9295,\n",
      "        1.8711, 2.2099, 1.9348, 2.1406, 1.9669, 1.9755, 1.9229, 1.7224, 1.8257,\n",
      "        1.9633, 1.8847, 2.1791, 1.8787, 1.8544, 1.7318, 2.0022, 1.7176, 2.0500,\n",
      "        2.1913, 2.0019, 2.2498, 1.9693, 1.9504, 2.0874, 1.9466, 2.1472, 2.1270,\n",
      "        2.1668, 2.0539, 2.2033, 1.9584, 1.9908, 1.9507, 2.0277, 2.0868, 2.0718,\n",
      "        1.8338, 1.7858, 2.1958, 2.1286, 2.0681, 1.9898, 2.1716, 1.9337, 2.0625,\n",
      "        1.8895, 1.9113, 2.2135, 2.0666, 1.9714, 2.1793, 2.0688, 1.8044, 2.1061,\n",
      "        1.8941, 1.9808, 1.9422, 1.9078, 1.8277, 2.1136, 2.1403, 2.0088, 2.2153,\n",
      "        1.9440, 1.8636, 1.9831, 1.9206, 1.9982, 1.9701, 1.8953, 2.3225, 2.1032,\n",
      "        1.9491, 1.9746, 1.9553, 2.0701, 1.9663, 1.8690, 2.1643, 2.0445, 1.9802,\n",
      "        1.9267, 2.1161, 1.9096, 1.8358, 1.9708, 2.2112, 1.9233, 2.0229, 2.0114,\n",
      "        2.1080, 1.9402, 2.0337, 2.1326, 2.1105, 2.0461, 2.0604, 1.9666, 1.9444,\n",
      "        1.7385, 2.0793, 2.0200, 1.8291, 1.8264, 1.8047, 2.0390, 2.3213, 1.9590,\n",
      "        1.8135, 1.9590, 2.0468, 1.9939, 2.0205, 2.0124, 1.9682, 1.9819, 2.0212,\n",
      "        1.9519, 2.0078, 1.8250, 2.7505, 2.0933, 1.9552, 2.0234, 1.9260, 1.8771,\n",
      "        2.0284, 1.9758, 1.9591, 2.0273, 1.9578, 2.1266, 1.9849, 2.0637, 2.0253,\n",
      "        1.7445, 1.9791, 1.9331, 1.8701, 2.0905, 2.0792, 2.1362, 2.2142, 2.0385,\n",
      "        1.8916, 2.0889, 1.6686, 1.8928, 1.8706, 2.0120, 1.9414, 2.1564, 2.1632,\n",
      "        2.0063, 2.0925, 2.0129, 1.9421, 2.0630, 1.8978, 1.7635, 2.0421, 1.8875,\n",
      "        2.0196, 2.0516, 1.8736, 1.8852, 2.0913, 2.0103, 1.9028, 1.8574, 2.0499,\n",
      "        2.0554, 2.0055, 1.8275, 2.0298, 2.0234, 1.9410, 1.9704, 1.9441, 1.9139,\n",
      "        2.0733, 1.7836, 1.9134, 1.9635, 1.9774, 1.7646, 1.7741, 2.0560, 2.2150,\n",
      "        2.1427, 2.0733, 1.9695, 1.9740, 2.0228, 2.0150, 2.5433, 2.0170, 1.9095,\n",
      "        1.8927, 2.1833, 1.8626, 1.9876, 1.9868, 1.9353, 2.1967, 1.8471, 1.9804,\n",
      "        2.0861, 1.9346, 1.9169, 2.0455, 2.0041, 2.0351, 2.0227, 2.0471, 1.8571,\n",
      "        1.9320, 1.8822, 1.8851, 2.1533, 1.9668, 2.1274, 1.7502, 2.0463, 2.1030,\n",
      "        1.9521, 1.8366, 1.8980, 2.0755, 2.0198, 2.1994, 1.9704, 2.0956, 1.9587,\n",
      "        2.0597, 1.7287, 1.8529, 2.1120, 2.0633, 1.8126, 1.8013, 2.2019, 2.1234,\n",
      "        1.9546, 2.0300, 1.9680, 2.1918, 1.9377], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0470,  0.0280,  0.0553,  ...,  0.0113, -0.0278, -0.0614],\n",
      "        [ 0.0095, -0.0056, -0.0575,  ..., -0.0605,  0.0136, -0.0188],\n",
      "        [ 0.0541,  0.0064,  0.0034,  ...,  0.0189,  0.0183, -0.0497],\n",
      "        ...,\n",
      "        [-0.0088, -0.0185, -0.0105,  ..., -0.0293, -0.0391, -0.0324],\n",
      "        [-0.0660, -0.0566, -0.0274,  ..., -0.0642, -0.0272,  0.0311],\n",
      "        [ 0.0204,  0.0440, -0.0111,  ..., -0.0313, -0.0074, -0.0552]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.6484e-03, -7.0201e-03,  6.2657e-03,  ..., -5.5984e-03,\n",
      "         -2.9503e-03,  1.5080e-03],\n",
      "        [-8.8363e-03,  1.0644e-02, -1.0035e-02,  ..., -1.6149e-03,\n",
      "          8.8471e-03, -5.0968e-03],\n",
      "        [ 7.7779e-03, -3.2601e-03,  7.8177e-03,  ...,  3.6784e-03,\n",
      "         -2.3366e-03,  6.2290e-03],\n",
      "        ...,\n",
      "        [-4.8688e-03,  5.9144e-03, -5.5879e-03,  ..., -6.5750e-03,\n",
      "          6.7749e-03, -4.4070e-03],\n",
      "        [ 1.2321e-03, -7.5860e-05, -1.3469e-03,  ...,  6.6812e-03,\n",
      "         -3.0520e-03,  3.9836e-03],\n",
      "        [ 2.6151e-03, -2.4448e-03, -1.5836e-03,  ..., -3.1970e-04,\n",
      "          1.8648e-03, -2.0074e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.1102, 1.8813, 2.1273,  ..., 2.6309, 1.8489, 1.9418],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-9.6311e-03,  1.9730e-03, -1.4534e-02,  ...,  1.4764e-02,\n",
      "          2.5682e-02,  2.9105e-02],\n",
      "        [ 4.8982e-03,  1.5203e-02,  8.8068e-03,  ...,  2.5654e-02,\n",
      "         -1.5699e-02,  1.9582e-02],\n",
      "        [-9.1986e-03,  9.9259e-03, -1.6470e-04,  ...,  9.8801e-05,\n",
      "         -7.3734e-03,  9.3270e-03],\n",
      "        ...,\n",
      "        [ 3.2326e-02, -1.0499e-02, -6.6166e-03,  ..., -1.0887e-02,\n",
      "          2.2918e-02,  2.1827e-02],\n",
      "        [-3.6465e-03, -2.1746e-02,  2.0695e-03,  ...,  2.2041e-02,\n",
      "         -1.1147e-02,  1.8934e-02],\n",
      "        [-9.5995e-03,  5.3186e-03,  1.1882e-02,  ...,  3.5943e-03,\n",
      "          3.1965e-02, -1.7919e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0054, -0.0049,  0.0045,  ..., -0.0132, -0.0011, -0.0098],\n",
      "        [-0.0034, -0.0082,  0.0067,  ..., -0.0049, -0.0004, -0.0053],\n",
      "        [ 0.0018,  0.0122, -0.0081,  ..., -0.0024,  0.0044, -0.0005],\n",
      "        ...,\n",
      "        [-0.0073, -0.0056,  0.0118,  ..., -0.0125, -0.0047, -0.0123],\n",
      "        [-0.0027, -0.0086,  0.0103,  ..., -0.0032, -0.0024, -0.0048],\n",
      "        [ 0.0024,  0.0071,  0.0014,  ..., -0.0042,  0.0024, -0.0051]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([4.3312, 4.4911, 4.1938, 4.4675, 4.8569, 4.3161, 4.4351, 4.2079, 4.5091,\n",
      "        4.4141, 4.1945, 4.3005, 4.4493, 4.0304, 4.5871, 4.5183, 4.1671, 4.3010,\n",
      "        4.0641, 4.4301, 4.6046, 4.6809, 4.1731, 4.3249, 4.1860, 4.2245, 4.1801,\n",
      "        4.3724, 4.5105, 4.6351, 4.2074, 3.8934, 4.7033, 4.2844, 4.3029, 4.2386,\n",
      "        3.9611, 4.2200, 4.2711, 4.8985, 4.0887, 4.3936, 4.3311, 4.2812, 4.6532,\n",
      "        4.3478, 5.5443, 4.1367, 4.4138, 4.5969, 4.6353, 4.4660, 4.1075, 3.9902,\n",
      "        4.3118, 4.3595, 4.4997, 4.3686, 5.4311, 4.3616, 4.5547, 4.3695, 9.4384,\n",
      "        4.3407, 4.5399, 4.3194, 4.4697, 4.0601, 4.6916, 4.5906, 4.0639, 4.1267,\n",
      "        4.0345, 4.6714, 4.3016, 4.2984, 4.2107, 4.3869, 4.5192, 3.8664, 4.2099,\n",
      "        4.0787, 4.2708, 4.8576, 4.3422, 4.2252, 4.0897, 4.4478, 3.9402, 4.4097,\n",
      "        4.5369, 4.0697, 5.1644, 4.4484, 4.4048, 4.7324, 4.3111, 4.4098, 4.3486,\n",
      "        4.8141, 4.5436, 4.6336, 4.3789, 4.4617, 4.4388, 4.3627, 4.1676, 4.3757,\n",
      "        4.3654, 4.1124, 4.6647, 4.3354, 4.2134, 4.2438, 4.2629, 4.0902, 4.6053,\n",
      "        4.2911, 4.6091, 4.7469, 4.4042, 4.2267, 4.4070, 4.3910, 4.2978, 4.7997,\n",
      "        4.3369, 4.2969, 4.3539, 3.8740, 4.1720, 4.6106, 4.4775, 4.5770, 4.6600,\n",
      "        4.3767, 4.1778, 4.5112, 4.5290, 4.7880, 3.9754, 4.1486, 4.5178, 4.3863,\n",
      "        4.6332, 4.6279, 4.5735, 4.3552, 4.2201, 4.3037, 5.0287, 4.3489, 4.3437,\n",
      "        4.2777, 4.4352, 4.5041, 4.3064, 4.4841, 4.9386, 4.5881, 4.3039, 4.5939,\n",
      "        4.4756, 4.2259, 4.2796, 4.4386, 4.2290, 4.4109, 4.3745, 4.4319, 4.1088,\n",
      "        4.3296, 4.3222, 4.4153, 4.4542, 4.1076, 4.1517, 4.4344, 4.7280, 4.2254,\n",
      "        4.5405, 4.0643, 4.6493, 4.3563, 4.3425, 4.6946, 4.3394, 4.5339, 4.3411,\n",
      "        4.3251, 4.2995, 4.0727, 6.6729, 4.3111, 4.3629, 4.2071, 4.5277, 4.2069,\n",
      "        4.5999, 4.6777, 4.4207, 4.4513, 4.6533, 4.2642, 4.5753, 4.5501, 4.1977,\n",
      "        3.8299, 4.4926, 4.1379, 4.4686, 4.4384, 4.4913, 4.4752, 4.7136, 4.4349,\n",
      "        4.2397, 4.6406, 3.8017, 4.1692, 4.3153, 4.1820, 4.4083, 4.8045, 4.7847,\n",
      "        4.4114, 4.5193, 4.2947, 4.2473, 4.3078, 4.2100, 3.9728, 4.4502, 4.3500,\n",
      "        4.1779, 4.6275, 4.3073, 4.1315, 4.3458, 4.4145, 4.0933, 4.3194, 4.1119,\n",
      "        4.1815, 4.2131, 4.1913, 5.0013, 4.1994, 4.6318, 4.4156, 4.3818, 4.5356,\n",
      "        4.3123, 4.1228, 4.3293, 4.4141, 4.5315, 4.0016, 3.9877, 4.6790, 4.2450,\n",
      "        4.5723, 4.6456, 4.3273, 4.2552, 4.5232, 4.7093, 6.7653, 4.1049, 4.0443,\n",
      "        4.4817, 4.0789, 4.3531, 4.1972, 4.2246, 4.3797, 4.5792, 4.3238, 4.4041,\n",
      "        4.3927, 4.0711, 4.3065, 4.3368, 4.6594, 4.3232, 4.2889, 4.5912, 4.2055,\n",
      "        4.0541, 4.3896, 4.6002, 4.6044, 4.1594, 4.6670, 4.1123, 4.3837, 4.4614,\n",
      "        4.1258, 4.2027, 4.2231, 4.2959, 4.1842, 4.4180, 4.3540, 4.5883, 4.3377,\n",
      "        4.7699, 4.0724, 4.1664, 4.3931, 4.2129, 4.2392, 3.9842, 4.3860, 4.3019,\n",
      "        4.2550, 4.4255, 4.3832, 4.5811, 4.3097], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0040, -0.0400,  0.0440,  ..., -0.0354,  0.0414,  0.0211],\n",
      "        [-0.0105,  0.0143, -0.0105,  ..., -0.0383, -0.0110,  0.0265],\n",
      "        [ 0.0463, -0.0042, -0.0493,  ..., -0.0445,  0.0120, -0.0580],\n",
      "        ...,\n",
      "        [-0.0445,  0.0233,  0.0538,  ..., -0.0160, -0.0365, -0.0230],\n",
      "        [-0.0464, -0.0345, -0.0475,  ..., -0.0281, -0.0447,  0.0076],\n",
      "        [-0.0163, -0.0501, -0.0338,  ..., -0.0185, -0.0613, -0.0415]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.0379e-03,  4.7179e-03, -5.4861e-03,  ...,  3.5808e-03,\n",
      "          4.9286e-03,  5.3305e-03],\n",
      "        [-2.8605e-03,  3.6608e-03, -4.8178e-03,  ...,  3.8517e-03,\n",
      "          3.3742e-03,  3.3136e-03],\n",
      "        [-5.4941e-04, -2.2453e-04,  1.1727e-03,  ..., -2.6737e-04,\n",
      "          9.8938e-05,  1.9895e-04],\n",
      "        ...,\n",
      "        [ 3.5150e-03, -2.6430e-03,  6.5286e-05,  ...,  1.6017e-03,\n",
      "         -3.3347e-03, -4.1022e-03],\n",
      "        [-4.5721e-03,  4.1563e-03, -3.5084e-03,  ...,  2.7114e-03,\n",
      "          4.6050e-03,  4.8467e-03],\n",
      "        [-1.5812e-02,  1.5209e-02, -1.4993e-02,  ...,  1.3083e-02,\n",
      "          1.6864e-02,  1.5349e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5264, 1.3322, 1.3691, 1.2994, 1.5740, 1.7071, 1.6216, 1.6124, 1.5894,\n",
      "        1.4219, 1.1341, 1.3566, 1.4217, 1.3101, 1.6178, 1.5713, 1.4984, 1.3502,\n",
      "        1.0186, 1.3311, 1.7003, 1.4909, 1.8946, 1.9725, 1.0796, 1.0305, 1.3189,\n",
      "        1.1719, 1.1130, 1.9578, 2.0739, 1.8629, 1.0689, 1.1823, 1.2028, 1.6870,\n",
      "        2.1561, 1.5533, 1.8085, 2.5763, 2.0205, 1.5049, 1.6655, 1.4735, 1.4898,\n",
      "        2.1919, 1.9483, 1.9762, 1.6558, 1.7454, 1.6999, 1.5561, 1.4431, 1.5149,\n",
      "        1.8744, 1.9660, 1.6975, 1.7634, 1.5798, 1.7498, 1.7274, 1.5489, 1.8746,\n",
      "        1.9387, 1.1767, 1.6737, 1.4370, 1.9061, 2.0107, 2.0937, 2.2817, 2.0181,\n",
      "        1.2039, 1.2512, 1.5229, 2.0872, 2.0639, 1.6344, 2.2574, 2.1140, 1.7939,\n",
      "        1.2919, 1.6478, 1.0757, 1.8529, 1.7200, 1.3029, 1.4445, 1.6270, 1.7332,\n",
      "        1.4039, 1.7902, 1.1491, 1.1824, 1.2893, 1.3195, 1.1454, 0.9932, 1.0768,\n",
      "        1.1325, 1.4885, 1.4812, 1.4499, 1.5958, 1.1527, 0.9397, 1.2806, 1.3822,\n",
      "        1.2558, 1.5463, 1.4141, 1.4549, 1.5750, 1.6693, 1.9294, 1.7007, 1.8065,\n",
      "        1.6664, 2.2356, 2.3306, 1.5041, 1.6977, 1.7663, 1.8276, 1.5928, 1.9898,\n",
      "        2.2294, 2.3325, 2.0241, 1.7007, 1.6829, 1.5099, 1.7030, 2.2771, 1.9600,\n",
      "        1.9498, 1.6451, 1.7233, 1.6487, 1.3708, 1.6928, 1.4034, 1.8723, 1.9080,\n",
      "        1.9135, 1.2767, 1.6988, 1.2249, 1.2832, 1.4067, 1.6093, 2.1303, 1.2906,\n",
      "        1.2664, 1.1704, 1.8383, 1.9701, 2.0086, 1.7269, 1.7962, 1.4639, 1.4094,\n",
      "        1.5983, 1.7461, 1.5393, 2.3302, 1.7915, 1.8295, 1.6463, 1.4398, 1.6927,\n",
      "        1.7062, 1.9013, 1.4905, 1.6655, 1.9770, 0.7673, 1.3519, 1.5142, 1.4586,\n",
      "        2.0805, 2.0070, 2.3841, 1.9876, 1.3955, 1.1177, 1.2259, 1.6606, 1.9128,\n",
      "        2.0591, 2.3340, 1.9332, 1.6635, 1.2019, 1.6207, 1.8853, 1.8616, 1.8711,\n",
      "        1.6489, 1.9403, 1.2415, 1.4285, 1.0682, 1.2076, 1.2400, 1.3823, 1.6548,\n",
      "        1.6106, 0.9770, 1.3335, 1.2992, 1.2693, 1.2408, 1.2194, 1.7642, 1.7796,\n",
      "        1.2144, 1.2090, 1.2945, 1.3845, 1.1726, 1.7543, 1.9150, 1.8010, 1.4407,\n",
      "        1.3100, 1.7254, 1.7172, 1.2715, 1.3156, 1.6929, 1.6268, 1.6548, 1.4706,\n",
      "        1.1021, 1.0992, 1.6583, 1.9272, 1.5642, 1.8781, 0.6995, 1.5608, 1.4544,\n",
      "        1.6603, 2.1848, 1.7559, 2.1349, 2.2517, 1.3528, 1.1983, 1.4606, 1.5392,\n",
      "        2.2124, 1.7769, 2.2678, 2.3871, 1.9310, 1.9870, 1.9327, 1.6299, 1.3922,\n",
      "        2.1230, 1.8092, 2.0190, 1.8515, 1.8179, 1.8106, 1.3281, 1.7536, 1.5297,\n",
      "        2.0371, 1.8864, 1.8175, 1.9069, 1.8948, 1.8607, 1.4900, 2.1567, 2.1045,\n",
      "        2.0863, 1.6795, 1.7379, 1.9814, 1.9804, 1.7434, 1.5776, 2.2154, 2.0109,\n",
      "        1.3045, 1.3176, 1.3986, 1.2890, 1.3493, 1.1409, 2.0049, 2.0529, 1.2402,\n",
      "        1.2701, 1.2576, 1.3792, 1.2625, 1.5750, 1.6123, 1.3892, 0.7870, 0.7964,\n",
      "        0.8366, 1.2122, 1.9422, 2.0880, 2.0296, 2.0130, 1.7664, 1.4060, 1.5221,\n",
      "        1.5413, 1.7700, 2.0653, 1.9738, 1.6750], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0334, -0.0440, -0.0263,  ..., -0.0674,  0.0019,  0.0269],\n",
      "        [-0.0652,  0.0365,  0.0114,  ...,  0.0290, -0.0551,  0.0245],\n",
      "        [ 0.0519, -0.0290,  0.0289,  ...,  0.0227, -0.0330, -0.0400],\n",
      "        ...,\n",
      "        [-0.0178, -0.0023, -0.0109,  ...,  0.0050,  0.0382,  0.0681],\n",
      "        [-0.0086,  0.0445,  0.0011,  ..., -0.0475, -0.0167, -0.0309],\n",
      "        [-0.0601, -0.0505,  0.0285,  ...,  0.0054, -0.0265, -0.0288]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0044, -0.0025,  0.0023,  ..., -0.0041, -0.0052, -0.0013],\n",
      "        [-0.0013, -0.0058,  0.0093,  ..., -0.0045,  0.0003,  0.0036],\n",
      "        [-0.0027, -0.0023,  0.0024,  ...,  0.0062,  0.0007,  0.0066],\n",
      "        ...,\n",
      "        [-0.0029, -0.0017, -0.0099,  ...,  0.0025,  0.0068, -0.0038],\n",
      "        [ 0.0027,  0.0048, -0.0018,  ..., -0.0074,  0.0076,  0.0029],\n",
      "        [-0.0002,  0.0022,  0.0017,  ..., -0.0074,  0.0040,  0.0060]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3953, 1.2999, 1.4070, 1.4471, 1.2199, 1.1657, 1.5666, 1.5769, 1.4618,\n",
      "        1.4148, 1.3096, 1.3256, 1.4241, 1.8668, 1.5530, 1.5302, 1.1096, 1.4896,\n",
      "        1.0687, 1.2160, 1.3869, 1.3762, 1.5782, 1.4396, 1.2999, 1.0517, 1.5198,\n",
      "        1.2520, 1.1933, 1.3424, 1.7652, 1.6463, 1.5366, 1.4974, 1.5538, 1.3118,\n",
      "        1.2842, 2.0818, 1.7389, 1.8666, 1.4804, 1.4544, 1.4455, 1.7073, 2.0753,\n",
      "        1.3193, 1.8702, 1.8507, 1.6592, 1.7415, 1.7658, 1.4928, 1.5287, 1.4575,\n",
      "        1.7345, 1.8600, 1.6080, 1.8420, 1.7803, 1.7160, 1.4418, 2.0542, 1.7394,\n",
      "        1.9196, 1.2078, 1.5798, 1.5878, 1.7252, 1.5827, 1.3221, 1.8507, 1.7460,\n",
      "        1.2190, 1.2624, 1.6506, 1.7566, 1.4003, 1.6291, 1.6826, 1.8971, 1.8025,\n",
      "        1.4692, 1.6009, 1.5679, 0.9525, 1.0081, 1.3878, 1.4720, 1.5910, 1.6910,\n",
      "        1.4565, 1.2673, 1.5792, 1.3784, 1.3531, 1.3031, 1.0246, 1.2510, 1.2420,\n",
      "        1.3105, 1.0516, 1.0976, 1.3898, 1.4808, 1.1984, 1.1348, 1.3466, 1.0740,\n",
      "        1.1366, 1.2954, 1.3655, 1.4490, 1.4693, 1.6005, 1.9474, 1.7910, 1.7274,\n",
      "        1.8165, 1.9756, 1.7758, 1.5548, 1.7226, 1.8010, 1.8195, 1.7563, 1.6163,\n",
      "        2.0087, 1.8944, 1.9245, 1.7433, 1.8092, 1.6186, 1.2533, 1.1342, 1.8320,\n",
      "        1.8362, 1.7156, 1.8479, 1.7774, 1.5038, 1.4321, 2.3204, 1.7516, 1.7673,\n",
      "        1.8125, 1.4629, 1.3221, 1.8485, 1.8290, 1.9440, 1.5437, 1.7125, 1.4686,\n",
      "        1.4848, 1.5748, 1.1300, 1.0469, 1.1878, 1.7042, 1.6831, 1.5906, 1.5397,\n",
      "        1.4372, 1.3916, 1.6661, 1.1764, 1.6637, 1.7152, 1.5200, 1.6228, 1.4837,\n",
      "        1.4285, 1.1874, 1.9746, 1.7098, 1.7022, 1.5651, 1.0723, 1.3185, 1.4823,\n",
      "        1.6206, 1.8888, 1.9532, 1.8460, 0.8338, 1.5036, 1.3462, 1.3641, 1.7228,\n",
      "        1.8107, 1.9350, 1.8167, 1.7776, 1.4232, 1.3868, 1.1352, 1.0344, 1.1609,\n",
      "        1.6023, 1.7566, 1.4021, 1.5129, 1.5327, 1.5912, 1.9726, 1.8396, 1.6602,\n",
      "        1.5310, 0.8824, 1.0038, 1.0188, 1.3068, 1.1603, 1.2995, 1.4820, 1.4510,\n",
      "        1.0494, 1.1633, 1.3936, 1.1914, 1.1941, 1.2318, 1.7633, 1.3454, 1.5607,\n",
      "        1.4244, 1.6214, 1.0646, 1.8665, 1.9175, 1.6339, 1.6019, 1.7819, 1.5967,\n",
      "        1.3212, 1.7149, 0.9985, 1.0730, 1.5422, 1.6410, 1.4993, 1.5465, 1.5559,\n",
      "        1.6668, 1.6309, 1.7610, 1.9288, 1.9313, 1.1203, 0.9583, 1.1522, 1.2512,\n",
      "        1.8527, 1.7193, 2.0614, 1.9530, 1.9161, 1.9200, 1.9724, 1.7094, 1.9502,\n",
      "        1.2408, 1.6251, 1.7598, 1.7784, 1.8894, 1.8177, 1.5900, 1.4693, 2.1830,\n",
      "        1.9091, 1.7087, 1.7700, 1.8955, 1.9271, 1.9076, 1.6145, 1.4685, 1.9596,\n",
      "        1.8644, 1.6744, 1.7082, 1.8654, 1.8632, 1.5796, 2.1356, 2.0599, 1.9832,\n",
      "        0.9895, 0.9289, 1.1902, 1.1324, 1.3236, 1.2623, 1.6768, 1.4619, 0.8577,\n",
      "        1.0234, 1.2677, 1.3422, 1.0936, 1.1832, 1.4750, 1.4557, 1.1718, 1.5170,\n",
      "        1.6340, 1.6197, 1.6862, 1.8008, 1.8727, 1.8621, 1.4649, 1.1385, 1.0341,\n",
      "        1.5120, 1.7269, 1.8613, 1.8730, 1.7504], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0222, -0.0189, -0.0231,  ..., -0.0470,  0.0062,  0.0128],\n",
      "        [ 0.0275,  0.0032,  0.0002,  ...,  0.0442,  0.0477, -0.0211],\n",
      "        [-0.0363,  0.0055,  0.0319,  ..., -0.0365,  0.0019, -0.0409],\n",
      "        ...,\n",
      "        [ 0.0446,  0.0016, -0.0005,  ...,  0.0190, -0.0207,  0.0192],\n",
      "        [ 0.0667,  0.0285, -0.0019,  ...,  0.0602,  0.0354, -0.0335],\n",
      "        [ 0.0093, -0.0456,  0.0526,  ...,  0.0452, -0.0173,  0.0250]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0004, -0.0026,  0.0033,  ...,  0.0080,  0.0041, -0.0027],\n",
      "        [-0.0006,  0.0010, -0.0047,  ..., -0.0072, -0.0042,  0.0032],\n",
      "        [-0.0020, -0.0035,  0.0048,  ...,  0.0082,  0.0054, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0155,  0.0088, -0.0069,  ..., -0.0071, -0.0068,  0.0094],\n",
      "        [-0.0115,  0.0004, -0.0030,  ..., -0.0016, -0.0028, -0.0002],\n",
      "        [-0.0086, -0.0097,  0.0084,  ...,  0.0088,  0.0088, -0.0098]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0690, 2.0830, 2.0637, 2.0709, 2.3413, 2.2433, 2.0851, 1.9707, 2.1058,\n",
      "        2.1583, 2.1751, 2.0327, 2.1054, 2.1630, 2.0346, 2.1524, 2.2866, 2.2326,\n",
      "        2.2250, 2.3661, 2.2507, 2.1171, 2.3057, 2.4056, 2.3103, 2.2783, 2.1835,\n",
      "        2.1110, 2.2788, 2.2091, 2.2831, 2.2410, 2.0496, 1.9383, 2.1280, 2.0765,\n",
      "        1.9752, 2.0236, 2.0876, 2.0746, 2.0640, 2.0579, 2.0097, 2.0048, 2.0473,\n",
      "        2.0210, 2.0258, 1.9680, 2.2092, 2.2582, 2.2463, 2.3268, 1.8867, 2.1631,\n",
      "        2.2229, 2.0421, 2.0282, 2.1751, 2.2649, 2.2352, 2.2946, 2.0112, 2.3065,\n",
      "        2.2171, 2.2473, 2.3380, 2.0965, 2.1424, 2.4347, 2.1980, 2.2315, 2.1895,\n",
      "        2.2316, 2.2459, 2.2366, 2.1484, 2.2289, 2.1322, 2.1628, 2.2505, 2.1123,\n",
      "        2.1281, 2.1952, 2.1328, 2.2939, 2.1233, 2.1087, 2.1092, 2.2902, 2.1044,\n",
      "        2.2139, 2.1883, 2.1215, 2.1964, 2.0728, 2.0896, 2.3282, 2.0195, 2.2356,\n",
      "        2.0898, 2.1865, 2.1176, 2.0579, 2.1094, 2.1656, 2.1508, 2.2444, 2.2056,\n",
      "        2.3938, 2.1898, 2.1451, 2.1792, 2.1650, 2.2258, 2.0862, 2.1367, 2.0370,\n",
      "        2.1886, 2.2243, 2.2662, 2.5237, 2.1113, 2.1935, 2.2419, 2.0118, 2.1930,\n",
      "        2.2413, 2.2954, 2.1793, 2.0786, 2.2123, 2.2187, 2.2543, 2.2982, 2.0754,\n",
      "        2.2202, 2.1381, 2.2469, 2.1165, 2.3123, 2.2456, 2.2251, 2.1358, 2.1344,\n",
      "        2.0640, 2.1107, 1.8164, 1.9497, 2.0397, 2.1043, 2.1305, 1.9586, 2.2636,\n",
      "        2.1760, 2.0619, 2.1083, 2.0075, 2.2071, 2.0732, 2.0171, 2.1651, 2.0782,\n",
      "        2.0464, 2.0997, 2.0774, 2.1234, 2.1322, 2.0828, 2.0643, 1.9991, 2.1272,\n",
      "        2.1212, 2.0676, 1.9549, 2.0256, 2.1326, 1.9793, 1.8310, 2.0169, 1.9620,\n",
      "        1.8471, 1.8997, 1.9454, 1.9398, 2.0133, 1.9978, 1.9418, 1.9086, 1.9241,\n",
      "        1.9705, 1.9479, 1.8556, 2.0778, 1.9217, 2.0176, 2.2051, 2.1756, 2.0090,\n",
      "        2.0320, 1.9759, 2.1804, 2.2158, 2.0024, 2.1187, 2.1886, 2.0139, 1.8604,\n",
      "        2.1888, 2.5980, 2.5602, 2.5321, 2.5142, 2.5452, 2.9650, 2.5337, 2.2834,\n",
      "        2.1162, 2.3587, 2.2819, 2.2944, 2.2341, 2.4700, 2.3702, 2.3395, 2.1349,\n",
      "        2.1516, 2.2368, 2.1792, 2.1105, 1.9427, 2.1925, 2.1454, 2.2558, 2.0827,\n",
      "        2.1482, 2.1635, 2.0834, 2.1733, 2.1039, 1.9265, 2.0097, 2.0291, 1.8808,\n",
      "        1.9422, 1.9698, 1.9524, 1.8951, 1.9907, 1.9289, 1.9247, 1.9782, 1.8931,\n",
      "        1.9060, 1.8674, 1.9873, 1.9648, 2.2740, 2.2034, 1.9953, 2.3166, 2.2142,\n",
      "        2.1034, 2.0774, 2.2107, 2.2083, 2.2258, 2.1128, 2.1928, 2.2349, 2.1139,\n",
      "        2.1910, 2.1016, 2.2075, 2.1705, 2.1824, 2.2879, 2.1498, 2.2071, 2.2085,\n",
      "        2.0860, 2.2530, 2.1820, 2.0948, 2.0393, 2.0170, 2.3078, 2.1236, 2.2138,\n",
      "        3.5402, 2.7024, 2.6260, 3.2144, 2.3731, 2.8214, 2.5147, 2.6994, 3.5802,\n",
      "        2.5271, 2.7213, 2.6830, 3.0147, 2.7982, 2.8036, 2.7479, 2.0899, 1.9811,\n",
      "        1.8617, 1.8791, 1.9183, 1.9061, 1.9263, 2.1311, 1.9661, 1.9705, 1.9255,\n",
      "        1.9107, 2.0881, 1.9592, 1.9542, 1.9507], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0439,  0.0234,  0.0248,  ..., -0.0015,  0.0193,  0.0307],\n",
      "        [-0.0401,  0.0218,  0.0387,  ...,  0.0488, -0.0378,  0.0374],\n",
      "        [-0.0283, -0.0426,  0.0255,  ...,  0.0323, -0.0223,  0.0508],\n",
      "        ...,\n",
      "        [-0.0462, -0.0081, -0.0515,  ...,  0.0259, -0.0140, -0.0042],\n",
      "        [ 0.0280,  0.0001,  0.0579,  ...,  0.0200,  0.0055,  0.0386],\n",
      "        [-0.0014,  0.0322,  0.0029,  ..., -0.0264, -0.0086, -0.0181]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0048,  0.0168,  0.0135,  ...,  0.0116, -0.0101, -0.0136],\n",
      "        [ 0.0021,  0.0166,  0.0104,  ...,  0.0067, -0.0078, -0.0101],\n",
      "        [ 0.0027, -0.0210, -0.0154,  ..., -0.0120,  0.0144,  0.0161],\n",
      "        ...,\n",
      "        [-0.0042,  0.0127,  0.0104,  ...,  0.0062, -0.0112, -0.0134],\n",
      "        [-0.0032,  0.0199,  0.0138,  ...,  0.0110, -0.0152, -0.0164],\n",
      "        [-0.0019, -0.0094, -0.0052,  ..., -0.0058,  0.0035,  0.0050]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2018, 2.0804, 2.1913, 2.2204, 2.5573, 2.2728, 2.1233, 2.3171, 2.3052,\n",
      "        2.1673, 2.1142, 2.4550, 2.5231, 2.0475, 2.3801, 2.1883, 2.2555, 2.1527,\n",
      "        2.3545, 2.0873, 2.4667, 2.4327, 2.0413, 2.1782, 2.1604, 1.9934, 2.2279,\n",
      "        2.1655, 2.4618, 2.2233, 2.0820, 2.1009, 2.4746, 2.1866, 2.1105, 2.0791,\n",
      "        2.0951, 2.2533, 2.3436, 2.1536, 2.0919, 2.3624, 2.3400, 2.1936, 2.3101,\n",
      "        2.1442, 3.1415, 1.9672, 2.2683, 2.5400, 2.2166, 2.2161, 2.0839, 2.3519,\n",
      "        2.0711, 2.1886, 2.2045, 2.3844, 2.6120, 2.3769, 2.2998, 2.0807, 6.8993,\n",
      "        2.1816, 2.2103, 2.1607, 2.4033, 2.6319, 2.3358, 2.1331, 2.2288, 2.1616,\n",
      "        2.0134, 2.3026, 2.1539, 2.3674, 2.2155, 2.4434, 1.9167, 1.9712, 2.0236,\n",
      "        2.1095, 2.3131, 2.2591, 2.0070, 2.1483, 2.4972, 2.4189, 1.9583, 2.2164,\n",
      "        2.1729, 2.0351, 2.4252, 2.2448, 2.2269, 2.3210, 2.1240, 2.3079, 2.1761,\n",
      "        2.3060, 2.3603, 2.2495, 2.4463, 2.4045, 2.3113, 2.3804, 2.1273, 2.2938,\n",
      "        1.9950, 2.3183, 2.2650, 2.2980, 2.2136, 2.1552, 2.4566, 2.1332, 2.2385,\n",
      "        2.3283, 2.4687, 2.5004, 2.4330, 2.2538, 2.2806, 2.2314, 2.0010, 2.5415,\n",
      "        2.1019, 2.1583, 2.2553, 2.3592, 2.1444, 2.0178, 2.1740, 2.1866, 2.4031,\n",
      "        2.1334, 2.2227, 2.4532, 2.1705, 2.3389, 2.1139, 2.0818, 2.2396, 2.3689,\n",
      "        2.3575, 2.3007, 2.1385, 2.3433, 2.1654, 2.2835, 2.4311, 2.2611, 2.3751,\n",
      "        2.1315, 2.2429, 2.3198, 2.0787, 2.3529, 2.2775, 2.1431, 2.0862, 2.2000,\n",
      "        2.3256, 2.3830, 2.3424, 2.3570, 2.2892, 2.1267, 2.3060, 2.1622, 2.2842,\n",
      "        2.3179, 2.2644, 2.1974, 2.2346, 2.0765, 2.2795, 2.2639, 2.1532, 1.9746,\n",
      "        2.0354, 2.1052, 2.1874, 2.1927, 2.3071, 2.1746, 2.3045, 2.2398, 1.9154,\n",
      "        2.2193, 2.1428, 2.0784, 3.3515, 2.1763, 2.2269, 2.2687, 2.4384, 2.2491,\n",
      "        2.3720, 2.3062, 2.2962, 2.3378, 2.2810, 2.2685, 2.2581, 2.3504, 1.9538,\n",
      "        2.2856, 2.6062, 2.1887, 2.1688, 2.3603, 2.4008, 2.3129, 2.3193, 2.1595,\n",
      "        2.1807, 2.0322, 2.0128, 2.1410, 2.0765, 2.0952, 2.3977, 2.4500, 2.3890,\n",
      "        2.2321, 2.3504, 2.1210, 1.9472, 2.3969, 2.1296, 2.1147, 2.0343, 2.3132,\n",
      "        2.0194, 2.1505, 2.2649, 2.2503, 2.0756, 2.2617, 2.1189, 2.1064, 2.3333,\n",
      "        2.0919, 2.2688, 2.0230, 2.3632, 1.9952, 2.0503, 2.4199, 2.2948, 2.1618,\n",
      "        2.0532, 1.9760, 2.2526, 2.2199, 2.3429, 2.0341, 2.3262, 2.2758, 2.0901,\n",
      "        2.1760, 2.2265, 2.1629, 2.1590, 2.2058, 2.3041, 2.8020, 2.3384, 2.0961,\n",
      "        1.9485, 2.1986, 2.0600, 2.4568, 2.4454, 2.3414, 2.1430, 2.1110, 2.2870,\n",
      "        2.1961, 2.2259, 2.1103, 2.2665, 2.5729, 2.1706, 2.2629, 2.2812, 2.3094,\n",
      "        2.1298, 2.1510, 2.1709, 2.2587, 2.2367, 2.3247, 2.0593, 2.0518, 2.1959,\n",
      "        2.0752, 2.0350, 2.3088, 2.2203, 2.1603, 2.1932, 2.1999, 2.3633, 2.2343,\n",
      "        2.3185, 2.0266, 2.0049, 2.0291, 2.3612, 1.9933, 1.9193, 2.3280, 2.3145,\n",
      "        2.1455, 2.3728, 2.1233, 2.2220, 1.9827], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0447,  0.0510, -0.0219,  ...,  0.0086, -0.0381,  0.0164],\n",
      "        [ 0.0335,  0.0086,  0.0163,  ..., -0.0567, -0.0035, -0.0332],\n",
      "        [-0.0318, -0.0421, -0.0561,  ..., -0.0134, -0.0451,  0.0214],\n",
      "        ...,\n",
      "        [-0.0258, -0.0276,  0.0257,  ...,  0.0457, -0.0259, -0.0298],\n",
      "        [ 0.0323, -0.0271,  0.0347,  ...,  0.0588, -0.0367, -0.0160],\n",
      "        [-0.0321, -0.0280, -0.0276,  ..., -0.0416,  0.0083,  0.0024]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 2.4240e-03,  2.2763e-05,  1.1144e-03,  ..., -1.5829e-03,\n",
      "         -2.0444e-04,  3.2786e-04],\n",
      "        [-9.1654e-04, -4.0508e-03,  4.2944e-03,  ..., -2.2664e-03,\n",
      "          2.9326e-03, -3.8429e-03],\n",
      "        [ 3.7555e-04, -1.4083e-02,  1.2326e-02,  ..., -1.2347e-02,\n",
      "          1.2981e-02, -1.3236e-02],\n",
      "        ...,\n",
      "        [ 1.4139e-02, -1.0499e-03,  2.1932e-03,  ..., -3.2106e-03,\n",
      "          5.4319e-04, -1.3972e-03],\n",
      "        [-5.8810e-04,  1.3346e-03, -2.0101e-03,  ...,  1.2073e-03,\n",
      "         -2.1014e-03,  1.6475e-03],\n",
      "        [ 1.3691e-03,  3.8936e-04, -8.7043e-04,  ...,  3.4049e-04,\n",
      "         -1.5991e-03,  1.3107e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.7313, 1.9081, 1.6071,  ..., 2.2157, 2.0472, 2.0312],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0027, -0.0046,  0.0032,  ...,  0.0052,  0.0046,  0.0193],\n",
      "        [-0.0208,  0.0210,  0.0315,  ..., -0.0159,  0.0163, -0.0168],\n",
      "        [ 0.0201, -0.0027, -0.0005,  ..., -0.0178, -0.0167,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0140, -0.0313,  0.0200,  ..., -0.0213, -0.0245, -0.0200],\n",
      "        [-0.0314, -0.0057,  0.0035,  ...,  0.0239, -0.0221, -0.0086],\n",
      "        [ 0.0106,  0.0257,  0.0235,  ...,  0.0040, -0.0088, -0.0170]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0149,  0.0101, -0.0166,  ..., -0.0170,  0.0149,  0.0158],\n",
      "        [-0.0051, -0.0079,  0.0078,  ...,  0.0049, -0.0063, -0.0090],\n",
      "        [-0.0176, -0.0127,  0.0174,  ...,  0.0167, -0.0162, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0144,  0.0077, -0.0129,  ..., -0.0149,  0.0129,  0.0109],\n",
      "        [ 0.0164,  0.0101, -0.0153,  ..., -0.0161,  0.0151,  0.0129],\n",
      "        [ 0.0143,  0.0107, -0.0137,  ..., -0.0131,  0.0130,  0.0117]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 5.2844,  4.9897,  4.9764,  3.5778,  5.1554,  5.9383,  6.1617,  6.2387,\n",
      "         5.7769,  5.4255,  5.9347,  5.5889,  5.7327,  5.2757,  5.4128,  5.9513,\n",
      "         5.3169,  5.9158,  5.2837,  5.7521,  5.4377,  6.0927,  4.7053,  6.1283,\n",
      "         5.5722,  5.8805,  4.9703,  5.4898,  5.4238,  5.5793,  5.5978,  4.1776,\n",
      "         6.1447,  4.6434,  5.7121,  5.7458,  4.5828,  6.0257,  5.8595,  5.1567,\n",
      "         5.6080,  5.8777,  5.5787,  5.7281,  6.0545,  5.7623,  5.3936,  5.3893,\n",
      "         5.3353,  5.9679,  5.2599,  6.1029,  5.5476,  5.8416,  5.4217,  6.0514,\n",
      "         5.3018,  6.2909,  5.2471,  4.7838,  6.3801,  5.5412, 12.0067,  5.7779,\n",
      "         6.0471,  5.5312,  5.6339,  5.9143,  5.7116,  5.7554,  5.4137,  5.8329,\n",
      "         5.1135,  5.4180,  5.4771,  6.1764,  5.6842,  6.3540,  4.9341,  5.6246,\n",
      "         5.1238,  5.0715,  5.6640,  5.0312,  5.7208,  5.8794,  5.9272,  5.6542,\n",
      "         5.4793,  5.3608,  5.7727,  4.9971,  5.7328,  5.9285,  5.2488,  6.6232,\n",
      "         6.0807,  5.4842,  4.1834,  5.4427,  5.7189,  6.0580,  5.9512,  5.5264,\n",
      "         5.8217,  5.8453,  5.4387,  5.9455,  5.6697,  5.4507,  6.3694,  5.6827,\n",
      "         5.6294,  6.0794,  5.6781,  5.8014,  5.4098,  5.2990,  6.0799,  5.5815,\n",
      "         5.9286,  5.5577,  5.4192,  4.4485,  4.0787,  5.3238,  6.0174,  5.4642,\n",
      "         5.6637,  5.4256,  5.8146,  5.0355,  3.9582,  5.2823,  6.1624,  5.2877,\n",
      "         5.7400,  6.2066,  5.4545,  4.8829,  5.1225,  4.9672,  5.8983,  6.0253,\n",
      "         6.6245,  6.0977,  6.2701,  6.2394,  6.1062,  5.9200,  5.8691,  5.8820,\n",
      "         5.3484,  5.8880,  5.4143,  6.1065,  5.0380,  5.9289,  6.0691,  5.8509,\n",
      "         5.6034,  5.5546,  5.4139,  6.1256,  5.6537,  6.1254,  6.0142,  5.6618,\n",
      "         5.3754,  5.9831,  4.8981,  5.9211,  5.5195,  5.6225,  5.5090,  5.0370,\n",
      "         5.6191,  6.2265,  5.2470,  5.0378,  5.2097,  5.0923,  5.5633,  6.0598,\n",
      "         5.7373,  5.7783,  5.7919,  6.2274,  5.4114,  5.6156,  5.2838,  5.5845,\n",
      "         4.9265,  5.7655,  5.5397,  5.9284,  6.2203,  5.9021,  5.9171,  5.6052,\n",
      "         5.6205,  6.1050,  6.1094,  5.9096,  5.8399,  6.0105,  5.6658,  3.5032,\n",
      "         6.0035,  4.9157,  5.7663,  6.0307,  5.9385,  5.6804,  5.9267,  6.2349,\n",
      "         5.6120,  5.3393,  4.8317,  5.2656,  5.7588,  5.6371,  5.5399,  5.6954,\n",
      "         6.5203,  4.6743,  5.9226,  5.4996,  5.8719,  5.8736,  5.9103,  5.2770,\n",
      "         5.4314,  5.1001,  5.6607,  5.7892,  5.5096,  6.1069,  5.5235,  5.8930,\n",
      "         5.0691,  5.9101,  5.8326,  5.5664,  6.0199,  5.6793,  5.1108,  5.7748,\n",
      "         4.9080,  5.8849,  5.5876,  5.4903,  5.2460,  5.3108,  5.7756,  5.0913,\n",
      "         5.4316,  5.5055,  5.1748,  5.3703,  5.7340,  5.6976,  5.8673,  5.6942,\n",
      "         5.9517,  5.7641,  5.8257,  5.3860,  5.3739,  5.6574,  3.9243,  5.7863,\n",
      "         5.2997,  5.9072,  5.7404,  5.7554,  6.3479,  5.4681,  5.6787,  5.9053,\n",
      "         5.4885,  5.9310,  6.0489,  6.2972,  5.2173,  5.5238,  6.0946,  5.9157,\n",
      "         3.8365,  5.4327,  5.8990,  6.0057,  6.0195,  5.6044,  5.6781,  6.2195,\n",
      "         5.4104,  4.7772,  5.7421,  5.6769,  5.7371,  5.6805,  5.4162,  5.8539,\n",
      "         6.3195,  5.9036,  6.2196,  5.0054,  4.9035,  6.0062,  5.6929,  5.0689,\n",
      "         5.0231,  5.7968,  5.2085,  5.4325,  5.9921,  5.6706,  5.5023,  3.4004],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 5.2048e-02,  5.7168e-02,  3.0671e-03,  7.6395e-03,  4.9073e-02,\n",
      "         -3.1210e-02,  6.1013e-02, -2.0448e-02, -4.3302e-02, -9.0638e-02,\n",
      "         -3.1305e-02,  8.1594e-03, -1.9025e-02,  6.1880e-02,  3.3327e-02,\n",
      "         -3.2257e-02, -5.1502e-02, -6.3603e-02,  4.7971e-02,  1.8586e-02,\n",
      "         -1.0727e-02, -9.0833e-02, -2.3802e-02, -4.8193e-02, -2.7362e-02,\n",
      "          4.1894e-02,  2.4672e-02,  3.4746e-02, -2.1268e-03,  6.4929e-02,\n",
      "         -4.7017e-02, -5.3023e-02,  7.5545e-02, -8.5475e-03, -1.8681e-02,\n",
      "          1.9075e-02, -3.6381e-02, -3.6849e-02,  6.7679e-02, -6.5267e-03,\n",
      "          5.4711e-02, -9.1222e-02, -8.6994e-03,  6.5640e-02,  7.8718e-02,\n",
      "         -1.8303e-02, -3.3819e-02, -5.8391e-02,  6.2718e-02, -7.2402e-02,\n",
      "         -2.5109e-02,  8.3493e-03,  6.9597e-02,  1.1976e-02, -2.4708e-02,\n",
      "         -3.9660e-02,  9.0227e-03,  4.8732e-03, -5.2985e-02,  5.2058e-02,\n",
      "         -4.4509e-02, -6.5427e-03, -7.9234e-02, -2.9316e-02, -4.5654e-02,\n",
      "          7.0541e-02, -2.9225e-02, -4.6966e-02,  1.9730e-02,  4.3822e-02,\n",
      "         -7.7667e-02,  3.8662e-02, -3.4669e-02,  2.9455e-02,  4.8630e-02,\n",
      "         -1.8747e-03, -4.0188e-02,  8.0200e-02,  3.1501e-02,  2.1120e-03,\n",
      "         -3.8209e-02, -1.7818e-02, -2.3472e-02,  2.2498e-02, -2.3097e-02,\n",
      "          2.0090e-03, -1.2906e-02, -9.4848e-03, -9.5199e-04, -3.1765e-02,\n",
      "          6.8914e-02, -2.4542e-02,  5.6995e-02,  7.3282e-02,  8.2201e-02,\n",
      "          1.6637e-02,  4.4703e-03,  6.5569e-03,  8.0486e-02,  8.2753e-02,\n",
      "          4.9726e-02,  4.7019e-03,  2.7874e-02, -2.8243e-02,  2.3701e-02,\n",
      "         -2.1732e-02, -8.0983e-02, -2.4880e-02,  1.1293e-02,  4.1167e-02,\n",
      "          1.5900e-02, -2.6957e-02, -2.3409e-03, -2.8293e-02,  6.9298e-02,\n",
      "          8.6540e-02,  2.7610e-02, -5.4482e-02,  7.8792e-02, -1.0626e-02],\n",
      "        [-4.3524e-02,  1.8156e-02,  8.7717e-02, -8.0032e-03, -8.4494e-02,\n",
      "          3.1344e-03, -6.1531e-02,  2.6168e-02, -4.7147e-02,  8.0173e-02,\n",
      "         -6.0429e-02, -1.4185e-02,  4.8383e-02,  9.0423e-02, -7.8544e-02,\n",
      "         -5.4365e-02, -6.4657e-02, -3.7951e-02, -6.7491e-02,  2.3218e-02,\n",
      "          3.1010e-02, -7.9554e-02,  8.9294e-02, -3.6038e-02, -6.0452e-02,\n",
      "         -8.4869e-02, -5.4067e-02,  1.2882e-02, -8.3364e-03,  3.1729e-02,\n",
      "          5.0105e-02, -1.4880e-02,  7.0933e-02,  7.4586e-02,  4.5191e-02,\n",
      "          2.1829e-02, -4.9204e-02, -3.6967e-02, -4.4102e-02,  1.2695e-03,\n",
      "          5.5328e-02, -2.6898e-02,  2.8673e-02,  7.6428e-02, -4.9325e-02,\n",
      "         -3.9762e-02, -5.5717e-02, -5.3646e-02,  5.1280e-02,  6.5921e-02,\n",
      "         -7.3198e-02,  4.0161e-02, -7.5979e-02,  4.1211e-03, -3.6509e-02,\n",
      "         -2.1323e-02,  1.8490e-03, -2.8367e-02, -6.8964e-02,  8.1847e-02,\n",
      "         -7.5619e-03, -5.9387e-02,  2.3774e-02, -1.0436e-03,  4.6711e-02,\n",
      "          3.2849e-02,  3.5498e-02, -6.8003e-02, -4.6913e-02,  6.8553e-02,\n",
      "          8.9541e-02,  7.6555e-02,  7.4681e-02,  5.8823e-02,  4.2131e-02,\n",
      "         -3.8928e-02, -6.3926e-02,  4.8324e-02, -3.6324e-02,  4.3068e-02,\n",
      "         -6.4895e-02, -5.1385e-02,  2.1957e-02,  2.9131e-02, -1.3269e-03,\n",
      "         -4.4508e-02,  8.5692e-02,  5.5540e-02,  6.6900e-02,  4.8650e-02,\n",
      "         -7.7568e-02, -6.1602e-02,  2.7745e-02,  2.5610e-02, -6.8598e-02,\n",
      "         -8.6124e-02, -8.1652e-02, -1.2628e-02, -6.9427e-02,  1.2360e-02,\n",
      "          7.5987e-02, -8.5291e-02, -2.0682e-02, -5.1821e-03, -2.8576e-02,\n",
      "         -3.9674e-02,  6.3789e-02,  6.9545e-02,  6.2593e-02,  8.1183e-03,\n",
      "          9.0493e-02,  8.0044e-02, -4.6054e-02, -1.4563e-02, -4.1635e-02,\n",
      "         -7.2867e-02,  3.0071e-02,  8.3747e-02, -2.4794e-02,  5.5450e-02],\n",
      "        [ 2.5866e-02,  7.9405e-02, -1.3064e-02,  6.1466e-02, -4.9401e-02,\n",
      "          5.4963e-02,  7.8275e-02, -2.8372e-02, -8.8238e-02,  1.7308e-02,\n",
      "          7.1522e-02, -2.3194e-03, -3.3775e-03, -2.5514e-02,  4.9239e-02,\n",
      "         -1.2804e-02,  1.4174e-02,  9.0242e-02,  1.7368e-03, -1.6282e-02,\n",
      "         -4.2620e-04,  6.4290e-02,  5.5246e-02,  5.0454e-02,  5.6458e-02,\n",
      "         -8.4477e-02, -4.5525e-03,  2.8170e-02, -5.2229e-02, -2.4550e-02,\n",
      "         -5.0321e-02, -2.2997e-02,  6.1522e-02, -1.9320e-02, -7.3430e-02,\n",
      "         -5.7278e-02,  5.0821e-02, -1.2768e-02,  7.8118e-02, -3.1079e-02,\n",
      "          7.7682e-02, -1.6414e-02,  6.5233e-02, -2.3002e-02, -6.5459e-02,\n",
      "         -8.0094e-02, -4.3684e-02,  7.1106e-02, -6.8804e-02, -7.2889e-02,\n",
      "          6.7741e-02,  8.4391e-02,  7.6311e-02,  7.9899e-02,  8.8938e-02,\n",
      "          8.3535e-02, -8.2436e-02, -3.5915e-03,  3.6392e-02,  2.2132e-02,\n",
      "          3.1260e-02,  6.6301e-02,  8.7614e-02,  2.0461e-02,  6.1568e-02,\n",
      "         -4.9511e-02,  3.3442e-02,  7.8538e-02,  2.1029e-02,  4.0005e-02,\n",
      "         -2.1998e-02, -9.1218e-02,  5.1963e-02, -4.5896e-02,  2.2432e-02,\n",
      "         -1.4790e-02,  4.9881e-02,  8.4726e-02, -1.6578e-02, -2.9716e-02,\n",
      "          8.3143e-02, -7.5306e-02,  2.5014e-02, -8.9941e-02, -3.9233e-02,\n",
      "          2.3438e-02, -7.7145e-04,  4.2188e-03, -8.8095e-02, -6.7135e-02,\n",
      "          4.9220e-02, -1.2141e-02, -6.6777e-02, -1.5566e-02, -2.3173e-02,\n",
      "         -5.1371e-02, -2.5879e-02, -8.1535e-02, -4.1013e-02, -4.7851e-02,\n",
      "         -6.6397e-02,  8.7765e-02,  8.3044e-02, -4.6970e-02, -2.6849e-02,\n",
      "          7.5462e-02,  7.9259e-03, -1.6544e-02, -6.8735e-02, -4.4598e-02,\n",
      "          4.9845e-04, -3.7067e-02, -8.1111e-02,  5.1616e-02,  6.2984e-02,\n",
      "          4.7545e-02,  3.6439e-02,  1.2973e-02, -5.8141e-02, -4.1015e-02],\n",
      "        [-7.0770e-02, -2.3912e-02, -5.4811e-02, -6.8518e-02,  5.3705e-02,\n",
      "         -1.7017e-02, -3.7094e-02, -3.2247e-02, -7.7896e-02,  8.7733e-02,\n",
      "         -7.5835e-02,  1.9084e-02, -1.7063e-02,  5.7932e-02,  7.0905e-02,\n",
      "          3.0538e-02,  7.1833e-02, -1.8382e-02, -8.6879e-02, -2.4391e-03,\n",
      "         -1.8961e-04,  6.5857e-02, -1.3579e-02, -8.7461e-02,  7.2151e-03,\n",
      "         -4.0687e-02, -4.0339e-02, -8.6915e-03,  6.8748e-02,  3.1219e-02,\n",
      "          6.6404e-02, -4.3203e-02, -9.0433e-02, -1.6474e-02,  9.1238e-02,\n",
      "          7.1505e-03, -4.5745e-02,  1.5214e-02,  7.8110e-02, -9.1125e-02,\n",
      "         -5.9617e-02,  3.8887e-02, -7.7777e-02,  3.2791e-02, -8.0137e-02,\n",
      "         -3.3836e-02,  5.9641e-03, -5.0180e-02,  6.7538e-02,  7.5382e-02,\n",
      "         -2.5732e-02, -5.5171e-02,  6.5320e-02,  4.2669e-02,  7.9218e-02,\n",
      "         -5.5700e-02, -8.2238e-02,  8.0124e-03,  3.1590e-03, -2.5704e-02,\n",
      "         -5.6741e-03,  4.9395e-04,  1.2604e-02,  3.7754e-02, -5.2252e-02,\n",
      "         -6.3988e-05, -1.9274e-02,  6.4993e-02,  3.3794e-02, -3.0669e-02,\n",
      "          8.3265e-02, -8.6009e-03, -2.9982e-02,  2.9370e-02,  5.1132e-02,\n",
      "         -2.7723e-03, -6.1257e-02,  5.1481e-03,  7.6030e-02, -1.1829e-02,\n",
      "          9.0769e-02, -3.5690e-02, -8.0731e-02,  3.9631e-02, -4.0781e-02,\n",
      "         -6.1122e-02, -2.0530e-02,  2.9503e-02, -8.8690e-02,  7.4762e-02,\n",
      "          4.4173e-02, -5.6355e-02, -8.4142e-02,  4.5109e-02,  5.6040e-02,\n",
      "          2.7298e-02, -4.0730e-02, -3.6586e-02, -6.3828e-02,  7.0806e-03,\n",
      "          5.3247e-02, -2.2195e-02,  3.6842e-02,  8.9839e-02, -4.8552e-03,\n",
      "         -8.5568e-02, -5.2697e-04, -7.2547e-02,  6.8513e-02,  8.0152e-02,\n",
      "         -4.0386e-02,  3.2646e-02, -6.3814e-02, -2.5210e-02,  5.9936e-02,\n",
      "         -7.8991e-02, -8.7769e-02,  3.5507e-02,  5.9511e-02, -8.5726e-02],\n",
      "        [ 4.8205e-02,  4.3737e-02,  5.5488e-02,  5.3117e-02, -6.8854e-02,\n",
      "         -1.3530e-02,  6.8259e-02, -2.2976e-02,  5.1076e-02,  4.4870e-02,\n",
      "         -5.5534e-02,  8.6940e-02,  7.3180e-02,  4.7798e-02,  7.6495e-02,\n",
      "         -6.4625e-02,  4.9923e-02,  4.1502e-02, -5.2620e-02, -1.0773e-03,\n",
      "          1.4424e-02, -1.8933e-02, -7.4799e-02,  2.4285e-02, -4.0002e-03,\n",
      "         -7.5474e-02, -7.4636e-03, -4.3678e-02, -2.8748e-02,  5.2301e-02,\n",
      "         -3.6687e-02,  8.5370e-02,  5.5577e-03, -1.4829e-02, -4.3796e-02,\n",
      "          7.7980e-02,  3.8031e-03, -5.7689e-02, -2.8371e-02, -1.1608e-02,\n",
      "          4.0987e-03, -6.7840e-02, -3.0961e-02, -6.1162e-02,  2.4371e-02,\n",
      "          6.4245e-02, -2.8524e-03, -3.7693e-03,  6.6016e-02, -7.4197e-02,\n",
      "         -6.9313e-02, -6.9040e-02,  6.8146e-02,  2.4434e-02,  3.0487e-02,\n",
      "          8.0080e-03, -2.9285e-03,  8.8121e-02, -6.8492e-02, -7.0580e-02,\n",
      "          4.6320e-02,  1.0411e-02, -3.9706e-02, -6.7674e-02, -3.0895e-02,\n",
      "          8.5238e-02,  8.3126e-02, -2.1042e-02, -5.0605e-02,  5.7306e-02,\n",
      "         -8.0013e-02,  3.9645e-02, -8.2224e-02,  6.4132e-02, -4.4297e-02,\n",
      "         -6.4583e-02,  6.2014e-03, -3.9227e-03,  2.5545e-02, -9.0680e-02,\n",
      "         -6.4731e-02, -8.2919e-02, -3.7788e-02,  9.1263e-02,  6.1929e-02,\n",
      "         -6.1832e-02,  1.5195e-02,  2.7057e-02,  1.3705e-02,  1.9044e-02,\n",
      "         -1.6152e-02, -5.4144e-02, -4.3002e-02, -6.1121e-02, -6.3252e-02,\n",
      "          6.1997e-02, -2.0995e-02,  1.8157e-03, -7.9598e-02,  6.1480e-02,\n",
      "         -3.8141e-03,  6.0415e-02, -2.9940e-02, -2.6105e-02, -2.5436e-02,\n",
      "          1.5003e-02,  7.9838e-02, -3.8747e-02,  8.9901e-02, -3.8925e-02,\n",
      "          1.9897e-02, -3.8338e-02,  4.5365e-02, -3.5450e-02,  4.2659e-02,\n",
      "         -3.1645e-02,  9.0453e-02, -7.4355e-02,  8.6922e-02, -3.2312e-02],\n",
      "        [ 7.8611e-02,  5.0023e-02,  3.1569e-02,  7.4060e-02,  3.4692e-02,\n",
      "         -3.8813e-02, -1.3483e-03,  6.5831e-02,  8.4281e-02, -6.6874e-02,\n",
      "         -2.8215e-02, -8.0277e-02, -4.5427e-02, -4.2700e-02,  4.4488e-02,\n",
      "         -8.3317e-02, -6.9992e-02, -3.6561e-02, -8.5886e-03,  5.8279e-04,\n",
      "          2.4995e-02,  5.7487e-02, -5.3198e-02,  9.0176e-03,  8.1387e-04,\n",
      "         -7.6916e-02,  8.6139e-02, -8.7993e-02,  8.3792e-02,  5.5021e-02,\n",
      "         -7.8043e-02, -1.0653e-02, -6.6272e-02,  7.9602e-02,  3.5707e-02,\n",
      "         -4.4018e-03,  2.7479e-02,  1.6917e-02, -5.4429e-02, -6.8105e-02,\n",
      "         -2.8116e-02,  3.8720e-02, -8.8406e-02,  2.8788e-02,  7.8700e-02,\n",
      "          3.1168e-02,  5.0301e-02,  7.2557e-02,  7.7166e-02,  5.1948e-02,\n",
      "         -1.3104e-02,  1.7543e-03, -4.1497e-02, -6.6288e-02,  2.4006e-02,\n",
      "         -6.7753e-02, -4.6466e-02,  7.9295e-02,  4.0833e-02,  6.2728e-02,\n",
      "         -5.4178e-02,  2.7313e-02, -7.6762e-02,  7.8231e-02, -4.7464e-02,\n",
      "         -3.3610e-02,  3.9732e-02,  8.1485e-02, -4.1542e-02,  2.6831e-02,\n",
      "         -2.6617e-02, -4.0696e-02,  6.0360e-02, -5.9014e-02,  3.0745e-02,\n",
      "         -8.7112e-03,  4.4217e-02,  8.5085e-02,  5.0352e-02,  8.7947e-02,\n",
      "         -7.5486e-02, -2.8877e-02,  7.8583e-02, -8.3780e-02,  9.0747e-03,\n",
      "         -7.3395e-04, -3.2927e-02,  1.5885e-02, -2.2105e-02,  7.7039e-02,\n",
      "         -4.3619e-02, -1.2271e-02, -4.4721e-02,  4.5569e-02,  6.4266e-02,\n",
      "         -3.8143e-02, -9.6399e-03, -5.2199e-02, -8.3583e-02, -6.2214e-02,\n",
      "          5.2032e-02, -2.3657e-02, -4.2494e-02,  2.2489e-02,  6.0503e-02,\n",
      "          8.6333e-02, -4.2516e-02, -8.4919e-02,  2.6706e-02, -6.3692e-02,\n",
      "          8.8765e-02,  1.0036e-02, -5.0520e-02, -5.6726e-02, -2.9083e-02,\n",
      "         -5.2542e-02,  8.8274e-02, -2.4867e-03, -8.9578e-02, -7.2576e-02],\n",
      "        [ 8.9572e-02, -2.3469e-02,  6.2593e-02, -9.8068e-03,  4.4818e-02,\n",
      "         -1.7806e-02,  7.7258e-02, -8.7955e-02,  2.0717e-02,  3.1078e-02,\n",
      "          5.9258e-02,  7.0024e-02,  1.4849e-02, -5.8861e-02, -1.1326e-02,\n",
      "         -6.5491e-02,  2.6469e-02, -2.5098e-02,  5.2014e-02,  1.3741e-02,\n",
      "         -1.3696e-02,  1.1992e-02, -5.7642e-02, -4.6052e-02, -3.6499e-02,\n",
      "         -3.3648e-02, -6.9920e-02, -3.0771e-02,  7.1880e-02,  8.9643e-02,\n",
      "          7.9920e-03,  1.2432e-02, -8.8859e-02, -2.9080e-02,  8.1511e-02,\n",
      "          3.3576e-02,  2.4451e-02, -8.1077e-02, -4.2202e-02,  6.4616e-02,\n",
      "          3.9724e-02, -5.9294e-02,  1.7537e-02,  2.1365e-02, -3.0884e-02,\n",
      "         -7.9861e-02,  5.2485e-02, -7.4168e-02, -8.7185e-02, -7.2453e-02,\n",
      "         -6.0077e-02, -6.7730e-02, -5.2165e-03, -6.1453e-02, -4.5829e-03,\n",
      "         -8.4484e-02, -3.0384e-02,  2.6585e-02,  8.9533e-02,  7.0750e-02,\n",
      "         -8.6600e-02, -5.7153e-02,  7.1428e-02, -5.2724e-02,  4.8475e-02,\n",
      "         -2.1627e-02, -6.5967e-02, -4.3553e-02, -1.8233e-02, -6.9964e-02,\n",
      "         -7.8359e-02, -6.5171e-02, -6.6544e-03, -4.6473e-02, -3.8572e-02,\n",
      "         -3.3438e-02,  7.8866e-02,  6.8237e-02,  7.6397e-02, -8.9299e-03,\n",
      "          2.1678e-02, -4.4581e-02,  8.8017e-02, -1.1254e-02, -1.0109e-02,\n",
      "          6.7245e-02,  8.8134e-02,  6.3138e-02,  2.1344e-02,  6.6490e-02,\n",
      "         -2.1411e-02, -4.8203e-03, -8.1019e-02,  8.3602e-03,  7.8788e-02,\n",
      "          6.3118e-02,  4.8445e-02,  2.6266e-02,  6.5160e-02,  3.6574e-02,\n",
      "         -4.3626e-02,  3.6461e-02, -7.2867e-02,  8.9647e-02, -4.8377e-02,\n",
      "         -2.7007e-02,  6.1859e-02, -7.2727e-02,  7.8305e-02, -1.4361e-02,\n",
      "          5.1413e-02,  7.4190e-02, -6.3165e-02, -8.4459e-02,  5.3581e-02,\n",
      "         -2.6601e-02, -8.2763e-02,  4.4337e-02,  6.2857e-02, -6.2388e-03],\n",
      "        [ 7.1211e-02,  6.6658e-02, -5.9428e-02, -4.2631e-02,  6.9394e-02,\n",
      "         -7.0819e-03, -3.5744e-02, -3.8549e-02, -6.2739e-02,  2.1319e-02,\n",
      "          5.5233e-02,  6.3317e-02, -3.8381e-02, -2.7255e-02, -1.4849e-02,\n",
      "          4.3682e-02, -4.0378e-02, -6.4587e-02,  6.8455e-02,  1.8486e-02,\n",
      "          6.8346e-02, -4.1347e-02, -1.5260e-02,  7.0921e-02, -8.5917e-02,\n",
      "         -6.5156e-02,  8.3500e-02, -2.0661e-02,  4.1561e-03,  1.4112e-02,\n",
      "         -4.4986e-02,  6.9690e-02,  1.8034e-02,  6.4943e-02, -5.2788e-02,\n",
      "          1.4952e-02,  7.5921e-02, -1.7352e-02, -2.1763e-02, -7.8053e-02,\n",
      "          6.3141e-02,  2.5997e-02, -6.3720e-03, -6.5469e-02, -7.4798e-02,\n",
      "         -7.9742e-02,  8.2475e-02, -4.2604e-02,  1.9424e-02,  1.5992e-02,\n",
      "          6.4784e-02, -2.1470e-03, -2.1254e-02,  1.1253e-02, -6.4680e-02,\n",
      "         -4.9694e-02,  8.1259e-03, -4.4849e-02,  7.1986e-03, -2.9087e-02,\n",
      "          4.8122e-02,  1.0728e-02, -7.5657e-02, -5.1599e-03, -4.0601e-02,\n",
      "         -8.7286e-02, -3.1102e-02, -6.0128e-02, -6.4449e-02,  4.5978e-02,\n",
      "          3.2670e-02,  2.4521e-02, -5.6797e-02,  2.0140e-02,  7.6288e-02,\n",
      "          4.9833e-02,  4.5984e-02,  5.2504e-02, -7.8745e-02,  2.0454e-02,\n",
      "         -5.5656e-02,  7.4411e-02,  5.9370e-02,  8.0931e-02, -5.5604e-02,\n",
      "          9.1125e-02,  8.0322e-02, -1.8972e-02,  2.6521e-03,  6.9167e-02,\n",
      "          6.1412e-02, -4.3952e-02, -1.4217e-02,  3.2643e-02, -6.6850e-02,\n",
      "          8.5213e-02, -8.7090e-02,  3.5398e-02,  3.6040e-02,  4.0841e-02,\n",
      "         -3.1422e-02,  5.7057e-02, -3.8533e-02,  8.3216e-02,  7.1672e-02,\n",
      "          4.7856e-02, -1.6386e-02, -2.1815e-02, -3.0048e-02,  3.7858e-02,\n",
      "         -4.7731e-02, -4.1232e-02, -1.9629e-02,  9.6535e-03,  2.4985e-02,\n",
      "         -5.5127e-02, -6.7492e-02, -8.4300e-02, -1.8194e-02, -4.7798e-02]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([42.3184], requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0562,  0.0294, -0.0143,  ..., -0.0432, -0.0331,  0.0290],\n",
      "        [-0.0670,  0.0472,  0.0270,  ..., -0.0391,  0.0353, -0.0081],\n",
      "        [ 0.0344,  0.0382,  0.0156,  ...,  0.0544,  0.0366, -0.0016],\n",
      "        ...,\n",
      "        [-0.0518,  0.0557,  0.0287,  ...,  0.0241,  0.0218, -0.0327],\n",
      "        [ 0.0378, -0.0404, -0.0468,  ..., -0.0469, -0.0127,  0.0257],\n",
      "        [ 0.0492, -0.0028, -0.0140,  ...,  0.0101,  0.0254, -0.0140]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0034, -0.0044,  0.0147,  ..., -0.0160,  0.0124,  0.0164],\n",
      "        [-0.0034, -0.0043,  0.0146,  ..., -0.0158,  0.0123,  0.0163],\n",
      "        [ 0.0034,  0.0043, -0.0144,  ...,  0.0156, -0.0121, -0.0160],\n",
      "        ...,\n",
      "        [-0.0040, -0.0048,  0.0151,  ..., -0.0162,  0.0127,  0.0166],\n",
      "        [-0.0037, -0.0045,  0.0145,  ..., -0.0156,  0.0123,  0.0160],\n",
      "        [ 0.0036,  0.0044, -0.0147,  ...,  0.0160, -0.0125, -0.0165]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3820, 0.3322, 0.3476, 0.3641, 0.3387, 0.3581, 0.3388, 0.3812, 0.3638,\n",
      "        0.3607, 0.3592, 0.3438, 0.3602, 0.3802, 0.3636, 0.3859, 0.3524, 0.3227,\n",
      "        0.3773, 0.3779, 0.3686, 0.3628, 0.3698, 0.3746, 0.3551, 0.3647, 0.3329,\n",
      "        0.3586, 0.3606, 0.3879, 0.3563, 0.3815, 0.3653, 0.3333, 0.3619, 0.3462,\n",
      "        0.3741, 0.3713, 0.3703, 0.3822, 0.3533, 0.3653, 0.3807, 0.3492, 0.3639,\n",
      "        0.3659, 0.3777, 0.3781, 0.3648, 0.3882, 0.3525, 0.3596, 0.3418, 0.3780,\n",
      "        0.3783, 0.3574, 0.3627, 0.3546, 0.3590, 0.3654, 0.3505, 0.3631, 0.3694,\n",
      "        0.3624, 0.3356, 0.3832, 0.3840, 0.3824, 0.3626, 0.3831, 0.3392, 0.3764,\n",
      "        0.3521, 0.3500, 0.3308, 0.3566, 0.3598, 0.3554, 0.3767, 0.3603, 0.3814,\n",
      "        0.3777, 0.3785, 0.3497, 0.3610, 0.3698, 0.3578, 0.3357, 0.3567, 0.3746,\n",
      "        0.3531, 0.3663, 0.3521, 0.3667, 0.3242, 0.3693, 0.3452, 0.3751, 0.3823,\n",
      "        0.3332, 0.3352, 0.3548, 0.3402, 0.3524, 0.3400, 0.3667, 0.3544, 0.3703,\n",
      "        0.3606, 0.3454, 0.3471, 0.3791, 0.3654, 0.3818, 0.3368, 0.3834, 0.3653,\n",
      "        0.4041, 0.3606, 0.3264, 0.3892, 0.3293, 0.3495, 0.3614, 0.3807, 0.3521,\n",
      "        0.3794, 0.3482, 0.3651, 0.3765, 0.3690, 0.3581, 0.3941, 0.3513, 0.3542,\n",
      "        0.3842, 0.3382, 0.3882, 0.3659, 0.3741, 0.3543, 0.3476, 0.3724, 0.3516,\n",
      "        0.3830, 0.3625, 0.3619, 0.3497, 0.3650, 0.3638, 0.3702, 0.3741, 0.3694,\n",
      "        0.3727, 0.3865, 0.3797, 0.3530, 0.3617, 0.3811, 0.3747, 0.3602, 0.3490,\n",
      "        0.3816, 0.3730, 0.3876, 0.3543, 0.3337, 0.3693, 0.3731, 0.3906, 0.3909,\n",
      "        0.3606, 0.3557, 0.3720, 0.3463, 0.3643, 0.3589, 0.3781, 0.3202, 0.4087,\n",
      "        0.3726, 0.3653, 0.3747, 0.3638, 0.3664, 0.3594, 0.3716, 0.3675, 0.3543,\n",
      "        0.3659, 0.3862, 0.3552, 0.3766, 0.3604, 0.3766, 0.3795, 0.3724, 0.3628,\n",
      "        0.3586, 0.3681, 0.3695, 0.3681, 0.3706, 0.3453, 0.3378, 0.3821, 0.3647,\n",
      "        0.3420, 0.3893, 0.3836, 0.3481, 0.3448, 0.3732, 0.3814, 0.3863, 0.3486,\n",
      "        0.3707, 0.3371, 0.3918, 0.3550, 0.3662, 0.3809, 0.3516, 0.3557, 0.3280,\n",
      "        0.3655, 0.3595, 0.3645, 0.3754, 0.3742, 0.3905, 0.3719, 0.3609, 0.3518,\n",
      "        0.3521, 0.3696, 0.3799, 0.3526, 0.3284, 0.3566, 0.3600, 0.3719, 0.3623,\n",
      "        0.3902, 0.3328, 0.3652, 0.3483, 0.3832, 0.3742, 0.3639, 0.3907, 0.3687,\n",
      "        0.3765, 0.3659, 0.3455, 0.3978, 0.3724, 0.3556, 0.3524, 0.3753, 0.3850,\n",
      "        0.3566, 0.3194, 0.3378, 0.3681, 0.3768, 0.3615, 0.3578, 0.3527, 0.3770,\n",
      "        0.3754, 0.3829, 0.3658, 0.3598, 0.3664, 0.3559, 0.3339, 0.3505, 0.3641,\n",
      "        0.3441, 0.3380, 0.3453, 0.3585, 0.3540, 0.3542, 0.3596, 0.3337, 0.3530,\n",
      "        0.3689, 0.3233, 0.3706, 0.3768, 0.3924, 0.3556, 0.3521, 0.3253, 0.3700,\n",
      "        0.3592, 0.3627, 0.3656, 0.3640, 0.3646, 0.3868, 0.3557, 0.3680, 0.3707,\n",
      "        0.3183, 0.3595, 0.3611, 0.3692, 0.3572, 0.3906, 0.3786, 0.3370, 0.3661,\n",
      "        0.3901, 0.3391, 0.3505, 0.3843, 0.3715], requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0229, -0.0032,  0.0024,  ...,  0.0399, -0.0422, -0.0317],\n",
      "        [-0.0360,  0.0442,  0.0251,  ...,  0.0456, -0.0438, -0.0039],\n",
      "        [ 0.0374,  0.0262, -0.0350,  ...,  0.0578,  0.0137, -0.0243],\n",
      "        ...,\n",
      "        [-0.0459,  0.0441,  0.0258,  ..., -0.0362,  0.0315, -0.0105],\n",
      "        [ 0.0675,  0.0431,  0.0039,  ...,  0.0300,  0.0256, -0.0270],\n",
      "        [ 0.0188,  0.0433,  0.0521,  ..., -0.0079,  0.0015,  0.0349]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0153,  0.0132,  0.0064,  0.0118, -0.0060, -0.0155,  0.0108, -0.0092],\n",
      "        [-0.0153, -0.0132, -0.0064, -0.0118,  0.0060,  0.0155, -0.0108,  0.0092]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3824, 0.3449], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in light_mod.model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0165,  0.0334, -0.0204,  ...,  0.0451,  0.0146,  0.0468],\n",
      "        [-0.0049, -0.0374,  0.0132,  ...,  0.0175,  0.0412, -0.0162],\n",
      "        [ 0.0576,  0.0062,  0.0150,  ..., -0.0121, -0.0313, -0.0231],\n",
      "        ...,\n",
      "        [-0.0432, -0.0117, -0.0119,  ..., -0.0162, -0.0306, -0.0155],\n",
      "        [ 0.0100,  0.0110,  0.0332,  ...,  0.0574,  0.0319,  0.0545],\n",
      "        [ 0.0162,  0.0118,  0.0055,  ..., -0.0026, -0.0303,  0.0268]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0053, -0.0035, -0.0028,  ..., -0.0029,  0.0065, -0.0030],\n",
      "        [ 0.0045, -0.0048,  0.0025,  ..., -0.0022,  0.0029, -0.0058],\n",
      "        [ 0.0019, -0.0031,  0.0016,  ...,  0.0032,  0.0028, -0.0031],\n",
      "        ...,\n",
      "        [ 0.0069, -0.0051,  0.0014,  ...,  0.0009,  0.0055, -0.0057],\n",
      "        [ 0.0010,  0.0009, -0.0001,  ..., -0.0010,  0.0010, -0.0009],\n",
      "        [ 0.0079, -0.0094,  0.0040,  ..., -0.0086,  0.0090, -0.0073]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5781, 0.9372, 0.8809, 1.0033, 0.8477, 1.7176, 0.8676, 1.3902, 0.7466,\n",
      "        0.6137, 0.8261, 1.1227, 1.2862, 0.9293, 1.2248, 1.8992, 2.3413, 1.6393,\n",
      "        1.3645, 0.8317, 2.1616, 0.5547, 1.0968, 1.0541, 0.8388, 0.7461, 1.4121,\n",
      "        1.6743, 1.0309, 2.0691, 1.1874, 2.2225, 1.3512, 1.6381, 1.6750, 1.6713,\n",
      "        1.5406, 0.9796, 1.3858, 1.1167, 1.3741, 0.7335, 0.9230, 0.7372, 0.9518,\n",
      "        1.7471, 1.5831, 1.0939, 0.5177, 0.8925, 0.7904, 0.7818, 1.0882, 1.2832,\n",
      "        0.8931, 1.1064, 0.6561, 0.8410, 1.0319, 1.2230, 1.3876, 0.4918, 1.3844,\n",
      "        1.1689, 1.1810, 0.8859, 0.9240, 1.6844, 1.8682, 0.6566, 1.6003, 1.1143,\n",
      "        1.7788, 1.4153, 1.4323, 0.7771, 0.4644, 1.7416, 1.6233, 1.4719, 1.1619,\n",
      "        1.2528, 0.9507, 1.2672, 0.6533, 1.1783, 1.5514, 1.6861, 1.6612, 1.1453,\n",
      "        1.5638, 1.3663, 1.6741, 1.8254, 1.0866, 1.4957, 0.5591, 0.5481, 0.7044,\n",
      "        0.8973, 0.6166, 1.2448, 2.0408, 1.5247, 0.6321, 0.5450, 0.5237, 1.4074,\n",
      "        1.0516, 1.1237, 1.2419, 1.6270, 0.5085, 0.4480, 0.6902, 0.5150, 0.6831,\n",
      "        1.0655, 1.0800, 0.9255, 0.4819, 0.5731, 0.6329, 0.6735, 1.2346, 0.5808,\n",
      "        1.3939, 1.2774, 0.8629, 0.6237, 1.5994, 1.5482, 1.0643, 0.9092, 1.0304,\n",
      "        0.7668, 0.3517, 0.8597, 1.4035, 0.9897, 1.2101, 1.2424, 1.0726, 0.7405,\n",
      "        1.8432, 1.2710, 1.5719, 1.6019, 2.3057, 0.4677, 1.6149, 1.8393, 1.5934,\n",
      "        1.2412, 1.2087, 0.5098, 0.4487, 2.5967, 1.4509, 0.9533, 0.7603, 0.8364,\n",
      "        1.1154, 1.1128, 1.3627, 1.5599, 1.6057, 0.7549, 1.1065, 1.0552, 0.8764,\n",
      "        1.3390, 1.8499, 1.4040, 0.7992, 0.7317, 1.3197, 0.9295, 1.2131, 1.9355,\n",
      "        1.4630, 1.3273, 0.9082, 1.6768, 0.7343, 0.8331, 1.0196, 1.9387, 1.3097,\n",
      "        1.2337, 1.4808, 1.2830, 0.5842, 1.1478, 0.8853, 1.1761, 1.2557, 0.6568,\n",
      "        1.0915, 1.7117, 0.4936, 0.9333, 1.2350, 0.9502, 0.7568, 1.0404, 1.7910,\n",
      "        0.9933, 0.4456, 0.6441, 1.4811, 0.9798, 0.8798, 0.7317, 0.6829, 0.8222,\n",
      "        0.5102, 0.5937, 0.3629, 1.1157, 1.1968, 1.3278, 0.7267, 0.6472, 3.4048,\n",
      "        2.2900, 2.3311, 2.2945, 1.4579, 2.1614, 1.5380, 1.7398, 3.1827, 2.2808,\n",
      "        2.9339, 1.6725, 2.8790, 1.8528, 2.1287, 2.2582, 1.1441, 0.9010, 0.5648,\n",
      "        1.4487, 1.9112, 0.8346, 1.1037, 1.2079, 0.7097, 1.1520, 1.4437, 1.2171,\n",
      "        0.4941, 1.9583, 1.0643, 0.8396, 0.5810, 1.1346, 1.4868, 1.2811, 1.6695,\n",
      "        0.5453, 1.7830, 0.8295, 1.0355, 0.9410, 0.5985, 1.5751, 0.7213, 1.8798,\n",
      "        1.8930, 0.8579, 1.3189, 1.5664, 1.6966, 1.4005, 1.2296, 0.7792, 1.7292,\n",
      "        1.2636, 0.9128, 1.0138, 2.0365, 1.9598, 2.2714, 2.4792, 1.1147, 1.1151,\n",
      "        1.6393, 0.7843, 1.6993, 1.9090, 1.2698, 0.7159, 1.6118, 1.0744, 0.8689,\n",
      "        1.3316, 0.5652, 1.0069, 1.1440, 2.0358, 1.8987, 1.1043, 0.5318, 0.5352,\n",
      "        1.4377, 0.4776, 0.4693, 2.1665, 1.5444, 1.1223, 1.5175, 1.9545, 1.1222,\n",
      "        1.5198, 2.3906, 0.5602, 0.8637, 1.0463], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0165, -0.0443, -0.0062,  ...,  0.0508, -0.0401,  0.0251],\n",
      "        [-0.0078,  0.0398, -0.0064,  ...,  0.0239, -0.0056, -0.0421],\n",
      "        [ 0.0171, -0.0079,  0.0491,  ...,  0.0132, -0.0199, -0.0104],\n",
      "        ...,\n",
      "        [-0.0023, -0.0676,  0.0467,  ...,  0.0102, -0.0015, -0.0049],\n",
      "        [-0.0561, -0.0355, -0.0491,  ..., -0.0451,  0.0463, -0.0394],\n",
      "        [ 0.0393,  0.0165, -0.0071,  ...,  0.0419, -0.0392, -0.0389]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.0704e-03,  6.0018e-03,  6.6971e-03,  ..., -1.1011e-02,\n",
      "         -4.1556e-03,  8.2221e-03],\n",
      "        [ 3.6563e-03, -1.2084e-03, -8.0422e-03,  ...,  4.3302e-03,\n",
      "          7.1306e-03, -8.9303e-03],\n",
      "        [ 5.9162e-03, -4.1235e-03, -1.2817e-03,  ...,  2.0689e-03,\n",
      "          8.2159e-03, -5.7432e-03],\n",
      "        ...,\n",
      "        [ 2.1906e-03,  5.7527e-03,  5.1682e-03,  ..., -5.8070e-03,\n",
      "         -3.8293e-03,  5.0486e-03],\n",
      "        [-5.6859e-04,  2.4162e-03,  1.2826e-03,  ...,  3.7534e-03,\n",
      "          3.2043e-03, -2.1172e-03],\n",
      "        [-2.6695e-03, -4.3248e-03,  3.6102e-03,  ..., -3.3616e-03,\n",
      "         -2.6608e-03, -3.1646e-05]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8562, 1.4884, 1.3027, 1.4847, 1.3219, 1.5533, 0.8277, 1.3269, 1.1479,\n",
      "        1.3291, 1.5935, 1.8990, 1.3801, 1.2164, 1.2423, 1.8232, 2.2486, 1.5217,\n",
      "        1.4243, 1.9165, 1.6938, 1.8138, 1.1293, 1.0799, 1.4728, 1.1220, 2.2932,\n",
      "        2.0826, 2.0504, 1.5527, 1.1548, 2.2669, 2.3088, 2.3812, 2.4061, 1.5857,\n",
      "        1.5192, 1.2384, 1.6925, 1.1454, 1.2906, 1.9257, 1.5601, 1.8094, 1.3440,\n",
      "        1.7490, 1.8426, 1.1575, 0.9391, 0.8119, 1.3102, 1.8890, 1.6294, 1.5461,\n",
      "        0.9728, 1.2629, 1.0748, 1.5004, 1.6988, 1.2041, 2.1587, 1.0513, 1.3584,\n",
      "        1.6943, 1.7824, 0.8956, 1.7069, 1.9571, 1.9105, 1.7749, 1.6152, 1.0300,\n",
      "        1.2924, 2.1525, 2.0250, 1.7705, 1.3397, 1.3393, 1.7937, 1.8697, 2.0307,\n",
      "        2.3262, 1.5618, 1.7114, 1.5982, 1.2545, 2.0150, 1.9792, 2.3378, 1.5460,\n",
      "        2.3679, 1.8560, 1.2163, 1.7519, 0.9059, 2.0972, 0.8416, 1.0954, 1.2628,\n",
      "        1.0814, 1.2229, 1.6286, 1.9311, 1.8545, 0.9502, 0.9968, 1.1143, 1.5576,\n",
      "        1.4853, 1.6469, 1.5948, 1.6463, 0.5573, 0.8292, 0.8956, 0.8480, 1.3123,\n",
      "        1.7994, 1.1347, 0.7854, 0.6315, 0.7666, 1.1879, 1.6057, 1.8517, 1.0453,\n",
      "        1.3332, 1.8357, 1.4420, 0.8323, 1.2533, 1.3455, 0.6991, 0.6949, 1.0533,\n",
      "        0.9432, 0.4396, 1.1569, 1.2242, 0.8096, 0.7423, 0.5525, 1.0910, 0.6294,\n",
      "        2.1230, 1.7292, 1.9409, 1.9630, 1.4585, 2.6214, 1.5315, 1.8813, 2.3809,\n",
      "        2.2048, 2.3169, 1.9177, 2.1372, 2.0299, 1.5897, 0.9222, 1.6061, 1.8197,\n",
      "        1.8303, 1.6938, 1.9638, 1.5399, 2.1513, 0.7072, 1.4340, 1.4315, 1.6164,\n",
      "        2.2310, 1.7971, 1.7271, 1.0786, 0.7714, 1.2121, 1.5603, 2.1402, 2.1698,\n",
      "        1.7229, 1.5031, 1.2509, 1.8936, 1.8184, 1.6031, 1.4551, 2.0146, 1.7322,\n",
      "        1.5267, 1.2333, 1.4950, 0.7937, 1.5306, 1.1809, 1.3249, 1.7870, 1.0798,\n",
      "        1.1167, 2.1665, 0.9292, 0.8947, 1.4685, 1.7166, 1.4174, 1.5906, 2.0177,\n",
      "        1.3240, 0.9308, 0.8247, 1.6919, 1.1438, 1.0444, 0.6807, 0.8172, 0.8324,\n",
      "        0.8163, 0.9968, 0.4942, 1.2175, 1.2153, 0.7825, 0.6389, 0.8285, 2.9348,\n",
      "        2.7219, 2.8567, 1.7094, 2.3167, 1.3959, 1.4203, 1.6888, 2.2562, 1.7832,\n",
      "        1.8490, 2.2756, 1.4417, 2.2137, 2.0288, 1.8278, 2.1035, 1.6682, 1.4848,\n",
      "        1.6561, 1.5076, 1.3575, 1.3674, 1.3221, 0.9124, 1.8806, 2.2732, 1.9356,\n",
      "        1.7632, 1.9201, 0.9784, 0.9087, 1.4321, 0.8496, 1.8241, 1.6485, 1.6313,\n",
      "        1.2649, 1.4303, 0.7446, 1.0029, 1.6803, 1.2023, 1.6057, 1.5424, 1.6240,\n",
      "        2.1106, 0.9067, 1.8017, 1.8683, 1.1523, 1.5284, 1.7782, 2.7731, 1.5426,\n",
      "        1.3896, 0.9869, 1.4265, 2.7190, 2.1871, 1.9915, 1.5247, 1.2344, 1.0865,\n",
      "        2.2350, 2.0913, 1.8349, 2.1330, 1.3986, 1.1405, 1.7397, 1.1469, 2.0471,\n",
      "        1.8656, 2.2223, 1.8872, 1.6362, 1.9923, 1.9347, 1.2233, 2.1704, 1.8013,\n",
      "        2.1269, 1.5759, 2.5413, 1.8513, 1.9647, 1.5304, 1.7308, 2.9181, 1.7253,\n",
      "        1.7173, 1.5746, 2.1705, 0.9038, 0.9707], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0604, -0.0216, -0.0072,  ..., -0.0167, -0.0378, -0.0315],\n",
      "        [ 0.0412,  0.0046, -0.0516,  ..., -0.0298,  0.0610,  0.0299],\n",
      "        [-0.0478, -0.0579, -0.0410,  ..., -0.0627, -0.0180,  0.0137],\n",
      "        ...,\n",
      "        [-0.0388, -0.0226,  0.0202,  ...,  0.0068,  0.0558, -0.0102],\n",
      "        [-0.0631,  0.0348, -0.0150,  ..., -0.0286,  0.0386,  0.0022],\n",
      "        [ 0.0469,  0.0447, -0.0168,  ..., -0.0463,  0.0157,  0.0109]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-3.8704e-03,  5.1367e-03,  5.2745e-03,  ...,  6.3268e-03,\n",
      "          5.5271e-03, -1.4753e-03],\n",
      "        [ 3.4987e-03, -2.2207e-03, -2.3228e-03,  ..., -2.5262e-03,\n",
      "         -2.2629e-03, -7.3282e-05],\n",
      "        [ 2.4868e-03, -3.2472e-03, -2.8333e-03,  ..., -3.2425e-03,\n",
      "         -3.7695e-03, -4.5113e-03],\n",
      "        ...,\n",
      "        [-5.1780e-03,  4.6521e-03,  3.5113e-03,  ...,  6.0224e-03,\n",
      "          4.6935e-03,  9.5624e-03],\n",
      "        [ 2.9812e-03, -3.3011e-03, -3.2452e-03,  ..., -3.0776e-03,\n",
      "         -3.2992e-03, -6.5655e-03],\n",
      "        [-1.7428e-04,  1.4428e-03,  2.6045e-03,  ...,  2.9375e-03,\n",
      "          1.4179e-03, -4.6577e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4780, 1.6108, 1.3982, 1.6252, 1.3500, 1.5400, 1.5952, 1.3383, 1.5163,\n",
      "        1.4929, 1.3239, 1.4070, 1.4894, 1.5046, 1.4668, 1.3923, 0.9275, 1.1038,\n",
      "        1.0281, 1.1335, 0.8842, 1.0843, 1.1988, 0.3961, 1.1992, 0.8608, 1.2151,\n",
      "        0.9856, 1.2417, 0.9286, 1.1677, 1.1772, 0.9175, 1.1091, 1.1163, 1.0201,\n",
      "        1.0649, 1.0961, 1.0805, 1.0401, 0.9907, 0.9631, 0.9961, 1.1976, 1.1096,\n",
      "        1.1807, 1.0590, 0.9646, 1.5708, 1.6094, 1.4278, 1.1238, 1.4999, 1.3880,\n",
      "        1.5130, 1.5872, 0.9999, 1.4184, 1.2471, 1.7635, 1.5394, 1.5037, 1.0313,\n",
      "        1.3806, 0.9310, 1.2895, 1.1545, 1.1520, 1.1760, 1.2543, 1.3837, 1.1468,\n",
      "        1.2825, 1.2902, 1.1524, 1.3172, 1.3036, 1.2052, 1.3096, 1.1870, 0.9731,\n",
      "        0.9958, 1.2621, 1.0509, 1.1910, 1.1812, 1.2481, 1.1847, 1.2931, 1.0173,\n",
      "        1.1854, 1.1154, 1.1099, 1.0842, 1.0695, 1.1317, 1.7177, 1.6532, 1.4591,\n",
      "        1.5465, 1.4904, 1.5124, 1.7511, 1.6645, 1.4575, 1.3655, 1.0590, 1.6619,\n",
      "        1.5855, 1.2693, 1.4485, 1.6938, 1.7327, 1.8132, 1.6442, 1.7913, 2.0512,\n",
      "        1.8217, 1.9818, 2.1410, 1.6798, 1.8747, 1.6601, 1.7587, 1.6135, 1.7930,\n",
      "        1.7483, 2.0972, 0.9979, 1.1503, 1.2001, 1.1058, 1.2194, 1.4867, 1.1972,\n",
      "        1.5486, 1.1252, 1.4479, 1.3301, 1.2140, 1.1101, 1.2164, 1.0939, 1.4256,\n",
      "        1.1501, 1.0336, 1.0882, 1.1199, 1.1030, 1.1024, 1.2014, 1.2261, 1.0974,\n",
      "        1.2042, 1.2186, 1.0978, 1.1150, 1.0125, 1.0808, 1.0509, 1.2360, 1.4687,\n",
      "        1.1721, 1.2829, 1.3397, 1.2569, 1.2998, 1.2600, 1.4954, 1.6611, 1.3423,\n",
      "        1.3498, 1.2534, 1.1317, 1.2729, 1.1790, 1.0203, 1.0282, 0.9040, 1.0477,\n",
      "        0.9774, 1.0713, 1.2966, 1.0557, 0.8679, 0.9935, 1.2097, 0.9946, 1.0994,\n",
      "        0.9434, 1.1573, 1.1345, 1.7631, 1.3893, 0.8681, 1.3174, 1.8748, 1.2667,\n",
      "        1.3171, 1.6820, 1.3684, 1.7846, 1.7203, 1.9524, 1.6945, 1.6949, 1.8300,\n",
      "        1.6862, 1.2826, 1.2606, 1.4032, 1.4754, 1.3755, 1.5547, 1.2915, 1.3935,\n",
      "        1.6004, 1.5261, 1.3266, 1.4307, 1.2269, 1.5733, 1.1570, 1.1838, 0.7396,\n",
      "        0.8214, 0.6792, 0.7919, 0.7220, 0.6879, 0.9169, 0.7888, 0.8879, 0.9555,\n",
      "        0.7431, 0.5834, 0.7252, 0.7654, 0.7087, 0.6744, 1.1871, 1.2561, 1.2494,\n",
      "        1.3623, 1.0393, 1.2673, 1.3066, 1.2073, 1.3197, 1.1599, 1.2065, 1.2715,\n",
      "        1.3805, 1.3587, 1.2121, 1.1862, 1.3221, 1.2601, 1.3603, 1.3424, 1.2555,\n",
      "        1.2875, 1.8086, 1.2441, 1.3181, 1.2054, 1.2843, 1.3196, 1.5537, 1.4206,\n",
      "        1.4053, 1.1218, 0.8883, 0.7981, 0.8878, 1.0484, 0.9816, 0.9583, 1.0059,\n",
      "        0.9688, 0.9325, 0.9224, 0.9470, 1.0909, 0.9131, 0.9833, 0.9367, 0.8725,\n",
      "        1.0532, 1.0072, 1.0006, 1.0311, 1.0455, 0.9003, 0.9752, 0.9772, 0.9819,\n",
      "        1.0679, 0.9240, 1.0460, 1.1475, 1.0296, 1.1379, 0.9610, 1.1654, 1.1018,\n",
      "        1.0517, 1.0309, 1.0954, 1.1818, 1.2164, 1.1743, 1.0756, 1.1872, 1.1692,\n",
      "        1.0835, 1.2720, 1.1234, 1.1976, 1.1878], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0547, -0.0341, -0.0013,  ...,  0.0037,  0.0356, -0.0429],\n",
      "        [ 0.0383, -0.0037,  0.0344,  ..., -0.0358,  0.0211, -0.0244],\n",
      "        [ 0.0128,  0.0237,  0.0315,  ..., -0.0397, -0.0010, -0.0481],\n",
      "        ...,\n",
      "        [-0.0092, -0.0461,  0.0540,  ...,  0.0337,  0.0516, -0.0336],\n",
      "        [ 0.0206,  0.0284, -0.0249,  ...,  0.0306,  0.0157,  0.0435],\n",
      "        [ 0.0513,  0.0430, -0.0518,  ...,  0.0034, -0.0613, -0.0056]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0053, -0.0054, -0.0034,  ..., -0.0018,  0.0022, -0.0013],\n",
      "        [ 0.0012,  0.0085, -0.0044,  ..., -0.0069,  0.0019,  0.0015],\n",
      "        [ 0.0013, -0.0014,  0.0013,  ...,  0.0035,  0.0028, -0.0017],\n",
      "        ...,\n",
      "        [-0.0003, -0.0006, -0.0038,  ..., -0.0047, -0.0003,  0.0006],\n",
      "        [-0.0051,  0.0063, -0.0094,  ...,  0.0004,  0.0044,  0.0100],\n",
      "        [-0.0076, -0.0021, -0.0108,  ...,  0.0002,  0.0024,  0.0047]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3693, 1.3727, 1.4928, 1.5238, 1.4587, 1.5678, 1.3965, 1.3984, 1.2999,\n",
      "        1.4422, 1.3422, 1.5060, 1.5142, 1.3618, 1.3520, 1.3791, 1.4222, 1.3489,\n",
      "        1.4884, 1.3439, 1.4284, 1.3918, 1.4144, 1.3608, 1.3927, 1.4368, 1.4507,\n",
      "        1.4120, 1.4238, 1.4602, 1.3775, 1.2959, 1.4418, 1.2838, 1.3527, 1.4002,\n",
      "        1.4288, 1.3833, 1.4195, 1.4678, 1.4184, 1.4216, 1.3538, 1.4405, 1.4364,\n",
      "        1.4387, 1.8065, 1.3609, 1.5689, 1.4240, 1.4307, 1.3740, 1.4553, 1.5503,\n",
      "        1.3851, 1.2347, 1.3881, 1.4166, 1.3762, 1.3280, 1.5093, 1.6860, 1.2540,\n",
      "        1.5188, 1.3909, 1.3765, 1.4444, 1.4344, 1.3785, 1.3008, 1.4154, 1.4720,\n",
      "        1.3620, 1.5189, 1.5118, 1.4027, 1.4252, 1.3065, 1.3849, 1.4717, 1.3981,\n",
      "        1.3712, 1.3154, 1.4254, 1.4516, 1.5631, 1.3648, 1.4719, 1.3792, 1.4000,\n",
      "        1.4940, 1.3992, 1.4623, 1.4317, 1.3651, 1.4280, 1.2941, 1.4475, 1.3953,\n",
      "        1.3520, 1.3779, 1.4314, 1.3806, 1.2821, 1.5183, 1.3299, 1.5249, 1.4923,\n",
      "        1.4138, 1.3594, 1.3637, 1.6161, 1.4735, 1.4628, 1.5049, 1.4030, 1.3122,\n",
      "        1.4433, 1.3368, 1.5374, 1.5857, 1.4170, 1.3670, 1.3374, 1.3371, 1.4505,\n",
      "        1.3576, 1.4379, 1.3631, 1.3362, 1.3812, 1.5802, 1.4366, 1.4766, 1.3673,\n",
      "        1.4628, 1.3891, 1.3098, 1.3899, 1.6490, 1.3459, 1.5399, 1.3592, 1.3353,\n",
      "        1.1601, 1.3863, 1.3833, 1.5057, 1.3813, 1.3663, 1.4469, 1.5443, 1.6018,\n",
      "        1.3784, 1.3914, 1.3002, 1.4242, 1.3742, 1.4720, 1.4043, 1.3359, 1.4279,\n",
      "        1.3887, 1.4868, 1.4784, 1.4958, 1.5025, 1.4489, 1.4078, 1.3756, 1.4479,\n",
      "        1.5951, 1.4621, 1.3898, 1.4045, 1.4506, 1.3304, 1.3616, 1.3701, 1.4323,\n",
      "        1.3984, 1.5215, 1.3616, 1.3573, 1.4541, 1.3159, 1.4270, 1.2429, 1.4397,\n",
      "        1.3750, 1.3870, 1.3348, 1.4754, 0.9675, 1.3853, 1.3266, 1.3873, 1.2985,\n",
      "        1.3716, 1.3408, 1.4509, 1.3735, 1.3250, 1.3521, 1.3948, 1.4501, 1.3732,\n",
      "        1.4163, 1.4263, 1.4618, 1.4106, 1.3210, 1.3010, 1.4134, 1.5012, 1.4097,\n",
      "        1.3851, 1.4110, 1.3754, 1.3660, 1.4073, 1.3749, 1.4599, 1.3237, 1.3390,\n",
      "        1.4037, 1.3953, 1.4789, 1.3787, 1.4518, 1.4373, 1.3417, 1.4627, 1.3392,\n",
      "        1.4334, 1.3937, 1.4734, 1.3757, 1.4065, 1.4321, 1.4429, 1.4656, 1.5410,\n",
      "        1.4245, 1.4255, 1.4013, 1.3907, 1.4346, 1.4922, 1.3253, 1.3423, 1.4285,\n",
      "        1.4685, 1.4175, 1.3168, 1.4481, 1.4598, 1.6882, 1.4263, 1.3777, 1.4180,\n",
      "        1.4411, 1.4594, 1.3457, 1.4814, 1.4150, 1.5158, 1.4069, 1.4245, 1.4898,\n",
      "        1.2895, 1.2389, 1.4660, 1.3547, 1.2691, 1.3782, 1.4596, 1.4506, 1.4229,\n",
      "        1.4110, 1.4626, 1.4503, 1.3405, 1.6353, 1.4537, 1.2704, 1.2913, 1.5206,\n",
      "        1.3730, 1.5006, 1.3993, 1.7568, 1.3867, 1.4226, 1.4776, 1.3382, 1.2745,\n",
      "        1.4039, 1.4153, 1.4187, 1.3860, 1.4273, 1.3839, 1.3642, 1.5231, 1.4203,\n",
      "        1.3778, 1.4265, 1.5658, 1.3919, 1.4841, 1.4917, 1.4336, 1.4523, 1.5174,\n",
      "        1.4439, 1.3852, 1.3806, 1.3919, 1.3717], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0264,  0.0543, -0.0028,  ..., -0.0047,  0.0070, -0.0090],\n",
      "        [-0.0009,  0.0099,  0.0268,  ...,  0.0311,  0.0296, -0.0009],\n",
      "        [ 0.0311, -0.0033,  0.0498,  ...,  0.0375, -0.0336,  0.0095],\n",
      "        ...,\n",
      "        [-0.0312, -0.0693, -0.0366,  ..., -0.0024, -0.0316,  0.0048],\n",
      "        [-0.0310,  0.0578,  0.0175,  ...,  0.0330,  0.0111, -0.0241],\n",
      "        [-0.0221,  0.0584,  0.0256,  ...,  0.0294,  0.0583, -0.0320]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0011, -0.0076, -0.0032,  ..., -0.0038,  0.0064,  0.0048],\n",
      "        [ 0.0076, -0.0080,  0.0036,  ..., -0.0053,  0.0005,  0.0058],\n",
      "        [-0.0015,  0.0010,  0.0030,  ..., -0.0004, -0.0010, -0.0021],\n",
      "        ...,\n",
      "        [-0.0011,  0.0040, -0.0003,  ...,  0.0023, -0.0012, -0.0042],\n",
      "        [ 0.0040, -0.0024,  0.0034,  ..., -0.0015,  0.0009,  0.0036],\n",
      "        [-0.0068,  0.0070,  0.0050,  ...,  0.0090,  0.0010, -0.0084]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3946, 2.1271, 1.3981,  ..., 2.5727, 1.6371, 1.6072],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0109,  0.0041, -0.0091,  ..., -0.0188, -0.0009, -0.0073],\n",
      "        [ 0.0038, -0.0126,  0.0136,  ...,  0.0139,  0.0013, -0.0057],\n",
      "        [ 0.0175, -0.0043,  0.0236,  ...,  0.0184, -0.0275,  0.0179],\n",
      "        ...,\n",
      "        [ 0.0138,  0.0079, -0.0207,  ...,  0.0081,  0.0253, -0.0167],\n",
      "        [ 0.0232,  0.0301,  0.0138,  ...,  0.0195,  0.0228,  0.0241],\n",
      "        [ 0.0081,  0.0181, -0.0194,  ..., -0.0014, -0.0084, -0.0230]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0009, -0.0019, -0.0031,  ...,  0.0035,  0.0015, -0.0003],\n",
      "        [ 0.0034, -0.0001, -0.0017,  ...,  0.0021, -0.0034,  0.0006],\n",
      "        [-0.0028,  0.0008, -0.0043,  ...,  0.0051,  0.0031, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0025,  0.0010, -0.0013,  ...,  0.0035, -0.0023, -0.0009],\n",
      "        [ 0.0019,  0.0019, -0.0061,  ...,  0.0038,  0.0013, -0.0029],\n",
      "        [ 0.0014, -0.0005,  0.0011,  ...,  0.0019, -0.0039, -0.0006]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.8371, 1.9826, 2.0237, 1.9819, 1.8091, 2.0150, 1.9608, 1.9405, 1.9741,\n",
      "        1.9505, 2.0388, 2.0150, 2.0721, 2.1165, 1.8905, 1.9687, 1.9349, 1.9508,\n",
      "        2.0705, 1.9902, 1.9687, 1.9116, 2.0157, 1.8077, 2.0457, 1.8459, 2.1015,\n",
      "        1.9758, 1.8150, 1.8410, 1.8759, 2.8103, 2.0663, 2.0147, 2.0445, 1.8726,\n",
      "        1.8909, 1.9940, 2.1080, 1.9511, 1.9529, 2.0401, 1.9420, 1.9972, 1.9156,\n",
      "        1.9914, 2.7756, 2.0026, 2.0288, 1.9676, 2.0590, 1.9568, 2.0084, 1.9251,\n",
      "        1.9852, 1.9247, 2.0373, 1.9354, 1.9698, 1.8840, 2.0070, 2.3287, 2.8747,\n",
      "        1.9883, 1.9243, 1.8857, 2.1168, 1.9656, 1.9229, 2.0137, 2.0070, 1.9988,\n",
      "        1.9541, 1.9249, 1.9944, 1.8679, 2.0414, 2.0267, 1.9817, 2.0314, 2.0817,\n",
      "        1.9975, 1.9345, 1.9355, 2.0097, 1.9369, 1.9504, 1.8691, 2.4246, 1.9853,\n",
      "        1.9272, 2.0267, 1.8649, 1.9649, 1.9700, 1.9757, 1.9156, 1.9348, 2.0330,\n",
      "        1.9207, 1.9021, 1.9554, 1.9485, 2.0019, 1.9372, 1.8687, 2.0532, 1.8787,\n",
      "        1.9086, 1.9872, 1.8079, 2.0175, 1.9483, 1.8909, 1.8595, 2.0778, 1.9356,\n",
      "        1.9330, 1.9844, 1.8582, 1.9824, 2.1164, 1.8573, 2.0590, 2.2029, 1.8545,\n",
      "        1.8786, 1.9130, 1.9295, 1.9873, 2.1451, 2.0217, 1.9004, 1.8963, 1.8056,\n",
      "        1.8804, 1.9814, 2.0328, 1.9518, 1.9149, 2.0056, 1.8909, 1.9127, 1.9278,\n",
      "        1.8757, 2.0353, 1.9326, 1.9259, 1.8412, 2.0143, 2.1199, 1.8653, 2.0949,\n",
      "        1.9992, 1.9730, 1.9702, 1.9496, 1.9186, 1.9013, 1.9285, 1.9398, 1.9300,\n",
      "        1.8944, 2.0322, 1.9783, 1.9857, 1.9232, 1.9431, 1.9079, 1.9928, 1.9249,\n",
      "        2.0109, 2.0350, 1.9970, 2.0050, 1.9790, 1.9869, 1.8482, 2.0077, 1.9966,\n",
      "        1.9332, 1.9277, 2.0169, 1.9966, 1.9601, 2.0484, 2.0139, 1.8990, 1.8976,\n",
      "        1.9427, 2.0532, 1.9699, 3.8607, 1.9088, 1.9314, 1.8814, 1.9257, 1.9383,\n",
      "        1.8685, 1.8785, 1.9409, 2.0177, 1.8885, 1.9869, 2.0006, 2.0288, 1.8374,\n",
      "        2.1219, 1.8761, 1.9090, 1.9907, 1.9911, 1.9158, 1.9254, 1.9319, 1.9119,\n",
      "        2.0283, 1.8619, 2.3745, 1.9967, 1.9819, 1.9481, 2.0201, 1.8172, 1.9971,\n",
      "        1.9888, 2.0693, 2.0039, 2.1622, 2.0101, 2.0805, 2.1791, 1.9472, 1.9406,\n",
      "        1.9665, 1.9918, 1.9665, 2.0316, 1.9135, 1.8858, 2.1398, 1.9567, 1.9561,\n",
      "        2.0392, 1.9217, 1.9400, 1.9134, 2.0102, 1.9831, 1.9237, 1.9482, 2.0512,\n",
      "        1.9016, 2.0288, 1.9860, 1.9812, 1.9713, 2.0094, 1.9320, 1.8652, 2.0388,\n",
      "        1.9386, 1.9723, 1.9744, 2.1703, 2.0228, 1.9906, 2.0024, 1.9691, 1.9744,\n",
      "        2.0169, 2.0044, 1.9585, 1.9455, 1.8848, 2.0357, 1.9122, 2.1596, 1.9767,\n",
      "        1.9199, 1.9592, 2.0026, 1.8574, 2.0931, 1.9923, 1.9953, 1.9577, 2.0482,\n",
      "        2.0598, 1.9599, 1.9298, 2.2504, 2.0060, 2.0904, 2.2088, 2.1562, 1.9106,\n",
      "        2.0249, 1.9183, 1.9320, 1.9556, 2.0502, 1.9483, 2.0520, 1.9014, 1.9343,\n",
      "        1.8770, 2.1013, 2.0339, 2.1124, 2.0417, 2.1470, 1.9591, 2.0202, 1.9465,\n",
      "        1.9024, 1.9100, 1.9277, 1.8693, 3.4729], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0312,  0.0311,  0.0338,  ...,  0.0250, -0.0127,  0.0178],\n",
      "        [ 0.0502, -0.0123,  0.0555,  ...,  0.0651, -0.0244, -0.0275],\n",
      "        [ 0.0420, -0.0539,  0.0179,  ...,  0.0362,  0.0323,  0.0204],\n",
      "        ...,\n",
      "        [ 0.0413, -0.0090,  0.0127,  ...,  0.0414, -0.0376,  0.0566],\n",
      "        [ 0.0001,  0.0543, -0.0106,  ..., -0.0090, -0.0301, -0.0261],\n",
      "        [-0.0647,  0.0291, -0.0339,  ..., -0.0329,  0.0588, -0.0225]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0057, -0.0066, -0.0064,  ..., -0.0042,  0.0025,  0.0047],\n",
      "        [ 0.0074, -0.0090, -0.0066,  ..., -0.0076,  0.0084,  0.0079],\n",
      "        [ 0.0049, -0.0009, -0.0026,  ...,  0.0009, -0.0026, -0.0021],\n",
      "        ...,\n",
      "        [-0.0016,  0.0006,  0.0012,  ...,  0.0025, -0.0022, -0.0012],\n",
      "        [ 0.0009, -0.0023,  0.0011,  ...,  0.0004,  0.0024,  0.0024],\n",
      "        [-0.0021,  0.0023, -0.0008,  ...,  0.0003, -0.0019, -0.0034]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3132, 1.7173, 1.7645, 1.6955, 2.5149, 3.0024, 2.8858, 2.6566, 2.2908,\n",
      "        1.9590, 1.9553, 2.6178, 2.1796, 2.3956, 2.8127, 2.6064, 3.4379, 2.5055,\n",
      "        1.9048, 2.3492, 2.5050, 2.6702, 3.1366, 3.6244, 1.8848, 2.0338, 2.8627,\n",
      "        2.0081, 2.3779, 2.5610, 3.1587, 3.2007, 2.4346, 2.0927, 2.8108, 2.7870,\n",
      "        2.9855, 2.3839, 3.4012, 3.9526, 1.9249, 2.1179, 2.2162, 2.5730, 2.9014,\n",
      "        2.8549, 3.3663, 3.0290, 1.3544, 1.9220, 1.0060, 1.2956, 2.2280, 2.3828,\n",
      "        2.7470, 2.6614, 3.4022, 1.9158, 2.7143, 2.7296, 2.5671, 2.4820, 3.0107,\n",
      "        2.9411, 1.9280, 2.0037, 3.3894, 2.7150, 2.4891, 3.1325, 2.7251, 3.2981,\n",
      "        2.3328, 1.2271, 1.0401, 1.5293, 1.9954, 2.4406, 3.1489, 3.0182, 2.3075,\n",
      "        2.3558, 2.9646, 2.9360, 2.8055, 3.2030, 3.3353, 3.4883, 2.3039, 2.4368,\n",
      "        2.4580, 2.6757, 2.7323, 2.6756, 3.3172, 2.9087, 2.4738, 2.3946, 3.3965,\n",
      "        3.8280, 3.7442, 2.3188, 2.3358, 3.7362, 2.6445, 3.0459, 3.8366, 3.6589,\n",
      "        2.9826, 3.8681, 3.2236, 4.1882, 1.3945, 1.2871, 2.6666, 2.4751, 2.4496,\n",
      "        2.5214, 2.7407, 2.6677, 1.8375, 2.5501, 1.9952, 1.9858, 2.2120, 2.5948,\n",
      "        2.7291, 3.1545, 1.9409, 1.9300, 2.1086, 1.8136, 2.3446, 2.9067, 2.7732,\n",
      "        2.8460, 2.0127, 1.9024, 2.4710, 3.4882, 2.5607, 2.4954, 2.8509, 2.8352,\n",
      "        3.2951, 1.9606, 2.5871, 2.1491, 2.3415, 2.2666, 3.0158, 2.7967, 1.0348,\n",
      "        1.5804, 1.8233, 2.1706, 2.2025, 2.9473, 2.9805, 3.2179, 1.9507, 2.3447,\n",
      "        2.0218, 2.2180, 2.3631, 2.9310, 2.8429, 2.8691, 3.0780, 2.0552, 2.6510,\n",
      "        2.3397, 2.6705, 2.5368, 3.0057, 3.3387, 1.4418, 1.4719, 2.4009, 1.6449,\n",
      "        1.9829, 2.7007, 2.4371, 2.7150, 1.2406, 1.5638, 1.4347, 2.3707, 2.0326,\n",
      "        2.1333, 2.5478, 2.7054, 1.2910, 1.7076, 1.7877, 1.4626, 2.0330, 1.9137,\n",
      "        2.8414, 2.6522, 2.2318, 2.0767, 2.1806, 2.6180, 2.1028, 2.2843, 2.8529,\n",
      "        2.8424, 2.6110, 2.3215, 2.3224, 3.2448, 3.4110, 1.9783, 3.0416, 3.1481,\n",
      "        1.8149, 3.1194, 3.6259, 4.3204, 3.5246, 4.1075, 3.2507, 3.7375, 1.3679,\n",
      "        1.4852, 2.5917, 1.3688, 1.7904, 2.1564, 2.4202, 3.5510, 4.4056, 2.1850,\n",
      "        1.3047, 2.3150, 2.1587, 3.0764, 2.5239, 3.0416, 2.1880, 2.4773, 2.2022,\n",
      "        3.2929, 2.0185, 2.6319, 3.0664, 3.0471, 2.2664, 1.9714, 2.0134, 1.3950,\n",
      "        2.7146, 2.5750, 2.9396, 2.9898, 3.0534, 2.1583, 2.2454, 2.0017, 2.1158,\n",
      "        2.3572, 3.3527, 3.1344, 2.5235, 2.4681, 2.2910, 3.0120, 2.7627, 2.2339,\n",
      "        3.4374, 2.4917, 1.9716, 2.4371, 2.9450, 2.3286, 1.9656, 2.5456, 3.3401,\n",
      "        3.3845, 2.0047, 2.6756, 2.9667, 3.0906, 3.1146, 2.6507, 3.1391, 3.0352,\n",
      "        2.2151, 3.0187, 2.9209, 3.2418, 2.1848, 3.1998, 3.3591, 3.4748, 2.8977,\n",
      "        3.4169, 2.8477, 2.3281, 3.2323, 2.4361, 3.8610, 3.3031, 1.4827, 2.1134,\n",
      "        2.3612, 2.0493, 2.0895, 2.4076, 2.4373, 2.8912, 1.6286, 1.4178, 1.4336,\n",
      "        2.7094, 2.0763, 2.5606, 2.6138, 2.7323], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0386, -0.0182,  0.0560,  ...,  0.0105, -0.0409, -0.0200],\n",
      "        [ 0.0004,  0.0074, -0.0412,  ..., -0.0063,  0.0097, -0.0424],\n",
      "        [ 0.0132,  0.0523, -0.0277,  ...,  0.0211, -0.0270, -0.0519],\n",
      "        ...,\n",
      "        [-0.0504,  0.0401,  0.0298,  ...,  0.0416,  0.0243,  0.0287],\n",
      "        [-0.0214, -0.0085, -0.0175,  ...,  0.0326,  0.0522, -0.0226],\n",
      "        [-0.0264,  0.0073, -0.0006,  ...,  0.0516,  0.0036, -0.0104]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0049, -0.0056, -0.0058,  ...,  0.0056, -0.0056, -0.0018],\n",
      "        [-0.0006, -0.0013,  0.0014,  ..., -0.0004, -0.0047, -0.0017],\n",
      "        [ 0.0010, -0.0028, -0.0056,  ...,  0.0039,  0.0040,  0.0023],\n",
      "        ...,\n",
      "        [-0.0038, -0.0018, -0.0020,  ..., -0.0056, -0.0008, -0.0010],\n",
      "        [-0.0018,  0.0004, -0.0001,  ..., -0.0106, -0.0027, -0.0045],\n",
      "        [-0.0047, -0.0021, -0.0030,  ..., -0.0067, -0.0048, -0.0002]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9355, 1.6595, 1.5052, 3.0938, 1.3189, 1.9806, 2.5234, 2.6337, 1.9127,\n",
      "        2.2493, 2.5819, 1.2669, 2.9026, 2.0828, 2.8359, 2.1341, 1.6952, 2.0768,\n",
      "        2.7115, 1.7996, 1.7195, 1.6958, 3.2474, 3.2042, 1.9306, 2.2202, 1.5979,\n",
      "        1.6775, 3.4225, 3.6305, 3.4521, 3.0795, 2.6968, 1.9000, 2.5169, 2.6545,\n",
      "        2.2836, 1.9568, 2.1335, 2.9574, 1.6712, 2.2165, 2.6095, 2.2353, 2.1769,\n",
      "        2.2767, 2.4171, 2.2881, 2.6350, 1.6676, 1.5193, 1.9029, 2.9607, 1.3216,\n",
      "        2.4109, 2.4460, 1.5262, 2.2485, 1.7489, 1.4531, 1.2975, 3.0666, 2.6537,\n",
      "        2.4638, 1.8051, 1.7297, 1.3659, 1.2415, 1.4568, 1.7143, 2.6433, 2.0876,\n",
      "        1.9756, 1.5239, 1.6540, 2.6221, 1.7102, 2.8184, 2.5065, 2.0695, 2.5138,\n",
      "        2.5822, 2.7172, 2.3594, 2.3596, 2.4280, 3.3028, 3.1754, 1.7344, 2.3234,\n",
      "        2.2353, 2.5107, 2.2225, 2.2735, 3.1587, 2.6137, 2.4295, 2.7121, 3.1676,\n",
      "        3.2651, 2.7599, 3.5813, 3.4607, 3.3769, 2.8245, 3.3161, 3.0389, 3.1942,\n",
      "        2.9130, 1.5739, 2.1815, 3.5271, 1.3093, 2.1218, 1.5939, 1.4697, 1.4169,\n",
      "        2.8606, 2.7257, 2.7311, 1.6525, 1.4942, 1.9866, 2.4160, 2.6571, 1.4780,\n",
      "        2.6359, 2.2529, 2.1745, 2.2122, 1.8305, 3.1778, 1.6469, 1.5326, 2.6082,\n",
      "        2.5091, 1.9769, 1.8809, 2.9077, 1.5565, 1.7613, 3.0680, 2.5788, 2.7729,\n",
      "        2.0450, 2.2570, 1.5476, 1.6049, 1.3730, 3.0297, 2.5852, 2.6634, 1.3636,\n",
      "        1.5394, 2.7367, 1.6563, 2.9917, 1.6059, 2.7016, 2.2696, 2.1796, 2.5006,\n",
      "        2.9639, 2.9879, 3.2698, 1.8579, 3.2318, 3.0376, 2.2887, 2.0467, 1.6365,\n",
      "        1.8156, 1.7510, 3.2853, 3.3668, 3.0763, 1.2632, 1.4512, 1.4253, 2.6549,\n",
      "        1.2931, 1.4109, 2.2352, 2.1941, 1.6392, 1.8403, 2.1379, 1.2020, 1.5305,\n",
      "        2.3420, 2.4163, 1.9267, 1.9542, 1.8629, 1.5791, 3.1029, 1.4603, 1.4755,\n",
      "        2.6691, 2.7737, 2.4022, 2.2475, 1.4692, 1.3345, 2.9720, 3.0549, 2.5617,\n",
      "        2.8263, 1.8803, 2.7296, 3.4154, 3.1212, 2.8622, 3.1566, 2.5818, 4.0234,\n",
      "        2.3520, 2.0394, 2.2469, 2.8539, 2.7644, 1.8528, 3.9054, 3.9386, 1.3859,\n",
      "        1.9597, 1.2661, 2.1112, 2.7458, 2.6032, 2.2531, 2.0803, 1.3054, 1.6406,\n",
      "        1.2607, 1.2919, 1.2119, 1.5367, 2.3146, 2.0619, 2.3220, 2.1385, 1.9829,\n",
      "        1.6096, 3.1381, 1.6745, 2.7701, 2.8149, 2.0405, 2.2006, 2.1121, 2.1225,\n",
      "        1.3869, 2.3394, 2.6503, 2.8830, 2.4248, 2.2364, 2.2009, 2.2043, 2.7972,\n",
      "        2.1918, 2.9710, 2.5163, 2.7948, 2.3751, 2.2837, 2.2109, 1.9405, 2.0493,\n",
      "        2.9035, 2.2758, 1.7640, 2.7674, 2.7056, 2.4542, 3.2111, 2.1260, 3.1088,\n",
      "        3.2277, 2.0244, 2.3725, 2.4495, 2.5379, 1.8316, 2.0504, 3.2617, 3.0671,\n",
      "        2.7885, 2.9536, 2.6211, 2.9679, 3.6945, 2.2301, 3.3379, 3.2026, 2.3238,\n",
      "        2.7256, 2.6821, 2.2830, 1.7252, 2.4220, 3.0817, 3.4615, 1.7612, 1.9585,\n",
      "        1.2566, 2.6949, 2.6589, 1.9439, 2.4050, 2.0593, 1.7354, 1.5327, 2.6415,\n",
      "        1.3026, 1.1757, 2.1075, 2.3039, 2.3884], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0507,  0.0457,  0.0476,  ..., -0.0267,  0.0105,  0.0019],\n",
      "        [ 0.0579,  0.0067, -0.0207,  ..., -0.0439,  0.0129,  0.0281],\n",
      "        [-0.0216, -0.0541,  0.0336,  ...,  0.0467, -0.0267, -0.0114],\n",
      "        ...,\n",
      "        [ 0.0396,  0.0185,  0.0421,  ...,  0.0340,  0.0391,  0.0311],\n",
      "        [-0.0388,  0.0430,  0.0416,  ..., -0.0061,  0.0486,  0.0457],\n",
      "        [ 0.0194,  0.0295,  0.0012,  ...,  0.0194, -0.0439, -0.0493]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0038,  0.0048,  0.0031,  ...,  0.0020,  0.0058, -0.0047],\n",
      "        [ 0.0032, -0.0028, -0.0036,  ..., -0.0021, -0.0026,  0.0053],\n",
      "        [ 0.0019,  0.0009, -0.0009,  ...,  0.0005,  0.0016,  0.0004],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0027, -0.0012,  ..., -0.0018, -0.0031,  0.0014],\n",
      "        [ 0.0015,  0.0003,  0.0024,  ..., -0.0016, -0.0013,  0.0005],\n",
      "        [ 0.0040, -0.0017, -0.0019,  ..., -0.0038,  0.0024,  0.0050]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5098, 1.3428, 1.4363, 1.4276, 1.3722, 1.4052, 1.3479, 1.3964, 1.4899,\n",
      "        1.4307, 1.4120, 1.4264, 1.4037, 1.4314, 1.4766, 1.4485, 1.2031, 1.1994,\n",
      "        1.1693, 1.2382, 1.1978, 1.1459, 1.2096, 1.1454, 1.2759, 1.1506, 1.1661,\n",
      "        1.1109, 1.1748, 1.1890, 1.1115, 1.2569, 1.8241, 1.9628, 1.9948, 1.9538,\n",
      "        2.1109, 2.2007, 2.1631, 1.9180, 2.1089, 2.0322, 1.9374, 2.1167, 2.0180,\n",
      "        1.9640, 1.9635, 2.1306, 1.3834, 1.5126, 1.3832, 1.4238, 1.5017, 1.5089,\n",
      "        1.3615, 1.5125, 1.4777, 1.4442, 1.4838, 1.4635, 1.5115, 1.5044, 1.4996,\n",
      "        1.4078, 1.2711, 1.2440, 1.2319, 1.3049, 1.3188, 1.3367, 1.2652, 1.3275,\n",
      "        1.2845, 1.2245, 1.3251, 1.2717, 1.1936, 1.2671, 1.2399, 1.1487, 2.0390,\n",
      "        2.0992, 2.0837, 1.9899, 2.1059, 2.1122, 2.1942, 2.1492, 2.2327, 2.1034,\n",
      "        2.0172, 2.1029, 2.0555, 2.1902, 2.0911, 1.9105, 1.6390, 1.7087, 1.7492,\n",
      "        1.7212, 1.7444, 1.7523, 1.7401, 1.7398, 1.6598, 1.6378, 1.7039, 1.7395,\n",
      "        1.7269, 1.6589, 1.6489, 1.7513, 1.2045, 1.2574, 1.2548, 1.2245, 1.2503,\n",
      "        1.3046, 1.1973, 1.2135, 1.3039, 1.2583, 1.2107, 1.2950, 1.2960, 1.2563,\n",
      "        1.2556, 1.2187, 1.6072, 1.4564, 1.5249, 1.5364, 1.5250, 1.4378, 1.5416,\n",
      "        1.5510, 1.5620, 1.4695, 1.5437, 1.4292, 1.5273, 1.6015, 1.5265, 1.4693,\n",
      "        1.2517, 1.2717, 1.1867, 1.2802, 1.2162, 1.2481, 1.2892, 1.2417, 1.3061,\n",
      "        1.3133, 1.1549, 1.3082, 1.2792, 1.2586, 1.2474, 1.2034, 1.1361, 1.2317,\n",
      "        1.3096, 1.1404, 1.1772, 1.1664, 1.2576, 1.1629, 1.1952, 1.1532, 1.1685,\n",
      "        1.2142, 1.2361, 1.3082, 1.1838, 1.2444, 1.4493, 1.4412, 1.4472, 1.4305,\n",
      "        1.4824, 1.4516, 1.4420, 1.4755, 1.4712, 1.3809, 1.4918, 1.4853, 1.4380,\n",
      "        1.4342, 1.4192, 1.5268, 1.5672, 1.5629, 1.6012, 1.6366, 1.5565, 1.5357,\n",
      "        1.5070, 1.5921, 1.5954, 1.6214, 1.5608, 1.6003, 1.6107, 1.5868, 1.5698,\n",
      "        1.5474, 1.4573, 1.4885, 1.5561, 1.5100, 1.4294, 1.5211, 1.5540, 1.4878,\n",
      "        1.6650, 1.5486, 1.4909, 1.4340, 1.5570, 1.3993, 1.5305, 1.5030, 1.3616,\n",
      "        1.3686, 1.3409, 1.3860, 1.3412, 1.3960, 1.4454, 1.4104, 1.3981, 1.4073,\n",
      "        1.4592, 1.3857, 1.3598, 1.3883, 1.4162, 1.3778, 1.6234, 1.6344, 1.6778,\n",
      "        1.6965, 1.7070, 1.6187, 1.6665, 1.6156, 1.6883, 1.6832, 1.7084, 1.6827,\n",
      "        1.6151, 1.7802, 1.6497, 1.7090, 1.8619, 1.8245, 1.7387, 1.7123, 1.7584,\n",
      "        1.8234, 1.7963, 1.8343, 1.8013, 1.8223, 1.8751, 1.8188, 1.8823, 1.7792,\n",
      "        1.6770, 1.7495, 1.7778, 1.7558, 1.9529, 1.8123, 1.8237, 1.7828, 1.8904,\n",
      "        1.7808, 1.7883, 1.7604, 1.8873, 1.7135, 1.7277, 1.8307, 1.8431, 1.8072,\n",
      "        1.7395, 1.7973, 1.7953, 1.8094, 1.8626, 1.7438, 1.7917, 1.8098, 1.8288,\n",
      "        1.6828, 1.7729, 1.8276, 1.7777, 1.8126, 1.7317, 1.8590, 1.4966, 1.4099,\n",
      "        1.4256, 1.3889, 1.4682, 1.3970, 1.3954, 1.3717, 1.4345, 1.3800, 1.4136,\n",
      "        1.3444, 1.3876, 1.4923, 1.4534, 1.4815], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0351, -0.0406, -0.0502,  ...,  0.0378, -0.0046, -0.0455],\n",
      "        [ 0.0107, -0.0003, -0.0013,  ...,  0.0049,  0.0055, -0.0124],\n",
      "        [ 0.0632, -0.0368, -0.0417,  ...,  0.0168,  0.0259,  0.0353],\n",
      "        ...,\n",
      "        [ 0.0250,  0.0406, -0.0145,  ..., -0.0409,  0.0007, -0.0567],\n",
      "        [ 0.0037, -0.0499,  0.0040,  ..., -0.0499, -0.0140,  0.0235],\n",
      "        [ 0.0162,  0.0497, -0.0386,  ...,  0.0039, -0.0165, -0.0024]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0016, -0.0087, -0.0014,  ...,  0.0040, -0.0021, -0.0026],\n",
      "        [-0.0005, -0.0003,  0.0016,  ...,  0.0034,  0.0022, -0.0024],\n",
      "        [-0.0046,  0.0020,  0.0004,  ..., -0.0088,  0.0010, -0.0063],\n",
      "        ...,\n",
      "        [ 0.0013,  0.0024,  0.0012,  ...,  0.0075, -0.0014,  0.0061],\n",
      "        [-0.0007, -0.0025,  0.0022,  ..., -0.0024,  0.0018, -0.0004],\n",
      "        [ 0.0018, -0.0008,  0.0033,  ...,  0.0083,  0.0028,  0.0022]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5151, 1.3401, 1.4406, 1.5555, 1.5715, 1.3727, 1.4330, 1.4005, 1.3943,\n",
      "        1.5413, 1.3969, 1.5340, 1.4176, 1.3922, 1.4163, 1.5175, 1.5430, 1.5220,\n",
      "        1.4075, 1.4062, 1.5464, 1.3906, 1.5464, 1.4136, 1.4475, 1.5200, 1.4801,\n",
      "        1.4423, 1.5452, 1.5151, 1.4707, 1.4880, 1.4380, 1.4062, 1.4664, 1.5837,\n",
      "        1.6137, 1.5035, 1.5553, 1.4884, 1.4375, 1.4902, 1.6757, 1.5633, 1.4270,\n",
      "        1.3748, 1.3079, 1.5158, 1.5316, 1.5482, 1.4403, 1.4235, 1.4339, 1.4801,\n",
      "        1.4532, 1.5172, 1.4194, 1.4796, 1.4729, 1.4188, 1.5039, 1.7786, 1.4402,\n",
      "        1.5766, 1.5081, 1.4633, 1.4175, 1.4406, 1.4725, 1.5703, 1.4597, 1.6191,\n",
      "        1.5974, 1.5227, 1.4323, 1.3342, 1.4648, 1.4439, 1.4202, 1.4725, 1.5170,\n",
      "        1.5607, 1.4448, 1.5346, 1.6038, 1.5237, 1.4342, 1.3530, 1.3634, 1.5913,\n",
      "        1.4620, 1.5344, 1.5434, 1.4272, 1.5004, 1.4811, 1.3042, 1.5367, 1.6194,\n",
      "        1.3964, 1.4712, 1.3765, 1.4164, 1.3238, 1.4351, 1.5392, 1.4704, 1.4078,\n",
      "        1.6345, 1.4005, 1.4246, 1.4880, 1.5552, 1.5272, 1.4343, 1.4652, 1.6053,\n",
      "        1.5465, 1.4444, 1.4666, 1.5178, 1.5750, 1.4457, 1.4763, 1.4908, 1.3823,\n",
      "        1.4198, 1.4375, 1.4752, 1.6520, 1.4647, 1.5199, 1.4610, 1.4879, 1.4385,\n",
      "        1.2688, 1.5131, 1.3334, 1.5633, 1.3824, 1.4484, 1.5189, 1.4605, 1.3965,\n",
      "        1.4227, 1.4134, 1.3834, 1.4107, 1.4890, 1.4877, 1.7318, 1.5302, 1.4646,\n",
      "        1.3499, 1.4393, 1.4989, 1.5787, 1.5313, 1.5501, 1.4894, 1.3801, 1.5105,\n",
      "        1.4861, 1.4635, 1.4833, 1.4895, 1.4796, 1.4381, 1.4112, 1.4790, 1.4623,\n",
      "        1.4632, 1.4982, 1.5290, 1.5579, 1.4324, 1.5014, 1.4799, 1.3883, 1.4502,\n",
      "        1.4668, 1.3827, 1.6434, 1.5531, 1.4438, 1.5007, 1.5309, 1.3341, 1.4030,\n",
      "        1.4162, 1.3900, 1.4824, 2.3219, 1.4178, 1.4911, 1.4204, 1.4497, 1.4230,\n",
      "        1.4588, 1.4771, 1.4652, 1.4840, 1.3967, 1.4846, 1.4229, 1.3916, 1.5696,\n",
      "        1.4475, 1.5143, 1.3630, 1.5070, 1.3878, 1.4586, 1.4723, 1.4275, 1.5121,\n",
      "        1.4608, 1.4348, 1.4623, 1.5180, 1.4721, 1.5448, 1.5110, 1.4563, 1.3817,\n",
      "        1.3434, 1.4634, 1.4455, 1.4989, 1.5383, 1.4984, 1.4742, 1.4706, 1.3982,\n",
      "        1.4342, 1.5140, 1.4813, 1.2873, 1.4518, 1.5383, 1.5827, 1.3492, 1.5013,\n",
      "        1.5160, 1.4210, 1.3570, 1.3462, 1.4532, 1.4510, 1.4979, 1.4083, 1.5287,\n",
      "        1.4999, 1.5614, 1.4159, 1.4939, 1.5797, 1.3966, 1.3387, 1.5087, 1.5131,\n",
      "        1.4336, 1.4443, 1.4929, 1.6143, 1.5734, 1.5189, 1.5558, 1.4644, 1.4888,\n",
      "        1.4422, 1.4876, 1.6829, 1.4446, 1.5126, 1.5055, 1.4766, 1.4103, 1.4352,\n",
      "        1.4509, 1.4766, 1.5255, 1.4107, 1.5383, 1.4188, 1.4595, 1.4247, 1.4495,\n",
      "        1.2975, 1.5295, 1.5493, 1.5705, 1.3766, 1.4951, 1.4948, 1.4913, 1.5726,\n",
      "        1.4885, 1.4277, 1.5017, 1.4506, 1.5342, 1.3889, 1.3446, 1.4614, 1.4581,\n",
      "        1.5769, 1.5077, 1.4598, 1.3480, 1.4739, 1.5295, 1.4425, 1.5424, 1.4128,\n",
      "        1.5599, 1.5041, 1.4341, 1.4178, 2.1279], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0333,  0.0021, -0.0338,  ...,  0.0312, -0.0253,  0.0519],\n",
      "        [-0.0022, -0.0543,  0.0069,  ...,  0.0256,  0.0386,  0.0128],\n",
      "        [ 0.0387,  0.0035, -0.0518,  ...,  0.0463, -0.0503,  0.0518],\n",
      "        ...,\n",
      "        [ 0.0339,  0.0500,  0.0269,  ...,  0.0240,  0.0292,  0.0293],\n",
      "        [-0.0390, -0.0484, -0.0562,  ..., -0.0591,  0.0333,  0.0508],\n",
      "        [-0.0269,  0.0145,  0.0028,  ..., -0.0614,  0.0538, -0.0010]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-4.8878e-03,  3.3231e-03, -3.9977e-03,  ...,  2.0056e-03,\n",
      "         -5.9200e-03,  6.1798e-03],\n",
      "        [ 1.6736e-03, -5.6377e-03,  1.1252e-03,  ..., -1.7838e-04,\n",
      "         -6.3642e-04, -1.2113e-03],\n",
      "        [-1.6400e-04, -1.3610e-03,  9.0726e-05,  ..., -1.1768e-03,\n",
      "          9.8020e-04, -4.7609e-04],\n",
      "        ...,\n",
      "        [ 6.8320e-04, -1.5922e-03,  2.9774e-04,  ...,  6.0998e-04,\n",
      "         -2.0076e-03,  1.3815e-04],\n",
      "        [ 7.8869e-03, -6.5845e-03,  7.9197e-03,  ..., -2.3341e-03,\n",
      "          9.9070e-03, -5.9489e-03],\n",
      "        [-4.9188e-04, -3.1961e-03, -1.9554e-03,  ...,  7.9817e-03,\n",
      "         -6.4822e-03,  3.2266e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.2354, 1.1939, 1.2675,  ..., 1.1296, 1.9021, 3.0284],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0207,  0.0195,  0.0001,  ..., -0.0212,  0.0141,  0.0262],\n",
      "        [-0.0140, -0.0095, -0.0265,  ...,  0.0138,  0.0154,  0.0125],\n",
      "        [-0.0316, -0.0104, -0.0038,  ..., -0.0200,  0.0019, -0.0221],\n",
      "        ...,\n",
      "        [-0.0109, -0.0302, -0.0118,  ..., -0.0173,  0.0127,  0.0147],\n",
      "        [-0.0089,  0.0020,  0.0121,  ...,  0.0302,  0.0146,  0.0173],\n",
      "        [-0.0076, -0.0206,  0.0084,  ...,  0.0058, -0.0062, -0.0244]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 3.4213e-03,  2.7873e-03,  2.8891e-03,  ..., -1.0825e-03,\n",
      "         -3.0777e-03,  2.8495e-03],\n",
      "        [ 7.1240e-03,  8.2690e-03,  4.5269e-03,  ...,  3.8422e-03,\n",
      "         -6.4782e-03,  9.6047e-03],\n",
      "        [-7.3098e-03, -2.4560e-03, -2.7313e-03,  ...,  4.2428e-03,\n",
      "          3.3066e-03, -4.2974e-04],\n",
      "        ...,\n",
      "        [ 8.0359e-03,  3.0304e-03,  5.4079e-03,  ..., -1.7025e-03,\n",
      "         -5.3814e-03,  2.3666e-03],\n",
      "        [ 4.7467e-04, -4.0395e-04, -1.8284e-04,  ...,  3.5112e-04,\n",
      "         -8.9206e-04,  3.0616e-05],\n",
      "        [ 1.3643e-03,  1.1466e-03,  1.6153e-03,  ..., -2.7570e-03,\n",
      "         -2.6088e-04, -2.5826e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.9627, 3.1115, 3.2687, 3.2839, 3.2422, 2.9855, 2.8641, 3.0383, 3.2100,\n",
      "        3.0361, 2.8575, 3.0005, 3.2483, 2.9130, 2.9464, 3.2039, 2.9153, 2.9506,\n",
      "        2.9051, 2.9377, 3.1469, 3.0680, 2.9553, 2.7964, 2.9013, 2.9233, 3.0191,\n",
      "        2.9756, 2.9021, 3.0429, 3.1984, 2.8299, 3.0898, 2.8210, 3.0550, 3.1875,\n",
      "        3.0454, 3.0078, 3.0385, 2.9954, 3.2129, 2.8643, 2.9120, 3.0750, 2.9905,\n",
      "        3.0976, 3.5245, 3.0204, 2.9755, 2.9728, 3.0884, 3.0205, 2.7820, 3.0296,\n",
      "        3.0998, 2.9577, 2.8916, 3.2129, 3.1356, 3.2631, 2.9748, 3.1223, 3.8604,\n",
      "        2.9460, 3.0999, 3.0238, 2.8726, 3.1957, 3.1532, 3.1138, 2.7881, 3.0630,\n",
      "        3.0615, 3.0013, 3.0765, 3.1101, 2.9101, 2.8679, 3.0198, 3.1840, 2.9237,\n",
      "        2.9667, 3.1111, 2.9478, 2.8327, 3.0835, 2.9885, 3.0522, 2.5975, 3.0055,\n",
      "        3.0371, 2.9081, 3.2536, 2.7695, 3.1025, 3.1314, 2.9029, 2.9060, 3.2290,\n",
      "        3.2153, 2.9696, 2.8985, 2.9836, 2.8837, 2.9264, 3.3201, 3.0397, 2.9391,\n",
      "        2.9243, 3.1682, 3.1422, 2.9643, 3.1307, 2.9923, 3.0266, 3.0551, 2.9717,\n",
      "        3.1797, 3.0568, 2.9039, 3.0134, 3.0197, 2.9179, 3.0952, 2.8765, 3.1029,\n",
      "        3.0313, 3.1412, 2.9556, 3.0338, 2.9078, 3.0962, 3.0211, 3.0498, 3.1526,\n",
      "        3.0790, 3.0031, 3.0518, 3.0896, 3.0753, 2.9725, 2.9050, 3.0922, 2.8889,\n",
      "        3.0322, 3.1181, 3.0155, 3.2714, 3.1099, 2.9977, 3.5673, 2.9367, 2.9617,\n",
      "        3.0898, 3.0814, 3.1866, 2.8066, 3.0315, 2.9378, 3.0083, 3.0686, 3.1476,\n",
      "        2.8016, 3.1394, 3.1248, 3.1146, 3.1667, 3.2098, 3.0455, 2.9242, 3.0705,\n",
      "        2.8096, 3.0418, 2.9524, 3.0527, 3.1693, 3.0748, 3.1457, 3.1407, 2.9228,\n",
      "        2.9801, 2.8620, 2.9364, 2.8313, 3.1410, 3.1007, 3.1966, 3.2055, 3.1437,\n",
      "        3.0682, 3.1554, 2.9922, 5.2875, 2.9458, 3.0970, 3.0926, 3.2426, 2.9912,\n",
      "        3.2172, 2.9936, 3.1528, 2.9115, 3.0701, 3.0004, 2.9791, 3.0638, 3.2364,\n",
      "        2.9578, 3.1449, 2.9747, 2.8723, 3.0261, 2.8431, 3.0581, 3.2044, 3.1121,\n",
      "        3.0251, 2.9204, 2.9007, 2.8924, 2.9766, 3.2887, 3.0727, 3.1894, 2.9404,\n",
      "        3.1939, 3.3012, 3.0138, 3.0119, 3.1254, 2.9134, 2.9042, 3.1518, 2.7848,\n",
      "        3.0618, 2.9701, 3.1545, 2.8757, 2.9555, 3.2382, 3.0639, 2.8759, 2.8863,\n",
      "        2.9031, 2.8911, 2.9305, 3.0610, 2.9579, 3.1239, 3.2133, 2.8835, 3.1089,\n",
      "        2.9427, 2.9445, 3.0422, 2.9225, 3.0208, 2.5521, 2.4516, 3.1231, 2.9925,\n",
      "        2.7937, 2.8829, 2.9095, 2.9891, 3.0775, 2.9922, 3.2363, 2.9352, 2.9199,\n",
      "        2.9033, 2.9941, 3.0903, 3.0141, 2.9721, 3.2147, 3.1333, 3.0290, 3.0268,\n",
      "        2.9138, 3.1151, 3.2431, 3.1393, 3.1929, 3.0055, 3.0859, 2.8129, 3.1223,\n",
      "        2.6289, 3.2687, 3.0555, 2.8451, 3.0440, 3.0484, 2.6835, 3.0561, 2.9532,\n",
      "        3.0337, 2.8151, 3.2350, 2.9175, 3.0411, 3.0818, 2.9187, 3.2556, 3.0251,\n",
      "        2.8826, 3.0841, 3.1265, 3.2198, 3.0610, 3.1799, 2.9638, 3.0580, 3.1450,\n",
      "        3.0016, 3.0138, 2.9580, 3.0331, 3.7531], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0526,  0.0637, -0.0029,  ..., -0.0663, -0.0336,  0.0017],\n",
      "        [ 0.0050,  0.0108, -0.0069,  ...,  0.0311, -0.0496,  0.0253],\n",
      "        [-0.0410, -0.0384, -0.0025,  ...,  0.0511,  0.0242,  0.0082],\n",
      "        ...,\n",
      "        [ 0.0135, -0.0485, -0.0035,  ...,  0.0334,  0.0087, -0.0146],\n",
      "        [ 0.0055, -0.0348, -0.0292,  ..., -0.0479, -0.0244,  0.0198],\n",
      "        [-0.0116, -0.0561,  0.0248,  ...,  0.0044, -0.0274,  0.0347]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 4.1183e-04,  5.1898e-04, -5.2915e-04,  ...,  1.6480e-03,\n",
      "         -4.9027e-06, -1.7321e-03],\n",
      "        [ 5.4938e-03, -9.6367e-03, -8.5315e-03,  ..., -8.7425e-03,\n",
      "         -8.3730e-03,  3.4823e-03],\n",
      "        [-5.3388e-03,  3.2801e-03,  2.6825e-03,  ...,  3.3941e-03,\n",
      "          3.8866e-03, -4.9495e-03],\n",
      "        ...,\n",
      "        [ 1.0240e-03,  8.4190e-04,  1.5552e-03,  ..., -7.2852e-04,\n",
      "         -4.1230e-03,  3.0812e-03],\n",
      "        [ 8.8475e-03, -7.8993e-03, -8.0972e-03,  ..., -7.8761e-03,\n",
      "         -1.0454e-02,  3.0546e-04],\n",
      "        [ 2.6974e-03, -1.3961e-03, -1.1606e-03,  ..., -7.7809e-04,\n",
      "         -4.1103e-03,  6.0352e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6251, 2.6459, 2.0950, 1.2526, 2.4734, 4.0939, 4.0528, 4.0984, 1.6258,\n",
      "        1.7498, 2.3940, 2.2293, 1.8260, 3.9749, 3.8749, 3.9437, 1.6965, 1.9970,\n",
      "        2.5529, 2.4945, 2.4270, 2.8197, 2.4923, 2.9178, 2.1323, 2.0425, 1.9697,\n",
      "        1.9504, 1.9860, 2.1116, 2.6534, 3.0697, 2.1063, 1.9648, 1.9621, 1.3702,\n",
      "        1.9452, 2.9877, 2.5310, 2.5641, 2.3064, 1.6216, 1.7931, 2.2795, 2.6160,\n",
      "        1.9646, 2.4352, 2.3954, 1.2609, 1.3968, 2.0674, 1.9271, 1.6272, 2.4845,\n",
      "        2.3867, 2.6704, 1.7382, 2.3025, 1.9814, 1.8835, 1.9664, 1.9693, 2.2375,\n",
      "        2.6577, 2.1699, 2.0699, 1.7503, 2.0047, 2.0979, 2.3143, 2.7781, 3.8691,\n",
      "        2.4042, 2.2303, 2.1068, 1.9244, 1.8057, 2.3460, 2.9915, 2.7822, 1.9239,\n",
      "        2.1179, 2.0796, 1.9913, 2.0829, 2.3717, 2.5766, 2.0374, 1.8274, 1.7050,\n",
      "        2.2797, 2.1600, 2.0022, 2.0650, 2.6553, 2.8205, 2.7373, 2.0058, 2.3082,\n",
      "        2.3597, 2.1232, 2.4632, 2.7316, 2.7595, 1.6165, 2.2072, 1.8075, 2.2850,\n",
      "        2.6018, 3.3129, 2.8237, 3.1153, 3.1512, 1.7643, 2.4407, 2.0310, 2.1901,\n",
      "        2.8640, 2.4919, 2.5470, 1.7588, 2.2823, 1.6629, 2.4428, 2.7711, 2.4074,\n",
      "        2.8385, 2.8638, 1.2774, 1.7800, 1.7666, 2.0585, 2.2304, 2.5650, 2.2350,\n",
      "        2.8436, 2.3583, 1.7385, 2.2623, 1.7376, 1.9687, 2.1867, 2.5075, 3.2102,\n",
      "        2.2480, 1.5015, 1.7398, 2.3254, 2.7109, 2.5342, 2.6040, 2.6346, 1.9131,\n",
      "        1.4668, 1.8390, 1.7457, 2.0670, 2.0639, 2.5260, 2.6283, 1.8806, 2.0316,\n",
      "        2.1635, 2.2540, 2.0373, 1.7298, 2.6085, 2.8535, 1.8382, 2.1000, 2.4409,\n",
      "        2.0900, 2.6051, 2.4532, 2.6759, 1.9697, 2.0989, 2.4524, 1.9675, 2.6125,\n",
      "        3.3689, 3.4088, 2.8976, 3.0807, 2.4242, 2.1250, 2.4137, 2.2678, 2.2166,\n",
      "        2.7274, 2.8038, 3.0275, 1.9476, 1.6724, 1.9972, 1.6096, 2.2213, 3.1477,\n",
      "        2.7641, 2.8963, 2.7089, 2.2615, 2.1392, 2.7213, 2.8540, 2.3423, 2.5243,\n",
      "        2.6484, 2.3531, 2.1841, 2.2417, 2.1521, 1.9504, 2.6564, 2.9010, 2.7402,\n",
      "        2.0689, 2.1705, 2.0229, 2.2793, 2.3591, 2.6975, 2.9014, 2.9326, 1.2779,\n",
      "        1.4844, 2.3054, 1.6093, 2.2985, 1.7351, 2.2441, 2.2157, 1.6390, 1.6877,\n",
      "        1.3590, 2.0729, 1.6051, 2.3609, 2.1178, 2.3231, 2.1813, 1.8422, 1.8371,\n",
      "        1.8594, 1.7772, 1.8731, 2.4048, 2.5751, 1.9013, 1.8157, 2.0794, 2.4594,\n",
      "        2.1461, 2.2528, 2.4476, 2.2963, 1.9243, 2.2980, 2.7347, 2.3037, 2.1110,\n",
      "        2.7301, 2.8535, 3.2572, 2.6108, 2.0600, 1.9121, 2.2110, 2.5729, 3.0840,\n",
      "        3.1170, 3.5771, 2.1859, 2.1502, 2.1911, 2.0514, 1.8502, 2.2366, 3.7195,\n",
      "        3.8732, 2.1959, 2.2586, 2.6611, 2.2685, 2.0664, 2.0182, 3.6214, 3.7151,\n",
      "        2.8974, 1.8913, 1.6529, 2.2098, 2.2944, 2.4037, 2.9225, 2.8887, 1.7557,\n",
      "        1.7449, 2.1590, 1.9214, 1.9640, 2.7504, 2.7541, 3.0107, 2.0338, 1.9682,\n",
      "        2.2781, 2.8427, 2.3850, 2.3192, 3.2396, 2.8082, 2.2207, 2.0076, 2.1172,\n",
      "        1.9178, 2.8429, 2.7808, 2.7981, 2.3343], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0079,  0.0196, -0.0602,  ...,  0.0236, -0.0495, -0.0393],\n",
      "        [ 0.0473, -0.0012,  0.0489,  ...,  0.0465, -0.0126, -0.0549],\n",
      "        [ 0.0642, -0.0291, -0.0519,  ...,  0.0335, -0.0361,  0.0045],\n",
      "        ...,\n",
      "        [-0.0202, -0.0266,  0.0355,  ..., -0.0162,  0.0148, -0.0285],\n",
      "        [ 0.0163, -0.0151,  0.0338,  ..., -0.0243,  0.0319,  0.0265],\n",
      "        [ 0.0240,  0.0292, -0.0046,  ...,  0.0022, -0.0145,  0.0181]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0002,  0.0013, -0.0027,  ...,  0.0044, -0.0039, -0.0034],\n",
      "        [-0.0064,  0.0049, -0.0016,  ...,  0.0002,  0.0025,  0.0026],\n",
      "        [-0.0016,  0.0066, -0.0018,  ..., -0.0013, -0.0003,  0.0018],\n",
      "        ...,\n",
      "        [-0.0078, -0.0015, -0.0034,  ..., -0.0022, -0.0031, -0.0009],\n",
      "        [-0.0092, -0.0089, -0.0081,  ..., -0.0029,  0.0062, -0.0019],\n",
      "        [-0.0089, -0.0018, -0.0041,  ..., -0.0012, -0.0035, -0.0018]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0705, 2.5428, 2.3508, 1.9760, 2.0991, 2.9845, 3.1099, 3.2076, 2.0326,\n",
      "        1.9360, 2.1854, 1.6982, 1.8092, 2.9088, 3.0336, 3.1842, 1.7676, 1.6798,\n",
      "        1.7784, 1.4614, 1.4361, 1.5470, 2.4343, 2.1582, 1.8641, 2.3713, 2.1831,\n",
      "        2.7708, 2.7214, 2.8038, 2.1937, 2.1177, 2.2337, 1.7458, 2.1283, 2.4211,\n",
      "        1.8971, 1.4590, 2.4361, 1.8712, 2.1058, 1.8827, 1.8491, 1.5297, 1.6714,\n",
      "        2.5513, 2.3186, 2.1279, 1.2376, 2.1820, 1.9844, 1.4009, 2.5583, 1.4676,\n",
      "        2.2705, 1.9684, 1.5102, 1.3544, 1.5301, 2.1755, 1.3011, 2.4472, 2.0719,\n",
      "        1.9877, 2.0071, 2.1089, 1.9090, 2.1962, 1.8062, 1.9300, 2.2513, 2.1558,\n",
      "        2.2494, 2.2295, 1.9751, 1.7442, 1.9602, 1.9665, 2.3400, 1.8728, 1.7940,\n",
      "        2.1360, 2.4510, 2.0427, 2.0027, 1.7558, 2.1464, 2.1257, 1.8868, 1.5204,\n",
      "        1.9364, 2.0315, 1.9444, 2.3798, 2.4317, 2.5076, 1.8621, 1.8623, 1.9703,\n",
      "        2.8948, 2.9952, 2.9549, 2.6740, 2.6723, 2.3808, 2.2558, 2.0334, 1.7987,\n",
      "        1.5428, 1.6599, 2.5902, 2.1694, 2.0567, 1.8703, 2.2366, 2.8172, 3.2906,\n",
      "        1.6099, 2.2535, 2.4842, 2.5101, 2.0728, 1.6054, 1.5813, 1.3896, 2.7549,\n",
      "        2.3377, 2.1157, 1.8416, 1.7870, 2.2301, 1.5017, 1.6241, 1.8215, 2.2420,\n",
      "        2.1349, 1.2369, 1.7084, 1.7365, 2.0014, 1.9713, 2.1656, 2.3596, 2.1385,\n",
      "        2.1685, 1.8456, 2.3087, 1.4073, 1.3531, 1.6128, 2.2556, 2.0454, 1.8942,\n",
      "        1.5752, 1.6171, 2.4392, 2.7460, 2.2130, 2.3558, 1.9544, 1.9751, 2.0513,\n",
      "        2.4317, 2.0491, 2.3031, 2.3000, 2.1649, 2.5608, 1.5359, 2.1554, 2.2521,\n",
      "        2.3143, 1.9976, 2.1674, 2.4492, 2.1731, 2.2896, 2.1228, 1.9004, 1.9294,\n",
      "        1.6358, 1.5909, 2.8386, 2.7222, 2.2226, 2.3623, 2.2717, 2.5050, 2.3296,\n",
      "        2.6293, 2.7097, 2.6849, 2.1296, 1.7786, 2.0160, 2.1816, 2.4280, 1.5430,\n",
      "        2.1482, 2.1172, 1.9056, 2.0488, 1.9572, 1.3843, 1.3501, 2.1146, 2.5132,\n",
      "        2.3750, 2.0737, 2.2170, 2.1236, 2.3145, 2.0413, 1.9640, 2.6743, 2.7371,\n",
      "        2.1716, 2.1373, 2.0513, 1.9630, 1.7619, 1.9278, 2.6878, 2.7538, 1.4357,\n",
      "        1.7803, 1.3488, 2.3593, 1.2206, 2.2874, 2.0997, 2.0660, 1.5893, 1.5398,\n",
      "        2.4441, 1.3093, 2.4074, 1.3033, 1.9746, 1.8836, 1.9593, 1.8228, 1.7753,\n",
      "        2.1170, 2.4332, 2.5551, 2.2448, 2.1305, 2.0972, 1.7915, 1.9935, 1.5466,\n",
      "        1.3920, 1.4355, 2.3625, 1.9377, 1.9587, 1.9965, 1.7765, 1.7974, 2.8305,\n",
      "        2.3684, 2.6687, 2.1497, 2.2049, 2.0840, 2.3904, 2.4306, 1.5878, 2.2392,\n",
      "        2.6330, 2.1911, 2.0811, 2.1465, 2.0247, 2.1139, 1.9245, 2.2276, 3.0404,\n",
      "        3.0677, 2.3161, 2.3813, 2.5214, 2.2125, 2.0549, 2.0084, 3.0050, 3.0132,\n",
      "        2.0597, 1.9055, 2.4398, 1.7407, 1.3676, 2.4017, 2.3252, 2.2493, 2.1084,\n",
      "        1.7722, 1.5507, 1.6412, 2.5152, 1.6760, 2.2049, 2.2541, 2.2650, 1.8736,\n",
      "        2.1891, 2.2645, 1.9403, 2.3514, 2.3254, 2.7769, 1.7965, 2.0821, 2.1158,\n",
      "        2.2760, 2.3232, 2.2892, 2.8385, 2.3254], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0126,  0.0255, -0.0413,  ..., -0.0395,  0.0487,  0.0353],\n",
      "        [-0.0305, -0.0427, -0.0048,  ..., -0.0401,  0.0325, -0.0366],\n",
      "        [ 0.0259, -0.0421, -0.0302,  ...,  0.0171,  0.0252,  0.0018],\n",
      "        ...,\n",
      "        [-0.0334, -0.0290,  0.0626,  ..., -0.0026,  0.0268, -0.0216],\n",
      "        [-0.0372,  0.0375, -0.0370,  ...,  0.0055,  0.0195, -0.0399],\n",
      "        [-0.0106,  0.0094,  0.0074,  ..., -0.0492,  0.0004, -0.0210]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0038, -0.0063,  0.0048,  ..., -0.0043,  0.0051, -0.0039],\n",
      "        [ 0.0050,  0.0030, -0.0036,  ...,  0.0047, -0.0042,  0.0046],\n",
      "        [-0.0035, -0.0041,  0.0038,  ..., -0.0031,  0.0043, -0.0053],\n",
      "        ...,\n",
      "        [ 0.0019,  0.0037, -0.0025,  ...,  0.0055, -0.0038,  0.0025],\n",
      "        [-0.0024, -0.0021,  0.0046,  ..., -0.0032,  0.0017, -0.0010],\n",
      "        [ 0.0027,  0.0024, -0.0012,  ...,  0.0050, -0.0002,  0.0088]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5365, 1.6515, 1.5967, 1.5537, 1.6421, 1.6285, 1.5041, 1.5537, 1.5362,\n",
      "        1.5709, 1.5089, 1.5683, 1.5184, 1.5014, 1.5707, 1.5312, 1.1628, 1.2364,\n",
      "        1.2089, 1.1110, 1.2377, 1.1826, 1.2507, 1.1834, 1.2354, 1.1921, 1.2145,\n",
      "        1.1792, 1.2113, 1.1790, 1.1572, 1.2247, 1.6577, 1.4829, 1.5938, 1.6214,\n",
      "        1.5915, 1.5520, 1.6633, 1.6032, 1.5963, 1.5659, 1.6108, 1.6440, 1.6196,\n",
      "        1.6412, 1.6302, 1.6070, 1.2701, 1.1565, 1.2511, 1.2151, 1.1801, 1.2283,\n",
      "        1.2193, 1.2472, 1.2597, 1.2975, 1.1828, 1.2415, 1.2150, 1.2186, 1.2618,\n",
      "        1.2166, 1.8286, 1.8192, 1.8198, 1.7568, 1.8907, 1.7810, 1.8324, 1.9419,\n",
      "        1.8722, 1.9157, 1.8333, 1.9758, 1.8602, 1.9514, 1.8608, 1.9549, 1.8185,\n",
      "        1.7905, 1.8470, 1.7529, 1.7361, 1.8922, 1.7595, 1.7414, 1.8564, 1.6888,\n",
      "        1.7761, 1.8164, 1.7732, 1.8574, 1.8286, 1.8306, 1.6610, 1.7971, 1.6306,\n",
      "        1.6942, 1.7758, 1.6793, 1.6948, 1.7042, 1.7354, 1.6514, 1.6520, 1.6711,\n",
      "        1.6607, 1.7248, 1.6270, 1.6508, 1.6171, 1.5577, 1.6077, 1.5498, 1.5634,\n",
      "        1.5629, 1.6464, 1.5889, 1.6094, 1.5799, 1.6177, 1.5118, 1.6200, 1.5716,\n",
      "        1.5640, 1.6459, 1.2302, 1.1850, 1.2367, 1.2238, 1.2226, 1.1751, 1.1717,\n",
      "        1.1568, 1.2080, 1.1526, 1.1714, 1.1738, 1.2004, 1.2005, 1.1585, 1.1970,\n",
      "        1.5958, 1.6151, 1.5888, 1.6019, 1.5255, 1.5965, 1.5973, 1.5506, 1.5928,\n",
      "        1.6158, 1.6245, 1.6272, 1.5965, 1.6401, 1.6243, 1.6506, 1.7572, 1.8343,\n",
      "        1.7464, 1.8561, 1.8222, 1.7837, 1.7687, 1.8379, 1.7084, 1.8166, 1.6981,\n",
      "        1.7919, 1.7170, 1.7927, 1.6823, 1.7745, 1.7433, 1.7506, 1.6572, 1.6326,\n",
      "        1.7790, 1.7256, 1.7681, 1.8328, 1.7304, 1.7689, 1.6888, 1.6225, 1.7242,\n",
      "        1.6816, 1.7363, 1.6585, 1.5288, 1.5655, 1.4681, 1.5090, 1.5145, 1.5574,\n",
      "        1.5514, 1.5550, 1.5526, 1.5707, 1.6610, 1.5776, 1.6089, 1.5914, 1.5118,\n",
      "        1.5099, 1.7942, 1.8313, 1.7402, 1.7738, 1.8102, 1.9072, 1.7051, 1.7233,\n",
      "        1.7040, 1.8166, 1.8600, 1.7234, 1.7673, 1.7723, 1.8241, 1.8085, 1.3474,\n",
      "        1.3518, 1.2818, 1.3629, 1.3453, 1.4532, 1.3673, 1.3719, 1.4564, 1.4978,\n",
      "        1.3781, 1.3548, 1.3925, 1.3155, 1.3461, 1.3736, 1.6737, 1.6571, 1.6887,\n",
      "        1.6277, 1.5657, 1.6417, 1.6357, 1.6849, 1.6142, 1.6503, 1.6814, 1.6300,\n",
      "        1.7039, 1.6548, 1.6993, 1.6701, 1.2539, 1.2764, 1.2900, 1.2242, 1.2461,\n",
      "        1.1963, 1.2445, 1.2062, 1.2651, 1.2331, 1.2561, 1.1629, 1.2826, 1.2599,\n",
      "        1.2665, 1.2178, 1.8126, 1.6607, 1.6833, 1.6833, 1.6807, 1.6514, 1.6210,\n",
      "        1.8369, 1.5600, 1.6461, 1.7167, 1.7704, 1.7386, 1.6039, 1.7404, 1.7772,\n",
      "        1.3174, 1.3024, 1.2545, 1.2825, 1.2582, 1.3100, 1.3466, 1.2184, 1.2667,\n",
      "        1.3335, 1.2801, 1.3133, 1.2878, 1.3026, 1.2481, 1.3123, 1.4768, 1.5412,\n",
      "        1.5726, 1.3971, 1.5619, 1.5192, 1.4278, 1.5137, 1.5211, 1.4671, 1.4959,\n",
      "        1.4411, 1.4316, 1.5327, 1.4572, 1.6477], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0084, -0.0394, -0.0403,  ..., -0.0429,  0.0201, -0.0060],\n",
      "        [-0.0365,  0.0047,  0.0154,  ...,  0.0012, -0.0356,  0.0005],\n",
      "        [-0.0115, -0.0406, -0.0085,  ...,  0.0435, -0.0467, -0.0364],\n",
      "        ...,\n",
      "        [ 0.0051, -0.0521,  0.0188,  ..., -0.0150, -0.0226,  0.0161],\n",
      "        [-0.0299, -0.0272, -0.0358,  ..., -0.0450, -0.0510, -0.0172],\n",
      "        [-0.0225,  0.0482,  0.0461,  ...,  0.0377,  0.0225,  0.0445]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-8.7675e-03, -5.1151e-03,  9.7319e-04,  ...,  2.7422e-03,\n",
      "         -5.0855e-04, -6.2907e-03],\n",
      "        [ 3.6475e-03,  4.5030e-03, -3.6968e-03,  ..., -3.7198e-03,\n",
      "         -2.0764e-03,  9.4974e-05],\n",
      "        [ 8.6947e-03,  4.0728e-03, -4.6027e-03,  ...,  7.1768e-04,\n",
      "         -7.5554e-04,  1.3800e-03],\n",
      "        ...,\n",
      "        [-1.0729e-02, -4.1713e-04,  3.6773e-03,  ...,  5.8027e-03,\n",
      "          4.1815e-03, -6.3043e-03],\n",
      "        [ 5.4869e-03,  7.8676e-03,  6.1966e-03,  ...,  7.7516e-04,\n",
      "         -3.3381e-04, -1.4271e-03],\n",
      "        [-7.1521e-04, -1.1882e-02, -4.6201e-03,  ...,  3.1847e-03,\n",
      "          8.9954e-04, -9.6641e-04]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4847, 1.5929, 1.5313, 1.4825, 1.6294, 1.4067, 1.4296, 1.4947, 1.4432,\n",
      "        1.4327, 1.4255, 1.5232, 1.5402, 1.4102, 1.6674, 1.4297, 1.3469, 1.5343,\n",
      "        1.4429, 1.4683, 1.5447, 1.4051, 1.5802, 1.4457, 1.4765, 1.4807, 1.4803,\n",
      "        1.5303, 1.5174, 1.4868, 1.4502, 1.4296, 1.6065, 1.4787, 1.4153, 1.4420,\n",
      "        1.6229, 1.4237, 1.3961, 1.4692, 1.6398, 1.5301, 1.4804, 1.4961, 1.4105,\n",
      "        1.3994, 1.3918, 1.6254, 1.4942, 1.5017, 1.4562, 1.5038, 1.3135, 1.3621,\n",
      "        1.3804, 1.3310, 1.3383, 1.4443, 1.6097, 1.5453, 1.3860, 1.6812, 2.7586,\n",
      "        1.4634, 1.4569, 1.4774, 1.4244, 1.5519, 1.5115, 1.5387, 1.3334, 1.4361,\n",
      "        1.4067, 1.4260, 1.5004, 1.5598, 1.4265, 1.5222, 1.4611, 1.5024, 1.4757,\n",
      "        1.5560, 1.4889, 1.5779, 1.4003, 1.4627, 1.4467, 1.5466, 1.3058, 1.5106,\n",
      "        1.4288, 1.5281, 1.5950, 1.5137, 1.4293, 1.5646, 1.4940, 1.5696, 1.5519,\n",
      "        1.5333, 1.5781, 1.5537, 1.6112, 1.4588, 1.5417, 1.6178, 1.4521, 1.4122,\n",
      "        1.4249, 1.4224, 1.5717, 1.3567, 1.4474, 1.4381, 1.5354, 1.4890, 1.5866,\n",
      "        1.4564, 1.4747, 1.4263, 1.5180, 1.5736, 1.5495, 1.5371, 1.3367, 1.5771,\n",
      "        1.5077, 1.5349, 1.4689, 1.4845, 1.3318, 1.3942, 1.5118, 1.5730, 1.5526,\n",
      "        1.3925, 1.4515, 1.5033, 1.3460, 1.3906, 1.4260, 1.4044, 1.4792, 1.4145,\n",
      "        1.3966, 1.4975, 1.4712, 1.5728, 1.4750, 1.5404, 2.1312, 1.5662, 1.5578,\n",
      "        1.5114, 1.4889, 1.5163, 1.4562, 1.5872, 1.5550, 1.4841, 1.3470, 1.4638,\n",
      "        1.5737, 1.5756, 1.6357, 1.5455, 1.3866, 1.4440, 1.3946, 1.4988, 1.4320,\n",
      "        1.6333, 1.5200, 1.4571, 1.4814, 1.3203, 1.5121, 1.5712, 1.4472, 1.5434,\n",
      "        1.4350, 1.3939, 1.4408, 1.5003, 1.5428, 1.4292, 1.4469, 1.4532, 1.4918,\n",
      "        1.6166, 1.3459, 1.3503, 2.1469, 1.3113, 1.4698, 1.4783, 1.5003, 1.4214,\n",
      "        1.5030, 1.5503, 1.5963, 1.3037, 1.4132, 1.3628, 1.3755, 1.5242, 1.5102,\n",
      "        1.3413, 1.5110, 1.6419, 1.5411, 1.5027, 1.4399, 1.6380, 1.4678, 1.5260,\n",
      "        1.5531, 1.5105, 1.3565, 1.5618, 1.4329, 1.5806, 1.5240, 1.4493, 1.4755,\n",
      "        1.4407, 1.3851, 1.4817, 1.4257, 1.4689, 1.3388, 1.3836, 1.3691, 1.4675,\n",
      "        1.4649, 1.7272, 1.4546, 1.3515, 1.4730, 1.6176, 1.5258, 1.4473, 1.3189,\n",
      "        1.4092, 1.3544, 1.5197, 1.4250, 1.5236, 1.4793, 1.4133, 1.5223, 1.4242,\n",
      "        1.4813, 1.4046, 1.4455, 1.4489, 1.3817, 1.4607, 1.2166, 1.4952, 1.4622,\n",
      "        1.4940, 1.6238, 1.4989, 1.4074, 1.4712, 1.5460, 1.6845, 1.3857, 1.5860,\n",
      "        1.3208, 1.5446, 1.2793, 1.6310, 1.4503, 1.5875, 1.5296, 1.3466, 1.5903,\n",
      "        1.4810, 1.4760, 1.4536, 1.5861, 1.6028, 1.5472, 1.4058, 1.4097, 1.4652,\n",
      "        1.2587, 1.3328, 1.5430, 1.5461, 1.5039, 1.5061, 1.2816, 1.4576, 1.5984,\n",
      "        1.5808, 1.4635, 1.5130, 1.5072, 1.5583, 1.4701, 1.3484, 1.4578, 1.4514,\n",
      "        1.4510, 1.5357, 1.4142, 1.4526, 1.4724, 1.3488, 1.5598, 1.5471, 1.5955,\n",
      "        1.4956, 1.5843, 1.5546, 1.3769, 1.6946], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0380, -0.0195, -0.0616,  ..., -0.0118, -0.0346, -0.0639],\n",
      "        [-0.0510,  0.0050, -0.0363,  ..., -0.0216, -0.0460, -0.0192],\n",
      "        [ 0.0685,  0.0114,  0.0390,  ...,  0.0223, -0.0623, -0.0453],\n",
      "        ...,\n",
      "        [ 0.0063,  0.0098, -0.0132,  ...,  0.0242,  0.0224, -0.0100],\n",
      "        [ 0.0232, -0.0126, -0.0143,  ...,  0.0039,  0.0524, -0.0293],\n",
      "        [ 0.0487,  0.0613,  0.0325,  ...,  0.0447,  0.0033,  0.0323]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.1816e-03, -1.5096e-03, -1.2411e-03,  ..., -5.4148e-03,\n",
      "         -2.9381e-03,  6.3840e-03],\n",
      "        [-2.7670e-03, -1.2711e-05,  6.4100e-03,  ...,  4.1453e-03,\n",
      "         -3.1416e-03,  1.3537e-03],\n",
      "        [ 1.0963e-03, -4.2313e-03, -2.4132e-03,  ..., -3.1423e-03,\n",
      "          1.1254e-03, -3.5998e-03],\n",
      "        ...,\n",
      "        [ 1.3117e-03, -5.5392e-03, -8.0036e-04,  ..., -6.8896e-03,\n",
      "          1.4623e-03,  5.2303e-03],\n",
      "        [ 4.1048e-03,  1.0363e-02, -5.9255e-03,  ..., -3.4513e-03,\n",
      "          1.2009e-04, -8.7777e-03],\n",
      "        [ 6.4115e-03,  5.1365e-03, -3.8479e-03,  ..., -5.1208e-03,\n",
      "         -6.1386e-03,  1.9917e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3860, 4.9359, 1.3603,  ..., 5.0172, 1.0020, 1.6571],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0189, -0.0192,  0.0186,  ...,  0.0009, -0.0175,  0.0314],\n",
      "        [ 0.0158,  0.0179,  0.0156,  ..., -0.0105,  0.0267,  0.0187],\n",
      "        [ 0.0109, -0.0102,  0.0042,  ..., -0.0081, -0.0184, -0.0021],\n",
      "        ...,\n",
      "        [-0.0005, -0.0067, -0.0376,  ..., -0.0254, -0.0210, -0.0129],\n",
      "        [ 0.0212, -0.0044,  0.0285,  ..., -0.0189, -0.0224,  0.0165],\n",
      "        [-0.0181,  0.0074,  0.0082,  ..., -0.0187,  0.0315, -0.0198]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0033,  0.0076,  0.0036,  ..., -0.0035,  0.0047,  0.0084],\n",
      "        [-0.0036, -0.0029,  0.0023,  ...,  0.0035, -0.0036, -0.0022],\n",
      "        [ 0.0041,  0.0073,  0.0009,  ...,  0.0003,  0.0030, -0.0012],\n",
      "        ...,\n",
      "        [-0.0018,  0.0006, -0.0028,  ..., -0.0019, -0.0010,  0.0006],\n",
      "        [-0.0027,  0.0071,  0.0003,  ...,  0.0030, -0.0012,  0.0098],\n",
      "        [-0.0035,  0.0008,  0.0020,  ...,  0.0037, -0.0023,  0.0027]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([3.8149, 3.7315, 3.7687, 3.6700, 3.7981, 3.5698, 3.6005, 3.6823, 3.7088,\n",
      "        3.7539, 3.6213, 3.5834, 3.6762, 3.6809, 3.7029, 3.8593, 3.7013, 3.7596,\n",
      "        3.6325, 3.6418, 3.6793, 3.7078, 3.6466, 3.7083, 3.5864, 3.8389, 3.4463,\n",
      "        3.5969, 3.9735, 3.5738, 3.7543, 4.7797, 3.7552, 3.5598, 3.6554, 3.8208,\n",
      "        3.7364, 3.7439, 3.3990, 3.7586, 3.8101, 3.8845, 3.8639, 3.4256, 3.7160,\n",
      "        3.7569, 3.9576, 3.6226, 3.6368, 3.7530, 4.0760, 3.7530, 3.4820, 3.7196,\n",
      "        4.0250, 3.5411, 3.8174, 3.8629, 3.8166, 3.6762, 3.3964, 3.8738, 8.6770,\n",
      "        3.7124, 3.6986, 3.7288, 3.6463, 3.8160, 3.7670, 3.8035, 3.6441, 3.7238,\n",
      "        3.3602, 3.7404, 3.7404, 3.5672, 3.5347, 3.6581, 3.6898, 3.6379, 3.6280,\n",
      "        3.7780, 3.6740, 3.7749, 3.7256, 3.9697, 3.5983, 3.7784, 3.2506, 3.5741,\n",
      "        3.6843, 3.7197, 4.1230, 3.5840, 3.8690, 3.7187, 3.7679, 3.6121, 3.8842,\n",
      "        3.9946, 3.8396, 3.7563, 3.6157, 3.5551, 3.6632, 3.8133, 3.5974, 3.8192,\n",
      "        3.7414, 3.6111, 3.8265, 3.5699, 3.7218, 3.6816, 3.8667, 4.0355, 3.9039,\n",
      "        3.6302, 3.8673, 3.7473, 3.6772, 3.5590, 3.8138, 3.7352, 3.6948, 3.7770,\n",
      "        3.7383, 3.9396, 3.7687, 3.7051, 3.3543, 3.6969, 3.7449, 4.0564, 3.7283,\n",
      "        3.6897, 3.5430, 3.7640, 3.5548, 3.5545, 3.5626, 3.6484, 3.6171, 3.6371,\n",
      "        3.7190, 3.4253, 3.7235, 3.6980, 3.6843, 3.7616, 3.9960, 3.5164, 3.7495,\n",
      "        3.7289, 3.7786, 3.6836, 3.5225, 3.6051, 3.5948, 3.9058, 3.5008, 3.6366,\n",
      "        3.8777, 3.7252, 3.8791, 4.0843, 3.6611, 3.8704, 3.7233, 3.8303, 3.7804,\n",
      "        3.4797, 3.6630, 3.7143, 3.6164, 3.6129, 3.6935, 3.7812, 3.8237, 3.5658,\n",
      "        3.8297, 3.8019, 3.6010, 3.6819, 3.9706, 3.7639, 3.6924, 3.7734, 3.7815,\n",
      "        3.6659, 3.5451, 3.6783, 7.5579, 3.6284, 3.6100, 3.5907, 3.7603, 3.7497,\n",
      "        3.7785, 3.9873, 3.6308, 3.5869, 3.6986, 3.5823, 3.7949, 3.5366, 3.7427,\n",
      "        3.2953, 3.6525, 3.6382, 3.6468, 3.5785, 3.7321, 3.6147, 3.6521, 3.6534,\n",
      "        3.5297, 3.7112, 3.4478, 3.6716, 3.8303, 3.4771, 3.8540, 3.7395, 3.7196,\n",
      "        3.6983, 3.7498, 3.6468, 3.5441, 3.8425, 3.7331, 3.8806, 3.7959, 3.7692,\n",
      "        3.7438, 3.9123, 3.4694, 3.6633, 3.7713, 3.7864, 3.6730, 3.6161, 3.6453,\n",
      "        3.6352, 3.6619, 3.6223, 3.7292, 3.6740, 3.7596, 3.8164, 3.6765, 3.5884,\n",
      "        3.8351, 3.3653, 3.7132, 3.7771, 3.7238, 3.1600, 3.4906, 3.7218, 3.6837,\n",
      "        3.6254, 3.7353, 3.9306, 4.0093, 3.4772, 3.9187, 4.2752, 3.7574, 3.6649,\n",
      "        3.6273, 3.8925, 3.7543, 3.7307, 3.6691, 3.6610, 3.8134, 3.6856, 3.6903,\n",
      "        3.6316, 3.5049, 3.7339, 3.7304, 3.7111, 3.8996, 3.7621, 3.6024, 3.8520,\n",
      "        3.3911, 3.8338, 3.8376, 3.6155, 3.7657, 3.7620, 3.5554, 3.6280, 3.5693,\n",
      "        3.7054, 3.5710, 3.7255, 3.6887, 3.6178, 3.8995, 3.6548, 3.7173, 3.8786,\n",
      "        3.6073, 3.6268, 3.7480, 3.7155, 3.6325, 3.6161, 3.7669, 3.4983, 3.8438,\n",
      "        3.5142, 3.8993, 3.7748, 3.6151, 3.8467], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0033,  0.0132,  0.0365,  ...,  0.0202,  0.0412, -0.0107],\n",
      "        [-0.0153,  0.0409,  0.0161,  ...,  0.0545, -0.0022, -0.0723],\n",
      "        [ 0.0422,  0.0250, -0.0572,  ..., -0.0181,  0.0479,  0.0297],\n",
      "        ...,\n",
      "        [-0.0588, -0.0539,  0.0434,  ..., -0.0382, -0.0204,  0.0213],\n",
      "        [-0.0007,  0.0106,  0.0208,  ..., -0.0460, -0.0020, -0.0602],\n",
      "        [-0.0256, -0.0612,  0.0660,  ...,  0.0010, -0.0607, -0.0399]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 7.1777e-04,  1.0107e-03, -2.5687e-04,  ...,  3.1009e-03,\n",
      "         -1.5622e-03, -3.9114e-03],\n",
      "        [ 8.6661e-03,  7.9020e-03,  7.3792e-03,  ..., -1.0475e-02,\n",
      "          7.6158e-03, -5.7935e-03],\n",
      "        [-1.4525e-04, -1.8623e-03,  4.7877e-03,  ..., -1.3071e-04,\n",
      "          8.3135e-04, -1.5976e-03],\n",
      "        ...,\n",
      "        [-5.5887e-03, -2.5704e-03, -2.3791e-03,  ...,  1.8088e-03,\n",
      "         -1.6268e-03,  3.3067e-03],\n",
      "        [ 4.6318e-03,  5.0659e-04, -2.0670e-04,  ..., -2.8731e-03,\n",
      "          4.0944e-03, -3.8617e-03],\n",
      "        [-9.4495e-04,  2.8302e-03,  3.6313e-05,  ..., -1.3287e-03,\n",
      "          4.0950e-04,  2.1146e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6871, 1.4526, 1.4351, 1.4024, 1.4268, 2.3852, 2.0736, 2.5694, 1.8121,\n",
      "        1.4658, 1.3666, 1.3725, 1.4985, 1.2941, 2.6005, 1.9778, 1.0946, 1.1285,\n",
      "        1.4597, 1.9606, 2.4150, 4.6460, 4.2533, 3.4192, 1.0164, 1.2230, 1.2697,\n",
      "        2.0283, 2.6129, 4.4712, 4.1415, 3.6047, 1.4896, 1.5277, 1.7731, 1.7966,\n",
      "        1.8904, 1.5432, 2.2209, 1.8780, 1.5062, 1.5607, 2.1811, 1.8150, 1.8523,\n",
      "        1.7297, 2.2179, 2.2731, 1.6199, 1.9287, 1.9438, 1.8144, 2.1108, 1.7567,\n",
      "        2.3158, 2.3083, 1.5412, 1.9419, 1.8362, 1.9638, 2.2470, 2.5333, 2.5669,\n",
      "        2.3425, 2.1427, 1.7635, 2.3592, 2.2677, 2.2606, 2.1520, 2.7258, 2.4155,\n",
      "        1.4766, 1.8940, 2.1557, 2.0275, 2.0003, 2.5118, 2.6909, 2.4162, 2.2252,\n",
      "        1.8601, 1.4370, 1.7513, 1.6127, 2.0905, 2.2929, 2.2310, 2.1556, 1.8539,\n",
      "        1.7142, 1.6086, 2.4307, 2.4353, 2.0882, 2.2586, 1.2930, 1.4454, 1.4001,\n",
      "        1.4142, 1.4159, 2.3297, 1.9839, 2.5975, 1.4049, 1.4948, 1.3935, 1.2872,\n",
      "        1.4183, 1.4360, 2.0558, 2.1910, 2.0336, 1.6276, 1.5143, 1.5623, 1.2417,\n",
      "        2.0317, 2.2051, 2.1520, 1.9363, 1.3326, 1.3782, 1.5574, 2.0338, 1.5844,\n",
      "        1.9743, 2.5284, 1.8455, 1.3974, 1.4992, 1.4091, 1.4330, 2.0765, 1.9490,\n",
      "        2.5149, 1.4942, 1.6683, 1.6125, 1.4124, 1.9578, 1.6786, 2.0531, 2.4802,\n",
      "        1.7447, 1.6264, 2.1102, 1.9689, 2.2007, 1.8780, 2.3857, 2.1843, 2.4149,\n",
      "        1.9493, 1.7460, 2.2348, 1.7408, 2.4936, 2.5201, 2.6852, 1.9608, 1.7359,\n",
      "        2.0534, 2.0076, 2.4374, 2.5983, 2.9817, 2.7885, 1.6619, 1.9112, 1.9765,\n",
      "        2.2281, 2.0940, 2.5230, 2.7824, 2.8213, 2.0322, 1.5943, 1.7566, 2.0962,\n",
      "        1.7254, 1.9171, 2.3698, 2.5297, 1.7274, 1.7787, 1.9995, 1.8577, 1.7525,\n",
      "        1.7882, 2.1802, 2.3978, 1.5140, 1.5569, 1.4216, 1.4660, 1.7978, 2.2260,\n",
      "        2.0362, 2.7466, 1.6412, 1.5185, 1.7459, 1.6913, 1.7925, 1.7132, 1.9373,\n",
      "        2.3290, 1.1439, 1.1450, 1.4831, 1.7441, 2.5057, 3.6659, 3.8161, 3.3104,\n",
      "        0.8287, 1.2223, 1.2778, 2.0544, 2.2225, 3.6710, 3.5317, 3.3299, 1.6439,\n",
      "        1.9347, 1.8999, 2.0499, 1.9679, 2.1601, 2.4444, 2.3433, 1.9394, 1.8300,\n",
      "        1.8855, 1.9967, 1.9451, 1.9240, 2.1914, 2.4545, 1.8550, 1.4453, 1.7742,\n",
      "        1.8116, 1.9268, 1.6398, 2.2911, 2.0152, 1.4429, 1.7404, 1.9054, 1.7377,\n",
      "        1.9494, 1.6748, 2.3530, 2.3627, 1.3776, 1.3428, 1.7945, 1.6995, 1.6546,\n",
      "        1.7550, 2.4635, 2.1288, 1.3825, 1.5683, 1.7315, 1.7646, 1.7170, 1.9074,\n",
      "        2.0470, 2.0770, 1.4225, 1.4862, 1.7946, 1.6706, 1.6385, 1.8275, 2.2847,\n",
      "        2.3543, 1.4846, 1.3737, 1.5001, 1.6398, 1.5561, 1.7707, 2.5246, 2.5135,\n",
      "        1.8847, 1.7240, 1.6909, 1.4999, 1.6617, 1.8414, 2.3312, 2.3273, 1.5929,\n",
      "        1.4534, 1.6078, 1.8392, 1.5384, 1.9866, 2.2148, 2.4222, 1.6033, 1.1833,\n",
      "        1.2311, 1.4226, 1.3535, 1.8578, 2.0727, 2.3582, 1.5245, 1.3240, 1.4471,\n",
      "        1.6675, 1.6368, 1.7552, 1.9553, 2.0807], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0259,  0.0148,  0.0269,  ...,  0.0243, -0.0443, -0.0591],\n",
      "        [-0.0123, -0.0204,  0.0366,  ...,  0.0250,  0.0338,  0.0382],\n",
      "        [-0.0486,  0.0084,  0.0461,  ..., -0.0285,  0.0198, -0.0169],\n",
      "        ...,\n",
      "        [-0.0225,  0.0065,  0.0294,  ..., -0.0341,  0.0481, -0.0048],\n",
      "        [-0.0236, -0.0467, -0.0067,  ...,  0.0590,  0.0187,  0.0084],\n",
      "        [-0.0341,  0.0264,  0.0343,  ...,  0.0150,  0.0527, -0.0121]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.5796e-06,  6.9217e-04, -1.5964e-03,  ..., -1.5364e-04,\n",
      "         -2.9192e-03, -2.7409e-03],\n",
      "        [ 7.5962e-03, -1.2124e-03,  5.4978e-03,  ..., -3.1802e-03,\n",
      "          9.6218e-03,  1.1493e-02],\n",
      "        [ 2.6537e-03, -2.9250e-03,  4.7768e-04,  ..., -3.2449e-03,\n",
      "          3.6641e-03,  1.8413e-03],\n",
      "        ...,\n",
      "        [ 5.0311e-03, -3.6959e-03, -3.8882e-04,  ..., -1.2281e-03,\n",
      "         -2.0591e-03,  3.4481e-03],\n",
      "        [ 4.2291e-03, -2.8448e-03, -2.5524e-03,  ...,  5.2200e-04,\n",
      "          1.0510e-03,  3.9964e-04],\n",
      "        [-4.8445e-03,  7.5215e-04,  2.8268e-03,  ...,  2.8266e-03,\n",
      "         -1.2314e-03, -1.4022e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6807, 1.4307, 1.7636, 1.6830, 1.6657, 1.3030, 1.8228, 1.7511, 1.8039,\n",
      "        1.7640, 1.6534, 1.7153, 1.6557, 2.2761, 1.6481, 1.7467, 1.1294, 1.4099,\n",
      "        1.3730, 2.0052, 1.8995, 3.2229, 2.9052, 2.8106, 1.1104, 0.9371, 1.4466,\n",
      "        1.9059, 2.2824, 3.0584, 2.9910, 2.7032, 1.4649, 1.5942, 1.9909, 1.8187,\n",
      "        1.9011, 2.2834, 1.9998, 1.9099, 1.4398, 1.4613, 1.8863, 1.8782, 1.8544,\n",
      "        1.6487, 2.1006, 2.1299, 1.6681, 2.0846, 1.8235, 1.7084, 1.7897, 2.2552,\n",
      "        2.3046, 2.0202, 1.4018, 1.6632, 1.9240, 1.9929, 1.8908, 1.5534, 2.1737,\n",
      "        2.0413, 1.8911, 1.7152, 2.0667, 2.0785, 1.8501, 2.0928, 2.4556, 2.1443,\n",
      "        1.7476, 1.9452, 2.2823, 2.0459, 1.9958, 1.9189, 2.5719, 2.2809, 1.7947,\n",
      "        1.8825, 1.7319, 1.7296, 1.9794, 1.9553, 2.0040, 2.0618, 2.2655, 1.8380,\n",
      "        1.9133, 1.8738, 1.3205, 1.4627, 2.1007, 1.9249, 1.4870, 1.4953, 1.7241,\n",
      "        1.8854, 1.4263, 1.2668, 1.8915, 1.5358, 1.3256, 1.7200, 1.7187, 1.5546,\n",
      "        1.7890, 1.9629, 1.8596, 1.6556, 1.8440, 1.6380, 1.7662, 1.6628, 1.9546,\n",
      "        1.4378, 1.9767, 2.0761, 2.0126, 1.7900, 1.7213, 1.8054, 1.3985, 2.2964,\n",
      "        2.1524, 1.7854, 1.8095, 1.5997, 1.6582, 1.7066, 1.8571, 1.3782, 1.9123,\n",
      "        1.8164, 1.4106, 1.6430, 1.8560, 1.7432, 1.3322, 1.9971, 2.0423, 1.8300,\n",
      "        2.0353, 1.7606, 1.9358, 2.0164, 1.7584, 2.1436, 1.9919, 2.0209, 1.5934,\n",
      "        1.7357, 1.8210, 1.9488, 2.0299, 1.8821, 2.2560, 2.2505, 1.9087, 1.8767,\n",
      "        1.8714, 2.0468, 2.0214, 1.9669, 2.8013, 2.6124, 1.6555, 1.7199, 2.1441,\n",
      "        2.0517, 2.0197, 2.3078, 2.5453, 2.3597, 1.7734, 1.5072, 1.9539, 1.9709,\n",
      "        1.9837, 2.0163, 2.4425, 2.4088, 1.8796, 1.8776, 1.7694, 2.0101, 1.7557,\n",
      "        1.7838, 2.0930, 2.2539, 1.7841, 1.6222, 1.7333, 1.8630, 1.5031, 1.2235,\n",
      "        1.9648, 1.8491, 1.9308, 1.7543, 1.5493, 1.3137, 1.3323, 2.1866, 1.9203,\n",
      "        1.9818, 1.1345, 1.2856, 1.5352, 1.9543, 2.2014, 2.7564, 2.7764, 2.5335,\n",
      "        0.8825, 1.1847, 1.1243, 1.7192, 1.8325, 2.8481, 2.5568, 2.6210, 1.7238,\n",
      "        1.8357, 1.9251, 1.8922, 1.6434, 1.8027, 2.2908, 2.2678, 1.7471, 1.9001,\n",
      "        1.8337, 2.0221, 1.8860, 1.7256, 1.9931, 2.2125, 1.7633, 1.5414, 1.7713,\n",
      "        1.8878, 1.8525, 1.9498, 2.1735, 1.9772, 1.3362, 1.6338, 1.8312, 1.7498,\n",
      "        1.9454, 1.8467, 2.2588, 2.3220, 1.2796, 1.5677, 1.8584, 1.8536, 1.7605,\n",
      "        1.8537, 1.9970, 2.1384, 1.5676, 1.4576, 1.7917, 1.8079, 1.9201, 1.7471,\n",
      "        2.4884, 1.9781, 1.2839, 1.3820, 1.7189, 1.7621, 1.7019, 1.7062, 2.1437,\n",
      "        1.8855, 1.0064, 1.3232, 1.6782, 1.9594, 1.7435, 1.7301, 1.9685, 1.9647,\n",
      "        1.7858, 1.7421, 1.7448, 1.7540, 1.7910, 1.7134, 2.2966, 2.0933, 1.6607,\n",
      "        1.6581, 1.8232, 1.8057, 1.6983, 1.5933, 2.0818, 2.2333, 1.4301, 1.5214,\n",
      "        1.4677, 1.3381, 1.8000, 1.2431, 1.4636, 1.5289, 1.7736, 1.4357, 1.4008,\n",
      "        1.2863, 1.1181, 1.2514, 1.6129, 1.5635], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0169, -0.0057, -0.0141,  ...,  0.0508, -0.0265,  0.0463],\n",
      "        [-0.0454, -0.0390,  0.0142,  ...,  0.0315, -0.0147, -0.0418],\n",
      "        [-0.0209,  0.0322,  0.0220,  ...,  0.0055, -0.0152,  0.0257],\n",
      "        ...,\n",
      "        [ 0.0269, -0.0349, -0.0469,  ...,  0.0569,  0.0462, -0.0007],\n",
      "        [-0.0212, -0.0460, -0.0420,  ..., -0.0385,  0.0417, -0.0500],\n",
      "        [ 0.0321,  0.0170,  0.0496,  ...,  0.0440, -0.0270, -0.0421]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0032, -0.0022,  0.0043,  ...,  0.0016, -0.0033,  0.0032],\n",
      "        [ 0.0052, -0.0057,  0.0060,  ...,  0.0032, -0.0073,  0.0075],\n",
      "        [ 0.0006, -0.0014,  0.0014,  ...,  0.0043,  0.0013,  0.0009],\n",
      "        ...,\n",
      "        [ 0.0013, -0.0027,  0.0049,  ...,  0.0003, -0.0023,  0.0027],\n",
      "        [-0.0006, -0.0019, -0.0001,  ..., -0.0019, -0.0003,  0.0032],\n",
      "        [-0.0059,  0.0039, -0.0048,  ..., -0.0038,  0.0055, -0.0074]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0157, 1.8019, 1.7274, 1.8741, 1.9388, 1.7895, 1.7706, 1.8351, 1.7671,\n",
      "        1.8792, 1.7175, 1.8124, 1.7629, 1.8196, 1.9635, 1.8564, 1.5505, 1.4399,\n",
      "        1.5271, 1.4752, 1.4976, 1.4683, 1.4122, 1.4246, 1.4647, 1.4120, 1.4259,\n",
      "        1.4778, 1.5551, 1.4697, 1.4932, 1.4528, 1.7829, 1.8704, 1.8244, 1.7359,\n",
      "        1.8889, 1.7156, 1.8566, 1.6853, 1.8763, 1.9312, 1.7365, 1.7814, 1.6848,\n",
      "        1.7775, 1.7902, 1.8418, 1.8000, 1.7595, 2.2120, 1.9459, 1.8395, 1.8849,\n",
      "        1.8604, 1.8619, 1.9627, 1.9837, 1.7978, 1.8078, 2.0202, 2.1606, 1.8939,\n",
      "        1.8782, 2.1685, 2.0821, 1.7798, 2.3172, 1.7910, 2.2750, 1.7583, 1.9139,\n",
      "        1.9447, 1.9798, 1.9792, 1.9179, 1.8361, 1.9155, 2.0399, 1.9603, 1.8064,\n",
      "        1.9186, 1.7186, 1.7648, 1.7463, 1.7378, 1.7486, 1.7444, 1.8732, 1.7531,\n",
      "        1.7841, 1.6663, 1.7606, 1.7739, 1.7353, 1.7846, 1.7696, 1.7919, 1.8656,\n",
      "        1.8842, 1.8700, 1.8850, 1.7273, 1.9108, 1.9347, 1.8787, 1.9328, 1.8454,\n",
      "        1.9100, 1.9108, 1.8761, 1.8545, 1.8353, 1.8158, 1.7044, 1.8782, 1.8593,\n",
      "        1.8788, 1.7721, 1.8355, 1.7718, 1.8029, 1.8199, 1.7489, 1.8203, 1.7040,\n",
      "        1.7886, 1.8478, 1.8516, 1.8242, 1.8020, 1.7064, 1.8558, 1.7449, 1.7347,\n",
      "        1.9073, 1.7177, 1.8140, 1.7344, 1.7803, 1.7619, 1.8497, 1.9177, 1.7890,\n",
      "        1.6578, 1.5925, 1.7129, 1.6639, 1.7793, 1.6799, 1.7249, 1.6437, 1.6969,\n",
      "        1.7042, 1.7601, 1.7942, 1.7408, 1.7412, 1.6820, 1.7866, 1.8983, 1.8643,\n",
      "        1.8802, 1.8284, 2.0615, 2.0613, 1.8835, 1.8383, 1.9284, 1.8492, 1.9284,\n",
      "        1.9081, 1.9335, 1.8283, 1.8829, 1.9498, 1.8861, 2.0301, 1.9009, 1.9513,\n",
      "        2.0681, 1.8736, 1.9404, 1.9392, 1.9203, 1.9185, 1.9599, 1.9055, 1.9942,\n",
      "        1.9380, 1.9500, 1.9337, 1.3754, 1.3890, 1.3077, 1.4203, 1.2986, 1.3203,\n",
      "        1.4207, 1.3317, 1.3794, 1.3424, 1.5064, 1.4101, 1.4242, 1.4025, 1.3395,\n",
      "        1.3613, 1.5976, 1.5292, 1.4052, 1.5621, 1.5117, 1.5260, 1.5065, 1.5066,\n",
      "        1.4992, 1.3663, 1.4227, 1.5165, 1.4228, 1.4534, 1.5027, 1.4374, 2.1221,\n",
      "        1.9334, 2.0105, 1.8866, 2.1236, 1.9795, 1.8556, 1.8910, 1.8606, 1.9011,\n",
      "        1.7899, 2.1173, 2.1776, 1.9726, 1.9106, 1.8646, 1.7307, 1.8132, 1.7280,\n",
      "        1.7809, 1.7179, 1.6925, 1.7793, 1.7728, 1.7715, 1.7731, 1.8334, 1.6572,\n",
      "        1.6935, 1.7497, 1.7228, 1.8197, 1.8315, 1.7464, 1.8162, 1.7579, 1.7040,\n",
      "        1.6778, 1.7605, 1.7791, 1.7488, 1.7348, 1.7164, 1.8160, 1.8196, 1.8014,\n",
      "        1.6610, 1.7425, 1.8720, 1.9847, 1.8933, 2.0537, 1.8459, 1.8784, 1.9121,\n",
      "        1.8506, 1.7790, 1.9241, 1.8651, 1.8870, 1.9662, 2.0304, 1.9246, 1.9625,\n",
      "        1.9097, 1.8095, 1.8948, 1.8818, 1.9004, 2.0653, 1.8721, 1.9872, 1.8330,\n",
      "        1.9091, 1.9717, 1.8692, 1.9356, 1.9202, 1.8306, 1.9235, 1.7483, 1.7828,\n",
      "        1.8043, 1.6252, 2.0728, 1.7454, 1.6731, 1.8337, 1.7958, 1.7485, 1.9289,\n",
      "        1.6809, 1.7414, 1.7384, 1.7585, 1.8604], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0159,  0.0425, -0.0004,  ...,  0.0434,  0.0239, -0.0472],\n",
      "        [-0.0457,  0.0283, -0.0137,  ...,  0.0103,  0.0145,  0.0185],\n",
      "        [ 0.0314,  0.0251, -0.0137,  ...,  0.0279, -0.0114, -0.0452],\n",
      "        ...,\n",
      "        [ 0.0036,  0.0490, -0.0268,  ..., -0.0008,  0.0233,  0.0407],\n",
      "        [ 0.0436, -0.0079,  0.0488,  ...,  0.0146,  0.0006, -0.0011],\n",
      "        [-0.0373, -0.0127,  0.0070,  ..., -0.0364, -0.0175,  0.0066]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0132, -0.0117,  0.0111,  ..., -0.0087,  0.0089, -0.0112],\n",
      "        [-0.0043,  0.0034, -0.0036,  ...,  0.0065,  0.0031,  0.0009],\n",
      "        [ 0.0119, -0.0123,  0.0112,  ..., -0.0110,  0.0116, -0.0129],\n",
      "        ...,\n",
      "        [-0.0038,  0.0044, -0.0047,  ...,  0.0013,  0.0019,  0.0044],\n",
      "        [-0.0033,  0.0013, -0.0044,  ...,  0.0062, -0.0023,  0.0010],\n",
      "        [-0.0040,  0.0009, -0.0017,  ...,  0.0082, -0.0099,  0.0033]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6067, 1.7704, 1.6566, 1.8597, 1.7667, 1.6433, 1.6774, 1.5701, 1.7817,\n",
      "        1.8153, 1.3653, 1.6064, 1.7092, 1.6978, 1.7449, 1.6615, 1.8366, 1.7884,\n",
      "        1.6595, 1.8355, 1.8281, 1.6543, 1.6978, 1.5922, 1.6456, 1.5314, 1.6054,\n",
      "        1.6990, 1.7374, 1.8622, 1.6635, 1.4337, 1.6517, 1.4495, 1.6933, 1.7534,\n",
      "        1.6826, 1.6821, 1.6162, 1.6630, 1.6874, 1.6798, 1.7752, 1.6176, 1.7568,\n",
      "        1.5831, 1.6128, 1.5385, 1.6769, 1.7641, 1.7594, 1.7489, 1.5140, 1.6572,\n",
      "        1.5922, 1.6767, 1.5473, 1.7119, 1.7849, 1.6380, 1.7101, 1.7547, 5.9812,\n",
      "        1.7400, 1.7186, 1.6974, 1.5837, 1.7088, 1.7222, 1.8712, 1.7129, 1.8223,\n",
      "        1.5689, 1.7573, 1.6328, 1.7721, 1.7698, 1.6757, 1.7404, 1.7506, 1.5328,\n",
      "        1.6649, 1.7036, 1.6356, 1.5937, 1.7000, 1.6787, 1.7230, 1.4770, 1.8228,\n",
      "        1.8348, 1.6120, 1.8995, 1.6431, 1.7100, 1.7357, 1.7318, 1.8198, 1.7678,\n",
      "        1.8610, 1.9062, 1.6743, 1.7490, 1.6111, 1.7737, 1.8217, 1.5183, 1.6614,\n",
      "        1.7369, 1.6534, 1.6528, 1.6588, 1.6085, 1.5338, 1.8365, 1.5659, 1.6737,\n",
      "        1.7157, 1.5990, 1.8325, 1.6130, 1.7011, 1.7031, 1.7500, 1.6300, 1.8079,\n",
      "        1.6797, 1.6583, 1.5062, 1.7787, 1.4906, 1.8386, 1.7237, 1.7641, 1.7320,\n",
      "        1.6439, 1.6969, 1.6698, 1.5787, 1.7563, 1.5464, 1.6715, 1.7082, 1.6443,\n",
      "        1.6638, 1.8756, 1.6881, 1.6553, 1.6846, 1.6713, 1.9620, 1.7405, 1.9372,\n",
      "        1.7599, 1.8437, 1.6799, 1.5606, 1.7587, 1.9374, 1.6921, 1.6447, 1.7213,\n",
      "        1.6494, 1.7068, 1.7641, 1.6637, 1.6982, 1.7280, 1.7438, 1.7407, 1.5813,\n",
      "        1.7129, 1.6833, 1.6193, 1.7266, 1.7137, 1.5375, 1.7093, 1.7323, 1.6162,\n",
      "        1.6115, 1.6578, 1.6698, 1.7406, 1.6051, 1.6396, 1.5645, 1.6327, 1.6098,\n",
      "        1.6609, 1.6683, 1.6097, 2.5952, 1.5879, 1.4720, 1.6662, 1.6685, 1.7379,\n",
      "        1.6936, 1.8702, 1.7704, 1.7825, 1.7057, 1.5661, 1.8266, 1.6566, 1.7289,\n",
      "        1.3421, 1.7034, 1.6460, 1.6635, 1.6247, 1.5934, 1.7699, 1.6676, 1.8142,\n",
      "        1.6528, 1.8195, 1.4090, 1.5664, 1.5072, 1.6499, 1.6215, 1.7249, 1.7308,\n",
      "        1.6453, 1.6428, 1.5970, 1.6127, 1.6770, 1.6305, 1.5997, 1.6801, 1.7204,\n",
      "        1.5961, 1.6472, 1.6017, 1.5721, 1.7311, 1.7864, 1.5595, 1.6808, 1.6743,\n",
      "        1.6613, 1.5589, 1.8072, 1.6187, 1.6327, 1.7599, 1.7200, 1.3839, 1.6988,\n",
      "        1.6993, 1.5867, 1.6323, 1.7548, 1.6845, 1.5426, 1.4912, 1.7405, 1.6063,\n",
      "        1.6585, 1.7396, 1.6008, 1.6472, 1.7417, 1.7493, 2.0602, 1.5561, 1.6574,\n",
      "        1.5844, 1.7866, 1.6164, 1.6381, 1.8137, 1.6449, 1.7252, 1.5740, 1.7416,\n",
      "        1.8183, 1.6935, 1.6699, 1.7051, 1.7405, 1.7519, 1.6943, 1.7957, 1.6995,\n",
      "        1.4572, 1.6589, 1.6567, 1.8010, 1.7020, 1.7282, 1.4622, 1.7940, 1.7505,\n",
      "        1.6543, 1.5468, 1.8006, 1.7825, 1.6824, 1.6195, 1.5545, 1.6330, 1.6590,\n",
      "        1.7078, 1.6709, 1.5965, 1.6844, 1.6884, 1.4929, 1.6187, 1.7026, 1.6952,\n",
      "        1.7263, 1.7908, 1.6135, 1.6903, 1.6452], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0472, -0.0120,  0.0471,  ...,  0.0190, -0.0100, -0.0648],\n",
      "        [-0.0455,  0.0083,  0.0616,  ..., -0.0499,  0.0150,  0.0438],\n",
      "        [-0.0222,  0.0209, -0.0164,  ...,  0.0532,  0.0111, -0.0110],\n",
      "        ...,\n",
      "        [-0.0215, -0.0431, -0.0165,  ...,  0.0482,  0.0448,  0.0186],\n",
      "        [-0.0470, -0.0289,  0.0128,  ..., -0.0127,  0.0568, -0.0021],\n",
      "        [-0.0586, -0.0109,  0.0397,  ...,  0.0491,  0.0495,  0.0369]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 6.0570e-03,  3.0275e-03, -3.6607e-03,  ..., -2.5852e-03,\n",
      "          4.8957e-03,  1.4018e-03],\n",
      "        [-7.3015e-03, -1.0378e-02,  9.9925e-03,  ...,  9.0027e-03,\n",
      "         -8.6360e-03, -9.7374e-03],\n",
      "        [-1.6266e-03, -2.6649e-03,  2.2445e-03,  ...,  2.0780e-03,\n",
      "         -2.9271e-03, -1.1321e-03],\n",
      "        ...,\n",
      "        [ 6.6193e-03, -8.9959e-03, -1.2115e-03,  ...,  2.0549e-03,\n",
      "          9.2343e-04,  4.1864e-03],\n",
      "        [ 2.0050e-03,  5.2059e-03, -6.5143e-03,  ..., -4.5720e-03,\n",
      "          5.1839e-03,  6.4788e-03],\n",
      "        [-1.9403e-03,  1.4193e-05,  1.7447e-03,  ...,  7.7181e-03,\n",
      "         -3.9025e-03, -1.2568e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6496, 1.5361, 4.6478,  ..., 1.6842, 1.6236, 1.7560],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0188, -0.0192,  0.0209,  ..., -0.0158,  0.0169, -0.0118],\n",
      "        [-0.0078, -0.0274,  0.0226,  ...,  0.0147,  0.0227,  0.0271],\n",
      "        [ 0.0026,  0.0237,  0.0254,  ...,  0.0171,  0.0172, -0.0031],\n",
      "        ...,\n",
      "        [-0.0045,  0.0039, -0.0002,  ...,  0.0137,  0.0229, -0.0244],\n",
      "        [ 0.0156,  0.0131, -0.0208,  ...,  0.0125,  0.0179,  0.0068],\n",
      "        [ 0.0033,  0.0057, -0.0069,  ..., -0.0079, -0.0142, -0.0046]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0040,  0.0008,  0.0010,  ..., -0.0017,  0.0010, -0.0056],\n",
      "        [-0.0035,  0.0043,  0.0021,  ..., -0.0053,  0.0013, -0.0018],\n",
      "        [ 0.0073, -0.0046, -0.0065,  ...,  0.0027, -0.0114,  0.0109],\n",
      "        ...,\n",
      "        [-0.0066,  0.0060,  0.0041,  ..., -0.0058,  0.0056, -0.0062],\n",
      "        [-0.0091,  0.0114,  0.0024,  ..., -0.0120, -0.0028, -0.0048],\n",
      "        [ 0.0022,  0.0029, -0.0090,  ..., -0.0045, -0.0136,  0.0111]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 3.5978,  3.7346,  3.5381,  3.7899,  3.9584,  3.5981,  3.6625,  3.5402,\n",
      "         3.6029,  3.7119,  3.4955,  3.8437,  3.7663,  3.5412,  3.8481,  3.6584,\n",
      "         3.5921,  3.5433,  3.5409,  3.6027,  3.7802,  3.9287,  3.6090,  3.6506,\n",
      "         3.4038,  3.6587,  3.5564,  3.6223,  3.6069,  3.6590,  3.6454,  4.5075,\n",
      "         3.7538,  3.5116,  3.6616,  3.6413,  3.6242,  3.4911,  3.5085,  3.8746,\n",
      "         3.5258,  3.6481,  3.6436,  3.5151,  3.9298,  3.5416,  5.2902,  3.5661,\n",
      "         3.5179,  3.9366,  3.7410,  3.6541,  3.4195,  3.6123,  3.6246,  3.5183,\n",
      "         3.8264,  3.8732,  4.0598,  3.5105,  3.5912,  3.8581, 12.3648,  3.7725,\n",
      "         3.8288,  3.7026,  3.7497,  3.5961,  3.8571,  3.8401,  3.7850,  3.7244,\n",
      "         3.4411,  3.9100,  3.7238,  3.5400,  3.5454,  3.5094,  3.6529,  3.4337,\n",
      "         3.6327,  3.5656,  3.7361,  3.7288,  3.6244,  3.5133,  3.6288,  3.7351,\n",
      "         3.3809,  3.7915,  3.8422,  3.7880,  4.4630,  3.7082,  3.7531,  3.7820,\n",
      "         3.6065,  3.8141,  3.7373,  3.8634,  3.7326,  3.7730,  3.7556,  3.5779,\n",
      "         3.5191,  3.8314,  3.6974,  3.6882,  3.7212,  3.6480,  3.7760,  3.8429,\n",
      "         3.7247,  3.6251,  3.7463,  3.4994,  3.7268,  3.7043,  3.5900,  3.7145,\n",
      "         3.7160,  3.6867,  3.7164,  3.8160,  3.7742,  3.6146,  3.6047,  3.6242,\n",
      "         3.6459,  3.4500,  3.4470,  3.6631,  3.8881,  3.7588,  3.6348,  3.6504,\n",
      "         3.6191,  3.7989,  3.6424,  3.6928,  3.5461,  3.5723,  3.5347,  3.7543,\n",
      "         3.7549,  3.7775,  3.7423,  3.6673,  3.7877,  3.7641,  5.2090,  3.6406,\n",
      "         3.6682,  3.7550,  3.9528,  3.6413,  3.7637,  3.9006,  4.0412,  3.6411,\n",
      "         3.7118,  3.8429,  3.5608,  3.6601,  3.7993,  3.9963,  3.5924,  3.6127,\n",
      "         3.5586,  3.6035,  3.4920,  3.5676,  3.5810,  3.8952,  3.6070,  3.4722,\n",
      "         3.6692,  3.6947,  3.8538,  3.8329,  3.6004,  3.5945,  3.6628,  3.5497,\n",
      "         3.7059,  3.7592,  3.5712,  3.7742,  3.7684,  3.7493,  3.6094,  3.3878,\n",
      "         6.3363,  3.6086,  3.6106,  3.5219,  3.6887,  3.7009,  3.7988,  3.8544,\n",
      "         3.5942,  3.6564,  3.8282,  3.6658,  3.7849,  3.7637,  3.5155,  3.2757,\n",
      "         3.6566,  3.5658,  3.7974,  3.5733,  3.5439,  4.2014,  3.4494,  3.6704,\n",
      "         3.6253,  3.7687,  3.3508,  3.5175,  3.4852,  3.6059,  3.9774,  3.6852,\n",
      "         3.5789,  3.7485,  3.5577,  3.7161,  3.6207,  3.4699,  3.5819,  3.4720,\n",
      "         3.6089,  3.7174,  3.7577,  3.8413,  3.7654,  3.4543,  3.7390,  3.8679,\n",
      "         3.5843,  3.6458,  3.5589,  3.5821,  3.3914,  3.5220,  3.6077,  3.5711,\n",
      "         3.7104,  3.8168,  3.6794,  3.7184,  3.9078,  3.4899,  3.6578,  3.8066,\n",
      "         3.5552,  3.3275,  3.2465,  4.0034,  3.6701,  3.7006,  3.6641,  3.6192,\n",
      "         3.8232,  3.7264,  3.9042,  5.4846,  3.5536,  3.5843,  3.6456,  3.5834,\n",
      "         3.6121,  3.8499,  3.6785,  3.7908,  3.7195,  3.4159,  3.8028,  3.6927,\n",
      "         3.5121,  3.6617,  3.6017,  3.6452,  3.8839,  3.8682,  3.7742,  3.6330,\n",
      "         3.5820,  3.9323,  3.6208,  3.8067,  3.6217,  3.9814,  3.4779,  3.5567,\n",
      "         3.6281,  3.6431,  3.4831,  3.6019,  3.5212,  3.6700,  3.6451,  3.5738,\n",
      "         3.6730,  3.4535,  3.8890,  3.5203,  3.5831,  3.8185,  3.5114,  3.5989,\n",
      "         3.5809,  3.6285,  3.5411,  4.0875,  3.7671,  3.6426,  3.4556,  3.8920],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0119,  0.0440, -0.0439,  ...,  0.0091, -0.0309, -0.0630],\n",
      "        [-0.0040,  0.0667,  0.0104,  ...,  0.0342,  0.0648, -0.0382],\n",
      "        [-0.0625,  0.0204,  0.0409,  ...,  0.0281,  0.0505,  0.0298],\n",
      "        ...,\n",
      "        [ 0.0340,  0.0382,  0.0151,  ..., -0.0180,  0.0185, -0.0409],\n",
      "        [ 0.0413,  0.0156, -0.0023,  ..., -0.0326, -0.0210,  0.0582],\n",
      "        [ 0.0317,  0.0195,  0.0134,  ...,  0.0512,  0.0514,  0.0094]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0043, -0.0031, -0.0023,  ..., -0.0022,  0.0023, -0.0037],\n",
      "        [ 0.0051,  0.0051,  0.0021,  ...,  0.0051, -0.0038,  0.0052],\n",
      "        [-0.0120, -0.0129, -0.0129,  ..., -0.0140,  0.0123, -0.0121],\n",
      "        ...,\n",
      "        [-0.0099, -0.0092, -0.0098,  ..., -0.0091,  0.0091, -0.0098],\n",
      "        [ 0.0030,  0.0028,  0.0043,  ...,  0.0023, -0.0025,  0.0028],\n",
      "        [-0.0032, -0.0038, -0.0032,  ..., -0.0040,  0.0040, -0.0035]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6347, 1.7477, 1.7440, 1.8784, 1.9181, 1.6816, 2.1900, 2.5722, 1.5431,\n",
      "        1.7020, 1.9422, 2.0768, 1.9874, 1.6498, 2.4465, 2.1087, 1.5202, 1.3482,\n",
      "        1.7116, 1.6758, 2.5338, 3.4355, 3.6089, 4.1409, 1.0344, 1.4874, 1.6849,\n",
      "        1.9782, 2.3204, 3.5023, 3.6239, 3.8123, 1.3655, 1.3264, 0.9898, 1.4213,\n",
      "        1.8973, 1.9518, 1.6483, 1.8330, 1.1164, 0.7407, 1.3872, 1.2660, 1.2936,\n",
      "        1.4150, 1.6957, 1.8440, 1.5375, 1.6607, 2.0458, 1.9637, 2.2102, 3.5229,\n",
      "        3.3109, 3.5561, 1.3116, 1.7409, 1.9969, 1.8843, 2.2777, 3.4235, 3.2384,\n",
      "        3.3016, 1.6208, 2.2510, 1.6738, 1.3802, 1.7906, 1.7044, 1.9756, 2.0585,\n",
      "        1.9599, 1.9064, 1.7661, 1.7074, 1.4144, 1.8839, 2.0782, 2.1382, 1.3944,\n",
      "        1.1502, 1.3702, 1.5767, 2.2107, 2.0483, 1.8678, 2.0895, 1.5775, 1.5028,\n",
      "        1.2353, 1.2250, 1.3722, 1.6780, 1.9683, 1.9624, 1.7231, 1.7372, 1.8042,\n",
      "        1.7375, 1.6263, 1.6131, 2.3609, 2.2448, 2.0275, 2.0645, 1.9987, 1.5553,\n",
      "        2.0495, 1.4765, 2.4698, 2.1948, 1.6977, 1.5273, 1.4997, 1.0005, 1.4125,\n",
      "        2.1094, 1.8512, 1.8882, 1.6380, 1.2772, 0.9755, 1.7383, 1.4337, 1.5577,\n",
      "        1.9070, 1.8767, 1.4304, 1.5129, 1.3566, 1.3904, 1.2466, 1.8215, 1.7879,\n",
      "        1.3408, 1.7547, 1.6808, 1.6451, 1.2966, 1.6422, 1.3191, 1.9997, 1.8823,\n",
      "        1.0547, 1.1075, 1.0863, 1.6316, 1.4015, 1.5369, 1.7903, 2.3059, 1.5980,\n",
      "        1.1232, 1.5287, 1.4182, 2.1047, 1.9431, 1.8165, 1.9708, 1.8390, 1.3895,\n",
      "        1.0250, 1.2817, 1.7975, 1.5367, 1.7394, 1.9296, 1.2120, 1.3922, 1.4866,\n",
      "        1.5159, 1.4446, 1.9964, 1.9117, 1.9593, 2.1112, 2.3143, 2.2221, 1.9694,\n",
      "        1.9762, 1.7550, 2.3741, 2.3431, 1.7834, 1.8120, 1.9284, 1.8660, 1.6517,\n",
      "        2.6516, 2.3782, 2.3300, 1.8451, 1.2319, 1.6000, 1.2749, 1.9332, 2.0454,\n",
      "        1.7667, 1.9873, 0.9248, 1.3314, 1.0776, 1.7732, 1.4615, 1.4835, 1.7375,\n",
      "        2.0057, 1.9932, 1.7338, 1.5357, 1.5910, 1.7158, 1.8224, 1.9420, 2.2921,\n",
      "        1.7647, 1.9700, 2.0115, 1.8213, 1.6438, 2.1969, 1.5287, 2.2566, 1.0362,\n",
      "        1.2024, 1.6454, 1.9614, 1.8456, 1.4230, 1.8704, 1.8272, 1.6628, 1.0373,\n",
      "        1.1278, 1.2387, 1.3742, 2.0040, 1.7978, 2.2198, 1.9867, 1.7284, 1.8440,\n",
      "        1.8674, 1.7964, 2.3654, 2.9857, 3.0239, 2.0915, 2.0161, 2.0863, 1.9179,\n",
      "        2.4762, 2.3602, 3.0395, 2.9668, 1.5852, 1.3298, 1.1664, 1.0862, 2.1685,\n",
      "        1.5791, 1.9750, 1.8797, 1.7756, 1.0227, 1.5319, 2.0977, 1.5381, 1.9972,\n",
      "        1.7844, 1.9191, 1.6474, 1.3457, 1.5272, 1.2283, 1.2762, 1.8996, 1.8916,\n",
      "        1.9457, 1.5386, 1.5242, 1.3582, 1.7246, 1.8551, 1.4814, 1.7626, 1.9146,\n",
      "        1.6269, 1.2864, 1.2301, 1.7471, 1.9743, 2.2713, 2.0534, 2.0212, 1.9128,\n",
      "        1.3160, 1.3228, 1.2712, 1.6712, 1.4660, 1.9638, 1.9478, 2.1536, 1.4944,\n",
      "        1.5921, 1.9516, 2.2610, 2.1663, 1.8905, 2.2260, 1.0674, 1.4787, 1.5075,\n",
      "        1.6366, 1.5711, 1.8105, 1.9937, 2.2593], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0025,  0.0387, -0.0509,  ...,  0.0368,  0.0354, -0.0507],\n",
      "        [ 0.0435,  0.0055,  0.0279,  ..., -0.0392, -0.0292,  0.0567],\n",
      "        [-0.0303,  0.0089, -0.0509,  ..., -0.0074, -0.0504,  0.0337],\n",
      "        ...,\n",
      "        [ 0.0179, -0.0153, -0.0026,  ..., -0.0056,  0.0040, -0.0458],\n",
      "        [-0.0420,  0.0106, -0.0107,  ...,  0.0197, -0.0091,  0.0046],\n",
      "        [ 0.0196,  0.0323,  0.0104,  ..., -0.0458, -0.0011,  0.0123]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0019, -0.0043,  0.0016,  ...,  0.0109,  0.0039,  0.0089],\n",
      "        [ 0.0011,  0.0038,  0.0030,  ...,  0.0043, -0.0035, -0.0002],\n",
      "        [ 0.0049, -0.0019, -0.0025,  ..., -0.0007, -0.0039, -0.0051],\n",
      "        ...,\n",
      "        [-0.0013,  0.0034, -0.0042,  ..., -0.0021, -0.0040, -0.0028],\n",
      "        [ 0.0014, -0.0047,  0.0082,  ...,  0.0045,  0.0014,  0.0006],\n",
      "        [ 0.0024,  0.0011, -0.0042,  ..., -0.0029, -0.0003,  0.0046]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5245, 1.6975, 1.9299, 2.0781, 1.8549, 1.7202, 2.2554, 2.1172, 1.6854,\n",
      "        1.6531, 1.7998, 1.7827, 2.0010, 2.2508, 2.0930, 2.0252, 1.4865, 1.3871,\n",
      "        1.6291, 1.8161, 2.2119, 2.5397, 2.6790, 2.5864, 0.8706, 1.4072, 1.6221,\n",
      "        1.7883, 1.9958, 2.4665, 2.6825, 2.2701, 1.5708, 1.3263, 1.4264, 1.2355,\n",
      "        1.0406, 1.2159, 1.5979, 1.7013, 1.5297, 1.5274, 1.6032, 1.4442, 1.9554,\n",
      "        1.8687, 1.6010, 1.5876, 1.4694, 1.6311, 1.9243, 2.0949, 2.0128, 2.5624,\n",
      "        2.7472, 2.8153, 1.4129, 1.7926, 2.0883, 1.7827, 2.0699, 2.4182, 2.6884,\n",
      "        2.6699, 1.7503, 2.0008, 1.7601, 1.5793, 1.5342, 2.0474, 1.9118, 1.9521,\n",
      "        1.7818, 1.9673, 1.8984, 2.0256, 1.7396, 1.4750, 1.9399, 1.9343, 1.8638,\n",
      "        1.4892, 1.6782, 1.3121, 1.1532, 1.2651, 1.8245, 1.7497, 1.6372, 1.7515,\n",
      "        1.5207, 1.7802, 2.2907, 1.9060, 1.7985, 1.8242, 1.7645, 1.5966, 1.7376,\n",
      "        1.8801, 2.1194, 2.0621, 2.1553, 2.0969, 1.8941, 2.0255, 2.0274, 1.8570,\n",
      "        1.5999, 1.7161, 2.0610, 2.0709, 1.6884, 1.7673, 1.5846, 1.9950, 2.1360,\n",
      "        1.2862, 1.7542, 1.8084, 1.7043, 1.5723, 1.8168, 1.3150, 1.2621, 2.0770,\n",
      "        1.7892, 1.7494, 1.5783, 1.6578, 1.7423, 1.7573, 1.7332, 1.3659, 1.7203,\n",
      "        1.7726, 1.5868, 1.7491, 1.6516, 1.5526, 1.5757, 1.8394, 1.8900, 1.8075,\n",
      "        1.4539, 1.4231, 1.7140, 1.4055, 2.1165, 1.8807, 1.7170, 1.5327, 1.7660,\n",
      "        1.6162, 1.4933, 1.3670, 1.1465, 1.2178, 1.6887, 1.7065, 1.6174, 1.4434,\n",
      "        1.5341, 1.5369, 1.2011, 2.1911, 1.6967, 1.6811, 2.0300, 1.8479, 1.5880,\n",
      "        1.5005, 1.8036, 1.2361, 1.7341, 2.0779, 1.9518, 2.0751, 2.0547, 1.8949,\n",
      "        1.4770, 2.2767, 2.2502, 2.2742, 1.8926, 1.9692, 1.9572, 1.9665, 2.2287,\n",
      "        1.3688, 2.1553, 2.1323, 1.7613, 1.5412, 1.4584, 1.9233, 1.2550, 1.1386,\n",
      "        1.6293, 1.7756, 1.6617, 1.7082, 1.7561, 1.2056, 1.7694, 2.1064, 1.5859,\n",
      "        1.7430, 1.6906, 1.8633, 1.7222, 1.6242, 1.4546, 1.9341, 1.9836, 2.0377,\n",
      "        1.9926, 1.8129, 1.7769, 1.5742, 1.6722, 1.4728, 1.6782, 1.9924, 1.2131,\n",
      "        1.5588, 1.6058, 1.1622, 1.1025, 2.0396, 1.7124, 1.6954, 2.1257, 1.4154,\n",
      "        1.4717, 1.8066, 1.9798, 1.2484, 1.7546, 1.8658, 2.1673, 1.8277, 2.0408,\n",
      "        1.7670, 1.8952, 1.7611, 2.2889, 2.4796, 1.8973, 1.9640, 1.8850, 2.0279,\n",
      "        1.7709, 1.7424, 2.4595, 2.2678, 1.7221, 1.6565, 1.8368, 1.9912, 1.1424,\n",
      "        1.9695, 1.6921, 1.8123, 1.8643, 1.6158, 1.5331, 1.0821, 1.9824, 1.1849,\n",
      "        1.7472, 1.8726, 1.7063, 1.6032, 1.7690, 1.7191, 2.2318, 1.1745, 1.7745,\n",
      "        1.8548, 1.7244, 1.8118, 1.5406, 1.2787, 1.1582, 2.0800, 1.7851, 1.7852,\n",
      "        1.9567, 1.6886, 1.6113, 1.3648, 1.2227, 1.1333, 1.9322, 1.8017, 1.7081,\n",
      "        1.6443, 1.8035, 1.5951, 1.6620, 2.1089, 1.7495, 1.8790, 1.8306, 1.6801,\n",
      "        1.6227, 1.4411, 1.1991, 1.2847, 1.8727, 1.8219, 1.5202, 1.5023, 1.6571,\n",
      "        1.6829, 2.0761, 2.1727, 1.7920, 1.7935], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0365,  0.0170,  0.0517,  ...,  0.0296, -0.0499,  0.0468],\n",
      "        [ 0.0041, -0.0661,  0.0270,  ..., -0.0088, -0.0373, -0.0141],\n",
      "        [-0.0431,  0.0020, -0.0050,  ..., -0.0213, -0.0488,  0.0181],\n",
      "        ...,\n",
      "        [ 0.0371, -0.0167, -0.0602,  ...,  0.0499, -0.0259,  0.0305],\n",
      "        [ 0.0332,  0.0362,  0.0341,  ...,  0.0393, -0.0085,  0.0491],\n",
      "        [-0.0145,  0.0207,  0.0197,  ...,  0.0223, -0.0313,  0.0073]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0009,  0.0014,  0.0013,  ...,  0.0022, -0.0026,  0.0018],\n",
      "        [ 0.0043,  0.0050, -0.0057,  ..., -0.0031,  0.0030,  0.0020],\n",
      "        [ 0.0022, -0.0014, -0.0009,  ..., -0.0037,  0.0037, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0007,  0.0049,  0.0013,  ..., -0.0015, -0.0001,  0.0031],\n",
      "        [ 0.0053,  0.0018, -0.0070,  ..., -0.0055,  0.0056, -0.0062],\n",
      "        [-0.0025, -0.0031,  0.0016,  ...,  0.0012, -0.0009, -0.0074]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0690, 2.1805, 2.1064, 2.1968, 2.1148, 2.0781, 2.2111, 2.2303, 2.1953,\n",
      "        2.2271, 2.1345, 2.0434, 2.2094, 2.3120, 2.1038, 2.0597, 1.9077, 2.0418,\n",
      "        1.9640, 1.9909, 1.9751, 2.0465, 2.0049, 1.9823, 2.0129, 1.9297, 1.9487,\n",
      "        1.9861, 1.9161, 1.9476, 2.0084, 1.9176, 2.3021, 2.0881, 2.2089, 2.0301,\n",
      "        2.2356, 2.1912, 2.2734, 2.1094, 2.1587, 2.1568, 2.1715, 2.3178, 2.2220,\n",
      "        2.2550, 2.2032, 2.2023, 2.1346, 2.0642, 2.0549, 2.0358, 2.1155, 2.1078,\n",
      "        2.0502, 2.1140, 2.0746, 2.0977, 2.0083, 2.1275, 2.1354, 2.2017, 2.1002,\n",
      "        2.0974, 2.0317, 2.1638, 2.0417, 2.1552, 2.2488, 2.1295, 2.0652, 2.0989,\n",
      "        2.0650, 2.1309, 2.1299, 2.0950, 2.0977, 2.0721, 2.0711, 2.0670, 2.1596,\n",
      "        2.1523, 2.1397, 2.1851, 1.9884, 2.1221, 2.0306, 1.9780, 2.2299, 2.0810,\n",
      "        2.0530, 1.9095, 2.1388, 1.9889, 2.2374, 2.0446, 2.2720, 2.0637, 1.9591,\n",
      "        2.0815, 2.1667, 2.0565, 2.0377, 2.0708, 2.1183, 2.1146, 2.0687, 2.1250,\n",
      "        2.1187, 2.0457, 2.1694, 2.1048, 2.1691, 2.1279, 2.1808, 2.1612, 2.1523,\n",
      "        2.1251, 2.0289, 2.1196, 2.2645, 2.1825, 2.1937, 2.1677, 2.0967, 2.1340,\n",
      "        2.0640, 2.1805, 2.2778, 2.3396, 2.3912, 2.2091, 2.4193, 2.3456, 2.2938,\n",
      "        2.3410, 2.2188, 2.4043, 2.1221, 2.3714, 2.2484, 2.4039, 2.2215, 2.4013,\n",
      "        1.8934, 2.0990, 2.1666, 2.2665, 1.9122, 2.0803, 2.1170, 2.0539, 1.9435,\n",
      "        2.1336, 2.0752, 2.0727, 2.1337, 2.1609, 1.9861, 2.1800, 1.9576, 2.0756,\n",
      "        2.1382, 2.1915, 2.0630, 1.9736, 2.0681, 2.0489, 2.0983, 2.0747, 2.0699,\n",
      "        2.0243, 2.1848, 1.9970, 2.0555, 2.1470, 2.0819, 2.1529, 2.1790, 2.1989,\n",
      "        2.2926, 2.0924, 2.1828, 2.2258, 2.0958, 2.3159, 2.1153, 2.1176, 2.3028,\n",
      "        2.1191, 2.0850, 2.1687, 1.9733, 1.9516, 2.0247, 2.0744, 2.0775, 2.1368,\n",
      "        2.0309, 2.0059, 2.0959, 1.9675, 2.1743, 2.0268, 2.0992, 2.0069, 2.0272,\n",
      "        1.9856, 2.1822, 2.0036, 2.1323, 2.0709, 2.1564, 2.1784, 2.0548, 2.1251,\n",
      "        2.1659, 2.3199, 2.2956, 2.1816, 2.1452, 2.1379, 2.1985, 2.2161, 1.9607,\n",
      "        1.9714, 2.0560, 1.9254, 1.9312, 2.0330, 2.0099, 1.8535, 2.0440, 1.9928,\n",
      "        2.0556, 1.9512, 1.9183, 2.0444, 2.0478, 1.9960, 2.2065, 2.1725, 2.0962,\n",
      "        2.3075, 2.1297, 2.3074, 2.0871, 1.7292, 2.1272, 2.3348, 2.0649, 2.1916,\n",
      "        2.1972, 2.1352, 2.2613, 2.0995, 2.1636, 2.1344, 2.1178, 2.0480, 2.0030,\n",
      "        2.1338, 2.0747, 2.0401, 2.0046, 2.1411, 1.9584, 2.0768, 2.1596, 2.0150,\n",
      "        2.0904, 2.1282, 1.9783, 2.0292, 2.0898, 2.0220, 2.0934, 2.1385, 2.2032,\n",
      "        2.1138, 2.1465, 2.0921, 2.0736, 2.0808, 2.0916, 1.9313, 2.0050, 2.1312,\n",
      "        2.0721, 2.0843, 2.1021, 2.1772, 2.0855, 2.1520, 2.1755, 2.0425, 2.0776,\n",
      "        2.0226, 2.1000, 2.0280, 2.0207, 2.1385, 1.9169, 2.0623, 1.9336, 1.7639,\n",
      "        1.8360, 1.7983, 1.7785, 1.9543, 1.7779, 1.8262, 1.8061, 1.8331, 1.8870,\n",
      "        1.7921, 1.8361, 1.8723, 1.7859, 1.7871], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0445, -0.0139, -0.0226,  ..., -0.0067, -0.0299, -0.0543],\n",
      "        [-0.0629, -0.0302,  0.0221,  ...,  0.0028,  0.0421, -0.0043],\n",
      "        [ 0.0376, -0.0438,  0.0084,  ..., -0.0286,  0.0046,  0.0512],\n",
      "        ...,\n",
      "        [-0.0509,  0.0557, -0.0410,  ..., -0.0253, -0.0268, -0.0222],\n",
      "        [-0.0347,  0.0400,  0.0168,  ...,  0.0212,  0.0518, -0.0284],\n",
      "        [-0.0307,  0.0488, -0.0416,  ..., -0.0221,  0.0155,  0.0282]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0116,  0.0071,  0.0063,  ...,  0.0070,  0.0011,  0.0122],\n",
      "        [-0.0066,  0.0046, -0.0026,  ...,  0.0021, -0.0016,  0.0035],\n",
      "        [ 0.0106, -0.0094, -0.0008,  ..., -0.0066, -0.0075, -0.0097],\n",
      "        ...,\n",
      "        [-0.0118,  0.0112,  0.0068,  ...,  0.0114,  0.0050,  0.0182],\n",
      "        [-0.0126,  0.0079,  0.0052,  ...,  0.0136,  0.0023,  0.0139],\n",
      "        [ 0.0081, -0.0109, -0.0025,  ..., -0.0060, -0.0083, -0.0041]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9915, 1.9685, 1.9719, 2.1403, 2.1753, 1.9527, 2.0714, 2.0038, 2.0477,\n",
      "        1.9532, 1.8231, 1.9680, 1.9493, 1.7907, 2.1472, 2.1172, 1.9321, 1.9931,\n",
      "        1.8297, 1.9081, 2.1902, 1.9498, 1.9356, 2.0226, 1.8427, 1.9276, 1.8430,\n",
      "        2.0361, 2.2028, 2.0867, 1.9442, 1.6365, 2.1565, 1.8766, 1.9391, 1.8225,\n",
      "        1.8976, 1.8823, 1.8782, 2.1606, 1.9334, 2.0300, 2.0379, 2.0179, 2.0318,\n",
      "        2.0337, 2.9432, 1.8862, 1.9594, 1.9888, 2.0109, 1.8992, 1.9445, 1.8399,\n",
      "        1.9966, 1.9082, 2.1574, 2.1459, 2.3493, 2.0480, 1.8882, 1.9530, 4.8682,\n",
      "        1.8900, 2.0767, 2.0952, 2.1589, 2.0277, 1.8864, 2.0190, 1.9502, 1.9295,\n",
      "        1.8711, 2.2099, 1.9348, 2.1406, 1.9669, 1.9755, 1.9229, 1.7224, 1.8257,\n",
      "        1.9633, 1.8847, 2.1791, 1.8787, 1.8544, 1.7318, 2.0022, 1.7176, 2.0500,\n",
      "        2.1913, 2.0019, 2.2498, 1.9693, 1.9504, 2.0874, 1.9466, 2.1472, 2.1270,\n",
      "        2.1668, 2.0539, 2.2033, 1.9584, 1.9908, 1.9507, 2.0277, 2.0868, 2.0718,\n",
      "        1.8338, 1.7858, 2.1958, 2.1286, 2.0681, 1.9898, 2.1716, 1.9337, 2.0625,\n",
      "        1.8895, 1.9113, 2.2135, 2.0666, 1.9714, 2.1793, 2.0688, 1.8044, 2.1061,\n",
      "        1.8941, 1.9808, 1.9422, 1.9078, 1.8277, 2.1136, 2.1403, 2.0088, 2.2153,\n",
      "        1.9440, 1.8636, 1.9831, 1.9206, 1.9982, 1.9701, 1.8953, 2.3225, 2.1032,\n",
      "        1.9491, 1.9746, 1.9553, 2.0701, 1.9663, 1.8690, 2.1643, 2.0445, 1.9802,\n",
      "        1.9267, 2.1161, 1.9096, 1.8358, 1.9708, 2.2112, 1.9233, 2.0229, 2.0114,\n",
      "        2.1080, 1.9402, 2.0337, 2.1326, 2.1105, 2.0461, 2.0604, 1.9666, 1.9444,\n",
      "        1.7385, 2.0793, 2.0200, 1.8291, 1.8264, 1.8047, 2.0390, 2.3213, 1.9590,\n",
      "        1.8135, 1.9590, 2.0468, 1.9939, 2.0205, 2.0124, 1.9682, 1.9819, 2.0212,\n",
      "        1.9519, 2.0078, 1.8250, 2.7505, 2.0933, 1.9552, 2.0234, 1.9260, 1.8771,\n",
      "        2.0284, 1.9758, 1.9591, 2.0273, 1.9578, 2.1266, 1.9849, 2.0637, 2.0253,\n",
      "        1.7445, 1.9791, 1.9331, 1.8701, 2.0905, 2.0792, 2.1362, 2.2142, 2.0385,\n",
      "        1.8916, 2.0889, 1.6686, 1.8928, 1.8706, 2.0120, 1.9414, 2.1564, 2.1632,\n",
      "        2.0063, 2.0925, 2.0129, 1.9421, 2.0630, 1.8978, 1.7635, 2.0421, 1.8875,\n",
      "        2.0196, 2.0516, 1.8736, 1.8852, 2.0913, 2.0103, 1.9028, 1.8574, 2.0499,\n",
      "        2.0554, 2.0055, 1.8275, 2.0298, 2.0234, 1.9410, 1.9704, 1.9441, 1.9139,\n",
      "        2.0733, 1.7836, 1.9134, 1.9635, 1.9774, 1.7646, 1.7741, 2.0560, 2.2150,\n",
      "        2.1427, 2.0733, 1.9695, 1.9740, 2.0228, 2.0150, 2.5433, 2.0170, 1.9095,\n",
      "        1.8927, 2.1833, 1.8626, 1.9876, 1.9868, 1.9353, 2.1967, 1.8471, 1.9804,\n",
      "        2.0861, 1.9346, 1.9169, 2.0455, 2.0041, 2.0351, 2.0227, 2.0471, 1.8571,\n",
      "        1.9320, 1.8822, 1.8851, 2.1533, 1.9668, 2.1274, 1.7502, 2.0463, 2.1030,\n",
      "        1.9521, 1.8366, 1.8980, 2.0755, 2.0198, 2.1994, 1.9704, 2.0956, 1.9587,\n",
      "        2.0597, 1.7287, 1.8529, 2.1120, 2.0633, 1.8126, 1.8013, 2.2019, 2.1234,\n",
      "        1.9546, 2.0300, 1.9680, 2.1918, 1.9377], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0470,  0.0280,  0.0553,  ...,  0.0113, -0.0278, -0.0614],\n",
      "        [ 0.0095, -0.0056, -0.0575,  ..., -0.0605,  0.0136, -0.0188],\n",
      "        [ 0.0541,  0.0064,  0.0034,  ...,  0.0189,  0.0183, -0.0497],\n",
      "        ...,\n",
      "        [-0.0088, -0.0185, -0.0105,  ..., -0.0293, -0.0391, -0.0324],\n",
      "        [-0.0660, -0.0566, -0.0274,  ..., -0.0642, -0.0272,  0.0311],\n",
      "        [ 0.0204,  0.0440, -0.0111,  ..., -0.0313, -0.0074, -0.0552]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 5.6484e-03, -7.0201e-03,  6.2657e-03,  ..., -5.5984e-03,\n",
      "         -2.9503e-03,  1.5080e-03],\n",
      "        [-8.8363e-03,  1.0644e-02, -1.0035e-02,  ..., -1.6149e-03,\n",
      "          8.8471e-03, -5.0968e-03],\n",
      "        [ 7.7779e-03, -3.2601e-03,  7.8177e-03,  ...,  3.6784e-03,\n",
      "         -2.3366e-03,  6.2290e-03],\n",
      "        ...,\n",
      "        [-4.8688e-03,  5.9144e-03, -5.5879e-03,  ..., -6.5750e-03,\n",
      "          6.7749e-03, -4.4070e-03],\n",
      "        [ 1.2321e-03, -7.5860e-05, -1.3469e-03,  ...,  6.6812e-03,\n",
      "         -3.0520e-03,  3.9836e-03],\n",
      "        [ 2.6151e-03, -2.4448e-03, -1.5836e-03,  ..., -3.1970e-04,\n",
      "          1.8648e-03, -2.0074e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.1102, 1.8813, 2.1273,  ..., 2.6309, 1.8489, 1.9418],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-9.6311e-03,  1.9730e-03, -1.4534e-02,  ...,  1.4764e-02,\n",
      "          2.5682e-02,  2.9105e-02],\n",
      "        [ 4.8982e-03,  1.5203e-02,  8.8068e-03,  ...,  2.5654e-02,\n",
      "         -1.5699e-02,  1.9582e-02],\n",
      "        [-9.1986e-03,  9.9259e-03, -1.6470e-04,  ...,  9.8801e-05,\n",
      "         -7.3734e-03,  9.3270e-03],\n",
      "        ...,\n",
      "        [ 3.2326e-02, -1.0499e-02, -6.6166e-03,  ..., -1.0887e-02,\n",
      "          2.2918e-02,  2.1827e-02],\n",
      "        [-3.6465e-03, -2.1746e-02,  2.0695e-03,  ...,  2.2041e-02,\n",
      "         -1.1147e-02,  1.8934e-02],\n",
      "        [-9.5995e-03,  5.3186e-03,  1.1882e-02,  ...,  3.5943e-03,\n",
      "          3.1965e-02, -1.7919e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0054, -0.0049,  0.0045,  ..., -0.0132, -0.0011, -0.0098],\n",
      "        [-0.0034, -0.0082,  0.0067,  ..., -0.0049, -0.0004, -0.0053],\n",
      "        [ 0.0018,  0.0122, -0.0081,  ..., -0.0024,  0.0044, -0.0005],\n",
      "        ...,\n",
      "        [-0.0073, -0.0056,  0.0118,  ..., -0.0125, -0.0047, -0.0123],\n",
      "        [-0.0027, -0.0086,  0.0103,  ..., -0.0032, -0.0024, -0.0048],\n",
      "        [ 0.0024,  0.0071,  0.0014,  ..., -0.0042,  0.0024, -0.0051]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([4.3312, 4.4911, 4.1938, 4.4675, 4.8569, 4.3161, 4.4351, 4.2079, 4.5091,\n",
      "        4.4141, 4.1945, 4.3005, 4.4493, 4.0304, 4.5871, 4.5183, 4.1671, 4.3010,\n",
      "        4.0641, 4.4301, 4.6046, 4.6809, 4.1731, 4.3249, 4.1860, 4.2245, 4.1801,\n",
      "        4.3724, 4.5105, 4.6351, 4.2074, 3.8934, 4.7033, 4.2844, 4.3029, 4.2386,\n",
      "        3.9611, 4.2200, 4.2711, 4.8985, 4.0887, 4.3936, 4.3311, 4.2812, 4.6532,\n",
      "        4.3478, 5.5443, 4.1367, 4.4138, 4.5969, 4.6353, 4.4660, 4.1075, 3.9902,\n",
      "        4.3118, 4.3595, 4.4997, 4.3686, 5.4311, 4.3616, 4.5547, 4.3695, 9.4384,\n",
      "        4.3407, 4.5399, 4.3194, 4.4697, 4.0601, 4.6916, 4.5906, 4.0639, 4.1267,\n",
      "        4.0345, 4.6714, 4.3016, 4.2984, 4.2107, 4.3869, 4.5192, 3.8664, 4.2099,\n",
      "        4.0787, 4.2708, 4.8576, 4.3422, 4.2252, 4.0897, 4.4478, 3.9402, 4.4097,\n",
      "        4.5369, 4.0697, 5.1644, 4.4484, 4.4048, 4.7324, 4.3111, 4.4098, 4.3486,\n",
      "        4.8141, 4.5436, 4.6336, 4.3789, 4.4617, 4.4388, 4.3627, 4.1676, 4.3757,\n",
      "        4.3654, 4.1124, 4.6647, 4.3354, 4.2134, 4.2438, 4.2629, 4.0902, 4.6053,\n",
      "        4.2911, 4.6091, 4.7469, 4.4042, 4.2267, 4.4070, 4.3910, 4.2978, 4.7997,\n",
      "        4.3369, 4.2969, 4.3539, 3.8740, 4.1720, 4.6106, 4.4775, 4.5770, 4.6600,\n",
      "        4.3767, 4.1778, 4.5112, 4.5290, 4.7880, 3.9754, 4.1486, 4.5178, 4.3863,\n",
      "        4.6332, 4.6279, 4.5735, 4.3552, 4.2201, 4.3037, 5.0287, 4.3489, 4.3437,\n",
      "        4.2777, 4.4352, 4.5041, 4.3064, 4.4841, 4.9386, 4.5881, 4.3039, 4.5939,\n",
      "        4.4756, 4.2259, 4.2796, 4.4386, 4.2290, 4.4109, 4.3745, 4.4319, 4.1088,\n",
      "        4.3296, 4.3222, 4.4153, 4.4542, 4.1076, 4.1517, 4.4344, 4.7280, 4.2254,\n",
      "        4.5405, 4.0643, 4.6493, 4.3563, 4.3425, 4.6946, 4.3394, 4.5339, 4.3411,\n",
      "        4.3251, 4.2995, 4.0727, 6.6729, 4.3111, 4.3629, 4.2071, 4.5277, 4.2069,\n",
      "        4.5999, 4.6777, 4.4207, 4.4513, 4.6533, 4.2642, 4.5753, 4.5501, 4.1977,\n",
      "        3.8299, 4.4926, 4.1379, 4.4686, 4.4384, 4.4913, 4.4752, 4.7136, 4.4349,\n",
      "        4.2397, 4.6406, 3.8017, 4.1692, 4.3153, 4.1820, 4.4083, 4.8045, 4.7847,\n",
      "        4.4114, 4.5193, 4.2947, 4.2473, 4.3078, 4.2100, 3.9728, 4.4502, 4.3500,\n",
      "        4.1779, 4.6275, 4.3073, 4.1315, 4.3458, 4.4145, 4.0933, 4.3194, 4.1119,\n",
      "        4.1815, 4.2131, 4.1913, 5.0013, 4.1994, 4.6318, 4.4156, 4.3818, 4.5356,\n",
      "        4.3123, 4.1228, 4.3293, 4.4141, 4.5315, 4.0016, 3.9877, 4.6790, 4.2450,\n",
      "        4.5723, 4.6456, 4.3273, 4.2552, 4.5232, 4.7093, 6.7653, 4.1049, 4.0443,\n",
      "        4.4817, 4.0789, 4.3531, 4.1972, 4.2246, 4.3797, 4.5792, 4.3238, 4.4041,\n",
      "        4.3927, 4.0711, 4.3065, 4.3368, 4.6594, 4.3232, 4.2889, 4.5912, 4.2055,\n",
      "        4.0541, 4.3896, 4.6002, 4.6044, 4.1594, 4.6670, 4.1123, 4.3837, 4.4614,\n",
      "        4.1258, 4.2027, 4.2231, 4.2959, 4.1842, 4.4180, 4.3540, 4.5883, 4.3377,\n",
      "        4.7699, 4.0724, 4.1664, 4.3931, 4.2129, 4.2392, 3.9842, 4.3860, 4.3019,\n",
      "        4.2550, 4.4255, 4.3832, 4.5811, 4.3097], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0040, -0.0400,  0.0440,  ..., -0.0354,  0.0414,  0.0211],\n",
      "        [-0.0105,  0.0143, -0.0105,  ..., -0.0383, -0.0110,  0.0265],\n",
      "        [ 0.0463, -0.0042, -0.0493,  ..., -0.0445,  0.0120, -0.0580],\n",
      "        ...,\n",
      "        [-0.0445,  0.0233,  0.0538,  ..., -0.0160, -0.0365, -0.0230],\n",
      "        [-0.0464, -0.0345, -0.0475,  ..., -0.0281, -0.0447,  0.0076],\n",
      "        [-0.0163, -0.0501, -0.0338,  ..., -0.0185, -0.0613, -0.0415]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-5.0379e-03,  4.7179e-03, -5.4861e-03,  ...,  3.5808e-03,\n",
      "          4.9286e-03,  5.3305e-03],\n",
      "        [-2.8605e-03,  3.6608e-03, -4.8178e-03,  ...,  3.8517e-03,\n",
      "          3.3742e-03,  3.3136e-03],\n",
      "        [-5.4941e-04, -2.2453e-04,  1.1727e-03,  ..., -2.6737e-04,\n",
      "          9.8938e-05,  1.9895e-04],\n",
      "        ...,\n",
      "        [ 3.5150e-03, -2.6430e-03,  6.5286e-05,  ...,  1.6017e-03,\n",
      "         -3.3347e-03, -4.1022e-03],\n",
      "        [-4.5721e-03,  4.1563e-03, -3.5084e-03,  ...,  2.7114e-03,\n",
      "          4.6050e-03,  4.8467e-03],\n",
      "        [-1.5812e-02,  1.5209e-02, -1.4993e-02,  ...,  1.3083e-02,\n",
      "          1.6864e-02,  1.5349e-02]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5264, 1.3322, 1.3691, 1.2994, 1.5740, 1.7071, 1.6216, 1.6124, 1.5894,\n",
      "        1.4219, 1.1341, 1.3566, 1.4217, 1.3101, 1.6178, 1.5713, 1.4984, 1.3502,\n",
      "        1.0186, 1.3311, 1.7003, 1.4909, 1.8946, 1.9725, 1.0796, 1.0305, 1.3189,\n",
      "        1.1719, 1.1130, 1.9578, 2.0739, 1.8629, 1.0689, 1.1823, 1.2028, 1.6870,\n",
      "        2.1561, 1.5533, 1.8085, 2.5763, 2.0205, 1.5049, 1.6655, 1.4735, 1.4898,\n",
      "        2.1919, 1.9483, 1.9762, 1.6558, 1.7454, 1.6999, 1.5561, 1.4431, 1.5149,\n",
      "        1.8744, 1.9660, 1.6975, 1.7634, 1.5798, 1.7498, 1.7274, 1.5489, 1.8746,\n",
      "        1.9387, 1.1767, 1.6737, 1.4370, 1.9061, 2.0107, 2.0937, 2.2817, 2.0181,\n",
      "        1.2039, 1.2512, 1.5229, 2.0872, 2.0639, 1.6344, 2.2574, 2.1140, 1.7939,\n",
      "        1.2919, 1.6478, 1.0757, 1.8529, 1.7200, 1.3029, 1.4445, 1.6270, 1.7332,\n",
      "        1.4039, 1.7902, 1.1491, 1.1824, 1.2893, 1.3195, 1.1454, 0.9932, 1.0768,\n",
      "        1.1325, 1.4885, 1.4812, 1.4499, 1.5958, 1.1527, 0.9397, 1.2806, 1.3822,\n",
      "        1.2558, 1.5463, 1.4141, 1.4549, 1.5750, 1.6693, 1.9294, 1.7007, 1.8065,\n",
      "        1.6664, 2.2356, 2.3306, 1.5041, 1.6977, 1.7663, 1.8276, 1.5928, 1.9898,\n",
      "        2.2294, 2.3325, 2.0241, 1.7007, 1.6829, 1.5099, 1.7030, 2.2771, 1.9600,\n",
      "        1.9498, 1.6451, 1.7233, 1.6487, 1.3708, 1.6928, 1.4034, 1.8723, 1.9080,\n",
      "        1.9135, 1.2767, 1.6988, 1.2249, 1.2832, 1.4067, 1.6093, 2.1303, 1.2906,\n",
      "        1.2664, 1.1704, 1.8383, 1.9701, 2.0086, 1.7269, 1.7962, 1.4639, 1.4094,\n",
      "        1.5983, 1.7461, 1.5393, 2.3302, 1.7915, 1.8295, 1.6463, 1.4398, 1.6927,\n",
      "        1.7062, 1.9013, 1.4905, 1.6655, 1.9770, 0.7673, 1.3519, 1.5142, 1.4586,\n",
      "        2.0805, 2.0070, 2.3841, 1.9876, 1.3955, 1.1177, 1.2259, 1.6606, 1.9128,\n",
      "        2.0591, 2.3340, 1.9332, 1.6635, 1.2019, 1.6207, 1.8853, 1.8616, 1.8711,\n",
      "        1.6489, 1.9403, 1.2415, 1.4285, 1.0682, 1.2076, 1.2400, 1.3823, 1.6548,\n",
      "        1.6106, 0.9770, 1.3335, 1.2992, 1.2693, 1.2408, 1.2194, 1.7642, 1.7796,\n",
      "        1.2144, 1.2090, 1.2945, 1.3845, 1.1726, 1.7543, 1.9150, 1.8010, 1.4407,\n",
      "        1.3100, 1.7254, 1.7172, 1.2715, 1.3156, 1.6929, 1.6268, 1.6548, 1.4706,\n",
      "        1.1021, 1.0992, 1.6583, 1.9272, 1.5642, 1.8781, 0.6995, 1.5608, 1.4544,\n",
      "        1.6603, 2.1848, 1.7559, 2.1349, 2.2517, 1.3528, 1.1983, 1.4606, 1.5392,\n",
      "        2.2124, 1.7769, 2.2678, 2.3871, 1.9310, 1.9870, 1.9327, 1.6299, 1.3922,\n",
      "        2.1230, 1.8092, 2.0190, 1.8515, 1.8179, 1.8106, 1.3281, 1.7536, 1.5297,\n",
      "        2.0371, 1.8864, 1.8175, 1.9069, 1.8948, 1.8607, 1.4900, 2.1567, 2.1045,\n",
      "        2.0863, 1.6795, 1.7379, 1.9814, 1.9804, 1.7434, 1.5776, 2.2154, 2.0109,\n",
      "        1.3045, 1.3176, 1.3986, 1.2890, 1.3493, 1.1409, 2.0049, 2.0529, 1.2402,\n",
      "        1.2701, 1.2576, 1.3792, 1.2625, 1.5750, 1.6123, 1.3892, 0.7870, 0.7964,\n",
      "        0.8366, 1.2122, 1.9422, 2.0880, 2.0296, 2.0130, 1.7664, 1.4060, 1.5221,\n",
      "        1.5413, 1.7700, 2.0653, 1.9738, 1.6750], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0334, -0.0440, -0.0263,  ..., -0.0674,  0.0019,  0.0269],\n",
      "        [-0.0652,  0.0365,  0.0114,  ...,  0.0290, -0.0551,  0.0245],\n",
      "        [ 0.0519, -0.0290,  0.0289,  ...,  0.0227, -0.0330, -0.0400],\n",
      "        ...,\n",
      "        [-0.0178, -0.0023, -0.0109,  ...,  0.0050,  0.0382,  0.0681],\n",
      "        [-0.0086,  0.0445,  0.0011,  ..., -0.0475, -0.0167, -0.0309],\n",
      "        [-0.0601, -0.0505,  0.0285,  ...,  0.0054, -0.0265, -0.0288]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0044, -0.0025,  0.0023,  ..., -0.0041, -0.0052, -0.0013],\n",
      "        [-0.0013, -0.0058,  0.0093,  ..., -0.0045,  0.0003,  0.0036],\n",
      "        [-0.0027, -0.0023,  0.0024,  ...,  0.0062,  0.0007,  0.0066],\n",
      "        ...,\n",
      "        [-0.0029, -0.0017, -0.0099,  ...,  0.0025,  0.0068, -0.0038],\n",
      "        [ 0.0027,  0.0048, -0.0018,  ..., -0.0074,  0.0076,  0.0029],\n",
      "        [-0.0002,  0.0022,  0.0017,  ..., -0.0074,  0.0040,  0.0060]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3953, 1.2999, 1.4070, 1.4471, 1.2199, 1.1657, 1.5666, 1.5769, 1.4618,\n",
      "        1.4148, 1.3096, 1.3256, 1.4241, 1.8668, 1.5530, 1.5302, 1.1096, 1.4896,\n",
      "        1.0687, 1.2160, 1.3869, 1.3762, 1.5782, 1.4396, 1.2999, 1.0517, 1.5198,\n",
      "        1.2520, 1.1933, 1.3424, 1.7652, 1.6463, 1.5366, 1.4974, 1.5538, 1.3118,\n",
      "        1.2842, 2.0818, 1.7389, 1.8666, 1.4804, 1.4544, 1.4455, 1.7073, 2.0753,\n",
      "        1.3193, 1.8702, 1.8507, 1.6592, 1.7415, 1.7658, 1.4928, 1.5287, 1.4575,\n",
      "        1.7345, 1.8600, 1.6080, 1.8420, 1.7803, 1.7160, 1.4418, 2.0542, 1.7394,\n",
      "        1.9196, 1.2078, 1.5798, 1.5878, 1.7252, 1.5827, 1.3221, 1.8507, 1.7460,\n",
      "        1.2190, 1.2624, 1.6506, 1.7566, 1.4003, 1.6291, 1.6826, 1.8971, 1.8025,\n",
      "        1.4692, 1.6009, 1.5679, 0.9525, 1.0081, 1.3878, 1.4720, 1.5910, 1.6910,\n",
      "        1.4565, 1.2673, 1.5792, 1.3784, 1.3531, 1.3031, 1.0246, 1.2510, 1.2420,\n",
      "        1.3105, 1.0516, 1.0976, 1.3898, 1.4808, 1.1984, 1.1348, 1.3466, 1.0740,\n",
      "        1.1366, 1.2954, 1.3655, 1.4490, 1.4693, 1.6005, 1.9474, 1.7910, 1.7274,\n",
      "        1.8165, 1.9756, 1.7758, 1.5548, 1.7226, 1.8010, 1.8195, 1.7563, 1.6163,\n",
      "        2.0087, 1.8944, 1.9245, 1.7433, 1.8092, 1.6186, 1.2533, 1.1342, 1.8320,\n",
      "        1.8362, 1.7156, 1.8479, 1.7774, 1.5038, 1.4321, 2.3204, 1.7516, 1.7673,\n",
      "        1.8125, 1.4629, 1.3221, 1.8485, 1.8290, 1.9440, 1.5437, 1.7125, 1.4686,\n",
      "        1.4848, 1.5748, 1.1300, 1.0469, 1.1878, 1.7042, 1.6831, 1.5906, 1.5397,\n",
      "        1.4372, 1.3916, 1.6661, 1.1764, 1.6637, 1.7152, 1.5200, 1.6228, 1.4837,\n",
      "        1.4285, 1.1874, 1.9746, 1.7098, 1.7022, 1.5651, 1.0723, 1.3185, 1.4823,\n",
      "        1.6206, 1.8888, 1.9532, 1.8460, 0.8338, 1.5036, 1.3462, 1.3641, 1.7228,\n",
      "        1.8107, 1.9350, 1.8167, 1.7776, 1.4232, 1.3868, 1.1352, 1.0344, 1.1609,\n",
      "        1.6023, 1.7566, 1.4021, 1.5129, 1.5327, 1.5912, 1.9726, 1.8396, 1.6602,\n",
      "        1.5310, 0.8824, 1.0038, 1.0188, 1.3068, 1.1603, 1.2995, 1.4820, 1.4510,\n",
      "        1.0494, 1.1633, 1.3936, 1.1914, 1.1941, 1.2318, 1.7633, 1.3454, 1.5607,\n",
      "        1.4244, 1.6214, 1.0646, 1.8665, 1.9175, 1.6339, 1.6019, 1.7819, 1.5967,\n",
      "        1.3212, 1.7149, 0.9985, 1.0730, 1.5422, 1.6410, 1.4993, 1.5465, 1.5559,\n",
      "        1.6668, 1.6309, 1.7610, 1.9288, 1.9313, 1.1203, 0.9583, 1.1522, 1.2512,\n",
      "        1.8527, 1.7193, 2.0614, 1.9530, 1.9161, 1.9200, 1.9724, 1.7094, 1.9502,\n",
      "        1.2408, 1.6251, 1.7598, 1.7784, 1.8894, 1.8177, 1.5900, 1.4693, 2.1830,\n",
      "        1.9091, 1.7087, 1.7700, 1.8955, 1.9271, 1.9076, 1.6145, 1.4685, 1.9596,\n",
      "        1.8644, 1.6744, 1.7082, 1.8654, 1.8632, 1.5796, 2.1356, 2.0599, 1.9832,\n",
      "        0.9895, 0.9289, 1.1902, 1.1324, 1.3236, 1.2623, 1.6768, 1.4619, 0.8577,\n",
      "        1.0234, 1.2677, 1.3422, 1.0936, 1.1832, 1.4750, 1.4557, 1.1718, 1.5170,\n",
      "        1.6340, 1.6197, 1.6862, 1.8008, 1.8727, 1.8621, 1.4649, 1.1385, 1.0341,\n",
      "        1.5120, 1.7269, 1.8613, 1.8730, 1.7504], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0222, -0.0189, -0.0231,  ..., -0.0470,  0.0062,  0.0128],\n",
      "        [ 0.0275,  0.0032,  0.0002,  ...,  0.0442,  0.0477, -0.0211],\n",
      "        [-0.0363,  0.0055,  0.0319,  ..., -0.0365,  0.0019, -0.0409],\n",
      "        ...,\n",
      "        [ 0.0446,  0.0016, -0.0005,  ...,  0.0190, -0.0207,  0.0192],\n",
      "        [ 0.0667,  0.0285, -0.0019,  ...,  0.0602,  0.0354, -0.0335],\n",
      "        [ 0.0093, -0.0456,  0.0526,  ...,  0.0452, -0.0173,  0.0250]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0004, -0.0026,  0.0033,  ...,  0.0080,  0.0041, -0.0027],\n",
      "        [-0.0006,  0.0010, -0.0047,  ..., -0.0072, -0.0042,  0.0032],\n",
      "        [-0.0020, -0.0035,  0.0048,  ...,  0.0082,  0.0054, -0.0042],\n",
      "        ...,\n",
      "        [ 0.0155,  0.0088, -0.0069,  ..., -0.0071, -0.0068,  0.0094],\n",
      "        [-0.0115,  0.0004, -0.0030,  ..., -0.0016, -0.0028, -0.0002],\n",
      "        [-0.0086, -0.0097,  0.0084,  ...,  0.0088,  0.0088, -0.0098]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0690, 2.0830, 2.0637, 2.0709, 2.3413, 2.2433, 2.0851, 1.9707, 2.1058,\n",
      "        2.1583, 2.1751, 2.0327, 2.1054, 2.1630, 2.0346, 2.1524, 2.2866, 2.2326,\n",
      "        2.2250, 2.3661, 2.2507, 2.1171, 2.3057, 2.4056, 2.3103, 2.2783, 2.1835,\n",
      "        2.1110, 2.2788, 2.2091, 2.2831, 2.2410, 2.0496, 1.9383, 2.1280, 2.0765,\n",
      "        1.9752, 2.0236, 2.0876, 2.0746, 2.0640, 2.0579, 2.0097, 2.0048, 2.0473,\n",
      "        2.0210, 2.0258, 1.9680, 2.2092, 2.2582, 2.2463, 2.3268, 1.8867, 2.1631,\n",
      "        2.2229, 2.0421, 2.0282, 2.1751, 2.2649, 2.2352, 2.2946, 2.0112, 2.3065,\n",
      "        2.2171, 2.2473, 2.3380, 2.0965, 2.1424, 2.4347, 2.1980, 2.2315, 2.1895,\n",
      "        2.2316, 2.2459, 2.2366, 2.1484, 2.2289, 2.1322, 2.1628, 2.2505, 2.1123,\n",
      "        2.1281, 2.1952, 2.1328, 2.2939, 2.1233, 2.1087, 2.1092, 2.2902, 2.1044,\n",
      "        2.2139, 2.1883, 2.1215, 2.1964, 2.0728, 2.0896, 2.3282, 2.0195, 2.2356,\n",
      "        2.0898, 2.1865, 2.1176, 2.0579, 2.1094, 2.1656, 2.1508, 2.2444, 2.2056,\n",
      "        2.3938, 2.1898, 2.1451, 2.1792, 2.1650, 2.2258, 2.0862, 2.1367, 2.0370,\n",
      "        2.1886, 2.2243, 2.2662, 2.5237, 2.1113, 2.1935, 2.2419, 2.0118, 2.1930,\n",
      "        2.2413, 2.2954, 2.1793, 2.0786, 2.2123, 2.2187, 2.2543, 2.2982, 2.0754,\n",
      "        2.2202, 2.1381, 2.2469, 2.1165, 2.3123, 2.2456, 2.2251, 2.1358, 2.1344,\n",
      "        2.0640, 2.1107, 1.8164, 1.9497, 2.0397, 2.1043, 2.1305, 1.9586, 2.2636,\n",
      "        2.1760, 2.0619, 2.1083, 2.0075, 2.2071, 2.0732, 2.0171, 2.1651, 2.0782,\n",
      "        2.0464, 2.0997, 2.0774, 2.1234, 2.1322, 2.0828, 2.0643, 1.9991, 2.1272,\n",
      "        2.1212, 2.0676, 1.9549, 2.0256, 2.1326, 1.9793, 1.8310, 2.0169, 1.9620,\n",
      "        1.8471, 1.8997, 1.9454, 1.9398, 2.0133, 1.9978, 1.9418, 1.9086, 1.9241,\n",
      "        1.9705, 1.9479, 1.8556, 2.0778, 1.9217, 2.0176, 2.2051, 2.1756, 2.0090,\n",
      "        2.0320, 1.9759, 2.1804, 2.2158, 2.0024, 2.1187, 2.1886, 2.0139, 1.8604,\n",
      "        2.1888, 2.5980, 2.5602, 2.5321, 2.5142, 2.5452, 2.9650, 2.5337, 2.2834,\n",
      "        2.1162, 2.3587, 2.2819, 2.2944, 2.2341, 2.4700, 2.3702, 2.3395, 2.1349,\n",
      "        2.1516, 2.2368, 2.1792, 2.1105, 1.9427, 2.1925, 2.1454, 2.2558, 2.0827,\n",
      "        2.1482, 2.1635, 2.0834, 2.1733, 2.1039, 1.9265, 2.0097, 2.0291, 1.8808,\n",
      "        1.9422, 1.9698, 1.9524, 1.8951, 1.9907, 1.9289, 1.9247, 1.9782, 1.8931,\n",
      "        1.9060, 1.8674, 1.9873, 1.9648, 2.2740, 2.2034, 1.9953, 2.3166, 2.2142,\n",
      "        2.1034, 2.0774, 2.2107, 2.2083, 2.2258, 2.1128, 2.1928, 2.2349, 2.1139,\n",
      "        2.1910, 2.1016, 2.2075, 2.1705, 2.1824, 2.2879, 2.1498, 2.2071, 2.2085,\n",
      "        2.0860, 2.2530, 2.1820, 2.0948, 2.0393, 2.0170, 2.3078, 2.1236, 2.2138,\n",
      "        3.5402, 2.7024, 2.6260, 3.2144, 2.3731, 2.8214, 2.5147, 2.6994, 3.5802,\n",
      "        2.5271, 2.7213, 2.6830, 3.0147, 2.7982, 2.8036, 2.7479, 2.0899, 1.9811,\n",
      "        1.8617, 1.8791, 1.9183, 1.9061, 1.9263, 2.1311, 1.9661, 1.9705, 1.9255,\n",
      "        1.9107, 2.0881, 1.9592, 1.9542, 1.9507], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0439,  0.0234,  0.0248,  ..., -0.0015,  0.0193,  0.0307],\n",
      "        [-0.0401,  0.0218,  0.0387,  ...,  0.0488, -0.0378,  0.0374],\n",
      "        [-0.0283, -0.0426,  0.0255,  ...,  0.0323, -0.0223,  0.0508],\n",
      "        ...,\n",
      "        [-0.0462, -0.0081, -0.0515,  ...,  0.0259, -0.0140, -0.0042],\n",
      "        [ 0.0280,  0.0001,  0.0579,  ...,  0.0200,  0.0055,  0.0386],\n",
      "        [-0.0014,  0.0322,  0.0029,  ..., -0.0264, -0.0086, -0.0181]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0048,  0.0168,  0.0135,  ...,  0.0116, -0.0101, -0.0136],\n",
      "        [ 0.0021,  0.0166,  0.0104,  ...,  0.0067, -0.0078, -0.0101],\n",
      "        [ 0.0027, -0.0210, -0.0154,  ..., -0.0120,  0.0144,  0.0161],\n",
      "        ...,\n",
      "        [-0.0042,  0.0127,  0.0104,  ...,  0.0062, -0.0112, -0.0134],\n",
      "        [-0.0032,  0.0199,  0.0138,  ...,  0.0110, -0.0152, -0.0164],\n",
      "        [-0.0019, -0.0094, -0.0052,  ..., -0.0058,  0.0035,  0.0050]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2018, 2.0804, 2.1913, 2.2204, 2.5573, 2.2728, 2.1233, 2.3171, 2.3052,\n",
      "        2.1673, 2.1142, 2.4550, 2.5231, 2.0475, 2.3801, 2.1883, 2.2555, 2.1527,\n",
      "        2.3545, 2.0873, 2.4667, 2.4327, 2.0413, 2.1782, 2.1604, 1.9934, 2.2279,\n",
      "        2.1655, 2.4618, 2.2233, 2.0820, 2.1009, 2.4746, 2.1866, 2.1105, 2.0791,\n",
      "        2.0951, 2.2533, 2.3436, 2.1536, 2.0919, 2.3624, 2.3400, 2.1936, 2.3101,\n",
      "        2.1442, 3.1415, 1.9672, 2.2683, 2.5400, 2.2166, 2.2161, 2.0839, 2.3519,\n",
      "        2.0711, 2.1886, 2.2045, 2.3844, 2.6120, 2.3769, 2.2998, 2.0807, 6.8993,\n",
      "        2.1816, 2.2103, 2.1607, 2.4033, 2.6319, 2.3358, 2.1331, 2.2288, 2.1616,\n",
      "        2.0134, 2.3026, 2.1539, 2.3674, 2.2155, 2.4434, 1.9167, 1.9712, 2.0236,\n",
      "        2.1095, 2.3131, 2.2591, 2.0070, 2.1483, 2.4972, 2.4189, 1.9583, 2.2164,\n",
      "        2.1729, 2.0351, 2.4252, 2.2448, 2.2269, 2.3210, 2.1240, 2.3079, 2.1761,\n",
      "        2.3060, 2.3603, 2.2495, 2.4463, 2.4045, 2.3113, 2.3804, 2.1273, 2.2938,\n",
      "        1.9950, 2.3183, 2.2650, 2.2980, 2.2136, 2.1552, 2.4566, 2.1332, 2.2385,\n",
      "        2.3283, 2.4687, 2.5004, 2.4330, 2.2538, 2.2806, 2.2314, 2.0010, 2.5415,\n",
      "        2.1019, 2.1583, 2.2553, 2.3592, 2.1444, 2.0178, 2.1740, 2.1866, 2.4031,\n",
      "        2.1334, 2.2227, 2.4532, 2.1705, 2.3389, 2.1139, 2.0818, 2.2396, 2.3689,\n",
      "        2.3575, 2.3007, 2.1385, 2.3433, 2.1654, 2.2835, 2.4311, 2.2611, 2.3751,\n",
      "        2.1315, 2.2429, 2.3198, 2.0787, 2.3529, 2.2775, 2.1431, 2.0862, 2.2000,\n",
      "        2.3256, 2.3830, 2.3424, 2.3570, 2.2892, 2.1267, 2.3060, 2.1622, 2.2842,\n",
      "        2.3179, 2.2644, 2.1974, 2.2346, 2.0765, 2.2795, 2.2639, 2.1532, 1.9746,\n",
      "        2.0354, 2.1052, 2.1874, 2.1927, 2.3071, 2.1746, 2.3045, 2.2398, 1.9154,\n",
      "        2.2193, 2.1428, 2.0784, 3.3515, 2.1763, 2.2269, 2.2687, 2.4384, 2.2491,\n",
      "        2.3720, 2.3062, 2.2962, 2.3378, 2.2810, 2.2685, 2.2581, 2.3504, 1.9538,\n",
      "        2.2856, 2.6062, 2.1887, 2.1688, 2.3603, 2.4008, 2.3129, 2.3193, 2.1595,\n",
      "        2.1807, 2.0322, 2.0128, 2.1410, 2.0765, 2.0952, 2.3977, 2.4500, 2.3890,\n",
      "        2.2321, 2.3504, 2.1210, 1.9472, 2.3969, 2.1296, 2.1147, 2.0343, 2.3132,\n",
      "        2.0194, 2.1505, 2.2649, 2.2503, 2.0756, 2.2617, 2.1189, 2.1064, 2.3333,\n",
      "        2.0919, 2.2688, 2.0230, 2.3632, 1.9952, 2.0503, 2.4199, 2.2948, 2.1618,\n",
      "        2.0532, 1.9760, 2.2526, 2.2199, 2.3429, 2.0341, 2.3262, 2.2758, 2.0901,\n",
      "        2.1760, 2.2265, 2.1629, 2.1590, 2.2058, 2.3041, 2.8020, 2.3384, 2.0961,\n",
      "        1.9485, 2.1986, 2.0600, 2.4568, 2.4454, 2.3414, 2.1430, 2.1110, 2.2870,\n",
      "        2.1961, 2.2259, 2.1103, 2.2665, 2.5729, 2.1706, 2.2629, 2.2812, 2.3094,\n",
      "        2.1298, 2.1510, 2.1709, 2.2587, 2.2367, 2.3247, 2.0593, 2.0518, 2.1959,\n",
      "        2.0752, 2.0350, 2.3088, 2.2203, 2.1603, 2.1932, 2.1999, 2.3633, 2.2343,\n",
      "        2.3185, 2.0266, 2.0049, 2.0291, 2.3612, 1.9933, 1.9193, 2.3280, 2.3145,\n",
      "        2.1455, 2.3728, 2.1233, 2.2220, 1.9827], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0447,  0.0510, -0.0219,  ...,  0.0086, -0.0381,  0.0164],\n",
      "        [ 0.0335,  0.0086,  0.0163,  ..., -0.0567, -0.0035, -0.0332],\n",
      "        [-0.0318, -0.0421, -0.0561,  ..., -0.0134, -0.0451,  0.0214],\n",
      "        ...,\n",
      "        [-0.0258, -0.0276,  0.0257,  ...,  0.0457, -0.0259, -0.0298],\n",
      "        [ 0.0323, -0.0271,  0.0347,  ...,  0.0588, -0.0367, -0.0160],\n",
      "        [-0.0321, -0.0280, -0.0276,  ..., -0.0416,  0.0083,  0.0024]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 2.4240e-03,  2.2763e-05,  1.1144e-03,  ..., -1.5829e-03,\n",
      "         -2.0444e-04,  3.2786e-04],\n",
      "        [-9.1654e-04, -4.0508e-03,  4.2944e-03,  ..., -2.2664e-03,\n",
      "          2.9326e-03, -3.8429e-03],\n",
      "        [ 3.7555e-04, -1.4083e-02,  1.2326e-02,  ..., -1.2347e-02,\n",
      "          1.2981e-02, -1.3236e-02],\n",
      "        ...,\n",
      "        [ 1.4139e-02, -1.0499e-03,  2.1932e-03,  ..., -3.2106e-03,\n",
      "          5.4319e-04, -1.3972e-03],\n",
      "        [-5.8810e-04,  1.3346e-03, -2.0101e-03,  ...,  1.2073e-03,\n",
      "         -2.1014e-03,  1.6475e-03],\n",
      "        [ 1.3691e-03,  3.8936e-04, -8.7043e-04,  ...,  3.4049e-04,\n",
      "         -1.5991e-03,  1.3107e-03]], requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.intermediate.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.7313, 1.9081, 1.6071,  ..., 2.2157, 2.0472, 2.0312],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0027, -0.0046,  0.0032,  ...,  0.0052,  0.0046,  0.0193],\n",
      "        [-0.0208,  0.0210,  0.0315,  ..., -0.0159,  0.0163, -0.0168],\n",
      "        [ 0.0201, -0.0027, -0.0005,  ..., -0.0178, -0.0167,  0.0206],\n",
      "        ...,\n",
      "        [ 0.0140, -0.0313,  0.0200,  ..., -0.0213, -0.0245, -0.0200],\n",
      "        [-0.0314, -0.0057,  0.0035,  ...,  0.0239, -0.0221, -0.0086],\n",
      "        [ 0.0106,  0.0257,  0.0235,  ...,  0.0040, -0.0088, -0.0170]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0149,  0.0101, -0.0166,  ..., -0.0170,  0.0149,  0.0158],\n",
      "        [-0.0051, -0.0079,  0.0078,  ...,  0.0049, -0.0063, -0.0090],\n",
      "        [-0.0176, -0.0127,  0.0174,  ...,  0.0167, -0.0162, -0.0151],\n",
      "        ...,\n",
      "        [ 0.0144,  0.0077, -0.0129,  ..., -0.0149,  0.0129,  0.0109],\n",
      "        [ 0.0164,  0.0101, -0.0153,  ..., -0.0161,  0.0151,  0.0129],\n",
      "        [ 0.0143,  0.0107, -0.0137,  ..., -0.0131,  0.0130,  0.0117]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([ 5.2844,  4.9897,  4.9764,  3.5778,  5.1554,  5.9383,  6.1617,  6.2387,\n",
      "         5.7769,  5.4255,  5.9347,  5.5889,  5.7327,  5.2757,  5.4128,  5.9513,\n",
      "         5.3169,  5.9158,  5.2837,  5.7521,  5.4377,  6.0927,  4.7053,  6.1283,\n",
      "         5.5722,  5.8805,  4.9703,  5.4898,  5.4238,  5.5793,  5.5978,  4.1776,\n",
      "         6.1447,  4.6434,  5.7121,  5.7458,  4.5828,  6.0257,  5.8595,  5.1567,\n",
      "         5.6080,  5.8777,  5.5787,  5.7281,  6.0545,  5.7623,  5.3936,  5.3893,\n",
      "         5.3353,  5.9679,  5.2599,  6.1029,  5.5476,  5.8416,  5.4217,  6.0514,\n",
      "         5.3018,  6.2909,  5.2471,  4.7838,  6.3801,  5.5412, 12.0067,  5.7779,\n",
      "         6.0471,  5.5312,  5.6339,  5.9143,  5.7116,  5.7554,  5.4137,  5.8329,\n",
      "         5.1135,  5.4180,  5.4771,  6.1764,  5.6842,  6.3540,  4.9341,  5.6246,\n",
      "         5.1238,  5.0715,  5.6640,  5.0312,  5.7208,  5.8794,  5.9272,  5.6542,\n",
      "         5.4793,  5.3608,  5.7727,  4.9971,  5.7328,  5.9285,  5.2488,  6.6232,\n",
      "         6.0807,  5.4842,  4.1834,  5.4427,  5.7189,  6.0580,  5.9512,  5.5264,\n",
      "         5.8217,  5.8453,  5.4387,  5.9455,  5.6697,  5.4507,  6.3694,  5.6827,\n",
      "         5.6294,  6.0794,  5.6781,  5.8014,  5.4098,  5.2990,  6.0799,  5.5815,\n",
      "         5.9286,  5.5577,  5.4192,  4.4485,  4.0787,  5.3238,  6.0174,  5.4642,\n",
      "         5.6637,  5.4256,  5.8146,  5.0355,  3.9582,  5.2823,  6.1624,  5.2877,\n",
      "         5.7400,  6.2066,  5.4545,  4.8829,  5.1225,  4.9672,  5.8983,  6.0253,\n",
      "         6.6245,  6.0977,  6.2701,  6.2394,  6.1062,  5.9200,  5.8691,  5.8820,\n",
      "         5.3484,  5.8880,  5.4143,  6.1065,  5.0380,  5.9289,  6.0691,  5.8509,\n",
      "         5.6034,  5.5546,  5.4139,  6.1256,  5.6537,  6.1254,  6.0142,  5.6618,\n",
      "         5.3754,  5.9831,  4.8981,  5.9211,  5.5195,  5.6225,  5.5090,  5.0370,\n",
      "         5.6191,  6.2265,  5.2470,  5.0378,  5.2097,  5.0923,  5.5633,  6.0598,\n",
      "         5.7373,  5.7783,  5.7919,  6.2274,  5.4114,  5.6156,  5.2838,  5.5845,\n",
      "         4.9265,  5.7655,  5.5397,  5.9284,  6.2203,  5.9021,  5.9171,  5.6052,\n",
      "         5.6205,  6.1050,  6.1094,  5.9096,  5.8399,  6.0105,  5.6658,  3.5032,\n",
      "         6.0035,  4.9157,  5.7663,  6.0307,  5.9385,  5.6804,  5.9267,  6.2349,\n",
      "         5.6120,  5.3393,  4.8317,  5.2656,  5.7588,  5.6371,  5.5399,  5.6954,\n",
      "         6.5203,  4.6743,  5.9226,  5.4996,  5.8719,  5.8736,  5.9103,  5.2770,\n",
      "         5.4314,  5.1001,  5.6607,  5.7892,  5.5096,  6.1069,  5.5235,  5.8930,\n",
      "         5.0691,  5.9101,  5.8326,  5.5664,  6.0199,  5.6793,  5.1108,  5.7748,\n",
      "         4.9080,  5.8849,  5.5876,  5.4903,  5.2460,  5.3108,  5.7756,  5.0913,\n",
      "         5.4316,  5.5055,  5.1748,  5.3703,  5.7340,  5.6976,  5.8673,  5.6942,\n",
      "         5.9517,  5.7641,  5.8257,  5.3860,  5.3739,  5.6574,  3.9243,  5.7863,\n",
      "         5.2997,  5.9072,  5.7404,  5.7554,  6.3479,  5.4681,  5.6787,  5.9053,\n",
      "         5.4885,  5.9310,  6.0489,  6.2972,  5.2173,  5.5238,  6.0946,  5.9157,\n",
      "         3.8365,  5.4327,  5.8990,  6.0057,  6.0195,  5.6044,  5.6781,  6.2195,\n",
      "         5.4104,  4.7772,  5.7421,  5.6769,  5.7371,  5.6805,  5.4162,  5.8539,\n",
      "         6.3195,  5.9036,  6.2196,  5.0054,  4.9035,  6.0062,  5.6929,  5.0689,\n",
      "         5.0231,  5.7968,  5.2085,  5.4325,  5.9921,  5.6706,  5.5023,  3.4004],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 5.2048e-02,  5.7168e-02,  3.0671e-03,  7.6395e-03,  4.9073e-02,\n",
      "         -3.1210e-02,  6.1013e-02, -2.0448e-02, -4.3302e-02, -9.0638e-02,\n",
      "         -3.1305e-02,  8.1594e-03, -1.9025e-02,  6.1880e-02,  3.3327e-02,\n",
      "         -3.2257e-02, -5.1502e-02, -6.3603e-02,  4.7971e-02,  1.8586e-02,\n",
      "         -1.0727e-02, -9.0833e-02, -2.3802e-02, -4.8193e-02, -2.7362e-02,\n",
      "          4.1894e-02,  2.4672e-02,  3.4746e-02, -2.1268e-03,  6.4929e-02,\n",
      "         -4.7017e-02, -5.3023e-02,  7.5545e-02, -8.5475e-03, -1.8681e-02,\n",
      "          1.9075e-02, -3.6381e-02, -3.6849e-02,  6.7679e-02, -6.5267e-03,\n",
      "          5.4711e-02, -9.1222e-02, -8.6994e-03,  6.5640e-02,  7.8718e-02,\n",
      "         -1.8303e-02, -3.3819e-02, -5.8391e-02,  6.2718e-02, -7.2402e-02,\n",
      "         -2.5109e-02,  8.3493e-03,  6.9597e-02,  1.1976e-02, -2.4708e-02,\n",
      "         -3.9660e-02,  9.0227e-03,  4.8732e-03, -5.2985e-02,  5.2058e-02,\n",
      "         -4.4509e-02, -6.5427e-03, -7.9234e-02, -2.9316e-02, -4.5654e-02,\n",
      "          7.0541e-02, -2.9225e-02, -4.6966e-02,  1.9730e-02,  4.3822e-02,\n",
      "         -7.7667e-02,  3.8662e-02, -3.4669e-02,  2.9455e-02,  4.8630e-02,\n",
      "         -1.8747e-03, -4.0188e-02,  8.0200e-02,  3.1501e-02,  2.1120e-03,\n",
      "         -3.8209e-02, -1.7818e-02, -2.3472e-02,  2.2498e-02, -2.3097e-02,\n",
      "          2.0090e-03, -1.2906e-02, -9.4848e-03, -9.5199e-04, -3.1765e-02,\n",
      "          6.8914e-02, -2.4542e-02,  5.6995e-02,  7.3282e-02,  8.2201e-02,\n",
      "          1.6637e-02,  4.4703e-03,  6.5569e-03,  8.0486e-02,  8.2753e-02,\n",
      "          4.9726e-02,  4.7019e-03,  2.7874e-02, -2.8243e-02,  2.3701e-02,\n",
      "         -2.1732e-02, -8.0983e-02, -2.4880e-02,  1.1293e-02,  4.1167e-02,\n",
      "          1.5900e-02, -2.6957e-02, -2.3409e-03, -2.8293e-02,  6.9298e-02,\n",
      "          8.6540e-02,  2.7610e-02, -5.4482e-02,  7.8792e-02, -1.0626e-02],\n",
      "        [-4.3524e-02,  1.8156e-02,  8.7717e-02, -8.0032e-03, -8.4494e-02,\n",
      "          3.1344e-03, -6.1531e-02,  2.6168e-02, -4.7147e-02,  8.0173e-02,\n",
      "         -6.0429e-02, -1.4185e-02,  4.8383e-02,  9.0423e-02, -7.8544e-02,\n",
      "         -5.4365e-02, -6.4657e-02, -3.7951e-02, -6.7491e-02,  2.3218e-02,\n",
      "          3.1010e-02, -7.9554e-02,  8.9294e-02, -3.6038e-02, -6.0452e-02,\n",
      "         -8.4869e-02, -5.4067e-02,  1.2882e-02, -8.3364e-03,  3.1729e-02,\n",
      "          5.0105e-02, -1.4880e-02,  7.0933e-02,  7.4586e-02,  4.5191e-02,\n",
      "          2.1829e-02, -4.9204e-02, -3.6967e-02, -4.4102e-02,  1.2695e-03,\n",
      "          5.5328e-02, -2.6898e-02,  2.8673e-02,  7.6428e-02, -4.9325e-02,\n",
      "         -3.9762e-02, -5.5717e-02, -5.3646e-02,  5.1280e-02,  6.5921e-02,\n",
      "         -7.3198e-02,  4.0161e-02, -7.5979e-02,  4.1211e-03, -3.6509e-02,\n",
      "         -2.1323e-02,  1.8490e-03, -2.8367e-02, -6.8964e-02,  8.1847e-02,\n",
      "         -7.5619e-03, -5.9387e-02,  2.3774e-02, -1.0436e-03,  4.6711e-02,\n",
      "          3.2849e-02,  3.5498e-02, -6.8003e-02, -4.6913e-02,  6.8553e-02,\n",
      "          8.9541e-02,  7.6555e-02,  7.4681e-02,  5.8823e-02,  4.2131e-02,\n",
      "         -3.8928e-02, -6.3926e-02,  4.8324e-02, -3.6324e-02,  4.3068e-02,\n",
      "         -6.4895e-02, -5.1385e-02,  2.1957e-02,  2.9131e-02, -1.3269e-03,\n",
      "         -4.4508e-02,  8.5692e-02,  5.5540e-02,  6.6900e-02,  4.8650e-02,\n",
      "         -7.7568e-02, -6.1602e-02,  2.7745e-02,  2.5610e-02, -6.8598e-02,\n",
      "         -8.6124e-02, -8.1652e-02, -1.2628e-02, -6.9427e-02,  1.2360e-02,\n",
      "          7.5987e-02, -8.5291e-02, -2.0682e-02, -5.1821e-03, -2.8576e-02,\n",
      "         -3.9674e-02,  6.3789e-02,  6.9545e-02,  6.2593e-02,  8.1183e-03,\n",
      "          9.0493e-02,  8.0044e-02, -4.6054e-02, -1.4563e-02, -4.1635e-02,\n",
      "         -7.2867e-02,  3.0071e-02,  8.3747e-02, -2.4794e-02,  5.5450e-02],\n",
      "        [ 2.5866e-02,  7.9405e-02, -1.3064e-02,  6.1466e-02, -4.9401e-02,\n",
      "          5.4963e-02,  7.8275e-02, -2.8372e-02, -8.8238e-02,  1.7308e-02,\n",
      "          7.1522e-02, -2.3194e-03, -3.3775e-03, -2.5514e-02,  4.9239e-02,\n",
      "         -1.2804e-02,  1.4174e-02,  9.0242e-02,  1.7368e-03, -1.6282e-02,\n",
      "         -4.2620e-04,  6.4290e-02,  5.5246e-02,  5.0454e-02,  5.6458e-02,\n",
      "         -8.4477e-02, -4.5525e-03,  2.8170e-02, -5.2229e-02, -2.4550e-02,\n",
      "         -5.0321e-02, -2.2997e-02,  6.1522e-02, -1.9320e-02, -7.3430e-02,\n",
      "         -5.7278e-02,  5.0821e-02, -1.2768e-02,  7.8118e-02, -3.1079e-02,\n",
      "          7.7682e-02, -1.6414e-02,  6.5233e-02, -2.3002e-02, -6.5459e-02,\n",
      "         -8.0094e-02, -4.3684e-02,  7.1106e-02, -6.8804e-02, -7.2889e-02,\n",
      "          6.7741e-02,  8.4391e-02,  7.6311e-02,  7.9899e-02,  8.8938e-02,\n",
      "          8.3535e-02, -8.2436e-02, -3.5915e-03,  3.6392e-02,  2.2132e-02,\n",
      "          3.1260e-02,  6.6301e-02,  8.7614e-02,  2.0461e-02,  6.1568e-02,\n",
      "         -4.9511e-02,  3.3442e-02,  7.8538e-02,  2.1029e-02,  4.0005e-02,\n",
      "         -2.1998e-02, -9.1218e-02,  5.1963e-02, -4.5896e-02,  2.2432e-02,\n",
      "         -1.4790e-02,  4.9881e-02,  8.4726e-02, -1.6578e-02, -2.9716e-02,\n",
      "          8.3143e-02, -7.5306e-02,  2.5014e-02, -8.9941e-02, -3.9233e-02,\n",
      "          2.3438e-02, -7.7145e-04,  4.2188e-03, -8.8095e-02, -6.7135e-02,\n",
      "          4.9220e-02, -1.2141e-02, -6.6777e-02, -1.5566e-02, -2.3173e-02,\n",
      "         -5.1371e-02, -2.5879e-02, -8.1535e-02, -4.1013e-02, -4.7851e-02,\n",
      "         -6.6397e-02,  8.7765e-02,  8.3044e-02, -4.6970e-02, -2.6849e-02,\n",
      "          7.5462e-02,  7.9259e-03, -1.6544e-02, -6.8735e-02, -4.4598e-02,\n",
      "          4.9845e-04, -3.7067e-02, -8.1111e-02,  5.1616e-02,  6.2984e-02,\n",
      "          4.7545e-02,  3.6439e-02,  1.2973e-02, -5.8141e-02, -4.1015e-02],\n",
      "        [-7.0770e-02, -2.3912e-02, -5.4811e-02, -6.8518e-02,  5.3705e-02,\n",
      "         -1.7017e-02, -3.7094e-02, -3.2247e-02, -7.7896e-02,  8.7733e-02,\n",
      "         -7.5835e-02,  1.9084e-02, -1.7063e-02,  5.7932e-02,  7.0905e-02,\n",
      "          3.0538e-02,  7.1833e-02, -1.8382e-02, -8.6879e-02, -2.4391e-03,\n",
      "         -1.8961e-04,  6.5857e-02, -1.3579e-02, -8.7461e-02,  7.2151e-03,\n",
      "         -4.0687e-02, -4.0339e-02, -8.6915e-03,  6.8748e-02,  3.1219e-02,\n",
      "          6.6404e-02, -4.3203e-02, -9.0433e-02, -1.6474e-02,  9.1238e-02,\n",
      "          7.1505e-03, -4.5745e-02,  1.5214e-02,  7.8110e-02, -9.1125e-02,\n",
      "         -5.9617e-02,  3.8887e-02, -7.7777e-02,  3.2791e-02, -8.0137e-02,\n",
      "         -3.3836e-02,  5.9641e-03, -5.0180e-02,  6.7538e-02,  7.5382e-02,\n",
      "         -2.5732e-02, -5.5171e-02,  6.5320e-02,  4.2669e-02,  7.9218e-02,\n",
      "         -5.5700e-02, -8.2238e-02,  8.0124e-03,  3.1590e-03, -2.5704e-02,\n",
      "         -5.6741e-03,  4.9395e-04,  1.2604e-02,  3.7754e-02, -5.2252e-02,\n",
      "         -6.3988e-05, -1.9274e-02,  6.4993e-02,  3.3794e-02, -3.0669e-02,\n",
      "          8.3265e-02, -8.6009e-03, -2.9982e-02,  2.9370e-02,  5.1132e-02,\n",
      "         -2.7723e-03, -6.1257e-02,  5.1481e-03,  7.6030e-02, -1.1829e-02,\n",
      "          9.0769e-02, -3.5690e-02, -8.0731e-02,  3.9631e-02, -4.0781e-02,\n",
      "         -6.1122e-02, -2.0530e-02,  2.9503e-02, -8.8690e-02,  7.4762e-02,\n",
      "          4.4173e-02, -5.6355e-02, -8.4142e-02,  4.5109e-02,  5.6040e-02,\n",
      "          2.7298e-02, -4.0730e-02, -3.6586e-02, -6.3828e-02,  7.0806e-03,\n",
      "          5.3247e-02, -2.2195e-02,  3.6842e-02,  8.9839e-02, -4.8552e-03,\n",
      "         -8.5568e-02, -5.2697e-04, -7.2547e-02,  6.8513e-02,  8.0152e-02,\n",
      "         -4.0386e-02,  3.2646e-02, -6.3814e-02, -2.5210e-02,  5.9936e-02,\n",
      "         -7.8991e-02, -8.7769e-02,  3.5507e-02,  5.9511e-02, -8.5726e-02],\n",
      "        [ 4.8205e-02,  4.3737e-02,  5.5488e-02,  5.3117e-02, -6.8854e-02,\n",
      "         -1.3530e-02,  6.8259e-02, -2.2976e-02,  5.1076e-02,  4.4870e-02,\n",
      "         -5.5534e-02,  8.6940e-02,  7.3180e-02,  4.7798e-02,  7.6495e-02,\n",
      "         -6.4625e-02,  4.9923e-02,  4.1502e-02, -5.2620e-02, -1.0773e-03,\n",
      "          1.4424e-02, -1.8933e-02, -7.4799e-02,  2.4285e-02, -4.0002e-03,\n",
      "         -7.5474e-02, -7.4636e-03, -4.3678e-02, -2.8748e-02,  5.2301e-02,\n",
      "         -3.6687e-02,  8.5370e-02,  5.5577e-03, -1.4829e-02, -4.3796e-02,\n",
      "          7.7980e-02,  3.8031e-03, -5.7689e-02, -2.8371e-02, -1.1608e-02,\n",
      "          4.0987e-03, -6.7840e-02, -3.0961e-02, -6.1162e-02,  2.4371e-02,\n",
      "          6.4245e-02, -2.8524e-03, -3.7693e-03,  6.6016e-02, -7.4197e-02,\n",
      "         -6.9313e-02, -6.9040e-02,  6.8146e-02,  2.4434e-02,  3.0487e-02,\n",
      "          8.0080e-03, -2.9285e-03,  8.8121e-02, -6.8492e-02, -7.0580e-02,\n",
      "          4.6320e-02,  1.0411e-02, -3.9706e-02, -6.7674e-02, -3.0895e-02,\n",
      "          8.5238e-02,  8.3126e-02, -2.1042e-02, -5.0605e-02,  5.7306e-02,\n",
      "         -8.0013e-02,  3.9645e-02, -8.2224e-02,  6.4132e-02, -4.4297e-02,\n",
      "         -6.4583e-02,  6.2014e-03, -3.9227e-03,  2.5545e-02, -9.0680e-02,\n",
      "         -6.4731e-02, -8.2919e-02, -3.7788e-02,  9.1263e-02,  6.1929e-02,\n",
      "         -6.1832e-02,  1.5195e-02,  2.7057e-02,  1.3705e-02,  1.9044e-02,\n",
      "         -1.6152e-02, -5.4144e-02, -4.3002e-02, -6.1121e-02, -6.3252e-02,\n",
      "          6.1997e-02, -2.0995e-02,  1.8157e-03, -7.9598e-02,  6.1480e-02,\n",
      "         -3.8141e-03,  6.0415e-02, -2.9940e-02, -2.6105e-02, -2.5436e-02,\n",
      "          1.5003e-02,  7.9838e-02, -3.8747e-02,  8.9901e-02, -3.8925e-02,\n",
      "          1.9897e-02, -3.8338e-02,  4.5365e-02, -3.5450e-02,  4.2659e-02,\n",
      "         -3.1645e-02,  9.0453e-02, -7.4355e-02,  8.6922e-02, -3.2312e-02],\n",
      "        [ 7.8611e-02,  5.0023e-02,  3.1569e-02,  7.4060e-02,  3.4692e-02,\n",
      "         -3.8813e-02, -1.3483e-03,  6.5831e-02,  8.4281e-02, -6.6874e-02,\n",
      "         -2.8215e-02, -8.0277e-02, -4.5427e-02, -4.2700e-02,  4.4488e-02,\n",
      "         -8.3317e-02, -6.9992e-02, -3.6561e-02, -8.5886e-03,  5.8279e-04,\n",
      "          2.4995e-02,  5.7487e-02, -5.3198e-02,  9.0176e-03,  8.1387e-04,\n",
      "         -7.6916e-02,  8.6139e-02, -8.7993e-02,  8.3792e-02,  5.5021e-02,\n",
      "         -7.8043e-02, -1.0653e-02, -6.6272e-02,  7.9602e-02,  3.5707e-02,\n",
      "         -4.4018e-03,  2.7479e-02,  1.6917e-02, -5.4429e-02, -6.8105e-02,\n",
      "         -2.8116e-02,  3.8720e-02, -8.8406e-02,  2.8788e-02,  7.8700e-02,\n",
      "          3.1168e-02,  5.0301e-02,  7.2557e-02,  7.7166e-02,  5.1948e-02,\n",
      "         -1.3104e-02,  1.7543e-03, -4.1497e-02, -6.6288e-02,  2.4006e-02,\n",
      "         -6.7753e-02, -4.6466e-02,  7.9295e-02,  4.0833e-02,  6.2728e-02,\n",
      "         -5.4178e-02,  2.7313e-02, -7.6762e-02,  7.8231e-02, -4.7464e-02,\n",
      "         -3.3610e-02,  3.9732e-02,  8.1485e-02, -4.1542e-02,  2.6831e-02,\n",
      "         -2.6617e-02, -4.0696e-02,  6.0360e-02, -5.9014e-02,  3.0745e-02,\n",
      "         -8.7112e-03,  4.4217e-02,  8.5085e-02,  5.0352e-02,  8.7947e-02,\n",
      "         -7.5486e-02, -2.8877e-02,  7.8583e-02, -8.3780e-02,  9.0747e-03,\n",
      "         -7.3395e-04, -3.2927e-02,  1.5885e-02, -2.2105e-02,  7.7039e-02,\n",
      "         -4.3619e-02, -1.2271e-02, -4.4721e-02,  4.5569e-02,  6.4266e-02,\n",
      "         -3.8143e-02, -9.6399e-03, -5.2199e-02, -8.3583e-02, -6.2214e-02,\n",
      "          5.2032e-02, -2.3657e-02, -4.2494e-02,  2.2489e-02,  6.0503e-02,\n",
      "          8.6333e-02, -4.2516e-02, -8.4919e-02,  2.6706e-02, -6.3692e-02,\n",
      "          8.8765e-02,  1.0036e-02, -5.0520e-02, -5.6726e-02, -2.9083e-02,\n",
      "         -5.2542e-02,  8.8274e-02, -2.4867e-03, -8.9578e-02, -7.2576e-02],\n",
      "        [ 8.9572e-02, -2.3469e-02,  6.2593e-02, -9.8068e-03,  4.4818e-02,\n",
      "         -1.7806e-02,  7.7258e-02, -8.7955e-02,  2.0717e-02,  3.1078e-02,\n",
      "          5.9258e-02,  7.0024e-02,  1.4849e-02, -5.8861e-02, -1.1326e-02,\n",
      "         -6.5491e-02,  2.6469e-02, -2.5098e-02,  5.2014e-02,  1.3741e-02,\n",
      "         -1.3696e-02,  1.1992e-02, -5.7642e-02, -4.6052e-02, -3.6499e-02,\n",
      "         -3.3648e-02, -6.9920e-02, -3.0771e-02,  7.1880e-02,  8.9643e-02,\n",
      "          7.9920e-03,  1.2432e-02, -8.8859e-02, -2.9080e-02,  8.1511e-02,\n",
      "          3.3576e-02,  2.4451e-02, -8.1077e-02, -4.2202e-02,  6.4616e-02,\n",
      "          3.9724e-02, -5.9294e-02,  1.7537e-02,  2.1365e-02, -3.0884e-02,\n",
      "         -7.9861e-02,  5.2485e-02, -7.4168e-02, -8.7185e-02, -7.2453e-02,\n",
      "         -6.0077e-02, -6.7730e-02, -5.2165e-03, -6.1453e-02, -4.5829e-03,\n",
      "         -8.4484e-02, -3.0384e-02,  2.6585e-02,  8.9533e-02,  7.0750e-02,\n",
      "         -8.6600e-02, -5.7153e-02,  7.1428e-02, -5.2724e-02,  4.8475e-02,\n",
      "         -2.1627e-02, -6.5967e-02, -4.3553e-02, -1.8233e-02, -6.9964e-02,\n",
      "         -7.8359e-02, -6.5171e-02, -6.6544e-03, -4.6473e-02, -3.8572e-02,\n",
      "         -3.3438e-02,  7.8866e-02,  6.8237e-02,  7.6397e-02, -8.9299e-03,\n",
      "          2.1678e-02, -4.4581e-02,  8.8017e-02, -1.1254e-02, -1.0109e-02,\n",
      "          6.7245e-02,  8.8134e-02,  6.3138e-02,  2.1344e-02,  6.6490e-02,\n",
      "         -2.1411e-02, -4.8203e-03, -8.1019e-02,  8.3602e-03,  7.8788e-02,\n",
      "          6.3118e-02,  4.8445e-02,  2.6266e-02,  6.5160e-02,  3.6574e-02,\n",
      "         -4.3626e-02,  3.6461e-02, -7.2867e-02,  8.9647e-02, -4.8377e-02,\n",
      "         -2.7007e-02,  6.1859e-02, -7.2727e-02,  7.8305e-02, -1.4361e-02,\n",
      "          5.1413e-02,  7.4190e-02, -6.3165e-02, -8.4459e-02,  5.3581e-02,\n",
      "         -2.6601e-02, -8.2763e-02,  4.4337e-02,  6.2857e-02, -6.2388e-03],\n",
      "        [ 7.1211e-02,  6.6658e-02, -5.9428e-02, -4.2631e-02,  6.9394e-02,\n",
      "         -7.0819e-03, -3.5744e-02, -3.8549e-02, -6.2739e-02,  2.1319e-02,\n",
      "          5.5233e-02,  6.3317e-02, -3.8381e-02, -2.7255e-02, -1.4849e-02,\n",
      "          4.3682e-02, -4.0378e-02, -6.4587e-02,  6.8455e-02,  1.8486e-02,\n",
      "          6.8346e-02, -4.1347e-02, -1.5260e-02,  7.0921e-02, -8.5917e-02,\n",
      "         -6.5156e-02,  8.3500e-02, -2.0661e-02,  4.1561e-03,  1.4112e-02,\n",
      "         -4.4986e-02,  6.9690e-02,  1.8034e-02,  6.4943e-02, -5.2788e-02,\n",
      "          1.4952e-02,  7.5921e-02, -1.7352e-02, -2.1763e-02, -7.8053e-02,\n",
      "          6.3141e-02,  2.5997e-02, -6.3720e-03, -6.5469e-02, -7.4798e-02,\n",
      "         -7.9742e-02,  8.2475e-02, -4.2604e-02,  1.9424e-02,  1.5992e-02,\n",
      "          6.4784e-02, -2.1470e-03, -2.1254e-02,  1.1253e-02, -6.4680e-02,\n",
      "         -4.9694e-02,  8.1259e-03, -4.4849e-02,  7.1986e-03, -2.9087e-02,\n",
      "          4.8122e-02,  1.0728e-02, -7.5657e-02, -5.1599e-03, -4.0601e-02,\n",
      "         -8.7286e-02, -3.1102e-02, -6.0128e-02, -6.4449e-02,  4.5978e-02,\n",
      "          3.2670e-02,  2.4521e-02, -5.6797e-02,  2.0140e-02,  7.6288e-02,\n",
      "          4.9833e-02,  4.5984e-02,  5.2504e-02, -7.8745e-02,  2.0454e-02,\n",
      "         -5.5656e-02,  7.4411e-02,  5.9370e-02,  8.0931e-02, -5.5604e-02,\n",
      "          9.1125e-02,  8.0322e-02, -1.8972e-02,  2.6521e-03,  6.9167e-02,\n",
      "          6.1412e-02, -4.3952e-02, -1.4217e-02,  3.2643e-02, -6.6850e-02,\n",
      "          8.5213e-02, -8.7090e-02,  3.5398e-02,  3.6040e-02,  4.0841e-02,\n",
      "         -3.1422e-02,  5.7057e-02, -3.8533e-02,  8.3216e-02,  7.1672e-02,\n",
      "          4.7856e-02, -1.6386e-02, -2.1815e-02, -3.0048e-02,  3.7858e-02,\n",
      "         -4.7731e-02, -4.1232e-02, -1.9629e-02,  9.6535e-03,  2.4985e-02,\n",
      "         -5.5127e-02, -6.7492e-02, -8.4300e-02, -1.8194e-02, -4.7798e-02]],\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_B.default.weight Parameter containing:\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0.]], requires_grad=True)\n",
      "base_model.model.esm.contact_head.regression.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([42.3184], requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0562,  0.0294, -0.0143,  ..., -0.0432, -0.0331,  0.0290],\n",
      "        [-0.0670,  0.0472,  0.0270,  ..., -0.0391,  0.0353, -0.0081],\n",
      "        [ 0.0344,  0.0382,  0.0156,  ...,  0.0544,  0.0366, -0.0016],\n",
      "        ...,\n",
      "        [-0.0518,  0.0557,  0.0287,  ...,  0.0241,  0.0218, -0.0327],\n",
      "        [ 0.0378, -0.0404, -0.0468,  ..., -0.0469, -0.0127,  0.0257],\n",
      "        [ 0.0492, -0.0028, -0.0140,  ...,  0.0101,  0.0254, -0.0140]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0034, -0.0044,  0.0147,  ..., -0.0160,  0.0124,  0.0164],\n",
      "        [-0.0034, -0.0043,  0.0146,  ..., -0.0158,  0.0123,  0.0163],\n",
      "        [ 0.0034,  0.0043, -0.0144,  ...,  0.0156, -0.0121, -0.0160],\n",
      "        ...,\n",
      "        [-0.0040, -0.0048,  0.0151,  ..., -0.0162,  0.0127,  0.0166],\n",
      "        [-0.0037, -0.0045,  0.0145,  ..., -0.0156,  0.0123,  0.0160],\n",
      "        [ 0.0036,  0.0044, -0.0147,  ...,  0.0160, -0.0125, -0.0165]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3820, 0.3322, 0.3476, 0.3641, 0.3387, 0.3581, 0.3388, 0.3812, 0.3638,\n",
      "        0.3607, 0.3592, 0.3438, 0.3602, 0.3802, 0.3636, 0.3859, 0.3524, 0.3227,\n",
      "        0.3773, 0.3779, 0.3686, 0.3628, 0.3698, 0.3746, 0.3551, 0.3647, 0.3329,\n",
      "        0.3586, 0.3606, 0.3879, 0.3563, 0.3815, 0.3653, 0.3333, 0.3619, 0.3462,\n",
      "        0.3741, 0.3713, 0.3703, 0.3822, 0.3533, 0.3653, 0.3807, 0.3492, 0.3639,\n",
      "        0.3659, 0.3777, 0.3781, 0.3648, 0.3882, 0.3525, 0.3596, 0.3418, 0.3780,\n",
      "        0.3783, 0.3574, 0.3627, 0.3546, 0.3590, 0.3654, 0.3505, 0.3631, 0.3694,\n",
      "        0.3624, 0.3356, 0.3832, 0.3840, 0.3824, 0.3626, 0.3831, 0.3392, 0.3764,\n",
      "        0.3521, 0.3500, 0.3308, 0.3566, 0.3598, 0.3554, 0.3767, 0.3603, 0.3814,\n",
      "        0.3777, 0.3785, 0.3497, 0.3610, 0.3698, 0.3578, 0.3357, 0.3567, 0.3746,\n",
      "        0.3531, 0.3663, 0.3521, 0.3667, 0.3242, 0.3693, 0.3452, 0.3751, 0.3823,\n",
      "        0.3332, 0.3352, 0.3548, 0.3402, 0.3524, 0.3400, 0.3667, 0.3544, 0.3703,\n",
      "        0.3606, 0.3454, 0.3471, 0.3791, 0.3654, 0.3818, 0.3368, 0.3834, 0.3653,\n",
      "        0.4041, 0.3606, 0.3264, 0.3892, 0.3293, 0.3495, 0.3614, 0.3807, 0.3521,\n",
      "        0.3794, 0.3482, 0.3651, 0.3765, 0.3690, 0.3581, 0.3941, 0.3513, 0.3542,\n",
      "        0.3842, 0.3382, 0.3882, 0.3659, 0.3741, 0.3543, 0.3476, 0.3724, 0.3516,\n",
      "        0.3830, 0.3625, 0.3619, 0.3497, 0.3650, 0.3638, 0.3702, 0.3741, 0.3694,\n",
      "        0.3727, 0.3865, 0.3797, 0.3530, 0.3617, 0.3811, 0.3747, 0.3602, 0.3490,\n",
      "        0.3816, 0.3730, 0.3876, 0.3543, 0.3337, 0.3693, 0.3731, 0.3906, 0.3909,\n",
      "        0.3606, 0.3557, 0.3720, 0.3463, 0.3643, 0.3589, 0.3781, 0.3202, 0.4087,\n",
      "        0.3726, 0.3653, 0.3747, 0.3638, 0.3664, 0.3594, 0.3716, 0.3675, 0.3543,\n",
      "        0.3659, 0.3862, 0.3552, 0.3766, 0.3604, 0.3766, 0.3795, 0.3724, 0.3628,\n",
      "        0.3586, 0.3681, 0.3695, 0.3681, 0.3706, 0.3453, 0.3378, 0.3821, 0.3647,\n",
      "        0.3420, 0.3893, 0.3836, 0.3481, 0.3448, 0.3732, 0.3814, 0.3863, 0.3486,\n",
      "        0.3707, 0.3371, 0.3918, 0.3550, 0.3662, 0.3809, 0.3516, 0.3557, 0.3280,\n",
      "        0.3655, 0.3595, 0.3645, 0.3754, 0.3742, 0.3905, 0.3719, 0.3609, 0.3518,\n",
      "        0.3521, 0.3696, 0.3799, 0.3526, 0.3284, 0.3566, 0.3600, 0.3719, 0.3623,\n",
      "        0.3902, 0.3328, 0.3652, 0.3483, 0.3832, 0.3742, 0.3639, 0.3907, 0.3687,\n",
      "        0.3765, 0.3659, 0.3455, 0.3978, 0.3724, 0.3556, 0.3524, 0.3753, 0.3850,\n",
      "        0.3566, 0.3194, 0.3378, 0.3681, 0.3768, 0.3615, 0.3578, 0.3527, 0.3770,\n",
      "        0.3754, 0.3829, 0.3658, 0.3598, 0.3664, 0.3559, 0.3339, 0.3505, 0.3641,\n",
      "        0.3441, 0.3380, 0.3453, 0.3585, 0.3540, 0.3542, 0.3596, 0.3337, 0.3530,\n",
      "        0.3689, 0.3233, 0.3706, 0.3768, 0.3924, 0.3556, 0.3521, 0.3253, 0.3700,\n",
      "        0.3592, 0.3627, 0.3656, 0.3640, 0.3646, 0.3868, 0.3557, 0.3680, 0.3707,\n",
      "        0.3183, 0.3595, 0.3611, 0.3692, 0.3572, 0.3906, 0.3786, 0.3370, 0.3661,\n",
      "        0.3901, 0.3391, 0.3505, 0.3843, 0.3715], requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0229, -0.0032,  0.0024,  ...,  0.0399, -0.0422, -0.0317],\n",
      "        [-0.0360,  0.0442,  0.0251,  ...,  0.0456, -0.0438, -0.0039],\n",
      "        [ 0.0374,  0.0262, -0.0350,  ...,  0.0578,  0.0137, -0.0243],\n",
      "        ...,\n",
      "        [-0.0459,  0.0441,  0.0258,  ..., -0.0362,  0.0315, -0.0105],\n",
      "        [ 0.0675,  0.0431,  0.0039,  ...,  0.0300,  0.0256, -0.0270],\n",
      "        [ 0.0188,  0.0433,  0.0521,  ..., -0.0079,  0.0015,  0.0349]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0153,  0.0132,  0.0064,  0.0118, -0.0060, -0.0155,  0.0108, -0.0092],\n",
      "        [-0.0153, -0.0132, -0.0064, -0.0118,  0.0060,  0.0155, -0.0108,  0.0092]],\n",
      "       requires_grad=True)\n",
      "base_model.model.classifier.out_proj.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.3824, 0.3449], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | PeftModel | 4.4 M \n",
      "------------------------------------\n",
      "302 K     Trainable params\n",
      "4.1 M     Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.623    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "085f16650fad4c26b25aada7209e94b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples found in target, recall is undefined. Setting recall to one for all thresholds.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('Val_MCC', ...)` in your `validation_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'Val_MCC': ...})` instead.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Average precision score for one or more classes was `nan`. Ignoring these classes in macro-average\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26f78f6eade443cbb1322f9119ce941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('Train_MCC', ...)` in your `training_step` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'Train_MCC': ...})` instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4740d052e9e941ab819243dac3a0ffe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved. New best score: -0.083\n",
      "Epoch 0, global step 93: 'Val_MCC' reached -0.08333 (best -0.08333), saving model to 'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=0-Val_MCC=-0.08.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04aace0ed37f4e70b66b9d0207777401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.333 >= min_delta = 0.005. New best score: 0.250\n",
      "Epoch 1, global step 186: 'Val_MCC' reached 0.25000 (best 0.25000), saving model to 'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=1-Val_MCC=0.25.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9de5e970634cd7ad8468479ed486ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 279: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb12ad01154445b3b8a9543a68700da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 372: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedaf6861cb94155aeb0e8309e258113",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.167 >= min_delta = 0.005. New best score: 0.417\n",
      "Epoch 4, global step 465: 'Val_MCC' reached 0.41667 (best 0.41667), saving model to 'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=4-Val_MCC=0.42.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f81e0b6e0f34304ba338d4714546f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 558: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df41eecd3afe418faa67445880a413f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.083 >= min_delta = 0.005. New best score: 0.500\n",
      "Epoch 6, global step 651: 'Val_MCC' reached 0.50000 (best 0.50000), saving model to 'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=6-Val_MCC=0.50.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eafc023a924d569a75f576e3954162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 744: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba413dcba7094d7f99ff66b45366480c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.083 >= min_delta = 0.005. New best score: 0.583\n",
      "Epoch 8, global step 837: 'Val_MCC' reached 0.58333 (best 0.58333), saving model to 'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=8-Val_MCC=0.58.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebf2f4c4e3244958fa5d051498fae00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 930: 'Val_MCC' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = ft.Trainer(callbacks=[checkpoint_callback, early_callback], default_root_dir=train_config.model_checkpoint_dir,\n",
    "                          fast_dev_run=bool(train_config.debug_mode_sample), max_epochs=10, \n",
    "                          max_time=train_config.max_time, precision=train_config.precision,\n",
    "                          accumulate_grad_batches=train_config.accumulate_grad_batches)\n",
    "        \n",
    "trainer.fit(model=light_mod, datamodule=data_module)\n",
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model_checkpoint/lightning_logs/version_3/checkpoints/epoch=8-Val_MCC=0.58.ckpt'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 120])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(best_model_path, map_location=lambda storage, loc: storage)\n",
    "checkpoint[\"state_dict\"][\"model.base_model.model.esm.contact_head.regression.lora_A.default.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ft.TransformerModule.load_from_checkpoint(best_model_path, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The classmethod `TransformerModule.load_from_checkpoint` cannot be called on an instance. Please call it on the class type and make sure the return value is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mod3 \u001b[38;5;241m=\u001b[39m \u001b[43mlight_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/utilities/model_helpers.py:121\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m is_scripting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m frameinfo\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;28;01mfor\u001b[39;00m frameinfo \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mstack())\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mTypeError\u001b[0m: The classmethod `TransformerModule.load_from_checkpoint` cannot be called on an instance. Please call it on the class type and make sure the return value is used."
     ]
    }
   ],
   "source": [
    "mod3 = light_mod.load_from_checkpoint(best_model_path, model=model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.prepare_data()\n",
    "data_module.setup(\"fit\")\n",
    "inputs = data_module.train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits_base = model(input_ids=batch[\"input_ids\"], \n",
    "                    attention_mask=batch[\"attention_mask\"]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
