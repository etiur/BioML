{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to finetune HuggingFace models on text data of any size and format with custom splitting (not random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to handle text data of any size and format with custom split because random splitting is not recommended for protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from datasets import Dataset, DatasetDict\n",
    "from BioML.utilities import split_methods\n",
    "from BioML.deep.embeddings import LLMConfig, TokenizeFasta\n",
    "from BioML.deep.utils import set_seed\n",
    "from peft import get_peft_model, LoraConfig\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to label the target values as labels so Trainer can recognize it.\n",
    "Dataset can actually be used for any usecases with large   files it doesn't depend on transformers  \n",
    "Although you would need to use PyTorch Dataloader to transform it into batches (but it only returns inputs ids and attention masks will it also return labels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "y = np.random.randint(0, 2, size=len(b))\n",
    "dataset = b.add_column(\"labels\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TokenizeFasta()\n",
    "tokens = tok.tokenize(\"../data/whole_sequence.fasta\", ([\"labels\", y],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom spliting with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = split_methods.ClusterSpliter(\"../data/resultsDB_clu.tsv\")\n",
    "train, test = cluster.train_test_split(range(len(dataset)), groups=dataset[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(range(len(dataset)), stratify=dataset[\"labels\"], test_size=0.2) # random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = DatasetDict({\"train\":dataset.select(train), \"test\":dataset.select(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, validation = cluster.train_test_split(range(len(new[\"train\"])), groups=new[\"train\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2 = DatasetDict({\"train\":new[\"train\"].select(train_), \"test\":dataset.select(test), \"validation\": new[\"train\"].select(validation)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the protein language models model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2) # torch_dtype=torch.bfloat16 to load in bfloat16 which is accepted by CPUs unlike float16\n",
    "\n",
    "def model_init2(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "\treturn AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f68842954984e87abc3735b7d39e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3b4ce46174a68bbf041a21f15b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new[\"train\"] = new[\"train\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)\n",
    "new[\"test\"] = new[\"test\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 1\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se use cpu to False whe you wan to use GPUs (it will automatically use GPUs), when f16 is True it will only use GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.2, lr_scheduler_type='cosine', fp16=False if device==\"cpu\" else True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to=['mlflow'],\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"matthews_correlation\", \n",
    "    save_total_limit=2, save_strategy=\"epoch\", seed=3242342, gradient_accumulation_steps=4, use_cpu=True if device==\"cpu\" else False) \n",
    "\n",
    "## The warmup step together with cosine learning rate scheduler turns to onecycle learning rate scheduler\n",
    "## weight decay for the Adam (AdamW) -> this is fast.Ai does\n",
    "## fp16 is half precision -> mixed training (using fp32 and fp16)\n",
    "## save_total_limit to 3 -> so only 3 models will be saved\n",
    "## each 500 steps will be saved a model\n",
    "## Save the report to mlflow\n",
    "# How to evaluate mlflow?\n",
    "# LR finder does not give reliable results for Transformers models https://github.com/huggingface/transformers/issues/16013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using several evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own function as an evaluation metric -> then you have to retun as an dict  \n",
    "Or you can use the evaluate library from hugging face to load different functions: [evaluate](https://huggingface.co/docs/evaluate/a_quick_tour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred):\n",
    "    metrics = [\"accuracy\", \"f1\", \"matthews_correlation\", \"precision\", \"recall\"]\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    loaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "    results = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "               for metric in metrics}\n",
    "\n",
    "    # the predictions from the models are logits (it also returns the labels, \n",
    "    # it also returns loss, attentions and hidden state but that is the classification model, for evalaution Trainer will only \n",
    "    # return logits and labels)\n",
    "    return results\n",
    "\n",
    "def compute_regression_metrics(eval_pred):\n",
    "\tmetrics = [\"mse\", \"mae\"]\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = logits\n",
    "\tloaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "\tresults = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "\t\t\t   for metric in metrics}\n",
    "\tresults[\"r2\"] = evaluate.load(\"r_squared\").compute(predictions=predictions, references=labels)\n",
    "\tresults[\"rmse\"] = loaded[\"mse\"].compute(predictions=predictions, references=labels, squared=False)[\"mse\"]\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        print(inputs)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.compute_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, args, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, compute_metrics=compute_classification_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters learning rate and batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8, 16]),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_classification_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_matthews_correlation\"]\n",
    "\n",
    "def compute_regression_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_r2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "model_init should have 0 or 1 argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we need to pass tokenized datasets\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_classification_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStoppingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:389\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;241m=\u001b[39m model_init\n\u001b[1;32m--> 389\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_model_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:1457\u001b[0m, in \u001b[0;36mTrainer.call_model_init\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init(trial)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should have 0 or 1 argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should not return None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: model_init should have 0 or 1 argument."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(None, args, model_init=model_init2, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, \n",
    "                  compute_metrics=compute_classification_metrics, \n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:27:53,211] A new study created in RDB with name: no-name-305c8b7d-b552-4cd6-b1d9-2e327a424486\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bff178a0fc74c9b9ffc182ef25172a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fd50fa88104f34832ff3bb22d0fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6917587518692017, 'eval_accuracy': 0.5666666666666667, 'eval_f1': 0.0, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 18.5898, 'eval_samples_per_second': 1.614, 'eval_steps_per_second': 0.807, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ddca6ef5484552b6fbd08fc014c515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7241240739822388, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 17.8967, 'eval_samples_per_second': 1.676, 'eval_steps_per_second': 0.838, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d8f83bc982433cbba6a578c3844d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7805582880973816, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 18.3333, 'eval_samples_per_second': 1.636, 'eval_steps_per_second': 0.818, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:36:15,996] Trial 0 finished with values: [0.7805582880973816, -0.008988968316207744] and parameters: {'learning_rate': 7.813286994811102e-05, 'gradient_accumulation_steps': 4}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 501.8248, 'train_samples_per_second': 0.933, 'train_steps_per_second': 0.231, 'train_loss': 0.6711522244859016, 'epoch': 2.97}\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "\tdirection=[\"minimize\", \"maximize\"],\n",
    "\tbackend=\"optuna\",\n",
    "\thp_space=optuna_hp_space,\n",
    "\tn_trials=1,\n",
    "\tcompute_objective=compute_classification_objective,\n",
    "    storage='sqlite:///my_optuna_studies.db',\n",
    "    load_if_exists=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.learning_rate = best_trials[0].hyperparameters[\"learning_rate\"]\n",
    "trainer.args.gradient_accumulation_steps = best_trials[0].hyperparameters[\"gradient_accumulation_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914d1c5b36242109903c985c86c6faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f5a46314b493cb66386ad59cc7861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7221236824989319, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.508, 'eval_samples_per_second': 0.952, 'eval_steps_per_second': 0.476, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e19a75be81b47138b51a3809d41f9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7303746938705444, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2142, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.481, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b5477268b8416aae340e6bca02f7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7445932030677795, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2265, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.48, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b70848b104408281ab0169afb06ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.753971517086029, 'eval_accuracy': 0.6, 'eval_f1': 0.7272727272727273, 'eval_matthews_correlation': 0.2857142857142857, 'eval_precision': 0.5714285714285714, 'eval_recall': 1.0, 'eval_runtime': 31.5365, 'eval_samples_per_second': 0.951, 'eval_steps_per_second': 0.476, 'epoch': 3.97}\n",
      "{'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=116, training_loss=0.6006371070598734, metrics={'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that we get the same results by evaluating the results once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for hyperparameters like the learning rate which is the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it is actually batch size and learning rate -> smaller batch sizes tend to work better than large batch sizes -> but learning rate is affected by batch as well -> higher abtch need higher learning rate.\n",
    "\n",
    "Fix everything else and tune the learning rate -> learning rate finder doesn'0t seem to work very well for transformers?  \n",
    "But teh idea of learning rate finder is just test different learning rates -> so I cannot test them?\n",
    "\n",
    "Ktrains: A wrapper to do many tasks and has a learning rate finder: [ktrains](https://github.com/amaiya/ktrain)\n",
    "\n",
    "Use pytorch lightning perhaps: [pytorch_lighningt_huggingface](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a552ef6cc74743f4b55e8d0d0bfe2327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b6497c18947b4a880757af2b28b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/44.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigscience/T0pp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\modeling_utils.py:3190\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3175\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3189\u001b[0m     }\n\u001b[1;32m-> 3190\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   3192\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3193\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3195\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    537\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model = AutoModel.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from lightning import LightningModule, LightningDataModule\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning import Trainer\n",
    "from torchmetrics.functional.classification import (\n",
    "    accuracy,\n",
    "    f1_score,\n",
    "    precision,\n",
    "    recall,\n",
    "    auroc,\n",
    "    average_precision,\n",
    "    cohen_kappa,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef\n",
    ") \n",
    "\n",
    "from torchmetrics.functional.regression import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    pearson_corrcoef,\n",
    "    kendall_rank_corrcoef,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_log_error)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from BioML.deep.train_config import LLMConfig\n",
    "from BioML.utilities import split_methods as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                     target: torch.tensor, num_classes: int=2, threshold: float=0.5):\n",
    "    task = \"binary\" if num_classes == 2 else \"multiclass\"\n",
    "    metrics = {\n",
    "                f\"{split}_Loss\": loss,\n",
    "                f\"{split}_Acc\": accuracy(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_F1\":f1_score(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Precision\": precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Recall\": recall(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\"\n",
    "                ),\n",
    "                f\"{split}_MCC\": matthews_corrcoef(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    threshold=threshold,\n",
    "                    task=task,\n",
    "                ),\n",
    "                f\"{split}_Confusion_Matrix\": confusion_matrix(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    normalize=\"true\",\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                ),\n",
    "                f\"{split}_AUROC\": auroc(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    thresholds=None,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Average_Precision\": average_precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Cohen_Kappa\": cohen_kappa(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                )}\n",
    "    return metrics\n",
    "\n",
    "def calculate_regression_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                 target: torch.tensor):\n",
    "    metrics = {f\"{split}_Loss\": loss,\n",
    "                f\"{split}_MAE\": mean_absolute_error(preds, target),\n",
    "                f\"{split}_MSE\": mean_squared_error(preds, target),\n",
    "                f\"{split}_RMSE\": mean_squared_error(preds, target, squared=False),\n",
    "                f\"{split}_R2\": r2_score(preds, target),\n",
    "                f\"{split}_Pearson\": pearson_corrcoef(preds, target),\n",
    "                f\"{split}_Kendall\": kendall_rank_corrcoef(preds, target),\n",
    "                f\"{split}_MAPE\": mean_absolute_percentage_error(preds, target),\n",
    "                f\"{split}_MSLE\": mean_squared_log_error(preds, target)}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values = (8, 16, 32, 64, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(x, y):\n",
    "    return x + y\n",
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.x = test\n",
    "        self.tes_ = partial(self.x, y=2)\n",
    "    def __call__(self, x):\n",
    "        return self.tes_(x)\n",
    "\n",
    "t = Test()\n",
    "t(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.esm.encoder.layer.0.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dropout']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_module_names_for_peft(model, filter_=\"key\"):\n",
    "    if isinstance(filter_, str):\n",
    "        filter_ = [filter_] # if it is a string, convert it to a list\n",
    "    module_names = []\n",
    "    for num, (name, module) in enumerate(model.named_modules()):\n",
    "        n = name.split(\".\")\n",
    "        if filter_ and set(n).intersection(filter_):\n",
    "            module_names.append(name)\n",
    "        elif not filter_:\n",
    "            module_names.append(name)\n",
    "    return module_names\n",
    "\n",
    "names = get_target_module_names_for_peft(model, filter_=\"output\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, replace_lora_weights_loftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1, \n",
    "                         target_modules=\"all-linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 285,144 || all params: 8,125,907 || trainable%: 3.509072894878073\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from BioML.deep import finetuning as ft\n",
    "from BioML.deep.utils import load_adapter\n",
    "from datasets import Dataset\n",
    "from Bio import SeqIO\n",
    "from safetensors import SafetensorError\n",
    "import torch\n",
    "from peft import replace_lora_weights_loftq, AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"../data/esterase_labels.csv\"\n",
    "lab = pd.read_csv(label, index_col=0)\n",
    "split_config = ft.SplitConfig()\n",
    "llm_config = ft.LLMConfig()\n",
    "train_config = ft.TrainConfig(2, batch_size=2, max_epochs=1, lora_rank=16, optimize=\"Val_MCC\")\n",
    "fasta_file = \"../data/whole_sequence.fasta\"\n",
    "label_regre = np.array(list(map(float, range(len(lab)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.target_modules = ['query', 'key', 'value', 'attention.output.dense']\n",
    "train_config.objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.TokenizeFasta(llm_config)\n",
    "data = tokenizer.tokenize(fasta_file, add_columns=[(\"labels\", lab.to_numpy().flatten())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 356,802 || all params: 8,197,565 || trainable%: 4.3525\n"
     ]
    }
   ],
   "source": [
    "splitter = ft.PrepareSplit(split_config.cluster_file, split_config.shuffle, split_config.random_seed, \n",
    "                            split_config.splitting_strategy, \n",
    "                            split_config.num_split, False)\n",
    "data_module = ft.DataModule(splitter, fasta_file, lab.to_numpy().flatten(), llm_config, train_config.batch_size)\n",
    "peft = ft.PreparePEFT(train_config, llm_config, \"pissa\")\n",
    "model = peft.prepare_model()\n",
    "light_mod = ft.TransformerModule(model, train_config, lr=1e-3)\n",
    "\n",
    "filename = f\"{{epoch}}-{{{train_config.optimize}:.2f}}\"\n",
    "checkpoint_callback = ft.ModelCheckpoint(filename=filename, monitor=train_config.optimize, \n",
    "                                              mode=train_config.optimize_mode, verbose=True, save_top_k=1)\n",
    "early_callback = ft.EarlyStopping(monitor=train_config.optimize, min_delta=train_config.min_delta, \n",
    "                                       patience=train_config.patience, verbose=True, mode=train_config.optimize_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 233,922 || all params: 8,074,685 || trainable%: 2.8970\n"
     ]
    }
   ],
   "source": [
    "peft = ft.PreparePEFT(train_config, llm_config)\n",
    "model2 = peft.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.7043e-01, -8.2647e-01,  2.4838e-02,  ...,  4.7189e-02,\n",
      "         -8.3786e-02,  2.5851e-02],\n",
      "        [-3.5719e-01,  2.8952e-02,  8.5006e-02,  ...,  5.7942e-02,\n",
      "          2.0922e-01, -2.6888e-01],\n",
      "        [ 7.5063e-02,  1.2485e-01,  6.2840e-02,  ..., -5.7912e-02,\n",
      "          8.7211e-02,  5.9190e-02],\n",
      "        ...,\n",
      "        [ 6.2899e-03,  2.6713e-02, -3.9605e-02,  ...,  1.0640e-01,\n",
      "          7.5123e-04, -5.9767e-02],\n",
      "        [ 4.4162e-02,  4.6904e-03,  8.3011e-02,  ...,  1.4559e-01,\n",
      "          2.7426e-02, -3.2084e-02],\n",
      "        [ 1.7775e-04, -1.3213e-02, -1.0379e-02,  ..., -9.3273e-02,\n",
      "          1.4039e-01, -3.1797e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0285, -0.0048, -0.0054,  ...,  0.0129, -0.0562,  0.0302],\n",
      "        [ 0.1384,  0.0217, -0.0380,  ...,  0.0082, -0.0462,  0.0579],\n",
      "        [ 0.1178,  0.0019,  0.0343,  ...,  0.0018,  0.0262, -0.0057],\n",
      "        ...,\n",
      "        [ 0.0227, -0.0180,  0.0703,  ...,  0.0152,  0.0150, -0.0309],\n",
      "        [-0.0003,  0.0125,  0.0585,  ..., -0.1054, -0.0656,  0.0119],\n",
      "        [ 0.0313, -0.0175, -0.0365,  ...,  0.0937, -0.1237, -0.0196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5873, 0.9400, 0.8834, 1.0011, 0.8351, 1.6801, 0.8591, 1.3857, 0.7457,\n",
      "        0.6151, 0.8057, 1.1133, 1.2734, 0.9145, 1.2189, 1.8857, 2.3503, 1.6414,\n",
      "        1.3544, 0.8303, 2.1570, 0.5538, 1.0793, 1.0558, 0.8334, 0.7435, 1.4016,\n",
      "        1.6589, 1.0253, 2.0550, 1.1909, 2.2155, 1.3714, 1.6381, 1.6177, 1.6315,\n",
      "        1.5257, 0.9768, 1.3578, 1.1207, 1.3782, 0.7250, 0.9063, 0.7317, 0.9410,\n",
      "        1.7056, 1.5990, 1.0967, 0.5155, 0.8857, 0.7932, 0.7848, 1.0841, 1.2641,\n",
      "        0.8809, 1.1028, 0.6627, 0.8378, 1.0029, 1.2287, 1.3782, 0.4884, 1.3836,\n",
      "        1.1725, 1.1586, 0.8844, 0.9265, 1.6501, 1.8563, 0.6485, 1.6144, 1.1125,\n",
      "        1.7792, 1.4165, 1.4069, 0.7714, 0.4669, 1.7467, 1.6306, 1.4739, 1.1572,\n",
      "        1.2335, 0.9374, 1.2641, 0.6478, 1.1517, 1.5467, 1.6535, 1.6447, 1.1364,\n",
      "        1.5612, 1.3593, 1.6656, 1.8102, 1.0880, 1.5107, 0.5529, 0.5377, 0.6984,\n",
      "        0.8919, 0.6094, 1.2246, 2.0509, 1.5282, 0.6284, 0.5584, 0.5203, 1.3957,\n",
      "        1.0501, 1.1043, 1.2537, 1.6188, 0.4961, 0.4438, 0.6898, 0.5104, 0.6881,\n",
      "        1.0709, 1.0670, 0.9157, 0.4899, 0.5834, 0.6328, 0.6624, 1.2185, 0.5718,\n",
      "        1.4091, 1.2455, 0.8632, 0.6179, 1.5918, 1.5366, 1.0506, 0.8979, 1.0382,\n",
      "        0.7717, 0.3410, 0.8559, 1.3751, 0.9877, 1.1721, 1.2376, 1.0866, 0.7256,\n",
      "        1.8115, 1.2615, 1.5351, 1.6051, 2.2750, 0.4583, 1.5959, 1.8159, 1.5986,\n",
      "        1.2386, 1.2219, 0.5143, 0.4552, 2.5652, 1.4507, 0.9394, 0.7338, 0.8319,\n",
      "        1.0903, 1.1077, 1.3615, 1.5297, 1.5645, 0.7571, 1.0886, 1.0412, 0.8878,\n",
      "        1.3356, 1.8191, 1.3875, 0.7934, 0.7322, 1.2949, 0.9306, 1.2011, 1.9200,\n",
      "        1.4277, 1.2990, 0.9323, 1.6592, 0.7322, 0.8158, 1.0060, 1.9004, 1.2868,\n",
      "        1.2296, 1.4783, 1.2853, 0.5865, 1.1468, 0.8820, 1.1706, 1.2369, 0.6446,\n",
      "        1.0860, 1.6630, 0.4944, 0.9224, 1.2387, 0.9313, 0.7568, 1.0396, 1.7756,\n",
      "        0.9973, 0.4420, 0.6338, 1.4626, 0.9706, 0.8584, 0.7334, 0.6792, 0.8190,\n",
      "        0.4998, 0.5863, 0.3591, 1.1064, 1.1684, 1.3195, 0.7124, 0.6288, 3.4058,\n",
      "        2.2894, 2.3039, 2.3184, 1.4553, 2.1919, 1.5340, 1.7372, 3.1814, 2.2748,\n",
      "        2.9761, 1.6589, 2.8641, 1.8701, 2.1067, 2.2280, 1.1328, 0.8959, 0.5703,\n",
      "        1.4181, 1.8598, 0.8204, 1.1057, 1.2013, 0.6950, 1.1179, 1.4333, 1.1852,\n",
      "        0.5071, 1.8742, 1.0409, 0.8568, 0.5709, 1.1396, 1.4776, 1.2673, 1.6857,\n",
      "        0.5480, 1.7507, 0.8236, 1.0195, 0.9362, 0.5892, 1.5747, 0.7228, 1.8882,\n",
      "        1.9171, 0.8700, 1.3362, 1.5765, 1.6774, 1.3801, 1.2409, 0.7780, 1.7569,\n",
      "        1.2493, 0.9076, 1.0342, 2.0558, 1.9531, 2.2741, 2.4913, 1.1082, 1.1169,\n",
      "        1.6298, 0.7658, 1.6727, 1.8809, 1.2403, 0.7066, 1.6247, 1.0856, 0.8546,\n",
      "        1.2991, 0.5652, 0.9862, 1.1118, 1.9982, 1.8534, 1.1159, 0.5336, 0.5210,\n",
      "        1.3930, 0.4745, 0.4598, 2.1504, 1.5160, 1.1264, 1.4919, 1.9552, 1.1071,\n",
      "        1.4930, 2.3807, 0.5510, 0.8693, 1.0496], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1005,  0.5498,  0.0729,  ..., -0.0110,  0.1209,  0.0658],\n",
      "        [ 0.0184,  0.0503,  0.0966,  ..., -0.0994, -0.3125, -0.1418],\n",
      "        [ 0.1038, -0.4246,  0.3924,  ...,  0.0704,  0.0173, -0.0551],\n",
      "        ...,\n",
      "        [-0.0880,  0.1564, -0.0328,  ...,  0.0017,  0.0048,  0.0033],\n",
      "        [ 0.2349,  0.0311, -0.0238,  ..., -0.0441, -0.0662,  0.0753],\n",
      "        [-0.1147,  0.0987, -0.0241,  ..., -0.0287,  0.1925, -0.0308]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0180, -0.0271, -0.0438,  ...,  0.0169,  0.0264, -0.0519],\n",
      "        [-0.0430,  0.0296,  0.2383,  ..., -0.0142,  0.0167,  0.0145],\n",
      "        [-0.0328,  0.0239, -0.0464,  ..., -0.1115, -0.0121, -0.0186],\n",
      "        ...,\n",
      "        [ 0.2671, -0.1343,  0.0173,  ...,  0.1614, -0.2601,  0.1452],\n",
      "        [ 0.0416, -0.0048, -0.0423,  ...,  0.0013,  0.0025, -0.0995],\n",
      "        [-0.0216,  0.0315, -0.0021,  ...,  0.0808, -0.0296, -0.0366]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8374, 1.5018, 1.3041, 1.5074, 1.3031, 1.5585, 0.8238, 1.3315, 1.1663,\n",
      "        1.3236, 1.5947, 1.8838, 1.3975, 1.2249, 1.2498, 1.8339, 2.2672, 1.5248,\n",
      "        1.4410, 1.9168, 1.6767, 1.8155, 1.1106, 1.0726, 1.4798, 1.1436, 2.3019,\n",
      "        2.0788, 2.0536, 1.5654, 1.1589, 2.2396, 2.3073, 2.3499, 2.3821, 1.6036,\n",
      "        1.5135, 1.2445, 1.7083, 1.1409, 1.3000, 1.9088, 1.5593, 1.8481, 1.3570,\n",
      "        1.7538, 1.8334, 1.1688, 0.9453, 0.7923, 1.3227, 1.8655, 1.6307, 1.5398,\n",
      "        0.9651, 1.2580, 1.0628, 1.5225, 1.7023, 1.2249, 2.1230, 1.0524, 1.3691,\n",
      "        1.7108, 1.7749, 0.9035, 1.7066, 1.9496, 1.9017, 1.7468, 1.6053, 1.0325,\n",
      "        1.2983, 2.1550, 2.0419, 1.7701, 1.3376, 1.3285, 1.7744, 1.8624, 2.0150,\n",
      "        2.3612, 1.5541, 1.7043, 1.5866, 1.2589, 2.0176, 1.9571, 2.3139, 1.5499,\n",
      "        2.3821, 1.8515, 1.2197, 1.7504, 0.8956, 2.0813, 0.8289, 1.1110, 1.2757,\n",
      "        1.0507, 1.2258, 1.6359, 1.9388, 1.8491, 0.9566, 0.9820, 1.1050, 1.5810,\n",
      "        1.4832, 1.6291, 1.5713, 1.6667, 0.5607, 0.8297, 0.9082, 0.8456, 1.2877,\n",
      "        1.8089, 1.1470, 0.7896, 0.6228, 0.7809, 1.1584, 1.6024, 1.8576, 1.0482,\n",
      "        1.3274, 1.8177, 1.4362, 0.8596, 1.2995, 1.3695, 0.6723, 0.6998, 1.0542,\n",
      "        0.9279, 0.4507, 1.1520, 1.2370, 0.8146, 0.7387, 0.5565, 1.1019, 0.6311,\n",
      "        2.1095, 1.7319, 1.9390, 1.9829, 1.4765, 2.6327, 1.5222, 1.8818, 2.4252,\n",
      "        2.2102, 2.2935, 1.9216, 2.1528, 2.0348, 1.5985, 0.9358, 1.5996, 1.8172,\n",
      "        1.8319, 1.7025, 1.9583, 1.5395, 2.1588, 0.7071, 1.4212, 1.4276, 1.6181,\n",
      "        2.2830, 1.7858, 1.7429, 1.0917, 0.7814, 1.2101, 1.5572, 2.1317, 2.1498,\n",
      "        1.7333, 1.5078, 1.2501, 1.9016, 1.8095, 1.5969, 1.4775, 2.0020, 1.7182,\n",
      "        1.5114, 1.2332, 1.5002, 0.7958, 1.5418, 1.1868, 1.3188, 1.7840, 1.0916,\n",
      "        1.1084, 2.1938, 0.9388, 0.8873, 1.4611, 1.7027, 1.4112, 1.5773, 2.0078,\n",
      "        1.3213, 0.9271, 0.8252, 1.7045, 1.1069, 1.0183, 0.6630, 0.8054, 0.8501,\n",
      "        0.8252, 0.9793, 0.4845, 1.2222, 1.2250, 0.7868, 0.6276, 0.8069, 2.9451,\n",
      "        2.7186, 2.8895, 1.7407, 2.3580, 1.3964, 1.4032, 1.6708, 2.2550, 1.8002,\n",
      "        1.8418, 2.2657, 1.4594, 2.2007, 2.0315, 1.8336, 2.1152, 1.6711, 1.5109,\n",
      "        1.6622, 1.5138, 1.3808, 1.3711, 1.3025, 0.9266, 1.8843, 2.3109, 1.9331,\n",
      "        1.7377, 1.9275, 0.9689, 0.9301, 1.3997, 0.8373, 1.8372, 1.6347, 1.6297,\n",
      "        1.2762, 1.4147, 0.7269, 0.9910, 1.6774, 1.1875, 1.5988, 1.5300, 1.6326,\n",
      "        2.1069, 0.9015, 1.7949, 1.8477, 1.1526, 1.5241, 1.7778, 2.7731, 1.5229,\n",
      "        1.3803, 0.9781, 1.4339, 2.6973, 2.2279, 2.0353, 1.5313, 1.2138, 1.0883,\n",
      "        2.2136, 2.0958, 1.8546, 2.1536, 1.4123, 1.1453, 1.7430, 1.1563, 2.0697,\n",
      "        1.8661, 2.2181, 1.8846, 1.6379, 1.9485, 1.9344, 1.2116, 2.1902, 1.7793,\n",
      "        2.1231, 1.5778, 2.5312, 1.8458, 1.9622, 1.5202, 1.7185, 2.9070, 1.7154,\n",
      "        1.6991, 1.5724, 2.1885, 0.9060, 0.9609], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1424, -0.0069,  0.2519,  ...,  0.0814, -0.1329, -0.0936],\n",
      "        [ 0.1260,  0.0035,  0.1406,  ...,  0.0060, -0.0241, -0.1546],\n",
      "        [ 0.0470,  0.0016,  0.0926,  ..., -0.0814, -0.0205, -0.0501],\n",
      "        ...,\n",
      "        [-0.0585,  0.0051, -0.0574,  ...,  0.0768, -0.0093, -0.0917],\n",
      "        [-0.0364, -0.0134,  0.0891,  ..., -0.1541,  0.0377, -0.0778],\n",
      "        [-0.1622,  0.0015, -0.0331,  ...,  0.0067, -0.0211,  0.1209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0765,  0.0980,  0.1722,  ..., -0.0251, -0.0698, -0.1424],\n",
      "        [ 0.1300, -0.0205, -0.1334,  ..., -0.0760,  0.1092,  0.0228],\n",
      "        [ 0.0560,  0.0779, -0.1371,  ..., -0.1142,  0.0742, -0.0733],\n",
      "        ...,\n",
      "        [ 0.0053,  0.0530,  0.0346,  ..., -0.0580,  0.0288, -0.1748],\n",
      "        [-0.0745,  0.0186,  0.1499,  ..., -0.0532, -0.0884, -0.0369],\n",
      "        [ 0.2171,  0.1043,  0.0594,  ...,  0.0359, -0.0330,  0.0201]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4927, 1.6146, 1.4119, 1.6148, 1.3547, 1.5420, 1.5957, 1.3482, 1.5070,\n",
      "        1.4946, 1.3155, 1.4255, 1.4958, 1.5160, 1.4856, 1.3993, 0.9258, 1.0924,\n",
      "        1.0338, 1.1353, 0.8695, 1.0913, 1.2008, 0.3927, 1.1997, 0.8729, 1.2249,\n",
      "        0.9813, 1.2411, 0.9379, 1.1826, 1.1731, 0.9171, 1.1147, 1.1085, 1.0160,\n",
      "        1.0529, 1.1061, 1.0714, 1.0326, 0.9978, 0.9595, 0.9963, 1.1957, 1.1058,\n",
      "        1.1943, 1.0503, 0.9583, 1.5632, 1.6116, 1.4257, 1.1160, 1.5282, 1.3789,\n",
      "        1.5325, 1.6005, 0.9923, 1.4162, 1.2519, 1.7454, 1.5428, 1.5050, 1.0535,\n",
      "        1.3630, 0.9217, 1.2765, 1.1505, 1.1329, 1.1758, 1.2591, 1.3913, 1.1450,\n",
      "        1.2662, 1.2610, 1.1588, 1.3159, 1.2937, 1.2024, 1.2959, 1.2083, 0.9873,\n",
      "        1.0030, 1.2463, 1.0519, 1.1947, 1.1835, 1.2502, 1.1807, 1.2851, 1.0200,\n",
      "        1.1974, 1.1204, 1.1151, 1.0728, 1.0995, 1.1339, 1.6850, 1.6453, 1.4477,\n",
      "        1.5283, 1.4608, 1.5123, 1.7606, 1.6790, 1.4536, 1.3615, 1.0539, 1.6766,\n",
      "        1.5956, 1.2666, 1.4661, 1.6749, 1.7290, 1.8016, 1.6522, 1.8008, 2.0479,\n",
      "        1.8339, 1.9653, 2.1335, 1.6599, 1.8508, 1.6515, 1.7320, 1.6060, 1.7662,\n",
      "        1.7140, 2.0657, 1.0041, 1.1502, 1.2057, 1.1104, 1.2182, 1.4887, 1.1868,\n",
      "        1.5332, 1.1300, 1.4410, 1.3467, 1.2089, 1.1034, 1.2099, 1.1175, 1.4389,\n",
      "        1.1561, 1.0273, 1.0741, 1.1234, 1.1185, 1.0851, 1.1877, 1.2212, 1.0896,\n",
      "        1.1967, 1.2134, 1.0944, 1.1203, 0.9993, 1.0941, 1.0678, 1.2287, 1.4793,\n",
      "        1.1623, 1.2790, 1.3448, 1.2633, 1.3038, 1.2589, 1.5259, 1.6493, 1.3567,\n",
      "        1.3565, 1.2454, 1.1212, 1.2549, 1.1834, 1.0260, 1.0332, 0.8878, 1.0279,\n",
      "        0.9775, 1.0704, 1.3009, 1.0478, 0.8605, 0.9796, 1.2201, 1.0061, 1.1037,\n",
      "        0.9611, 1.1367, 1.1465, 1.7780, 1.3971, 0.8716, 1.3058, 1.8646, 1.2762,\n",
      "        1.3038, 1.6715, 1.3641, 1.7771, 1.7013, 1.9484, 1.6952, 1.7031, 1.7976,\n",
      "        1.6869, 1.2911, 1.2696, 1.3963, 1.4774, 1.3635, 1.5648, 1.2879, 1.3984,\n",
      "        1.5977, 1.5329, 1.3327, 1.4316, 1.2458, 1.5834, 1.1738, 1.1879, 0.7350,\n",
      "        0.8385, 0.6717, 0.7792, 0.7465, 0.6887, 0.9211, 0.7858, 0.8819, 0.9442,\n",
      "        0.7440, 0.5943, 0.7240, 0.7652, 0.7081, 0.6787, 1.1885, 1.2643, 1.2546,\n",
      "        1.3681, 1.0384, 1.2648, 1.2786, 1.2049, 1.3213, 1.1544, 1.2183, 1.2704,\n",
      "        1.3843, 1.3557, 1.2026, 1.1787, 1.3437, 1.2421, 1.3807, 1.3408, 1.2601,\n",
      "        1.2891, 1.8066, 1.2268, 1.3248, 1.2082, 1.3135, 1.3192, 1.5398, 1.4319,\n",
      "        1.3836, 1.1218, 0.8959, 0.7929, 0.9015, 1.0226, 0.9853, 0.9635, 1.0151,\n",
      "        0.9750, 0.9321, 0.9203, 0.9476, 1.0982, 0.9120, 0.9769, 0.9376, 0.8773,\n",
      "        1.0452, 1.0148, 0.9932, 1.0126, 1.0655, 0.9017, 0.9699, 0.9783, 0.9918,\n",
      "        1.0670, 0.9244, 1.0514, 1.1607, 1.0265, 1.1332, 0.9821, 1.1537, 1.0887,\n",
      "        1.0461, 1.0413, 1.0952, 1.2016, 1.2325, 1.1638, 1.0600, 1.1822, 1.1641,\n",
      "        1.0915, 1.2743, 1.1293, 1.2005, 1.1988], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0240, -0.0543,  0.0521,  ..., -0.0150, -0.0552,  0.0442],\n",
      "        [ 0.0177, -0.0445, -0.1237,  ...,  0.0905,  0.0007,  0.0203],\n",
      "        [-0.0405,  0.0197,  0.0028,  ...,  0.0454,  0.0579,  0.0052],\n",
      "        ...,\n",
      "        [ 0.0658,  0.0374, -0.1228,  ...,  0.0194,  0.0028, -0.0704],\n",
      "        [-0.0010,  0.2788,  0.0367,  ..., -0.0402, -0.1133, -0.0133],\n",
      "        [-0.0667, -0.0574, -0.0855,  ...,  0.0544,  0.0049, -0.0901]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0671,  0.0215,  0.0105,  ...,  0.0306,  0.0409, -0.0089],\n",
      "        [ 0.0036, -0.0268, -0.0548,  ..., -0.0542, -0.1474, -0.0509],\n",
      "        [-0.0697, -0.0670, -0.0350,  ...,  0.0679,  0.0767,  0.0862],\n",
      "        ...,\n",
      "        [-0.0031,  0.0227, -0.0127,  ...,  0.0467,  0.0503, -0.0179],\n",
      "        [-0.0062, -0.0149,  0.0577,  ..., -0.1676,  0.1078, -0.0591],\n",
      "        [-0.0316, -0.0409, -0.0787,  ...,  0.0371,  0.1069, -0.0039]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3747, 1.3894, 1.4854, 1.5076, 1.4498, 1.5789, 1.4099, 1.4072, 1.3064,\n",
      "        1.4274, 1.3434, 1.5063, 1.5148, 1.3795, 1.3653, 1.3729, 1.4341, 1.3506,\n",
      "        1.4613, 1.3473, 1.3978, 1.4063, 1.4204, 1.3573, 1.4028, 1.4448, 1.4522,\n",
      "        1.3809, 1.4116, 1.4559, 1.3596, 1.3025, 1.4531, 1.2708, 1.3587, 1.3878,\n",
      "        1.4148, 1.3787, 1.4404, 1.4551, 1.4135, 1.4298, 1.3495, 1.4463, 1.4323,\n",
      "        1.4360, 1.8061, 1.3503, 1.5649, 1.4131, 1.4355, 1.3738, 1.4393, 1.5773,\n",
      "        1.3801, 1.2495, 1.3841, 1.4282, 1.3481, 1.3407, 1.4928, 1.6891, 1.2522,\n",
      "        1.5080, 1.3808, 1.3823, 1.4502, 1.4289, 1.3966, 1.3128, 1.4076, 1.4644,\n",
      "        1.3594, 1.5224, 1.5123, 1.4181, 1.4212, 1.3225, 1.3818, 1.4714, 1.3919,\n",
      "        1.3776, 1.3128, 1.4218, 1.4323, 1.5548, 1.3659, 1.4452, 1.3946, 1.4262,\n",
      "        1.4827, 1.3974, 1.4582, 1.4218, 1.3798, 1.4249, 1.2987, 1.4325, 1.3801,\n",
      "        1.3422, 1.3673, 1.4486, 1.3536, 1.2743, 1.5162, 1.3325, 1.5177, 1.4820,\n",
      "        1.4218, 1.3542, 1.3831, 1.6063, 1.4634, 1.4684, 1.5416, 1.4143, 1.3137,\n",
      "        1.4473, 1.3444, 1.5344, 1.5863, 1.4146, 1.3642, 1.3577, 1.3317, 1.4611,\n",
      "        1.3587, 1.4405, 1.3654, 1.3447, 1.4065, 1.5712, 1.4418, 1.4660, 1.3712,\n",
      "        1.4646, 1.3927, 1.3188, 1.3750, 1.6459, 1.3480, 1.5365, 1.3741, 1.3423,\n",
      "        1.1471, 1.3893, 1.3614, 1.5097, 1.3817, 1.3700, 1.4585, 1.5309, 1.5796,\n",
      "        1.3792, 1.3830, 1.3127, 1.4157, 1.3674, 1.4844, 1.3998, 1.3517, 1.4317,\n",
      "        1.4065, 1.4833, 1.4850, 1.4964, 1.5006, 1.4470, 1.3830, 1.3691, 1.4570,\n",
      "        1.5999, 1.4686, 1.3846, 1.4037, 1.4579, 1.3285, 1.3512, 1.3857, 1.4283,\n",
      "        1.3920, 1.5174, 1.3553, 1.3516, 1.4557, 1.3244, 1.4279, 1.2325, 1.4418,\n",
      "        1.3500, 1.3720, 1.3385, 1.4989, 0.9617, 1.3869, 1.3330, 1.3944, 1.3058,\n",
      "        1.3582, 1.3430, 1.4276, 1.3860, 1.3119, 1.3375, 1.3986, 1.4417, 1.3726,\n",
      "        1.3996, 1.4161, 1.4537, 1.3882, 1.3350, 1.3058, 1.4176, 1.4913, 1.4282,\n",
      "        1.3890, 1.4206, 1.3563, 1.3812, 1.4005, 1.3811, 1.4505, 1.3248, 1.3475,\n",
      "        1.4196, 1.3802, 1.4666, 1.3961, 1.4665, 1.4254, 1.3331, 1.4709, 1.3260,\n",
      "        1.4457, 1.4008, 1.4648, 1.3595, 1.4141, 1.4384, 1.4431, 1.4605, 1.5257,\n",
      "        1.4278, 1.4500, 1.3984, 1.3878, 1.4334, 1.4737, 1.3175, 1.3484, 1.4413,\n",
      "        1.4654, 1.4015, 1.3200, 1.4629, 1.4865, 1.6980, 1.4436, 1.3686, 1.4155,\n",
      "        1.4526, 1.4338, 1.3436, 1.4876, 1.4125, 1.5029, 1.4164, 1.4158, 1.4855,\n",
      "        1.2872, 1.2392, 1.4545, 1.3489, 1.2877, 1.3852, 1.4283, 1.4458, 1.4448,\n",
      "        1.3937, 1.4512, 1.4746, 1.3522, 1.6366, 1.4459, 1.2592, 1.2784, 1.5182,\n",
      "        1.3845, 1.5055, 1.3924, 1.7714, 1.3866, 1.4244, 1.4894, 1.3473, 1.2799,\n",
      "        1.3888, 1.4305, 1.4076, 1.3896, 1.4325, 1.3978, 1.3426, 1.5426, 1.4078,\n",
      "        1.3793, 1.4331, 1.5409, 1.3797, 1.4806, 1.4996, 1.4277, 1.4412, 1.5105,\n",
      "        1.4543, 1.3805, 1.3937, 1.3924, 1.3659], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.7003e-01,  5.2951e-02, -1.2185e-01,  ...,  1.6899e-01,\n",
      "         -1.6760e-01,  6.4166e-01],\n",
      "        [ 9.1949e-02, -2.4223e-02, -9.0878e-02,  ...,  1.2673e-01,\n",
      "         -1.6266e-02,  7.7130e-02],\n",
      "        [-1.1728e-01, -5.2174e-02, -1.0652e-01,  ..., -2.3536e-01,\n",
      "         -2.0896e-02, -6.1848e-04],\n",
      "        ...,\n",
      "        [-1.5016e-02, -2.1608e-02,  4.3276e-02,  ..., -4.3280e-02,\n",
      "          1.7819e-01,  1.4624e-03],\n",
      "        [ 5.3831e-02,  8.2512e-02,  6.2805e-02,  ..., -4.7947e-02,\n",
      "         -1.5457e-01,  5.5668e-03],\n",
      "        [ 5.6542e-02, -1.4761e-01,  5.1521e-03,  ..., -9.4848e-02,\n",
      "          2.7665e-02,  1.6270e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0140,  0.0227,  0.0292,  ..., -0.0078,  0.0373, -0.0193],\n",
      "        [-0.1493, -0.1240, -0.0335,  ..., -0.0307,  0.0295,  0.0436],\n",
      "        [-0.1208, -0.1387, -0.0054,  ...,  0.0378, -0.0256,  0.0409],\n",
      "        ...,\n",
      "        [-0.0766, -0.0316, -0.1350,  ...,  0.0253, -0.0189,  0.1318],\n",
      "        [-0.0207,  0.1333,  0.1310,  ..., -0.0532, -0.0373, -0.0041],\n",
      "        [ 0.0695, -0.1402, -0.1067,  ..., -0.0060, -0.0811,  0.0301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3228, 1.7167, 1.7892, 1.6899, 2.5000, 3.0245, 2.9065, 2.6829, 2.2675,\n",
      "        1.9312, 1.9584, 2.6223, 2.1600, 2.3743, 2.8331, 2.5622, 3.4464, 2.4802,\n",
      "        1.9317, 2.3686, 2.5087, 2.6261, 3.1150, 3.6424, 1.8733, 2.0167, 2.8550,\n",
      "        2.0079, 2.3863, 2.5467, 3.1784, 3.1810, 2.4409, 2.1004, 2.8212, 2.7785,\n",
      "        2.9947, 2.3737, 3.3397, 3.9800, 1.9109, 2.1063, 2.1877, 2.5391, 2.8875,\n",
      "        2.8831, 3.3345, 3.0384, 1.3548, 1.9214, 1.0094, 1.3006, 2.2433, 2.4186,\n",
      "        2.7794, 2.6735, 3.4520, 1.8977, 2.6700, 2.7093, 2.5843, 2.4909, 3.0235,\n",
      "        2.8959, 1.9297, 2.0115, 3.3876, 2.7099, 2.5087, 3.1259, 2.7278, 3.3231,\n",
      "        2.3439, 1.2477, 1.0489, 1.5469, 2.0124, 2.4396, 3.1604, 3.0250, 2.3145,\n",
      "        2.3602, 2.9603, 2.9110, 2.8074, 3.2007, 3.3319, 3.4903, 2.2887, 2.4577,\n",
      "        2.4752, 2.6465, 2.7445, 2.6685, 3.3151, 2.8944, 2.4961, 2.3645, 3.4088,\n",
      "        3.8269, 3.7414, 2.3287, 2.3481, 3.7498, 2.6456, 3.0459, 3.8318, 3.6100,\n",
      "        3.0284, 3.8170, 3.1901, 4.1504, 1.3845, 1.2867, 2.6492, 2.5096, 2.4507,\n",
      "        2.4978, 2.7566, 2.6403, 1.8373, 2.5612, 1.9942, 1.9798, 2.1954, 2.5927,\n",
      "        2.7389, 3.2072, 1.9173, 1.9458, 2.1176, 1.8394, 2.3602, 2.8897, 2.8006,\n",
      "        2.8387, 2.0002, 1.8972, 2.4731, 3.4814, 2.5468, 2.5005, 2.8950, 2.8635,\n",
      "        3.2978, 1.9449, 2.6062, 2.1491, 2.3243, 2.2584, 2.9874, 2.8292, 1.0420,\n",
      "        1.5749, 1.8214, 2.1740, 2.2250, 2.9403, 2.9848, 3.2168, 1.9528, 2.3546,\n",
      "        2.0247, 2.2130, 2.3638, 2.9385, 2.8415, 2.9179, 3.0707, 2.0531, 2.6329,\n",
      "        2.3713, 2.7022, 2.5385, 2.9941, 3.3606, 1.4401, 1.4543, 2.3839, 1.6381,\n",
      "        1.9803, 2.7022, 2.4293, 2.7150, 1.2429, 1.5818, 1.4417, 2.3670, 2.0308,\n",
      "        2.1258, 2.5203, 2.7178, 1.2981, 1.7122, 1.7755, 1.4587, 2.0367, 1.9014,\n",
      "        2.8426, 2.6285, 2.2243, 2.0749, 2.1893, 2.6212, 2.0866, 2.2924, 2.8065,\n",
      "        2.8627, 2.6159, 2.3010, 2.3303, 3.2447, 3.3884, 1.9804, 3.0135, 3.1311,\n",
      "        1.8345, 3.0779, 3.6628, 4.3906, 3.5270, 4.1559, 3.2852, 3.7826, 1.3692,\n",
      "        1.4752, 2.5798, 1.3959, 1.7982, 2.1479, 2.4733, 3.5554, 4.4083, 2.1676,\n",
      "        1.3069, 2.3187, 2.1601, 3.0743, 2.5531, 3.0497, 2.1873, 2.4900, 2.1923,\n",
      "        3.3045, 2.0328, 2.6299, 3.0715, 3.0616, 2.2864, 1.9831, 2.0289, 1.3874,\n",
      "        2.7274, 2.5959, 2.9740, 2.9868, 3.0506, 2.1440, 2.2682, 1.9939, 2.1247,\n",
      "        2.3379, 3.3555, 3.0993, 2.5158, 2.4700, 2.3264, 3.0261, 2.7810, 2.2113,\n",
      "        3.4599, 2.4841, 1.9812, 2.4335, 2.9191, 2.3438, 1.9682, 2.5484, 3.3898,\n",
      "        3.3511, 2.0106, 2.7205, 2.9787, 3.0968, 3.1368, 2.6210, 3.1475, 3.0286,\n",
      "        2.1818, 3.0290, 2.9021, 3.2458, 2.1815, 3.1650, 3.3631, 3.4817, 2.8906,\n",
      "        3.4028, 2.8311, 2.3375, 3.2507, 2.4482, 3.8870, 3.3289, 1.4857, 2.1173,\n",
      "        2.3755, 2.0315, 2.0787, 2.4021, 2.4472, 2.8881, 1.6336, 1.4086, 1.4457,\n",
      "        2.7128, 2.0880, 2.6069, 2.5944, 2.6837], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0261, -0.0791,  0.0779,  ..., -0.0908,  0.0643, -0.8742],\n",
      "        [-0.0095,  0.0145, -0.0314,  ..., -0.1198,  0.1143, -0.0831],\n",
      "        [ 0.0731,  0.0915, -0.0615,  ...,  0.0194, -0.1439, -0.0835],\n",
      "        ...,\n",
      "        [-0.0482,  0.0060, -0.0387,  ..., -0.0089,  0.1657, -0.0550],\n",
      "        [-0.0996, -0.0224,  0.0575,  ...,  0.1377,  0.0201,  0.0148],\n",
      "        [-0.0543, -0.1113,  0.1015,  ..., -0.0554,  0.0636, -0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0595,  0.0690, -0.0912,  ...,  0.1025, -0.0276,  0.0540],\n",
      "        [-0.1470,  0.1043, -0.1274,  ..., -0.1458, -0.0419, -0.0748],\n",
      "        [ 0.2249, -0.0237,  0.0069,  ...,  0.0293,  0.0029, -0.0273],\n",
      "        ...,\n",
      "        [ 0.1627, -0.0626,  0.0183,  ..., -0.0717, -0.0676, -0.0076],\n",
      "        [ 0.1088,  0.0133,  0.0367,  ...,  0.0800,  0.1615, -0.0504],\n",
      "        [-0.0880, -0.0830, -0.0804,  ..., -0.0791,  0.0564,  0.0468]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9286, 1.6597, 1.4979, 3.0950, 1.3135, 1.9884, 2.4981, 2.6347, 1.9370,\n",
      "        2.2319, 2.5965, 1.2604, 2.9126, 2.0885, 2.8187, 2.0993, 1.6835, 2.0353,\n",
      "        2.7469, 1.7892, 1.7414, 1.7080, 3.2592, 3.1716, 1.9322, 2.2077, 1.6041,\n",
      "        1.6927, 3.4304, 3.6145, 3.4989, 3.0751, 2.6691, 1.9014, 2.4965, 2.6552,\n",
      "        2.2616, 1.9448, 2.1471, 2.9531, 1.6711, 2.2426, 2.5966, 2.2534, 2.1863,\n",
      "        2.2688, 2.4121, 2.3003, 2.6514, 1.6639, 1.4983, 1.9049, 2.9210, 1.3303,\n",
      "        2.4300, 2.5021, 1.5209, 2.2663, 1.7641, 1.4543, 1.2990, 3.0472, 2.6548,\n",
      "        2.4604, 1.7938, 1.7288, 1.3517, 1.2307, 1.4785, 1.7055, 2.6775, 2.0548,\n",
      "        1.9992, 1.5352, 1.6418, 2.5871, 1.7270, 2.8056, 2.5010, 2.0641, 2.4760,\n",
      "        2.5371, 2.7687, 2.3909, 2.3907, 2.4419, 3.2883, 3.1403, 1.7158, 2.2924,\n",
      "        2.2236, 2.5198, 2.2084, 2.2473, 3.1399, 2.6018, 2.4450, 2.6955, 3.1731,\n",
      "        3.2542, 2.8014, 3.6047, 3.4755, 3.3723, 2.8549, 3.3023, 3.0562, 3.1537,\n",
      "        2.9184, 1.5708, 2.2127, 3.5159, 1.2727, 2.1176, 1.5690, 1.4803, 1.4221,\n",
      "        2.8364, 2.7163, 2.7532, 1.6368, 1.5102, 1.9866, 2.4181, 2.6728, 1.4938,\n",
      "        2.6703, 2.2478, 2.1770, 2.2326, 1.8161, 3.1776, 1.6185, 1.5432, 2.6151,\n",
      "        2.5157, 1.9495, 1.8913, 2.9080, 1.5766, 1.7408, 3.0165, 2.5628, 2.7255,\n",
      "        2.0530, 2.2424, 1.5414, 1.6124, 1.3750, 3.0514, 2.5983, 2.6611, 1.3656,\n",
      "        1.5337, 2.7956, 1.6592, 3.0017, 1.6040, 2.7046, 2.2361, 2.1649, 2.5139,\n",
      "        2.9802, 3.0001, 3.2523, 1.8604, 3.2542, 3.0079, 2.2947, 2.0338, 1.6555,\n",
      "        1.8088, 1.7434, 3.2782, 3.3688, 3.0948, 1.2517, 1.4562, 1.4179, 2.6662,\n",
      "        1.2921, 1.4032, 2.2301, 2.1944, 1.6668, 1.8524, 2.1097, 1.1742, 1.5424,\n",
      "        2.3609, 2.4085, 1.9309, 1.9570, 1.8866, 1.5751, 3.1065, 1.4607, 1.4751,\n",
      "        2.6641, 2.7696, 2.4266, 2.2500, 1.4690, 1.3197, 2.9568, 3.0756, 2.5313,\n",
      "        2.8138, 1.8780, 2.7134, 3.3902, 3.1343, 2.8007, 3.1773, 2.5950, 4.0254,\n",
      "        2.3379, 2.0384, 2.2772, 2.7928, 2.7757, 1.8408, 3.9358, 3.9438, 1.3422,\n",
      "        1.9955, 1.2537, 2.1084, 2.7412, 2.6115, 2.2369, 2.0776, 1.2937, 1.6644,\n",
      "        1.2519, 1.2760, 1.2236, 1.5240, 2.2907, 2.0159, 2.3497, 2.1433, 1.9996,\n",
      "        1.6093, 3.1090, 1.6821, 2.7933, 2.8005, 2.0622, 2.2073, 2.0933, 2.1428,\n",
      "        1.3878, 2.3520, 2.6652, 2.8492, 2.4219, 2.2370, 2.2071, 2.1815, 2.8091,\n",
      "        2.1777, 3.0069, 2.5210, 2.8037, 2.3782, 2.2917, 2.2170, 1.9472, 2.0228,\n",
      "        2.9038, 2.3139, 1.7494, 2.7830, 2.7074, 2.4657, 3.2157, 2.1404, 3.1377,\n",
      "        3.2406, 2.0065, 2.3585, 2.4612, 2.5304, 1.8208, 2.0458, 3.2857, 3.0259,\n",
      "        2.7799, 2.9550, 2.6324, 2.9411, 3.7359, 2.2172, 3.3163, 3.2191, 2.3201,\n",
      "        2.7323, 2.6764, 2.2569, 1.7368, 2.4221, 3.0890, 3.4691, 1.7528, 1.9669,\n",
      "        1.2395, 2.7236, 2.6974, 1.9431, 2.4018, 2.0678, 1.7160, 1.5182, 2.6530,\n",
      "        1.3027, 1.1570, 2.1144, 2.2997, 2.4038], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0302,  0.0437,  0.1214,  ...,  0.0126,  0.0679, -0.0008],\n",
      "        [-0.1016,  0.0215, -0.0513,  ..., -0.0735, -0.0524, -0.0078],\n",
      "        [ 0.0997,  0.0311,  0.1339,  ...,  0.0097, -0.0245,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0551, -0.0566, -0.0324,  ..., -0.0236,  0.0844, -0.0093],\n",
      "        [ 0.0892,  0.2403, -0.0423,  ...,  0.1555,  0.0503, -0.0009],\n",
      "        [ 0.0319,  0.0144,  0.0019,  ..., -0.0038,  0.0024, -0.0006]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0196, -0.0285,  0.0052,  ...,  0.0950, -0.0904,  0.0049],\n",
      "        [-0.0085, -0.0143,  0.1247,  ..., -0.0727,  0.0395,  0.0079],\n",
      "        [-0.1056,  0.0185,  0.0115,  ...,  0.0196, -0.1344, -0.0975],\n",
      "        ...,\n",
      "        [-0.0363, -0.0705,  0.0230,  ...,  0.0657, -0.1139,  0.0757],\n",
      "        [-0.0149,  0.0176, -0.0516,  ..., -0.0228, -0.0085,  0.0473],\n",
      "        [ 0.0056,  0.1576, -0.1515,  ..., -0.0367, -0.0122, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4993, 1.3476, 1.4469, 1.4356, 1.3921, 1.4253, 1.3475, 1.3992, 1.4736,\n",
      "        1.4392, 1.4102, 1.4343, 1.4128, 1.4302, 1.4738, 1.4424, 1.1952, 1.2019,\n",
      "        1.1851, 1.2134, 1.1942, 1.1493, 1.1944, 1.1528, 1.2793, 1.1387, 1.1779,\n",
      "        1.0983, 1.1763, 1.2063, 1.1155, 1.2548, 1.8259, 1.9658, 1.9917, 1.9290,\n",
      "        2.1091, 2.1928, 2.1768, 1.9590, 2.1324, 2.0345, 1.9358, 2.1147, 2.0026,\n",
      "        1.9803, 1.9682, 2.1186, 1.3952, 1.5002, 1.3788, 1.4128, 1.4860, 1.5012,\n",
      "        1.3699, 1.5264, 1.4839, 1.4494, 1.4857, 1.4571, 1.4977, 1.5067, 1.4853,\n",
      "        1.4155, 1.2766, 1.2417, 1.2617, 1.2971, 1.3214, 1.3286, 1.2780, 1.3287,\n",
      "        1.2603, 1.2176, 1.3094, 1.2633, 1.2080, 1.2647, 1.2592, 1.1700, 2.0403,\n",
      "        2.1187, 2.0816, 1.9920, 2.1230, 2.1298, 2.1993, 2.1632, 2.2220, 2.1032,\n",
      "        2.0196, 2.1157, 2.0299, 2.1863, 2.0824, 1.9219, 1.6350, 1.7297, 1.7399,\n",
      "        1.7099, 1.7632, 1.7662, 1.7406, 1.7121, 1.6636, 1.6426, 1.7014, 1.7518,\n",
      "        1.7105, 1.6557, 1.6358, 1.7484, 1.2066, 1.2424, 1.2631, 1.2280, 1.2606,\n",
      "        1.3033, 1.2010, 1.2022, 1.3080, 1.2521, 1.2003, 1.2689, 1.3170, 1.2624,\n",
      "        1.2587, 1.2285, 1.6081, 1.4722, 1.5109, 1.5398, 1.5213, 1.4310, 1.5557,\n",
      "        1.5547, 1.5511, 1.4765, 1.5584, 1.4371, 1.5441, 1.5903, 1.5369, 1.4705,\n",
      "        1.2628, 1.2742, 1.1872, 1.2943, 1.2327, 1.2563, 1.2880, 1.2513, 1.2882,\n",
      "        1.3168, 1.1640, 1.3027, 1.2704, 1.2639, 1.2473, 1.2138, 1.1424, 1.2293,\n",
      "        1.3036, 1.1335, 1.1833, 1.1583, 1.2376, 1.1815, 1.1947, 1.1560, 1.1682,\n",
      "        1.2242, 1.2274, 1.3154, 1.1884, 1.2220, 1.4578, 1.4571, 1.4496, 1.4295,\n",
      "        1.4834, 1.4492, 1.4315, 1.4714, 1.4800, 1.4078, 1.5120, 1.4859, 1.4432,\n",
      "        1.4355, 1.4302, 1.5152, 1.5692, 1.5594, 1.6091, 1.6369, 1.5520, 1.5656,\n",
      "        1.5105, 1.5801, 1.5916, 1.6341, 1.5665, 1.5994, 1.5867, 1.5837, 1.5863,\n",
      "        1.5473, 1.4517, 1.4992, 1.5409, 1.5012, 1.4251, 1.5205, 1.5316, 1.5059,\n",
      "        1.6549, 1.5527, 1.5018, 1.4358, 1.5477, 1.4191, 1.5175, 1.5237, 1.3556,\n",
      "        1.3576, 1.3311, 1.3826, 1.3560, 1.4141, 1.4361, 1.4116, 1.3973, 1.4189,\n",
      "        1.4681, 1.3850, 1.3540, 1.3955, 1.3962, 1.3742, 1.6136, 1.6316, 1.6667,\n",
      "        1.6941, 1.7123, 1.6411, 1.6649, 1.5852, 1.6681, 1.6766, 1.7108, 1.6858,\n",
      "        1.6217, 1.7520, 1.6430, 1.6982, 1.8724, 1.8083, 1.7239, 1.7069, 1.7412,\n",
      "        1.8341, 1.7948, 1.8121, 1.8127, 1.8227, 1.8570, 1.8260, 1.8708, 1.7828,\n",
      "        1.6911, 1.7559, 1.7640, 1.7519, 1.9516, 1.8071, 1.8303, 1.8054, 1.8920,\n",
      "        1.7751, 1.7910, 1.7499, 1.8676, 1.7230, 1.7257, 1.8327, 1.8592, 1.7925,\n",
      "        1.7741, 1.8035, 1.8036, 1.8015, 1.8489, 1.7842, 1.7836, 1.8231, 1.8144,\n",
      "        1.6899, 1.7658, 1.8264, 1.7925, 1.8126, 1.7412, 1.8792, 1.4864, 1.4154,\n",
      "        1.4228, 1.3774, 1.4762, 1.3867, 1.4033, 1.3672, 1.4363, 1.3737, 1.4069,\n",
      "        1.3530, 1.3883, 1.4635, 1.4446, 1.5020], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0217,  0.0229,  ...,  0.0115,  0.0520, -0.0495],\n",
      "        [ 0.0202, -0.0492,  0.0130,  ...,  0.0468, -0.0377,  0.0192],\n",
      "        [ 0.0090,  0.0069, -0.0064,  ..., -0.0227,  0.0403, -0.0175],\n",
      "        ...,\n",
      "        [-0.0451,  0.0256, -0.0389,  ...,  0.0549,  0.0195,  0.0403],\n",
      "        [-0.0086, -0.0007,  0.0650,  ...,  0.0418, -0.0590, -0.1459],\n",
      "        [ 0.0106,  0.0533,  0.0497,  ...,  0.0615,  0.1177,  0.0070]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0705, -0.0781,  0.0237,  ...,  0.0851,  0.1432,  0.0945],\n",
      "        [ 0.0221,  0.0467, -0.0008,  ...,  0.0623, -0.0340, -0.0129],\n",
      "        [ 0.0817,  0.0056,  0.0281,  ...,  0.0459, -0.1102, -0.0282],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0699, -0.0239,  ..., -0.0942, -0.0153, -0.0297],\n",
      "        [ 0.0115,  0.0195,  0.0018,  ..., -0.0898,  0.0362,  0.0512],\n",
      "        [ 0.5918, -0.0390,  0.2050,  ..., -0.0053, -0.0623,  0.0609]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5014, 1.3327, 1.4554, 1.5530, 1.5741, 1.3886, 1.4239, 1.3876, 1.3847,\n",
      "        1.5476, 1.4273, 1.5368, 1.4240, 1.3810, 1.4078, 1.5239, 1.5372, 1.5113,\n",
      "        1.3997, 1.4292, 1.5568, 1.3909, 1.5455, 1.4469, 1.4205, 1.5497, 1.4665,\n",
      "        1.4476, 1.5210, 1.5334, 1.4755, 1.4735, 1.4614, 1.4325, 1.4512, 1.5678,\n",
      "        1.6011, 1.5061, 1.5713, 1.5010, 1.4320, 1.4921, 1.7000, 1.5624, 1.4234,\n",
      "        1.3821, 1.3155, 1.5083, 1.5360, 1.5495, 1.4496, 1.4356, 1.4506, 1.4838,\n",
      "        1.4546, 1.5012, 1.4049, 1.4990, 1.4502, 1.4335, 1.5002, 1.7995, 1.4395,\n",
      "        1.5972, 1.5242, 1.4567, 1.4125, 1.4498, 1.4517, 1.5820, 1.4537, 1.6206,\n",
      "        1.5965, 1.5183, 1.4551, 1.3322, 1.4610, 1.4729, 1.4255, 1.4739, 1.5041,\n",
      "        1.5618, 1.4337, 1.5347, 1.5926, 1.5278, 1.4356, 1.3712, 1.3611, 1.5967,\n",
      "        1.4402, 1.5318, 1.5628, 1.4050, 1.5145, 1.4789, 1.2983, 1.5420, 1.6201,\n",
      "        1.3993, 1.4770, 1.3783, 1.4142, 1.3155, 1.4459, 1.5723, 1.4921, 1.4083,\n",
      "        1.6247, 1.4190, 1.4377, 1.4869, 1.5547, 1.5288, 1.4442, 1.4670, 1.6050,\n",
      "        1.5413, 1.4641, 1.4590, 1.5244, 1.5841, 1.4634, 1.4977, 1.4885, 1.3851,\n",
      "        1.3901, 1.4511, 1.4613, 1.6734, 1.4604, 1.5296, 1.4768, 1.4887, 1.4561,\n",
      "        1.2820, 1.5037, 1.3580, 1.5477, 1.3842, 1.4473, 1.5225, 1.4472, 1.4212,\n",
      "        1.4155, 1.4108, 1.3950, 1.4272, 1.4968, 1.4843, 1.7360, 1.5123, 1.4630,\n",
      "        1.3554, 1.4336, 1.4874, 1.5849, 1.5262, 1.5496, 1.5090, 1.3878, 1.4919,\n",
      "        1.4874, 1.4610, 1.4888, 1.5007, 1.4774, 1.4255, 1.4060, 1.4851, 1.4931,\n",
      "        1.4960, 1.5238, 1.5347, 1.5565, 1.4501, 1.5244, 1.4789, 1.3946, 1.4409,\n",
      "        1.4717, 1.3813, 1.6351, 1.5331, 1.4500, 1.4904, 1.5212, 1.3473, 1.3938,\n",
      "        1.4162, 1.3886, 1.4642, 2.2825, 1.4273, 1.4872, 1.4280, 1.4431, 1.4219,\n",
      "        1.4591, 1.4861, 1.4576, 1.4935, 1.3943, 1.4882, 1.4089, 1.4106, 1.5969,\n",
      "        1.4638, 1.5316, 1.3555, 1.5203, 1.3868, 1.4587, 1.4645, 1.4181, 1.5176,\n",
      "        1.4714, 1.4654, 1.4757, 1.5123, 1.4654, 1.5540, 1.5448, 1.4473, 1.3781,\n",
      "        1.3392, 1.4533, 1.4458, 1.4954, 1.5407, 1.5017, 1.4586, 1.4709, 1.4263,\n",
      "        1.4444, 1.5165, 1.4833, 1.3000, 1.4598, 1.5327, 1.5552, 1.3605, 1.5023,\n",
      "        1.5033, 1.4213, 1.3591, 1.3550, 1.4752, 1.4481, 1.4843, 1.4088, 1.5462,\n",
      "        1.4859, 1.5445, 1.4289, 1.5068, 1.5817, 1.3894, 1.3336, 1.5011, 1.4988,\n",
      "        1.4432, 1.4598, 1.5096, 1.6152, 1.5682, 1.5423, 1.5538, 1.4566, 1.4902,\n",
      "        1.4528, 1.4790, 1.6919, 1.4428, 1.5100, 1.5214, 1.4966, 1.4362, 1.4471,\n",
      "        1.4545, 1.4768, 1.5430, 1.3868, 1.5182, 1.4363, 1.4779, 1.4320, 1.4402,\n",
      "        1.2930, 1.5281, 1.5409, 1.5837, 1.3817, 1.4880, 1.4943, 1.5104, 1.5852,\n",
      "        1.4879, 1.4388, 1.4799, 1.4350, 1.5431, 1.3986, 1.3522, 1.4398, 1.4663,\n",
      "        1.5889, 1.5170, 1.4585, 1.3570, 1.4707, 1.5160, 1.4306, 1.5722, 1.4120,\n",
      "        1.5698, 1.5195, 1.4282, 1.4076, 2.1103], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1913,  0.1439, -0.0587,  ...,  0.1206, -0.0412,  0.1710],\n",
      "        [ 0.1080, -0.1282,  0.0175,  ...,  0.0437,  0.0472, -0.0279],\n",
      "        [-0.0331, -0.0769, -0.0039,  ...,  0.0006, -0.1255, -0.0912],\n",
      "        ...,\n",
      "        [-0.1786, -0.0246, -0.0410,  ..., -0.0425,  0.0283, -0.0534],\n",
      "        [-0.0601, -0.1179, -0.1459,  ..., -0.0513, -0.0413,  0.0630],\n",
      "        [-0.1071,  0.0360,  0.0248,  ...,  0.0250,  0.0601,  0.0684]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0176, -0.0285,  0.0359,  ..., -0.1464,  0.1971, -0.1429],\n",
      "        [ 0.0213,  0.1127,  0.0043,  ..., -0.0422,  0.0240, -0.0810],\n",
      "        [ 0.0328, -0.0235,  0.0089,  ..., -0.0473,  0.0231,  0.0545],\n",
      "        ...,\n",
      "        [-0.1333, -0.1791,  0.1892,  ..., -0.0054,  0.0455, -0.0115],\n",
      "        [ 0.0381,  0.1481, -0.3332,  ...,  0.0029, -0.0540,  0.0920],\n",
      "        [-0.1313, -0.1393,  0.0490,  ..., -0.1403, -0.0163, -0.0613]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6502, 2.6156, 2.0932, 1.2639, 2.4448, 4.1155, 4.0457, 4.0813, 1.6421,\n",
      "        1.7426, 2.3876, 2.2295, 1.8275, 3.9988, 3.8468, 3.9548, 1.6908, 1.9997,\n",
      "        2.5554, 2.4655, 2.4225, 2.8384, 2.4930, 2.8896, 2.1552, 2.0546, 1.9411,\n",
      "        1.9582, 1.9826, 2.1164, 2.6452, 3.0319, 2.0923, 1.9524, 1.9136, 1.3721,\n",
      "        1.9326, 2.9358, 2.5857, 2.5309, 2.3159, 1.6121, 1.7857, 2.2958, 2.6083,\n",
      "        1.9384, 2.4236, 2.3950, 1.2545, 1.4059, 2.0667, 1.9373, 1.6634, 2.5145,\n",
      "        2.3794, 2.6810, 1.7396, 2.3242, 1.9650, 1.8965, 1.9688, 1.9418, 2.2488,\n",
      "        2.6895, 2.1461, 2.1112, 1.7653, 2.0296, 2.1150, 2.2905, 2.7678, 3.8653,\n",
      "        2.3992, 2.2087, 2.1105, 1.9179, 1.7957, 2.3470, 2.9652, 2.7732, 1.9223,\n",
      "        2.1029, 2.0688, 1.9646, 2.0856, 2.3693, 2.5675, 2.0249, 1.8247, 1.6944,\n",
      "        2.2734, 2.1682, 2.0256, 2.0651, 2.6483, 2.8148, 2.7232, 2.0216, 2.3221,\n",
      "        2.3505, 2.1293, 2.4979, 2.7553, 2.7774, 1.6059, 2.2108, 1.7986, 2.3020,\n",
      "        2.6188, 3.3310, 2.7673, 3.0991, 3.1531, 1.7765, 2.4253, 2.0373, 2.1842,\n",
      "        2.8903, 2.4858, 2.5315, 1.7635, 2.2949, 1.6692, 2.4804, 2.7635, 2.4357,\n",
      "        2.8048, 2.8515, 1.2723, 1.8122, 1.7715, 2.0678, 2.2418, 2.5657, 2.2642,\n",
      "        2.8708, 2.3819, 1.7237, 2.2664, 1.7338, 1.9725, 2.1832, 2.4724, 3.1641,\n",
      "        2.2298, 1.5323, 1.7500, 2.3365, 2.7462, 2.5351, 2.5812, 2.5951, 1.8959,\n",
      "        1.4568, 1.8616, 1.7385, 2.0547, 2.0577, 2.5471, 2.6174, 1.9045, 2.0368,\n",
      "        2.1491, 2.2578, 2.0064, 1.7262, 2.6151, 2.8401, 1.8364, 2.0855, 2.4266,\n",
      "        2.0812, 2.6259, 2.4519, 2.6943, 1.9615, 2.0984, 2.4450, 1.9669, 2.6365,\n",
      "        3.3529, 3.3635, 2.8954, 3.0783, 2.4246, 2.1224, 2.4412, 2.2906, 2.2350,\n",
      "        2.7289, 2.7601, 2.9762, 1.9270, 1.6923, 1.9806, 1.6168, 2.1872, 3.1525,\n",
      "        2.7998, 2.9103, 2.6990, 2.2466, 2.1372, 2.7138, 2.8328, 2.3670, 2.5533,\n",
      "        2.6239, 2.3611, 2.1765, 2.2233, 2.1408, 1.9627, 2.6408, 2.8932, 2.7491,\n",
      "        2.0639, 2.1844, 2.0121, 2.2453, 2.3933, 2.6866, 2.9182, 2.9554, 1.2797,\n",
      "        1.4910, 2.3079, 1.6123, 2.3203, 1.7228, 2.2731, 2.2295, 1.6505, 1.6896,\n",
      "        1.3521, 2.0697, 1.6070, 2.3539, 2.1010, 2.3073, 2.1981, 1.8335, 1.8523,\n",
      "        1.8404, 1.7815, 1.8730, 2.4490, 2.6054, 1.9097, 1.7991, 2.0760, 2.4617,\n",
      "        2.1500, 2.2592, 2.4519, 2.3295, 1.9338, 2.3007, 2.7088, 2.2805, 2.1295,\n",
      "        2.6968, 2.8910, 3.2615, 2.6082, 2.0596, 1.8855, 2.2187, 2.5806, 3.1001,\n",
      "        3.1367, 3.6361, 2.1971, 2.1435, 2.1802, 2.0894, 1.8538, 2.2296, 3.7110,\n",
      "        3.8970, 2.2108, 2.2536, 2.6539, 2.3088, 2.0942, 2.0067, 3.6656, 3.6851,\n",
      "        2.9023, 1.8844, 1.6712, 2.2201, 2.2959, 2.3754, 2.8909, 2.8768, 1.7485,\n",
      "        1.7711, 2.1843, 1.9280, 1.9510, 2.7384, 2.7710, 3.0087, 2.0421, 1.9873,\n",
      "        2.2730, 2.8562, 2.3922, 2.3298, 3.2295, 2.8123, 2.2061, 2.0193, 2.1244,\n",
      "        1.9207, 2.8432, 2.7413, 2.7872, 2.3487], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0528,  0.0363, -0.1404,  ...,  0.1749, -0.0095,  0.2133],\n",
      "        [ 0.0687, -0.0108, -0.0267,  ...,  0.1657, -0.0969, -0.1916],\n",
      "        [-0.0832, -0.0244,  0.0423,  ..., -0.0532, -0.0036,  0.1286],\n",
      "        ...,\n",
      "        [ 0.0253,  0.2217,  0.1188,  ...,  0.0197, -0.0818, -0.0353],\n",
      "        [-0.0240,  0.1141,  0.0670,  ...,  0.0692,  0.0048, -0.1398],\n",
      "        [ 0.2032, -0.0093,  0.0479,  ..., -0.0212, -0.0887, -0.0002]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0136, -0.0452,  0.0064,  ...,  0.0516,  0.0515,  0.0320],\n",
      "        [ 0.0608,  0.0408, -0.0213,  ...,  0.0114, -0.2708, -0.0407],\n",
      "        [ 0.0469,  0.0192,  0.1265,  ..., -0.0443, -0.1521, -0.0255],\n",
      "        ...,\n",
      "        [-0.0878, -0.1330,  0.1468,  ..., -0.0283,  0.0971,  0.0644],\n",
      "        [-0.1082, -0.1558, -0.2830,  ...,  0.0050,  0.0208, -0.0580],\n",
      "        [ 0.0882, -0.0245,  0.0646,  ..., -0.0613,  0.1311, -0.1207]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0801, 2.5454, 2.3534, 1.9563, 2.0947, 2.9690, 3.0816, 3.2038, 2.0320,\n",
      "        1.9005, 2.1902, 1.7012, 1.8005, 2.9080, 2.9956, 3.1893, 1.7702, 1.6986,\n",
      "        1.7789, 1.4597, 1.4505, 1.5532, 2.4262, 2.1200, 1.8313, 2.3842, 2.1493,\n",
      "        2.7785, 2.7044, 2.7936, 2.1872, 2.0606, 2.2343, 1.7282, 2.1142, 2.4230,\n",
      "        1.8947, 1.4490, 2.4256, 1.8532, 2.1351, 1.9019, 1.8645, 1.5370, 1.6852,\n",
      "        2.5597, 2.3226, 2.1420, 1.2325, 2.1807, 1.9795, 1.3838, 2.5246, 1.4686,\n",
      "        2.2704, 1.9798, 1.5304, 1.3496, 1.5547, 2.1629, 1.2913, 2.4373, 2.0808,\n",
      "        2.0001, 2.0338, 2.1033, 1.9229, 2.1926, 1.7963, 1.9357, 2.2573, 2.1702,\n",
      "        2.2783, 2.2245, 1.9967, 1.7335, 1.9597, 1.9497, 2.3237, 1.8741, 1.7868,\n",
      "        2.1621, 2.4480, 2.0349, 2.0122, 1.7537, 2.1366, 2.1363, 1.8818, 1.5387,\n",
      "        1.9156, 2.0270, 1.9629, 2.4083, 2.4379, 2.5233, 1.8670, 1.8546, 1.9981,\n",
      "        2.8820, 3.0257, 2.9527, 2.7151, 2.6997, 2.3930, 2.2379, 2.0177, 1.8048,\n",
      "        1.5508, 1.6605, 2.5677, 2.1661, 2.0660, 1.8363, 2.2414, 2.8375, 3.2821,\n",
      "        1.6042, 2.2681, 2.4918, 2.4985, 2.0704, 1.5898, 1.5874, 1.3801, 2.7167,\n",
      "        2.3488, 2.1238, 1.8422, 1.7717, 2.1960, 1.5201, 1.6356, 1.8005, 2.2272,\n",
      "        2.1135, 1.2367, 1.6991, 1.7485, 2.0175, 2.0073, 2.1669, 2.3764, 2.1196,\n",
      "        2.1707, 1.8195, 2.2821, 1.4093, 1.3516, 1.6107, 2.2630, 2.0711, 1.8645,\n",
      "        1.6004, 1.6279, 2.4121, 2.7284, 2.2204, 2.3359, 1.9434, 1.9885, 2.0685,\n",
      "        2.4383, 2.0437, 2.3073, 2.2920, 2.1596, 2.5379, 1.5089, 2.1324, 2.2579,\n",
      "        2.3212, 1.9888, 2.1639, 2.4237, 2.1530, 2.2684, 2.0758, 1.9215, 1.9331,\n",
      "        1.6264, 1.6005, 2.7913, 2.7354, 2.2101, 2.3566, 2.2811, 2.4981, 2.3413,\n",
      "        2.5897, 2.6791, 2.7004, 2.1357, 1.7810, 2.0047, 2.1804, 2.4103, 1.5509,\n",
      "        2.1535, 2.1462, 1.9084, 2.0515, 1.9552, 1.3726, 1.3420, 2.1230, 2.5501,\n",
      "        2.3498, 2.0701, 2.2193, 2.1227, 2.2995, 2.0020, 1.9695, 2.6695, 2.7156,\n",
      "        2.1324, 2.1671, 2.0408, 1.9931, 1.7616, 1.9403, 2.6549, 2.7555, 1.4491,\n",
      "        1.7870, 1.3653, 2.3537, 1.2332, 2.3115, 2.1162, 2.1013, 1.6064, 1.5327,\n",
      "        2.4257, 1.3044, 2.4427, 1.3136, 1.9825, 1.8853, 1.9633, 1.8179, 1.7905,\n",
      "        2.1321, 2.4386, 2.5614, 2.2665, 2.1441, 2.0938, 1.7983, 1.9854, 1.5365,\n",
      "        1.3738, 1.4349, 2.3596, 1.9332, 1.9685, 1.9769, 1.7733, 1.7968, 2.8031,\n",
      "        2.3921, 2.6747, 2.1273, 2.2021, 2.0743, 2.3725, 2.4030, 1.5839, 2.2402,\n",
      "        2.6566, 2.1743, 2.0850, 2.1494, 2.0353, 2.1392, 1.9327, 2.2130, 3.0331,\n",
      "        3.0326, 2.3201, 2.3914, 2.5229, 2.1936, 2.0664, 1.9892, 3.0062, 3.0446,\n",
      "        2.0394, 1.9070, 2.4598, 1.7389, 1.3756, 2.3826, 2.3162, 2.2542, 2.1030,\n",
      "        1.7690, 1.5512, 1.6548, 2.5122, 1.6617, 2.2071, 2.2569, 2.2578, 1.8992,\n",
      "        2.1725, 2.2870, 1.9433, 2.3382, 2.3189, 2.7745, 1.7905, 2.0525, 2.1216,\n",
      "        2.2870, 2.3020, 2.2691, 2.8605, 2.3341], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0283, -0.0195, -0.0466,  ...,  0.1461, -0.0745, -0.0679],\n",
      "        [-0.0280, -0.0835,  0.0401,  ..., -0.0004,  0.0150,  0.0179],\n",
      "        [-0.0676,  0.1491,  0.0185,  ..., -0.0324, -0.1147, -0.0558],\n",
      "        ...,\n",
      "        [-0.0194,  0.1029, -0.1721,  ..., -0.0643,  0.1024,  0.0681],\n",
      "        [-0.0606, -0.1340,  0.0140,  ..., -0.0180,  0.1068, -0.0306],\n",
      "        [ 0.1068, -0.0672, -0.0874,  ...,  0.0060, -0.0572, -0.0716]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.1007,  0.0420,  0.0728,  ..., -0.0412, -0.0170,  0.0073],\n",
      "        [ 0.0628, -0.2094, -0.0632,  ..., -0.0234, -0.0946,  0.1010],\n",
      "        [ 0.1231,  0.1951, -0.0433,  ..., -0.0022,  0.0793, -0.0088],\n",
      "        ...,\n",
      "        [ 0.0425,  0.0623, -0.0830,  ...,  0.0406,  0.0985,  0.0282],\n",
      "        [-0.1121,  0.0135,  0.0106,  ...,  0.0666, -0.0313, -0.0144],\n",
      "        [ 0.1298, -0.0779, -0.0233,  ..., -0.0352,  0.0530, -0.0361]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5272, 1.6399, 1.5915, 1.5357, 1.6434, 1.6291, 1.5056, 1.5451, 1.5269,\n",
      "        1.5868, 1.5215, 1.5710, 1.5106, 1.5398, 1.5559, 1.5410, 1.1699, 1.2181,\n",
      "        1.2137, 1.1217, 1.2262, 1.1804, 1.2341, 1.1829, 1.2592, 1.1774, 1.2031,\n",
      "        1.1680, 1.1794, 1.1864, 1.1715, 1.2289, 1.6772, 1.4956, 1.6138, 1.6294,\n",
      "        1.6175, 1.5474, 1.6580, 1.6204, 1.6013, 1.5853, 1.6135, 1.6444, 1.6257,\n",
      "        1.6408, 1.6433, 1.6140, 1.2809, 1.1577, 1.2673, 1.2278, 1.1961, 1.2151,\n",
      "        1.2261, 1.2386, 1.2627, 1.2773, 1.1744, 1.2442, 1.2154, 1.2196, 1.2574,\n",
      "        1.2187, 1.8325, 1.8472, 1.8343, 1.7411, 1.8783, 1.8034, 1.8382, 1.9294,\n",
      "        1.8843, 1.9070, 1.8260, 1.9665, 1.8537, 1.9500, 1.8820, 1.9473, 1.7951,\n",
      "        1.7867, 1.8390, 1.7401, 1.7464, 1.9000, 1.7550, 1.7559, 1.8500, 1.6767,\n",
      "        1.7674, 1.8292, 1.7742, 1.8637, 1.8388, 1.8370, 1.6519, 1.7779, 1.6414,\n",
      "        1.6772, 1.7805, 1.6661, 1.6829, 1.7039, 1.7318, 1.6569, 1.6671, 1.6501,\n",
      "        1.6658, 1.7283, 1.6355, 1.6675, 1.6172, 1.5656, 1.6039, 1.5498, 1.5469,\n",
      "        1.5747, 1.6256, 1.5783, 1.5952, 1.5724, 1.5939, 1.5260, 1.6177, 1.5769,\n",
      "        1.5712, 1.6337, 1.2462, 1.1890, 1.2367, 1.2357, 1.2237, 1.1823, 1.1748,\n",
      "        1.1755, 1.2078, 1.1587, 1.1635, 1.1740, 1.2130, 1.1948, 1.1655, 1.2109,\n",
      "        1.5826, 1.6088, 1.5890, 1.5887, 1.5365, 1.5827, 1.5901, 1.5458, 1.5769,\n",
      "        1.6117, 1.6281, 1.6499, 1.5696, 1.6306, 1.6181, 1.6560, 1.7826, 1.8544,\n",
      "        1.7580, 1.8587, 1.8124, 1.7649, 1.7524, 1.8455, 1.7268, 1.8060, 1.6977,\n",
      "        1.8009, 1.6819, 1.8048, 1.6808, 1.7871, 1.7265, 1.7473, 1.6593, 1.6386,\n",
      "        1.7736, 1.7244, 1.7483, 1.8189, 1.7296, 1.7639, 1.7033, 1.6299, 1.7335,\n",
      "        1.6635, 1.7585, 1.6489, 1.5190, 1.5598, 1.4873, 1.5216, 1.5170, 1.5483,\n",
      "        1.5518, 1.5543, 1.5473, 1.5790, 1.6520, 1.5943, 1.6216, 1.5828, 1.5278,\n",
      "        1.5094, 1.7989, 1.8334, 1.7350, 1.7699, 1.8159, 1.8816, 1.7130, 1.7366,\n",
      "        1.6951, 1.8245, 1.8671, 1.7377, 1.7824, 1.7490, 1.8031, 1.7918, 1.3409,\n",
      "        1.3331, 1.2977, 1.3627, 1.3447, 1.4610, 1.3613, 1.3682, 1.4541, 1.4836,\n",
      "        1.3857, 1.3482, 1.3708, 1.3107, 1.3525, 1.3842, 1.6655, 1.6543, 1.6791,\n",
      "        1.6352, 1.5863, 1.6455, 1.6618, 1.6931, 1.6158, 1.6577, 1.6700, 1.6240,\n",
      "        1.7081, 1.6579, 1.7181, 1.6521, 1.2660, 1.2770, 1.2893, 1.2335, 1.2419,\n",
      "        1.2057, 1.2556, 1.2190, 1.2743, 1.2448, 1.2616, 1.1654, 1.2908, 1.2702,\n",
      "        1.2628, 1.2279, 1.8248, 1.6240, 1.6945, 1.6764, 1.6567, 1.6677, 1.6097,\n",
      "        1.8228, 1.5692, 1.6430, 1.7216, 1.7609, 1.7230, 1.6166, 1.7402, 1.7759,\n",
      "        1.3061, 1.2980, 1.2656, 1.2814, 1.2790, 1.3002, 1.3535, 1.2208, 1.2732,\n",
      "        1.3230, 1.2968, 1.3123, 1.2997, 1.3005, 1.2601, 1.3309, 1.4792, 1.5306,\n",
      "        1.5492, 1.4329, 1.5518, 1.5265, 1.4427, 1.4884, 1.5333, 1.4654, 1.5130,\n",
      "        1.4176, 1.4364, 1.5398, 1.4649, 1.6545], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0072,  0.0275, -0.0987,  ...,  0.0838, -0.0675,  0.2627],\n",
      "        [ 0.0284, -0.2144,  0.0263,  ...,  0.1146,  0.0788,  0.0401],\n",
      "        [ 0.1191, -0.0373, -0.0527,  ..., -0.0537, -0.0905,  0.1126],\n",
      "        ...,\n",
      "        [ 0.1226,  0.1020,  0.0064,  ..., -0.0435,  0.0303, -0.0754],\n",
      "        [-0.0416, -0.0597,  0.0929,  ..., -0.0577, -0.0103,  0.0725],\n",
      "        [ 0.1775, -0.0602,  0.0101,  ..., -0.0578,  0.1327,  0.0175]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0888,  0.0128,  0.0299,  ...,  0.0219,  0.0374, -0.0780],\n",
      "        [-0.0329,  0.1707,  0.1076,  ..., -0.0068, -0.0087, -0.1130],\n",
      "        [ 0.0619,  0.0029,  0.0427,  ...,  0.0916, -0.0434, -0.0564],\n",
      "        ...,\n",
      "        [ 0.0194, -0.1103, -0.0349,  ..., -0.1215,  0.0031,  0.1362],\n",
      "        [ 0.0195, -0.1059,  0.0562,  ..., -0.0463, -0.0015, -0.0162],\n",
      "        [ 0.0733, -0.1120, -0.0374,  ..., -0.0589,  0.0424,  0.0280]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4852, 1.5863, 1.5307, 1.5088, 1.6193, 1.3944, 1.4274, 1.5202, 1.4342,\n",
      "        1.4310, 1.4320, 1.5326, 1.5587, 1.4011, 1.6347, 1.4332, 1.3336, 1.5348,\n",
      "        1.4366, 1.4599, 1.5344, 1.4167, 1.5995, 1.4376, 1.4715, 1.4841, 1.4711,\n",
      "        1.5340, 1.4947, 1.4911, 1.4454, 1.4229, 1.6320, 1.4771, 1.4091, 1.4460,\n",
      "        1.6202, 1.4149, 1.3964, 1.4694, 1.6347, 1.4984, 1.4732, 1.4960, 1.3767,\n",
      "        1.4153, 1.3913, 1.6091, 1.5136, 1.5012, 1.4560, 1.5075, 1.3090, 1.3775,\n",
      "        1.3879, 1.3591, 1.3411, 1.4378, 1.6290, 1.5532, 1.3855, 1.6763, 2.7814,\n",
      "        1.4581, 1.4423, 1.4788, 1.4278, 1.5659, 1.5388, 1.5121, 1.3352, 1.4405,\n",
      "        1.4037, 1.4332, 1.4897, 1.5940, 1.4385, 1.5158, 1.4677, 1.5055, 1.4844,\n",
      "        1.5357, 1.5033, 1.5741, 1.4117, 1.4569, 1.4445, 1.5497, 1.3004, 1.5096,\n",
      "        1.4159, 1.5419, 1.5935, 1.5185, 1.4364, 1.5835, 1.4850, 1.5966, 1.5393,\n",
      "        1.5207, 1.5843, 1.5451, 1.6172, 1.4659, 1.5571, 1.6062, 1.4274, 1.4081,\n",
      "        1.4243, 1.4254, 1.5863, 1.3596, 1.4392, 1.4486, 1.5461, 1.4971, 1.5840,\n",
      "        1.4750, 1.4543, 1.4336, 1.4925, 1.5821, 1.5370, 1.5233, 1.3452, 1.5704,\n",
      "        1.5224, 1.5331, 1.4827, 1.4917, 1.3546, 1.3926, 1.5097, 1.5815, 1.5642,\n",
      "        1.4026, 1.4565, 1.4945, 1.3433, 1.3841, 1.4233, 1.4171, 1.4768, 1.4137,\n",
      "        1.4062, 1.4882, 1.4657, 1.5479, 1.4730, 1.5519, 2.1133, 1.5775, 1.5598,\n",
      "        1.4972, 1.4767, 1.5169, 1.4652, 1.5629, 1.5577, 1.4844, 1.3515, 1.4537,\n",
      "        1.5758, 1.5780, 1.6583, 1.5675, 1.3802, 1.4450, 1.4097, 1.5219, 1.4319,\n",
      "        1.6426, 1.5335, 1.4466, 1.4913, 1.3297, 1.4979, 1.5517, 1.4700, 1.5537,\n",
      "        1.4411, 1.3917, 1.4599, 1.4890, 1.5446, 1.4041, 1.4665, 1.4746, 1.4919,\n",
      "        1.6231, 1.3502, 1.3560, 2.1453, 1.3052, 1.4609, 1.4697, 1.4984, 1.4280,\n",
      "        1.5222, 1.5612, 1.5971, 1.3091, 1.4373, 1.3624, 1.3663, 1.5522, 1.5311,\n",
      "        1.3408, 1.5385, 1.6565, 1.5588, 1.4876, 1.4312, 1.6053, 1.4554, 1.5336,\n",
      "        1.5547, 1.5143, 1.3529, 1.5455, 1.4272, 1.5759, 1.5304, 1.4447, 1.4745,\n",
      "        1.4382, 1.3927, 1.5012, 1.4370, 1.4487, 1.3536, 1.3955, 1.3746, 1.4724,\n",
      "        1.4584, 1.7232, 1.4524, 1.3445, 1.4818, 1.6252, 1.5196, 1.4486, 1.3228,\n",
      "        1.3826, 1.3533, 1.5325, 1.4179, 1.5299, 1.4783, 1.4135, 1.5139, 1.4146,\n",
      "        1.4979, 1.3969, 1.4388, 1.4577, 1.3766, 1.4539, 1.2180, 1.5059, 1.4594,\n",
      "        1.5009, 1.6209, 1.5234, 1.4064, 1.4583, 1.5368, 1.6806, 1.3966, 1.5973,\n",
      "        1.3467, 1.5388, 1.2857, 1.6064, 1.4255, 1.6169, 1.5345, 1.3505, 1.5718,\n",
      "        1.4807, 1.4838, 1.4700, 1.6023, 1.6147, 1.5629, 1.3989, 1.4161, 1.4603,\n",
      "        1.2450, 1.3561, 1.5340, 1.5493, 1.5026, 1.5171, 1.2936, 1.4623, 1.5910,\n",
      "        1.5688, 1.4752, 1.4966, 1.5027, 1.5506, 1.4831, 1.3553, 1.4801, 1.4337,\n",
      "        1.4599, 1.5498, 1.4339, 1.4534, 1.4642, 1.3374, 1.5551, 1.5341, 1.5673,\n",
      "        1.5014, 1.5767, 1.5708, 1.3815, 1.6792], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0974,  0.0594, -0.1145,  ..., -0.1115, -0.0819,  0.1815],\n",
      "        [ 0.1114, -0.1549, -0.0573,  ...,  0.1229, -0.0385, -0.0939],\n",
      "        [ 0.1785, -0.0444, -0.1038,  ..., -0.0657, -0.0241,  0.0197],\n",
      "        ...,\n",
      "        [-0.0357, -0.0714,  0.0728,  ...,  0.0254, -0.1044,  0.1380],\n",
      "        [-0.0301, -0.0758,  0.0531,  ..., -0.0502,  0.0368,  0.0036],\n",
      "        [-0.0124, -0.1155,  0.0185,  ..., -0.1466, -0.0767,  0.1884]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0701, -0.0195, -0.0521,  ..., -0.0232, -0.0614, -0.0142],\n",
      "        [-0.1042, -0.0104, -0.0218,  ..., -0.0150, -0.0675,  0.0020],\n",
      "        [ 0.0911, -0.0028, -0.1154,  ...,  0.0119,  0.0328,  0.0210],\n",
      "        ...,\n",
      "        [-0.1355, -0.0670,  0.0020,  ..., -0.0172,  0.0272,  0.0401],\n",
      "        [-0.0281, -0.0774, -0.1560,  ..., -0.0121,  0.1091, -0.0975],\n",
      "        [ 0.1272,  0.1299, -0.0124,  ...,  0.1130,  0.0355, -0.0015]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6579, 1.4362, 1.4333, 1.3821, 1.4182, 2.3624, 2.0713, 2.5590, 1.8090,\n",
      "        1.4647, 1.3471, 1.3601, 1.5070, 1.2785, 2.5912, 2.0349, 1.1011, 1.1287,\n",
      "        1.4390, 1.9767, 2.4265, 4.6501, 4.2600, 3.4385, 1.0099, 1.2299, 1.2878,\n",
      "        2.0306, 2.6301, 4.4378, 4.2134, 3.6061, 1.4914, 1.5164, 1.7802, 1.7877,\n",
      "        1.8833, 1.5644, 2.2301, 1.8761, 1.4867, 1.5687, 2.1623, 1.8227, 1.8690,\n",
      "        1.7264, 2.2270, 2.2885, 1.5816, 1.9156, 1.9380, 1.8436, 2.0991, 1.7544,\n",
      "        2.3045, 2.3098, 1.5279, 1.9601, 1.8580, 1.9616, 2.2451, 2.5637, 2.5662,\n",
      "        2.3488, 2.1455, 1.7758, 2.3912, 2.2600, 2.2763, 2.1790, 2.7780, 2.3885,\n",
      "        1.4835, 1.8715, 2.1361, 2.0398, 2.0120, 2.5187, 2.6739, 2.4436, 2.2261,\n",
      "        1.8579, 1.4257, 1.7337, 1.6311, 2.0758, 2.3112, 2.2074, 2.1480, 1.8446,\n",
      "        1.7081, 1.6120, 2.4526, 2.4516, 2.1117, 2.2415, 1.2832, 1.4362, 1.4073,\n",
      "        1.3875, 1.4291, 2.3041, 1.9971, 2.5825, 1.4081, 1.5164, 1.3988, 1.2904,\n",
      "        1.4126, 1.4232, 2.0267, 2.1985, 2.0410, 1.6312, 1.5168, 1.5718, 1.2360,\n",
      "        2.0262, 2.1945, 2.1226, 1.9331, 1.3425, 1.3781, 1.5630, 2.0269, 1.5729,\n",
      "        1.9759, 2.5323, 1.8478, 1.3920, 1.4999, 1.4121, 1.4388, 2.0795, 1.9729,\n",
      "        2.5244, 1.4903, 1.6789, 1.5856, 1.4161, 1.9491, 1.6637, 2.0832, 2.4958,\n",
      "        1.7441, 1.6366, 2.1077, 1.9513, 2.2123, 1.8839, 2.4128, 2.1732, 2.4303,\n",
      "        1.9687, 1.7449, 2.2393, 1.7446, 2.4663, 2.5513, 2.6849, 1.9608, 1.7362,\n",
      "        2.0739, 2.0064, 2.4483, 2.5913, 2.9753, 2.8067, 1.6456, 1.9218, 1.9801,\n",
      "        2.2326, 2.0853, 2.5448, 2.7821, 2.7880, 2.0124, 1.5923, 1.7471, 2.1180,\n",
      "        1.7262, 1.9367, 2.3641, 2.5234, 1.7433, 1.7845, 1.9987, 1.8459, 1.7459,\n",
      "        1.7800, 2.2015, 2.4129, 1.5191, 1.5551, 1.4129, 1.4837, 1.7969, 2.2280,\n",
      "        2.0381, 2.7568, 1.6470, 1.5248, 1.7329, 1.6961, 1.7936, 1.7156, 1.9406,\n",
      "        2.3345, 1.1458, 1.1443, 1.4758, 1.7356, 2.5206, 3.6379, 3.7720, 3.2780,\n",
      "        0.8240, 1.1919, 1.2837, 2.0729, 2.2404, 3.7330, 3.5249, 3.3430, 1.6637,\n",
      "        1.9500, 1.9017, 2.0359, 1.9665, 2.1687, 2.4457, 2.3560, 1.9452, 1.8432,\n",
      "        1.9107, 2.0104, 1.9273, 1.9114, 2.1928, 2.4267, 1.8683, 1.4589, 1.7653,\n",
      "        1.8170, 1.9382, 1.6376, 2.3274, 2.0293, 1.4497, 1.7161, 1.8546, 1.7381,\n",
      "        1.9599, 1.6720, 2.3729, 2.3810, 1.3835, 1.3518, 1.7803, 1.7010, 1.6470,\n",
      "        1.7547, 2.4688, 2.1257, 1.3666, 1.5720, 1.7306, 1.7808, 1.6935, 1.9183,\n",
      "        2.0585, 2.0541, 1.4105, 1.4780, 1.8007, 1.6688, 1.6210, 1.8251, 2.2875,\n",
      "        2.3727, 1.4843, 1.3994, 1.4690, 1.6379, 1.5442, 1.7736, 2.5260, 2.5035,\n",
      "        1.8963, 1.7503, 1.7025, 1.4933, 1.6592, 1.8671, 2.3274, 2.2930, 1.6071,\n",
      "        1.4596, 1.6012, 1.8346, 1.5373, 1.9718, 2.2165, 2.4272, 1.5861, 1.1843,\n",
      "        1.2226, 1.4178, 1.3635, 1.8357, 2.0726, 2.3337, 1.4956, 1.3248, 1.4438,\n",
      "        1.6852, 1.6455, 1.7410, 1.9528, 2.0677], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1011,  0.0472,  0.0762,  ...,  0.0725,  0.1155, -0.0312],\n",
      "        [ 0.0930, -0.0944, -0.0129,  ...,  0.0377,  0.0660, -0.0120],\n",
      "        [ 0.0839, -0.0321,  0.0353,  ...,  0.1766,  0.0375, -0.1734],\n",
      "        ...,\n",
      "        [-0.1094,  0.0767, -0.0775,  ..., -0.0416,  0.0441, -0.0375],\n",
      "        [ 0.0675, -0.0954, -0.0132,  ...,  0.0429,  0.0707,  0.0600],\n",
      "        [-0.0133,  0.0353, -0.0130,  ...,  0.0328,  0.0228, -0.0165]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-9.6924e-04,  1.4808e-02, -5.4056e-02,  ..., -5.9756e-05,\n",
      "          1.1257e-01,  6.8136e-02],\n",
      "        [-4.6217e-02, -7.3824e-03, -5.6782e-02,  ...,  5.3237e-02,\n",
      "          5.3616e-02,  6.6409e-02],\n",
      "        [-3.6919e-02,  9.3368e-02, -4.5654e-02,  ..., -1.5445e-01,\n",
      "         -8.1055e-03,  8.5169e-02],\n",
      "        ...,\n",
      "        [-2.3618e-04, -2.3092e-02,  2.8901e-02,  ...,  1.4425e-04,\n",
      "         -3.9764e-02, -4.2556e-02],\n",
      "        [-1.9380e-01, -2.0590e-02, -8.4090e-02,  ..., -3.4319e-02,\n",
      "         -9.5992e-02, -1.8172e-02],\n",
      "        [ 2.5444e-01, -2.3175e-02, -1.2144e-02,  ..., -8.6750e-03,\n",
      "          3.4835e-02, -1.6897e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6901, 1.4298, 1.7453, 1.6625, 1.6545, 1.2896, 1.8225, 1.7626, 1.8045,\n",
      "        1.7439, 1.6363, 1.7145, 1.6673, 2.3117, 1.6647, 1.7685, 1.1484, 1.3900,\n",
      "        1.3753, 2.0084, 1.9245, 3.2050, 2.9254, 2.7812, 1.1120, 0.9392, 1.4350,\n",
      "        1.8692, 2.2257, 3.0266, 3.0223, 2.7161, 1.4762, 1.5824, 1.9992, 1.8014,\n",
      "        1.9028, 2.2570, 1.9999, 1.9345, 1.4294, 1.4515, 1.8875, 1.8812, 1.8538,\n",
      "        1.6526, 2.1032, 2.1460, 1.6828, 2.0841, 1.8332, 1.7057, 1.8017, 2.2720,\n",
      "        2.2901, 2.0170, 1.4014, 1.6636, 1.9481, 1.9804, 1.8881, 1.5372, 2.1759,\n",
      "        2.0352, 1.8895, 1.7117, 2.0542, 2.0979, 1.8531, 2.0740, 2.4378, 2.1064,\n",
      "        1.7551, 1.9543, 2.2812, 2.0551, 1.9989, 1.8952, 2.5715, 2.3170, 1.7885,\n",
      "        1.8901, 1.7296, 1.7175, 1.9981, 1.9724, 2.0101, 2.0759, 2.2895, 1.8547,\n",
      "        1.9279, 1.8706, 1.3204, 1.4631, 2.0985, 1.9076, 1.4760, 1.5220, 1.7244,\n",
      "        1.9040, 1.4430, 1.2648, 1.8934, 1.5211, 1.3224, 1.7244, 1.6960, 1.5561,\n",
      "        1.7947, 1.9424, 1.8680, 1.6467, 1.8423, 1.6644, 1.7456, 1.6977, 1.9615,\n",
      "        1.4442, 1.9819, 2.0908, 2.0244, 1.7846, 1.7368, 1.8221, 1.3797, 2.2888,\n",
      "        2.1304, 1.7870, 1.8181, 1.6051, 1.6508, 1.6809, 1.8541, 1.3600, 1.9013,\n",
      "        1.8085, 1.4148, 1.6455, 1.8709, 1.7390, 1.3277, 2.0001, 2.0476, 1.8211,\n",
      "        2.0402, 1.7590, 1.9451, 2.0128, 1.7764, 2.1293, 1.9919, 1.9868, 1.5772,\n",
      "        1.7232, 1.7962, 1.9615, 2.0382, 1.8888, 2.2557, 2.2683, 1.9020, 1.8857,\n",
      "        1.8800, 2.0686, 2.0127, 1.9537, 2.7780, 2.6227, 1.6432, 1.7251, 2.1520,\n",
      "        2.0491, 2.0441, 2.2843, 2.5111, 2.3604, 1.7844, 1.4995, 1.9423, 1.9569,\n",
      "        1.9644, 2.0299, 2.4345, 2.4030, 1.8794, 1.8724, 1.7823, 2.0091, 1.7452,\n",
      "        1.7731, 2.1050, 2.2811, 1.7665, 1.6329, 1.7372, 1.8654, 1.5278, 1.2061,\n",
      "        1.9632, 1.8588, 1.9148, 1.7430, 1.5462, 1.3146, 1.3501, 2.2016, 1.9226,\n",
      "        1.9767, 1.1053, 1.2718, 1.5245, 1.9349, 2.1981, 2.7698, 2.8139, 2.5206,\n",
      "        0.8822, 1.1700, 1.1164, 1.7197, 1.8152, 2.8380, 2.5741, 2.6274, 1.7296,\n",
      "        1.8357, 1.9123, 1.8853, 1.6653, 1.7978, 2.2881, 2.2627, 1.7328, 1.8854,\n",
      "        1.8349, 2.0321, 1.8805, 1.7104, 1.9848, 2.1853, 1.7704, 1.5614, 1.7693,\n",
      "        1.8813, 1.8705, 1.9470, 2.1772, 1.9979, 1.3471, 1.6604, 1.8429, 1.7438,\n",
      "        1.9345, 1.8578, 2.2558, 2.3191, 1.2602, 1.5813, 1.8244, 1.8505, 1.7577,\n",
      "        1.8526, 1.9940, 2.1371, 1.5627, 1.4654, 1.7877, 1.8041, 1.9242, 1.7531,\n",
      "        2.5125, 1.9818, 1.2792, 1.3785, 1.7182, 1.7653, 1.6841, 1.7082, 2.1210,\n",
      "        1.8657, 1.0020, 1.3304, 1.6737, 1.9252, 1.7378, 1.7295, 1.9686, 1.9735,\n",
      "        1.8157, 1.7665, 1.7366, 1.7667, 1.7726, 1.7117, 2.3074, 2.0739, 1.6763,\n",
      "        1.6542, 1.8008, 1.8245, 1.7083, 1.6116, 2.1011, 2.2264, 1.4053, 1.5233,\n",
      "        1.4744, 1.3481, 1.8235, 1.2385, 1.4698, 1.5212, 1.7525, 1.4489, 1.4020,\n",
      "        1.2768, 1.1129, 1.2385, 1.6282, 1.5777], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.3337,  0.0604,  0.0433,  ..., -0.0721,  0.0114,  0.0046],\n",
      "        [ 0.0230,  0.1058,  0.0094,  ...,  0.0869, -0.0825,  0.0831],\n",
      "        [-0.0539, -0.0305, -0.3123,  ...,  0.0936, -0.0085,  0.0292],\n",
      "        ...,\n",
      "        [-0.1622, -0.0076, -0.0787,  ..., -0.1303, -0.1678, -0.0611],\n",
      "        [-0.0361, -0.0996,  0.0459,  ...,  0.0450,  0.0341, -0.0721],\n",
      "        [ 0.1475,  0.0459, -0.0538,  ...,  0.0524,  0.0190,  0.0563]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0893,  0.1474,  0.1481,  ..., -0.0069, -0.1107, -0.0561],\n",
      "        [-0.1553, -0.0772,  0.0110,  ...,  0.0783, -0.0595,  0.1390],\n",
      "        [-0.0306,  0.0049, -0.0071,  ...,  0.0093,  0.1346, -0.0018],\n",
      "        ...,\n",
      "        [-0.1086, -0.0581, -0.0051,  ...,  0.1052,  0.0394,  0.1085],\n",
      "        [-0.0931,  0.1065, -0.0347,  ..., -0.0974, -0.1386, -0.1102],\n",
      "        [-0.1270,  0.0642,  0.0119,  ...,  0.0213, -0.0573,  0.0164]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9919, 1.7890, 1.7176, 1.8750, 1.9581, 1.7886, 1.7659, 1.8095, 1.7721,\n",
      "        1.9024, 1.7131, 1.8031, 1.7861, 1.8008, 1.9807, 1.8473, 1.5640, 1.4496,\n",
      "        1.5711, 1.4845, 1.5061, 1.4802, 1.4278, 1.4410, 1.4632, 1.4218, 1.4365,\n",
      "        1.4804, 1.5405, 1.4633, 1.4815, 1.4422, 1.7774, 1.8959, 1.8243, 1.7406,\n",
      "        1.8590, 1.7296, 1.8627, 1.6864, 1.8537, 1.9351, 1.7567, 1.7654, 1.7072,\n",
      "        1.7669, 1.8065, 1.8350, 1.7768, 1.7679, 2.2139, 1.9217, 1.8609, 1.8836,\n",
      "        1.8377, 1.8473, 1.9907, 1.9871, 1.7807, 1.8057, 2.0076, 2.1678, 1.8832,\n",
      "        1.8731, 2.1739, 2.0793, 1.7738, 2.3126, 1.8005, 2.3029, 1.7925, 1.9511,\n",
      "        1.9189, 1.9744, 1.9697, 1.9211, 1.8326, 1.9079, 2.0179, 1.9617, 1.7883,\n",
      "        1.9182, 1.7291, 1.7717, 1.7293, 1.7402, 1.7344, 1.7481, 1.8791, 1.7537,\n",
      "        1.7666, 1.6620, 1.7439, 1.7990, 1.7408, 1.7928, 1.7621, 1.7997, 1.8673,\n",
      "        1.8825, 1.8624, 1.8952, 1.7478, 1.9294, 1.9456, 1.8838, 1.9201, 1.8593,\n",
      "        1.9086, 1.9129, 1.8995, 1.8468, 1.8383, 1.8199, 1.7046, 1.8690, 1.8564,\n",
      "        1.8729, 1.7816, 1.8405, 1.7967, 1.8034, 1.8343, 1.7804, 1.8097, 1.7025,\n",
      "        1.7832, 1.8502, 1.8559, 1.8258, 1.7999, 1.6973, 1.8518, 1.7492, 1.7396,\n",
      "        1.9084, 1.7251, 1.8305, 1.7479, 1.7693, 1.7640, 1.8348, 1.9066, 1.7992,\n",
      "        1.6686, 1.6091, 1.7045, 1.6545, 1.7440, 1.6695, 1.7185, 1.6539, 1.6978,\n",
      "        1.6911, 1.7498, 1.7961, 1.7188, 1.7555, 1.6919, 1.7758, 1.8816, 1.8563,\n",
      "        1.8754, 1.8535, 2.0495, 2.0666, 1.8723, 1.8519, 1.9132, 1.8670, 1.9367,\n",
      "        1.9084, 1.9452, 1.8295, 1.9156, 1.9522, 1.9169, 2.0352, 1.8821, 1.9448,\n",
      "        2.0415, 1.8521, 1.9346, 1.9559, 1.9095, 1.9264, 1.9529, 1.8688, 1.9952,\n",
      "        1.9381, 1.9476, 1.9223, 1.3918, 1.4041, 1.3231, 1.4014, 1.2949, 1.3361,\n",
      "        1.4246, 1.3314, 1.3987, 1.3450, 1.5080, 1.4212, 1.4308, 1.4066, 1.3388,\n",
      "        1.3824, 1.5949, 1.5467, 1.4164, 1.5498, 1.4930, 1.5235, 1.5245, 1.4963,\n",
      "        1.4844, 1.3989, 1.4030, 1.5052, 1.4110, 1.4510, 1.4994, 1.4400, 2.1339,\n",
      "        1.9643, 2.0098, 1.9048, 2.1455, 1.9863, 1.8460, 1.9036, 1.8581, 1.9033,\n",
      "        1.8104, 2.1055, 2.1381, 1.9730, 1.9288, 1.8745, 1.7297, 1.7898, 1.7144,\n",
      "        1.7614, 1.7276, 1.7171, 1.8054, 1.7696, 1.7638, 1.7718, 1.8358, 1.6484,\n",
      "        1.7218, 1.7484, 1.7296, 1.8218, 1.8349, 1.7400, 1.8221, 1.7744, 1.7213,\n",
      "        1.6710, 1.7598, 1.8056, 1.7394, 1.7058, 1.7129, 1.8056, 1.8048, 1.7937,\n",
      "        1.6661, 1.7370, 1.8977, 1.9739, 1.8809, 2.0181, 1.8587, 1.8976, 1.9081,\n",
      "        1.8559, 1.7674, 1.8911, 1.8586, 1.8933, 1.9555, 2.0221, 1.9254, 1.9472,\n",
      "        1.9198, 1.8275, 1.8947, 1.8802, 1.9084, 2.0417, 1.8569, 1.9944, 1.8177,\n",
      "        1.9094, 1.9581, 1.8851, 1.9276, 1.9164, 1.8274, 1.9169, 1.7560, 1.7797,\n",
      "        1.8239, 1.6376, 2.0625, 1.7377, 1.6877, 1.8417, 1.8131, 1.7284, 1.8926,\n",
      "        1.6946, 1.7420, 1.7298, 1.7581, 1.8551], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.2649, -0.0890,  0.0675,  ..., -0.0717, -0.1811,  0.1666],\n",
      "        [-0.0697,  0.0303,  0.0303,  ..., -0.1414,  0.0624,  0.0833],\n",
      "        [-0.0070, -0.0207,  0.0639,  ...,  0.0318,  0.0641, -0.0440],\n",
      "        ...,\n",
      "        [-0.0555, -0.1256, -0.0432,  ..., -0.0047, -0.0023,  0.0331],\n",
      "        [ 0.0637, -0.1111,  0.0545,  ...,  0.0418,  0.0602, -0.0185],\n",
      "        [ 0.0140, -0.0386, -0.0045,  ...,  0.1037, -0.0839, -0.0737]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0072,  0.0051, -0.0387,  ...,  0.0063,  0.1297, -0.0130],\n",
      "        [-0.0630, -0.1436, -0.0281,  ...,  0.0100, -0.0442,  0.0720],\n",
      "        [ 0.0377, -0.0843,  0.0020,  ...,  0.0204, -0.1304,  0.0368],\n",
      "        ...,\n",
      "        [-0.0079,  0.0269,  0.0040,  ...,  0.0354,  0.0964, -0.0893],\n",
      "        [ 0.0157, -0.1492, -0.1592,  ..., -0.0159,  0.0884,  0.0334],\n",
      "        [-0.0033, -0.0794,  0.0264,  ...,  0.0229,  0.0525,  0.0873]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5964, 1.7750, 1.6466, 1.8556, 1.7690, 1.6466, 1.6775, 1.5701, 1.7983,\n",
      "        1.8266, 1.3759, 1.5952, 1.7119, 1.6862, 1.7469, 1.6866, 1.8569, 1.7962,\n",
      "        1.6551, 1.8480, 1.8246, 1.6774, 1.7044, 1.5884, 1.6442, 1.5355, 1.5958,\n",
      "        1.7036, 1.7652, 1.8322, 1.6739, 1.4304, 1.6619, 1.4391, 1.7018, 1.7702,\n",
      "        1.6612, 1.6867, 1.5918, 1.6550, 1.6842, 1.6901, 1.7625, 1.6067, 1.7787,\n",
      "        1.5810, 1.6056, 1.5458, 1.6893, 1.7588, 1.7383, 1.7630, 1.4993, 1.6647,\n",
      "        1.5729, 1.6971, 1.5488, 1.7025, 1.7909, 1.6277, 1.7180, 1.7459, 5.9657,\n",
      "        1.7474, 1.7299, 1.7038, 1.5956, 1.7353, 1.7232, 1.8430, 1.7103, 1.8162,\n",
      "        1.5539, 1.7732, 1.6409, 1.7836, 1.7436, 1.6761, 1.7520, 1.7571, 1.5346,\n",
      "        1.6484, 1.6952, 1.6431, 1.5776, 1.6875, 1.6540, 1.7211, 1.4713, 1.8457,\n",
      "        1.8090, 1.6045, 1.9106, 1.6500, 1.7166, 1.7487, 1.7148, 1.8254, 1.7693,\n",
      "        1.8611, 1.8819, 1.6629, 1.7560, 1.6224, 1.7851, 1.8255, 1.5220, 1.6499,\n",
      "        1.7284, 1.6605, 1.6525, 1.6673, 1.6053, 1.5435, 1.8526, 1.5717, 1.6661,\n",
      "        1.6997, 1.5872, 1.8104, 1.6107, 1.7039, 1.7082, 1.7426, 1.6314, 1.7977,\n",
      "        1.6669, 1.6655, 1.5098, 1.7733, 1.5138, 1.8477, 1.7176, 1.7784, 1.7476,\n",
      "        1.6622, 1.7188, 1.6773, 1.5834, 1.7559, 1.5528, 1.6364, 1.7077, 1.6455,\n",
      "        1.6567, 1.8731, 1.6939, 1.6362, 1.6733, 1.6718, 1.9434, 1.7441, 1.9019,\n",
      "        1.7602, 1.8292, 1.6832, 1.5443, 1.7428, 1.9316, 1.6991, 1.6669, 1.7201,\n",
      "        1.6199, 1.6992, 1.7471, 1.6642, 1.7006, 1.7301, 1.7655, 1.7177, 1.5834,\n",
      "        1.7193, 1.6808, 1.6058, 1.7037, 1.7061, 1.5287, 1.7106, 1.7347, 1.6057,\n",
      "        1.6091, 1.6595, 1.6693, 1.7268, 1.6284, 1.6324, 1.5833, 1.6379, 1.6101,\n",
      "        1.6652, 1.6718, 1.6060, 2.5919, 1.6003, 1.4726, 1.6650, 1.6707, 1.7373,\n",
      "        1.6747, 1.8651, 1.7673, 1.7775, 1.7196, 1.5756, 1.8251, 1.6449, 1.7327,\n",
      "        1.3527, 1.6778, 1.6322, 1.6627, 1.6080, 1.6040, 1.7854, 1.6487, 1.8082,\n",
      "        1.6624, 1.8348, 1.4219, 1.5654, 1.5196, 1.6465, 1.6206, 1.7468, 1.7356,\n",
      "        1.6517, 1.6620, 1.5854, 1.6148, 1.6876, 1.6166, 1.5845, 1.6772, 1.7044,\n",
      "        1.5869, 1.6469, 1.6190, 1.5716, 1.7281, 1.7825, 1.5593, 1.6752, 1.6691,\n",
      "        1.6460, 1.5620, 1.8063, 1.6064, 1.6436, 1.7409, 1.7083, 1.4048, 1.6869,\n",
      "        1.7109, 1.5984, 1.6265, 1.7553, 1.7012, 1.5377, 1.4939, 1.7354, 1.6266,\n",
      "        1.6588, 1.7281, 1.6112, 1.6529, 1.7351, 1.7509, 2.0513, 1.5761, 1.6615,\n",
      "        1.5907, 1.7857, 1.6159, 1.6534, 1.8055, 1.6535, 1.7446, 1.5624, 1.7473,\n",
      "        1.8098, 1.6876, 1.6906, 1.6989, 1.7606, 1.7472, 1.7068, 1.8167, 1.6962,\n",
      "        1.4691, 1.6598, 1.6596, 1.8147, 1.7002, 1.7283, 1.4625, 1.7969, 1.7486,\n",
      "        1.6421, 1.5465, 1.8117, 1.7740, 1.6889, 1.6157, 1.5733, 1.6133, 1.6651,\n",
      "        1.7071, 1.6808, 1.5862, 1.6860, 1.6872, 1.4883, 1.6161, 1.6948, 1.7144,\n",
      "        1.7136, 1.7942, 1.6294, 1.6943, 1.6341], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0517,  0.0622,  0.1105,  ...,  0.0655, -0.0859,  0.0634],\n",
      "        [-0.1135,  0.1362, -0.0370,  ..., -0.1070, -0.1524,  0.0273],\n",
      "        [ 0.0609, -0.0011, -0.0382,  ...,  0.0540, -0.1280, -0.0369],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0075, -0.0670,  ..., -0.0294, -0.0261,  0.0454],\n",
      "        [ 0.1140,  0.0563,  0.0533,  ...,  0.1218,  0.0815, -0.0206],\n",
      "        [ 0.0196, -0.0849, -0.1175,  ..., -0.0299,  0.0895, -0.0432]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0218,  0.0902, -0.0317,  ..., -0.0773,  0.0044, -0.0755],\n",
      "        [-0.0013,  0.0249,  0.0186,  ..., -0.1475, -0.0492,  0.0744],\n",
      "        [-0.0335,  0.0805, -0.0374,  ...,  0.0396,  0.0109, -0.0805],\n",
      "        ...,\n",
      "        [-0.0392, -0.0383,  0.0070,  ..., -0.0899,  0.0326,  0.0249],\n",
      "        [-0.0166, -0.0003,  0.1070,  ...,  0.0023, -0.0075, -0.0619],\n",
      "        [-0.0208, -0.0453, -0.0823,  ..., -0.1111,  0.1887,  0.0999]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6337, 1.7600, 1.7629, 1.8963, 1.9098, 1.6751, 2.1733, 2.6076, 1.5449,\n",
      "        1.6958, 1.9653, 2.0703, 1.9918, 1.6585, 2.4516, 2.1484, 1.5309, 1.3538,\n",
      "        1.6992, 1.6709, 2.5066, 3.3846, 3.5920, 4.1216, 1.0307, 1.4845, 1.6685,\n",
      "        1.9913, 2.2828, 3.4988, 3.6315, 3.8500, 1.3643, 1.3228, 0.9992, 1.4191,\n",
      "        1.9344, 1.9588, 1.6406, 1.8346, 1.1080, 0.7372, 1.3827, 1.2633, 1.2873,\n",
      "        1.4144, 1.6797, 1.8822, 1.5576, 1.6729, 2.0475, 1.9566, 2.2402, 3.5156,\n",
      "        3.3188, 3.5732, 1.3067, 1.7685, 1.9725, 1.8790, 2.2742, 3.4703, 3.2260,\n",
      "        3.3209, 1.6189, 2.2401, 1.6556, 1.3670, 1.8052, 1.7063, 1.9953, 2.0377,\n",
      "        1.9500, 1.9010, 1.7561, 1.7332, 1.4086, 1.8552, 2.0625, 2.1352, 1.3984,\n",
      "        1.1396, 1.3981, 1.6107, 2.2101, 2.0423, 1.9024, 2.0720, 1.6050, 1.4876,\n",
      "        1.2246, 1.2394, 1.3747, 1.6754, 1.9947, 1.9357, 1.7249, 1.7370, 1.7914,\n",
      "        1.7440, 1.6500, 1.6080, 2.3578, 2.2440, 2.0404, 2.0412, 2.0147, 1.5548,\n",
      "        2.0393, 1.4602, 2.4878, 2.2049, 1.6869, 1.5122, 1.5080, 1.0083, 1.4087,\n",
      "        2.0937, 1.8564, 1.8915, 1.6557, 1.2708, 0.9796, 1.7265, 1.4343, 1.5847,\n",
      "        1.8911, 1.8790, 1.4347, 1.5057, 1.3525, 1.3807, 1.2588, 1.8227, 1.8048,\n",
      "        1.3318, 1.7523, 1.6641, 1.6454, 1.2975, 1.6354, 1.3294, 2.0041, 1.8899,\n",
      "        1.0600, 1.1079, 1.0697, 1.6158, 1.4061, 1.5240, 1.7901, 2.3184, 1.5918,\n",
      "        1.1358, 1.5190, 1.4117, 2.0765, 1.9414, 1.8248, 1.9604, 1.8536, 1.3922,\n",
      "        1.0155, 1.2763, 1.8027, 1.5505, 1.7617, 1.9545, 1.2159, 1.3947, 1.4961,\n",
      "        1.5048, 1.4424, 2.0258, 1.9112, 1.9610, 2.1253, 2.3290, 2.2206, 1.9620,\n",
      "        1.9712, 1.7326, 2.3938, 2.3578, 1.7861, 1.8462, 1.9090, 1.8549, 1.6395,\n",
      "        2.6132, 2.3810, 2.3269, 1.8619, 1.2375, 1.5966, 1.2859, 1.9249, 2.0485,\n",
      "        1.7793, 1.9945, 0.9056, 1.3242, 1.0726, 1.7650, 1.4618, 1.4797, 1.7451,\n",
      "        2.0096, 2.0175, 1.7332, 1.5400, 1.6013, 1.7008, 1.8693, 1.9584, 2.2831,\n",
      "        1.7529, 2.0045, 2.0085, 1.8197, 1.6432, 2.1935, 1.5392, 2.2392, 1.0377,\n",
      "        1.2115, 1.6351, 1.9476, 1.8446, 1.4346, 1.8432, 1.8279, 1.6863, 1.0471,\n",
      "        1.1405, 1.2439, 1.3783, 2.0083, 1.7855, 2.1968, 1.9842, 1.7413, 1.8590,\n",
      "        1.8665, 1.8045, 2.3448, 2.9954, 3.0288, 2.0846, 2.0311, 2.0887, 1.9372,\n",
      "        2.4821, 2.3633, 3.0243, 2.9675, 1.5588, 1.3154, 1.1663, 1.0990, 2.1888,\n",
      "        1.5730, 1.9676, 1.8778, 1.7880, 1.0257, 1.5217, 2.1233, 1.5294, 2.0089,\n",
      "        1.8003, 1.9225, 1.6347, 1.3483, 1.5454, 1.2217, 1.2806, 1.9207, 1.8523,\n",
      "        1.9190, 1.5480, 1.5318, 1.3424, 1.6983, 1.8449, 1.4883, 1.7564, 1.8995,\n",
      "        1.6189, 1.2694, 1.2178, 1.7529, 1.9763, 2.2447, 2.0339, 2.0146, 1.9455,\n",
      "        1.3152, 1.3260, 1.2605, 1.6739, 1.4883, 1.9536, 1.9307, 2.1591, 1.5101,\n",
      "        1.5884, 1.9242, 2.2654, 2.2242, 1.8935, 2.2635, 1.0623, 1.4864, 1.4776,\n",
      "        1.6304, 1.5762, 1.8336, 1.9985, 2.2520], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0448,  0.0019,  0.0529,  ..., -0.0895,  0.0385,  0.0121],\n",
      "        [ 0.1131, -0.0248, -0.0683,  ...,  0.0796,  0.1477, -0.0684],\n",
      "        [ 0.0132, -0.0822,  0.1646,  ..., -0.0319,  0.0211,  0.0143],\n",
      "        ...,\n",
      "        [ 0.0876,  0.0296, -0.0593,  ..., -0.0558,  0.0331,  0.0839],\n",
      "        [ 0.1090,  0.0368,  0.0678,  ..., -0.0397, -0.0424, -0.0278],\n",
      "        [-0.0228, -0.0715,  0.1695,  ..., -0.1912, -0.0186,  0.0759]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0146,  0.0669,  0.0628,  ...,  0.0699,  0.0609, -0.0460],\n",
      "        [-0.0259,  0.0440,  0.0093,  ..., -0.0763,  0.0814, -0.0719],\n",
      "        [ 0.0260,  0.0115,  0.0263,  ..., -0.0786, -0.0040, -0.0887],\n",
      "        ...,\n",
      "        [ 0.0031,  0.0294, -0.1311,  ...,  0.1427, -0.0770, -0.1167],\n",
      "        [ 0.0920,  0.1251,  0.0147,  ...,  0.0269, -0.0227, -0.1059],\n",
      "        [-0.2414, -0.0328, -0.0356,  ...,  0.0523,  0.0088, -0.0184]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5527, 1.6836, 1.9057, 2.0700, 1.8872, 1.7274, 2.2482, 2.1215, 1.6831,\n",
      "        1.6709, 1.8061, 1.7953, 2.0014, 2.2334, 2.0840, 2.0100, 1.4847, 1.3700,\n",
      "        1.6244, 1.8292, 2.2017, 2.5366, 2.6763, 2.5882, 0.8686, 1.4008, 1.6317,\n",
      "        1.7958, 1.9868, 2.4663, 2.6693, 2.2738, 1.5796, 1.3381, 1.4306, 1.2349,\n",
      "        1.0504, 1.2101, 1.5936, 1.6932, 1.5338, 1.5388, 1.6129, 1.4604, 1.9296,\n",
      "        1.8910, 1.6164, 1.5873, 1.4555, 1.6198, 1.9103, 2.0822, 2.0350, 2.5993,\n",
      "        2.7536, 2.8014, 1.4121, 1.8084, 2.0293, 1.7922, 2.1079, 2.4175, 2.7038,\n",
      "        2.6871, 1.7676, 1.9740, 1.7641, 1.5678, 1.5016, 2.0393, 1.9101, 1.9341,\n",
      "        1.7609, 1.9777, 1.8762, 2.0067, 1.7348, 1.4864, 1.9480, 1.9633, 1.8620,\n",
      "        1.4972, 1.6819, 1.2940, 1.1250, 1.2779, 1.8183, 1.7357, 1.6343, 1.7543,\n",
      "        1.5193, 1.7725, 2.2930, 1.9039, 1.8066, 1.8178, 1.7647, 1.5868, 1.7468,\n",
      "        1.8785, 2.1126, 2.0701, 2.1605, 2.0891, 1.8717, 2.0131, 2.0067, 1.8450,\n",
      "        1.5731, 1.6982, 2.0890, 2.0686, 1.6830, 1.7705, 1.5698, 2.0105, 2.1196,\n",
      "        1.2902, 1.7660, 1.8228, 1.7187, 1.5760, 1.8091, 1.3199, 1.2591, 2.0989,\n",
      "        1.7923, 1.7583, 1.5653, 1.6532, 1.7333, 1.7522, 1.7245, 1.3768, 1.7222,\n",
      "        1.7858, 1.5699, 1.7473, 1.6486, 1.5443, 1.5633, 1.8391, 1.9007, 1.8051,\n",
      "        1.4656, 1.4234, 1.7060, 1.4061, 2.1120, 1.8643, 1.7289, 1.5457, 1.7553,\n",
      "        1.6197, 1.4914, 1.3728, 1.1500, 1.2303, 1.7243, 1.7132, 1.6175, 1.4258,\n",
      "        1.5289, 1.5401, 1.2048, 2.1726, 1.7051, 1.6495, 2.0435, 1.8500, 1.5929,\n",
      "        1.4828, 1.8095, 1.2305, 1.7223, 2.0698, 1.9618, 2.0583, 2.0588, 1.9041,\n",
      "        1.4948, 2.3093, 2.2549, 2.2688, 1.9004, 1.9927, 1.9676, 1.9637, 2.2409,\n",
      "        1.3802, 2.1645, 2.1262, 1.7710, 1.5278, 1.4636, 1.9029, 1.2497, 1.1409,\n",
      "        1.6232, 1.7810, 1.6648, 1.7164, 1.7614, 1.2102, 1.7844, 2.0994, 1.6078,\n",
      "        1.7682, 1.6891, 1.8583, 1.7302, 1.6331, 1.4540, 1.9424, 1.9737, 2.0331,\n",
      "        1.9947, 1.8180, 1.7956, 1.5962, 1.7092, 1.4826, 1.6836, 1.9929, 1.2224,\n",
      "        1.5670, 1.6229, 1.1581, 1.1049, 2.0730, 1.7103, 1.7277, 2.1365, 1.4394,\n",
      "        1.4532, 1.8331, 1.9981, 1.2285, 1.7264, 1.8582, 2.1642, 1.8281, 2.0565,\n",
      "        1.7690, 1.9134, 1.7711, 2.2791, 2.4941, 1.9043, 1.9575, 1.8916, 2.0040,\n",
      "        1.7748, 1.7361, 2.4646, 2.2605, 1.7605, 1.6842, 1.8603, 1.9928, 1.1350,\n",
      "        1.9951, 1.7126, 1.8104, 1.8551, 1.6262, 1.5254, 1.0824, 1.9858, 1.1804,\n",
      "        1.7378, 1.8603, 1.7102, 1.5969, 1.8054, 1.6991, 2.2189, 1.1911, 1.7671,\n",
      "        1.8433, 1.7125, 1.8152, 1.5446, 1.2737, 1.1487, 2.0897, 1.7754, 1.8121,\n",
      "        1.9613, 1.6838, 1.6091, 1.3819, 1.2131, 1.1501, 1.9194, 1.8058, 1.7156,\n",
      "        1.6409, 1.7919, 1.5859, 1.6545, 2.1121, 1.7657, 1.8743, 1.8266, 1.6771,\n",
      "        1.6245, 1.4244, 1.2024, 1.2840, 1.8309, 1.8192, 1.5122, 1.5164, 1.6434,\n",
      "        1.6908, 2.1068, 2.1800, 1.8020, 1.7915], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1718, -0.0859,  0.0626,  ..., -0.0738, -0.0044, -0.1601],\n",
      "        [ 0.0806, -0.1421, -0.0522,  ..., -0.0081, -0.0670, -0.0989],\n",
      "        [ 0.0815,  0.2038,  0.0710,  ..., -0.1239,  0.1307, -0.0535],\n",
      "        ...,\n",
      "        [-0.0707, -0.0609,  0.0387,  ..., -0.0982, -0.1527,  0.0611],\n",
      "        [ 0.0489,  0.1235, -0.0628,  ..., -0.0616,  0.0903, -0.0107],\n",
      "        [-0.0358,  0.1172,  0.0172,  ..., -0.0789,  0.0425,  0.0145]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0233, -0.0073,  0.0339,  ..., -0.0433,  0.0200, -0.0178],\n",
      "        [-0.0347,  0.0100, -0.0310,  ..., -0.0817,  0.0731, -0.1374],\n",
      "        [-0.0671,  0.0209, -0.0325,  ..., -0.0387,  0.1345,  0.0349],\n",
      "        ...,\n",
      "        [ 0.1500,  0.0596,  0.0145,  ..., -0.0394, -0.0082, -0.0411],\n",
      "        [ 0.0905, -0.0880,  0.0774,  ...,  0.0236,  0.0559,  0.0292],\n",
      "        [-0.1805,  0.1048,  0.0474,  ..., -0.0185, -0.0503,  0.0819]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0482, 2.2054, 2.1130, 2.1690, 2.1033, 2.1033, 2.1896, 2.2411, 2.2094,\n",
      "        2.2062, 2.1566, 2.0390, 2.2411, 2.3306, 2.1092, 2.0593, 1.9134, 2.0421,\n",
      "        1.9810, 1.9861, 1.9764, 2.0428, 1.9879, 2.0051, 1.9920, 1.9401, 1.9586,\n",
      "        2.0162, 1.9205, 1.9393, 2.0056, 1.9118, 2.2752, 2.0620, 2.2300, 2.0310,\n",
      "        2.2491, 2.1708, 2.2792, 2.1153, 2.1641, 2.1633, 2.1612, 2.3271, 2.1935,\n",
      "        2.2529, 2.1785, 2.1978, 2.1322, 2.0636, 2.0524, 2.0556, 2.1083, 2.0846,\n",
      "        2.0446, 2.1149, 2.0666, 2.0849, 2.0137, 2.1390, 2.1111, 2.1890, 2.1029,\n",
      "        2.0855, 2.0210, 2.1745, 2.0110, 2.1265, 2.2169, 2.1051, 2.0760, 2.1005,\n",
      "        2.0326, 2.1207, 2.1206, 2.0985, 2.1123, 2.0757, 2.0717, 2.0485, 2.1681,\n",
      "        2.1364, 2.1675, 2.1683, 2.0006, 2.1257, 2.0500, 1.9842, 2.2526, 2.0743,\n",
      "        2.0438, 1.9284, 2.1485, 1.9768, 2.2405, 2.0458, 2.2747, 2.0617, 1.9371,\n",
      "        2.0708, 2.1509, 2.0495, 2.0280, 2.0986, 2.1464, 2.1382, 2.0966, 2.1338,\n",
      "        2.1161, 2.0509, 2.1437, 2.0878, 2.1791, 2.1567, 2.1656, 2.1607, 2.1816,\n",
      "        2.1440, 2.0449, 2.1404, 2.2582, 2.2058, 2.1648, 2.1728, 2.0921, 2.1581,\n",
      "        2.0928, 2.1445, 2.2917, 2.3270, 2.3724, 2.2153, 2.4160, 2.3167, 2.2819,\n",
      "        2.3519, 2.2251, 2.3708, 2.1061, 2.4067, 2.2176, 2.3679, 2.2010, 2.4127,\n",
      "        1.9146, 2.0938, 2.1696, 2.2662, 1.9167, 2.0657, 2.1099, 2.0635, 1.9511,\n",
      "        2.1411, 2.0539, 2.0871, 2.1541, 2.1513, 1.9938, 2.1734, 1.9720, 2.0564,\n",
      "        2.1374, 2.1709, 2.0768, 2.0241, 2.0354, 2.0808, 2.0879, 2.0835, 2.0600,\n",
      "        2.0384, 2.1927, 2.0334, 2.0393, 2.1262, 2.0929, 2.1409, 2.1634, 2.1845,\n",
      "        2.2695, 2.1263, 2.1816, 2.2454, 2.1058, 2.3415, 2.1144, 2.1534, 2.3145,\n",
      "        2.1172, 2.0954, 2.1621, 1.9731, 1.9773, 2.0125, 2.0745, 2.0563, 2.1125,\n",
      "        2.0364, 1.9882, 2.0921, 1.9555, 2.1685, 2.0167, 2.0927, 2.0390, 2.0627,\n",
      "        1.9650, 2.1938, 2.0023, 2.1007, 2.1025, 2.1417, 2.1735, 2.0449, 2.1532,\n",
      "        2.1784, 2.3249, 2.2909, 2.2143, 2.1204, 2.1510, 2.2076, 2.2003, 1.9442,\n",
      "        1.9668, 2.0574, 1.9579, 1.9148, 2.0424, 2.0019, 1.8696, 2.0179, 1.9948,\n",
      "        2.0719, 1.9325, 1.9205, 2.0282, 2.0161, 1.9891, 2.2015, 2.1748, 2.1023,\n",
      "        2.3245, 2.1440, 2.3064, 2.0869, 1.7306, 2.1436, 2.3464, 2.0865, 2.1621,\n",
      "        2.1862, 2.1456, 2.2508, 2.0938, 2.1479, 2.1038, 2.1359, 2.0511, 2.0187,\n",
      "        2.1498, 2.0779, 2.0487, 2.0279, 2.1443, 1.9305, 2.0750, 2.1543, 2.0195,\n",
      "        2.0664, 2.0958, 1.9939, 2.0505, 2.1063, 2.0129, 2.0719, 2.1502, 2.2006,\n",
      "        2.1159, 2.1666, 2.0913, 2.0745, 2.0868, 2.0683, 1.9278, 1.9997, 2.1085,\n",
      "        2.0804, 2.0655, 2.1112, 2.1723, 2.0991, 2.1548, 2.1594, 2.0304, 2.0639,\n",
      "        2.0324, 2.1243, 2.0022, 2.0328, 2.1483, 1.9289, 2.0476, 1.9602, 1.7725,\n",
      "        1.8443, 1.7937, 1.7828, 1.9617, 1.7623, 1.8179, 1.7928, 1.8182, 1.8827,\n",
      "        1.8228, 1.8497, 1.8697, 1.7796, 1.8182], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0033,  0.2054,  0.0521,  ..., -0.0430, -0.0789, -0.0032],\n",
      "        [-0.0494,  0.0213,  0.0519,  ..., -0.0713, -0.0614, -0.0077],\n",
      "        [ 0.0576,  0.0239, -0.0174,  ..., -0.0183,  0.0689, -0.0190],\n",
      "        ...,\n",
      "        [-0.0003,  0.0031, -0.1341,  ..., -0.1379, -0.0011,  0.0655],\n",
      "        [ 0.0082,  0.0113, -0.0407,  ...,  0.0450,  0.0100,  0.0192],\n",
      "        [ 0.0840,  0.0197,  0.0107,  ..., -0.0154,  0.0061,  0.0379]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0058, -0.0679,  0.0396,  ..., -0.0469,  0.0028, -0.0857],\n",
      "        [ 0.0336,  0.1391, -0.1784,  ...,  0.0617,  0.1397, -0.0124],\n",
      "        [ 0.0215,  0.1162, -0.0030,  ..., -0.0232,  0.0618, -0.0355],\n",
      "        ...,\n",
      "        [-0.0154, -0.0004,  0.0499,  ..., -0.0895,  0.0407,  0.1043],\n",
      "        [-0.0195, -0.0594, -0.0106,  ...,  0.1714,  0.0258, -0.1175],\n",
      "        [-0.0417,  0.0827, -0.1193,  ..., -0.0509, -0.1264, -0.0498]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0040, 1.9559, 1.9719, 2.1341, 2.1760, 1.9445, 2.0996, 1.9815, 2.0508,\n",
      "        1.9584, 1.8002, 1.9554, 1.9745, 1.8003, 2.1552, 2.0786, 1.9150, 1.9798,\n",
      "        1.8310, 1.8955, 2.1815, 1.9694, 1.9566, 2.0163, 1.8500, 1.9120, 1.8482,\n",
      "        2.0386, 2.1839, 2.1060, 1.9633, 1.6475, 2.1423, 1.8811, 1.9456, 1.8412,\n",
      "        1.8994, 1.8917, 1.8796, 2.1834, 1.9456, 2.0173, 2.0474, 2.0117, 2.0106,\n",
      "        2.0075, 2.9191, 1.9125, 1.9543, 1.9893, 2.0150, 1.9304, 1.9379, 1.8482,\n",
      "        1.9900, 1.9346, 2.1602, 2.1297, 2.3495, 2.0662, 1.8931, 1.9441, 4.8674,\n",
      "        1.8816, 2.0862, 2.1221, 2.1362, 2.0283, 1.8881, 2.0475, 1.9672, 1.9226,\n",
      "        1.8503, 2.2211, 1.9469, 2.1239, 1.9504, 1.9870, 1.9125, 1.7402, 1.8235,\n",
      "        1.9516, 1.8790, 2.1946, 1.8776, 1.8752, 1.7352, 2.0060, 1.7244, 2.0466,\n",
      "        2.1788, 1.9950, 2.2474, 1.9243, 1.9560, 2.0585, 1.9626, 2.1690, 2.1201,\n",
      "        2.1559, 2.0604, 2.1946, 1.9485, 2.0176, 1.9604, 2.0090, 2.0718, 2.0529,\n",
      "        1.8487, 1.7985, 2.2043, 2.1143, 2.0630, 1.9901, 2.1701, 1.9707, 2.0658,\n",
      "        1.9131, 1.9332, 2.2194, 2.0729, 1.9535, 2.1774, 2.0564, 1.8340, 2.1281,\n",
      "        1.9420, 1.9687, 1.9398, 1.9034, 1.8167, 2.1144, 2.1498, 1.9992, 2.1961,\n",
      "        1.9690, 1.8544, 2.0149, 1.8851, 1.9738, 1.9919, 1.8912, 2.3173, 2.1065,\n",
      "        1.9866, 1.9809, 1.9712, 2.1023, 1.9959, 1.8835, 2.1337, 2.0374, 1.9877,\n",
      "        1.9378, 2.1066, 1.9457, 1.8648, 1.9330, 2.1748, 1.9181, 1.9934, 2.0136,\n",
      "        2.1109, 1.9425, 2.0450, 2.1345, 2.1150, 2.0668, 2.0774, 1.9717, 1.9636,\n",
      "        1.7347, 2.0716, 2.0190, 1.8438, 1.8391, 1.8263, 2.0583, 2.3159, 1.9855,\n",
      "        1.8125, 1.9918, 2.0440, 2.0084, 2.0004, 1.9904, 1.9613, 2.0108, 1.9930,\n",
      "        1.9525, 2.0195, 1.8131, 2.7143, 2.0945, 1.9469, 2.0026, 1.9517, 1.8786,\n",
      "        2.0359, 1.9882, 1.9751, 2.0598, 1.9443, 2.1008, 1.9952, 2.0906, 2.0281,\n",
      "        1.7348, 1.9644, 1.9214, 1.8692, 2.1121, 2.0779, 2.1423, 2.2565, 2.0303,\n",
      "        1.8940, 2.0688, 1.6688, 1.8827, 1.8633, 2.0113, 1.9551, 2.1850, 2.1410,\n",
      "        2.0469, 2.0941, 2.0045, 1.9320, 2.0846, 1.8719, 1.7587, 2.0429, 1.8645,\n",
      "        2.0160, 2.0406, 1.8866, 1.8828, 2.0873, 2.0077, 1.8897, 1.8515, 2.0193,\n",
      "        2.0525, 2.0216, 1.8177, 2.0155, 2.0024, 1.9471, 1.9866, 1.9463, 1.9293,\n",
      "        2.0859, 1.7917, 1.9248, 1.9185, 1.9749, 1.7910, 1.7858, 2.0815, 2.1918,\n",
      "        2.1333, 2.0849, 1.9805, 1.9842, 2.0301, 2.0212, 2.5474, 2.0420, 1.9107,\n",
      "        1.9044, 2.1810, 1.8701, 2.0022, 1.9817, 1.9358, 2.1984, 1.8543, 1.9793,\n",
      "        2.0779, 1.9338, 1.9107, 2.0659, 2.0024, 2.0126, 2.0324, 2.0306, 1.8533,\n",
      "        1.9070, 1.8813, 1.8921, 2.1766, 1.9688, 2.1409, 1.7569, 2.0445, 2.1129,\n",
      "        1.9598, 1.8595, 1.9245, 2.0670, 2.0063, 2.1707, 1.9824, 2.1156, 1.9823,\n",
      "        2.0775, 1.7144, 1.8626, 2.1228, 2.0479, 1.8072, 1.7946, 2.1915, 2.1204,\n",
      "        1.9599, 2.0327, 1.9645, 2.1784, 1.9447], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 6.9096e-03,  9.0093e-03,  4.5108e-02,  ...,  3.1682e-02,\n",
      "          4.4049e-02,  1.1927e-03],\n",
      "        [ 3.6081e-02,  5.6901e-02,  4.8275e-02,  ..., -1.3207e-01,\n",
      "          1.5424e-01, -5.4517e-02],\n",
      "        [ 1.0428e-01, -3.7398e-02,  1.1350e-01,  ..., -1.7103e-02,\n",
      "         -6.5586e-02,  2.9993e-02],\n",
      "        ...,\n",
      "        [-3.9185e-02,  9.8685e-03,  1.2522e-01,  ..., -7.1830e-02,\n",
      "          1.2110e-04, -2.1157e-02],\n",
      "        [-4.6519e-02, -8.7204e-02,  7.1968e-02,  ...,  3.2845e-03,\n",
      "         -1.2215e-01,  7.9669e-02],\n",
      "        [ 1.8704e-01,  5.7959e-02, -6.8179e-02,  ...,  5.0360e-02,\n",
      "         -1.4938e-02,  5.5053e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0467,  0.0254,  0.0032,  ...,  0.0329, -0.1292, -0.1989],\n",
      "        [-0.0130,  0.0239,  0.0279,  ..., -0.0043, -0.0127,  0.0392],\n",
      "        [-0.0999,  0.0127, -0.0081,  ..., -0.0279,  0.0270, -0.0032],\n",
      "        ...,\n",
      "        [-0.0325, -0.0455, -0.0247,  ..., -0.0798, -0.2549, -0.1018],\n",
      "        [-0.0014,  0.0118,  0.0049,  ...,  0.0606,  0.1390, -0.0190],\n",
      "        [-0.0111,  0.0297, -0.0052,  ..., -0.0325,  0.0013, -0.0695]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5381, 1.3241, 1.3606, 1.3117, 1.5673, 1.7181, 1.6286, 1.6203, 1.5947,\n",
      "        1.4249, 1.1294, 1.3563, 1.4150, 1.3214, 1.6308, 1.5715, 1.5084, 1.3545,\n",
      "        1.0088, 1.3435, 1.6906, 1.4723, 1.8878, 1.9731, 1.0818, 1.0348, 1.3316,\n",
      "        1.1658, 1.1225, 1.9560, 2.0871, 1.8467, 1.0503, 1.1944, 1.1954, 1.6989,\n",
      "        2.1670, 1.5665, 1.8076, 2.5810, 2.0163, 1.5129, 1.6467, 1.4793, 1.4955,\n",
      "        2.1777, 1.9528, 1.9467, 1.6761, 1.7553, 1.7100, 1.5580, 1.4536, 1.5034,\n",
      "        1.8631, 1.9654, 1.6999, 1.7867, 1.5859, 1.7625, 1.7016, 1.5586, 1.8923,\n",
      "        1.9431, 1.1625, 1.6653, 1.4213, 1.9189, 2.0206, 2.0994, 2.3131, 2.0310,\n",
      "        1.2126, 1.2500, 1.5079, 2.0828, 2.0491, 1.6412, 2.2873, 2.1174, 1.7807,\n",
      "        1.3064, 1.6816, 1.0816, 1.8376, 1.7215, 1.3270, 1.4578, 1.6459, 1.7349,\n",
      "        1.3885, 1.8085, 1.1415, 1.1752, 1.3183, 1.3184, 1.1481, 0.9861, 1.0752,\n",
      "        1.1474, 1.4940, 1.4680, 1.4413, 1.5983, 1.1378, 0.9281, 1.2734, 1.3845,\n",
      "        1.2613, 1.5516, 1.4273, 1.4636, 1.5497, 1.6500, 1.9046, 1.7097, 1.8287,\n",
      "        1.6713, 2.2520, 2.3423, 1.5076, 1.6906, 1.7861, 1.8009, 1.5837, 1.9797,\n",
      "        2.2663, 2.3626, 2.0189, 1.7018, 1.6993, 1.5177, 1.6955, 2.2770, 1.9597,\n",
      "        1.9259, 1.6370, 1.7430, 1.6526, 1.3704, 1.7034, 1.3912, 1.8728, 1.9157,\n",
      "        1.9089, 1.2782, 1.6891, 1.2273, 1.2931, 1.4038, 1.6021, 2.1480, 1.2879,\n",
      "        1.2745, 1.1777, 1.8134, 1.9397, 2.0201, 1.7271, 1.7985, 1.4744, 1.4170,\n",
      "        1.6002, 1.7605, 1.5343, 2.2958, 1.8005, 1.8268, 1.6362, 1.4320, 1.6918,\n",
      "        1.7405, 1.8883, 1.4965, 1.6768, 1.9683, 0.7655, 1.3468, 1.5213, 1.4466,\n",
      "        2.0985, 2.0150, 2.4076, 2.0058, 1.3755, 1.1222, 1.2161, 1.6605, 1.9287,\n",
      "        2.0339, 2.3360, 1.9510, 1.6895, 1.2065, 1.6309, 1.8707, 1.8506, 1.8763,\n",
      "        1.6442, 1.9454, 1.2582, 1.4368, 1.0756, 1.2356, 1.2497, 1.3988, 1.6670,\n",
      "        1.6304, 1.0059, 1.3316, 1.2845, 1.2817, 1.2350, 1.2023, 1.7394, 1.7847,\n",
      "        1.2201, 1.1940, 1.2930, 1.3842, 1.1869, 1.7422, 1.9143, 1.8138, 1.4444,\n",
      "        1.3211, 1.7328, 1.7142, 1.2688, 1.3226, 1.6830, 1.6361, 1.6761, 1.4869,\n",
      "        1.0866, 1.1001, 1.6807, 1.9232, 1.5747, 1.8789, 0.7083, 1.5451, 1.4541,\n",
      "        1.6945, 2.1873, 1.7500, 2.1163, 2.2851, 1.3602, 1.1812, 1.4561, 1.5435,\n",
      "        2.2452, 1.7591, 2.2909, 2.3954, 1.9507, 1.9902, 1.9600, 1.6377, 1.3966,\n",
      "        2.1447, 1.8187, 2.0263, 1.8509, 1.8431, 1.8020, 1.3191, 1.7604, 1.5306,\n",
      "        1.9956, 1.9023, 1.8232, 1.9189, 1.9032, 1.8481, 1.4953, 2.1604, 2.1009,\n",
      "        2.0643, 1.7075, 1.7333, 1.9907, 1.9706, 1.7323, 1.5755, 2.2104, 2.0081,\n",
      "        1.2970, 1.3091, 1.3988, 1.2809, 1.3336, 1.1335, 2.0048, 2.0325, 1.2376,\n",
      "        1.2608, 1.2689, 1.3746, 1.2606, 1.5977, 1.5886, 1.3858, 0.7806, 0.7912,\n",
      "        0.8362, 1.1981, 1.9292, 2.0808, 2.0394, 1.9664, 1.7727, 1.4186, 1.5101,\n",
      "        1.5409, 1.7831, 2.0739, 1.9813, 1.6855], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0232,  0.0128,  0.0074,  ...,  0.0509,  0.0133, -0.0386],\n",
      "        [-0.0883,  0.0237, -0.0643,  ...,  0.0992, -0.0433,  0.0486],\n",
      "        [-0.0292, -0.0248, -0.0834,  ..., -0.0787,  0.1826, -0.1204],\n",
      "        ...,\n",
      "        [ 0.0445, -0.0735,  0.0530,  ...,  0.0662, -0.0577, -0.0873],\n",
      "        [ 0.0491, -0.0735, -0.0313,  ...,  0.0187,  0.0054, -0.0281],\n",
      "        [-0.0102, -0.0021,  0.0063,  ..., -0.0803,  0.0854, -0.0696]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0805,  0.0673,  0.0426,  ..., -0.0207,  0.0048,  0.0317],\n",
      "        [-0.0556,  0.0072,  0.0265,  ...,  0.0056,  0.0443, -0.0146],\n",
      "        [-0.0879, -0.0427, -0.0578,  ...,  0.1052,  0.0684, -0.0826],\n",
      "        ...,\n",
      "        [-0.0928, -0.0110,  0.0696,  ..., -0.1158,  0.0105, -0.0457],\n",
      "        [ 0.0775, -0.0020,  0.0036,  ...,  0.0620,  0.0231, -0.0015],\n",
      "        [-0.0437, -0.0561,  0.0451,  ...,  0.0499,  0.0158,  0.0429]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4120, 1.2885, 1.4003, 1.4359, 1.2012, 1.1708, 1.5482, 1.5687, 1.4945,\n",
      "        1.4145, 1.3142, 1.3258, 1.4146, 1.8676, 1.5420, 1.5307, 1.1158, 1.5155,\n",
      "        1.0557, 1.2026, 1.3733, 1.3775, 1.5734, 1.4394, 1.2882, 1.0482, 1.5250,\n",
      "        1.2323, 1.2005, 1.3294, 1.7931, 1.6491, 1.5698, 1.5052, 1.5440, 1.3188,\n",
      "        1.2876, 2.0958, 1.7369, 1.8793, 1.4784, 1.4597, 1.4362, 1.7059, 2.0755,\n",
      "        1.3089, 1.8648, 1.8775, 1.6831, 1.7312, 1.7440, 1.5016, 1.5092, 1.4459,\n",
      "        1.7224, 1.8467, 1.6303, 1.8358, 1.7772, 1.7181, 1.4353, 2.0451, 1.7525,\n",
      "        1.9263, 1.1927, 1.5767, 1.5799, 1.7375, 1.5925, 1.3498, 1.8650, 1.7685,\n",
      "        1.2294, 1.2721, 1.6534, 1.7710, 1.4073, 1.6553, 1.6974, 1.8732, 1.8131,\n",
      "        1.4575, 1.6273, 1.5673, 0.9564, 1.0141, 1.3880, 1.4738, 1.5901, 1.6948,\n",
      "        1.4297, 1.2684, 1.5858, 1.3848, 1.3454, 1.3128, 1.0242, 1.2420, 1.2504,\n",
      "        1.3265, 1.0533, 1.0974, 1.3863, 1.4605, 1.2098, 1.1468, 1.3285, 1.0935,\n",
      "        1.1326, 1.2770, 1.3693, 1.4441, 1.4769, 1.5894, 1.9289, 1.7987, 1.7325,\n",
      "        1.7949, 1.9599, 1.8069, 1.5693, 1.7317, 1.7642, 1.8238, 1.7539, 1.6203,\n",
      "        1.9929, 1.8863, 1.9249, 1.7468, 1.8039, 1.6011, 1.2425, 1.1449, 1.8424,\n",
      "        1.8389, 1.7078, 1.8506, 1.7749, 1.5125, 1.4091, 2.3307, 1.7605, 1.7613,\n",
      "        1.8183, 1.4697, 1.3213, 1.8501, 1.8321, 1.9360, 1.5376, 1.7318, 1.4935,\n",
      "        1.4940, 1.5666, 1.1238, 1.0510, 1.2036, 1.6913, 1.6986, 1.5926, 1.5324,\n",
      "        1.4486, 1.3799, 1.6664, 1.1669, 1.6408, 1.7325, 1.5452, 1.6070, 1.4553,\n",
      "        1.4355, 1.1947, 1.9876, 1.6986, 1.7164, 1.5539, 1.0645, 1.3167, 1.4890,\n",
      "        1.6279, 1.8737, 1.9548, 1.8404, 0.8375, 1.5047, 1.3291, 1.3541, 1.7082,\n",
      "        1.8385, 1.9628, 1.8327, 1.7749, 1.4125, 1.3953, 1.1282, 1.0333, 1.1730,\n",
      "        1.6001, 1.7618, 1.3852, 1.5273, 1.5377, 1.5882, 1.9737, 1.8627, 1.6450,\n",
      "        1.5467, 0.8895, 0.9862, 1.0044, 1.3033, 1.1541, 1.2802, 1.4819, 1.4580,\n",
      "        1.0285, 1.1637, 1.3768, 1.2034, 1.1819, 1.2230, 1.7715, 1.3497, 1.5540,\n",
      "        1.4253, 1.6287, 1.0641, 1.8440, 1.9160, 1.6406, 1.5991, 1.7898, 1.5941,\n",
      "        1.3458, 1.7170, 1.0052, 1.0840, 1.5320, 1.6331, 1.5172, 1.5524, 1.5585,\n",
      "        1.6600, 1.6216, 1.7651, 1.9542, 1.9153, 1.1167, 0.9682, 1.1384, 1.2487,\n",
      "        1.8588, 1.7127, 2.0839, 1.9503, 1.9314, 1.9193, 1.9716, 1.7119, 1.9399,\n",
      "        1.2639, 1.6187, 1.7545, 1.7953, 1.8933, 1.8136, 1.6014, 1.4758, 2.1887,\n",
      "        1.8906, 1.7025, 1.7860, 1.9090, 1.9333, 1.9106, 1.6137, 1.4672, 1.9806,\n",
      "        1.8559, 1.6878, 1.6937, 1.8634, 1.8420, 1.5637, 2.0973, 2.0649, 1.9817,\n",
      "        0.9981, 0.9282, 1.1798, 1.1234, 1.3285, 1.2654, 1.6858, 1.4549, 0.8565,\n",
      "        1.0240, 1.2684, 1.3303, 1.1138, 1.1891, 1.4737, 1.4395, 1.1795, 1.5165,\n",
      "        1.6334, 1.6215, 1.6741, 1.8065, 1.8570, 1.8657, 1.4551, 1.1416, 1.0379,\n",
      "        1.5223, 1.7389, 1.8636, 1.8447, 1.7293], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1136,  0.1394,  0.0808,  ...,  0.1340, -0.0458,  0.1404],\n",
      "        [ 0.0126,  0.1288, -0.1405,  ..., -0.2244, -0.0030, -0.0677],\n",
      "        [ 0.0645, -0.0071, -0.0293,  ..., -0.0629,  0.0924,  0.0253],\n",
      "        ...,\n",
      "        [-0.0759, -0.1033,  0.0147,  ...,  0.0384, -0.0330,  0.0149],\n",
      "        [-0.1311,  0.0460,  0.0167,  ..., -0.0687, -0.0850,  0.0366],\n",
      "        [ 0.2005, -0.0571,  0.1022,  ..., -0.0224,  0.1006, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-7.7648e-02, -6.2766e-03, -5.0111e-03,  ..., -4.1308e-03,\n",
      "          3.2643e-05, -2.3375e-02],\n",
      "        [ 6.6529e-02,  4.9720e-02,  5.4404e-02,  ..., -7.5455e-02,\n",
      "         -1.8741e-01, -2.4146e-02],\n",
      "        [ 3.5613e-02, -1.5745e-02,  6.3330e-02,  ..., -5.0738e-02,\n",
      "         -7.5435e-02, -6.5039e-02],\n",
      "        ...,\n",
      "        [ 1.5077e-03, -3.9175e-02, -2.5339e-02,  ..., -1.6529e-01,\n",
      "         -3.0051e-02,  7.5463e-02],\n",
      "        [ 3.3585e-02,  1.4387e-01,  1.9341e-02,  ..., -1.6217e-01,\n",
      "          2.7490e-02, -7.6189e-02],\n",
      "        [-1.2541e-01,  6.7943e-02,  1.3112e-02,  ..., -1.1398e-01,\n",
      "         -6.8334e-02,  3.3773e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0459, 2.0872, 2.0457, 2.0850, 2.3455, 2.2168, 2.0853, 1.9858, 2.1007,\n",
      "        2.1373, 2.1752, 2.0311, 2.1220, 2.1631, 2.0555, 2.1820, 2.2886, 2.2388,\n",
      "        2.2620, 2.3479, 2.2422, 2.1314, 2.3345, 2.4123, 2.2920, 2.2968, 2.2117,\n",
      "        2.1382, 2.2910, 2.1872, 2.2931, 2.2569, 2.0402, 1.9761, 2.1097, 2.0977,\n",
      "        1.9650, 2.0153, 2.0831, 2.0870, 2.0587, 2.0829, 1.9905, 2.0311, 2.0403,\n",
      "        2.0348, 2.0150, 1.9851, 2.1885, 2.2447, 2.2228, 2.3064, 1.8946, 2.1378,\n",
      "        2.2134, 2.0713, 2.0203, 2.1733, 2.2562, 2.2405, 2.2758, 2.0138, 2.3362,\n",
      "        2.2121, 2.2693, 2.3481, 2.1181, 2.1267, 2.4583, 2.1993, 2.2224, 2.1675,\n",
      "        2.2083, 2.2359, 2.2512, 2.1663, 2.2373, 2.1530, 2.1574, 2.2369, 2.1073,\n",
      "        2.1448, 2.2113, 2.1324, 2.2754, 2.1430, 2.1214, 2.1327, 2.2852, 2.0837,\n",
      "        2.1750, 2.1983, 2.1430, 2.2045, 2.0853, 2.0761, 2.3358, 2.0172, 2.2436,\n",
      "        2.1088, 2.2148, 2.1317, 2.0736, 2.1319, 2.1974, 2.1190, 2.2420, 2.2013,\n",
      "        2.3971, 2.2047, 2.1358, 2.1751, 2.1783, 2.2657, 2.0836, 2.1345, 2.0512,\n",
      "        2.1883, 2.2334, 2.2579, 2.5340, 2.1016, 2.1879, 2.2572, 2.0334, 2.1877,\n",
      "        2.2357, 2.2800, 2.2116, 2.0817, 2.2148, 2.2371, 2.2617, 2.2827, 2.0812,\n",
      "        2.2271, 2.1301, 2.2714, 2.1242, 2.3077, 2.2771, 2.2564, 2.1113, 2.1436,\n",
      "        2.0739, 2.1041, 1.8338, 1.9468, 2.0473, 2.0995, 2.1161, 1.9709, 2.2373,\n",
      "        2.1643, 2.0425, 2.1024, 2.0105, 2.1975, 2.0431, 2.0336, 2.1611, 2.0961,\n",
      "        2.0551, 2.0954, 2.1029, 2.1169, 2.1227, 2.0961, 2.0525, 2.0018, 2.1081,\n",
      "        2.1327, 2.0566, 1.9604, 2.0457, 2.1595, 1.9850, 1.8274, 2.0199, 1.9642,\n",
      "        1.8366, 1.9036, 1.9247, 1.9568, 2.0136, 2.0041, 1.9606, 1.8684, 1.9361,\n",
      "        1.9552, 1.9374, 1.8698, 2.0734, 1.8850, 2.0125, 2.1891, 2.1604, 2.0338,\n",
      "        2.0337, 1.9774, 2.1750, 2.2208, 2.0105, 2.1371, 2.1542, 2.0531, 1.8514,\n",
      "        2.1638, 2.5341, 2.5410, 2.5105, 2.5094, 2.5290, 2.9251, 2.5418, 2.2712,\n",
      "        2.1183, 2.3824, 2.2876, 2.2975, 2.2318, 2.4948, 2.3729, 2.3485, 2.1484,\n",
      "        2.1590, 2.2384, 2.1686, 2.1431, 1.9848, 2.1943, 2.1450, 2.2909, 2.0974,\n",
      "        2.1466, 2.1537, 2.0876, 2.1705, 2.1065, 1.9446, 2.0186, 2.0394, 1.9057,\n",
      "        1.9337, 1.9500, 1.9467, 1.9051, 1.9885, 1.9479, 1.9157, 1.9688, 1.9108,\n",
      "        1.8772, 1.8621, 1.9732, 1.9708, 2.2738, 2.2149, 1.9944, 2.2762, 2.1991,\n",
      "        2.1562, 2.0752, 2.2249, 2.2091, 2.2391, 2.1431, 2.1852, 2.2309, 2.1233,\n",
      "        2.1733, 2.0893, 2.2208, 2.1557, 2.1793, 2.2890, 2.1551, 2.2233, 2.2100,\n",
      "        2.1162, 2.2384, 2.2032, 2.1036, 2.0494, 2.0176, 2.3342, 2.1241, 2.2456,\n",
      "        3.5250, 2.7166, 2.6705, 3.1644, 2.3545, 2.8128, 2.5481, 2.6732, 3.5540,\n",
      "        2.5471, 2.7344, 2.6662, 3.0300, 2.7949, 2.7925, 2.7367, 2.1109, 1.9796,\n",
      "        1.9016, 1.8760, 1.9274, 1.9137, 1.9299, 2.1599, 1.9690, 1.9874, 1.9013,\n",
      "        1.9380, 2.1150, 1.9397, 1.9751, 1.9484], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0963, -0.0767,  0.0546,  ..., -0.0051, -0.0175, -0.0335],\n",
      "        [ 0.0110,  0.0416,  0.0049,  ..., -0.0334,  0.0195,  0.0284],\n",
      "        [-0.0472,  0.0821, -0.0153,  ..., -0.0571,  0.0504, -0.0197],\n",
      "        ...,\n",
      "        [-0.0540, -0.0311,  0.0638,  ...,  0.0519,  0.0094, -0.2150],\n",
      "        [ 0.0354,  0.0642, -0.0220,  ..., -0.0137,  0.2641,  0.0333],\n",
      "        [-0.0912, -0.0097, -0.0008,  ...,  0.0239, -0.0368,  0.0488]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0470,  0.0703,  0.1437,  ..., -0.0295, -0.0685, -0.0047],\n",
      "        [-0.0250, -0.0849,  0.1145,  ...,  0.0571, -0.0512,  0.0459],\n",
      "        [ 0.0214,  0.0659, -0.1185,  ..., -0.0499, -0.0136, -0.0093],\n",
      "        ...,\n",
      "        [-0.0645,  0.0273, -0.0748,  ...,  0.1247, -0.0636, -0.0256],\n",
      "        [ 0.0135,  0.0323,  0.0477,  ..., -0.0681, -0.0011,  0.0257],\n",
      "        [ 0.0150, -0.0669,  0.0234,  ...,  0.0196,  0.0166, -0.1589]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2006, 2.0657, 2.1802, 2.2342, 2.5547, 2.2691, 2.1380, 2.3179, 2.2961,\n",
      "        2.1777, 2.1096, 2.4494, 2.5215, 2.0670, 2.3622, 2.1919, 2.2548, 2.1264,\n",
      "        2.4321, 2.0805, 2.4589, 2.4542, 2.0407, 2.1860, 2.1341, 1.9930, 2.2346,\n",
      "        2.2173, 2.4632, 2.2399, 2.1041, 2.0852, 2.4457, 2.1679, 2.1058, 2.0830,\n",
      "        2.1188, 2.2574, 2.3751, 2.1786, 2.1387, 2.3536, 2.3232, 2.2077, 2.3385,\n",
      "        2.1442, 3.1453, 1.9829, 2.3005, 2.5675, 2.2675, 2.2296, 2.0703, 2.3497,\n",
      "        2.0743, 2.1796, 2.2047, 2.3915, 2.6277, 2.4032, 2.3249, 2.1075, 6.9186,\n",
      "        2.1943, 2.2000, 2.1898, 2.3786, 2.6556, 2.3278, 2.1597, 2.2228, 2.1402,\n",
      "        2.0080, 2.3187, 2.1237, 2.3826, 2.2431, 2.4328, 1.9139, 1.9923, 2.0537,\n",
      "        2.1299, 2.3023, 2.2714, 2.0234, 2.1668, 2.4934, 2.4340, 1.9738, 2.2208,\n",
      "        2.1318, 2.0382, 2.4105, 2.2387, 2.2245, 2.2876, 2.1124, 2.2899, 2.1746,\n",
      "        2.2922, 2.3415, 2.2709, 2.4396, 2.4025, 2.3144, 2.3605, 2.1049, 2.2974,\n",
      "        2.0244, 2.3103, 2.2656, 2.2653, 2.2157, 2.1713, 2.4520, 2.1407, 2.2505,\n",
      "        2.3006, 2.4731, 2.5070, 2.4000, 2.2931, 2.2897, 2.2192, 2.0109, 2.5255,\n",
      "        2.1196, 2.1500, 2.2468, 2.3480, 2.1511, 2.0098, 2.1829, 2.1692, 2.4278,\n",
      "        2.1292, 2.2237, 2.4373, 2.1596, 2.3053, 2.1168, 2.1038, 2.2343, 2.3608,\n",
      "        2.3900, 2.2983, 2.1589, 2.3392, 2.1760, 2.2740, 2.4360, 2.2611, 2.3880,\n",
      "        2.1533, 2.2852, 2.2903, 2.0797, 2.3667, 2.2895, 2.1738, 2.0741, 2.2175,\n",
      "        2.3321, 2.3705, 2.3487, 2.3343, 2.3052, 2.1094, 2.3291, 2.1580, 2.3033,\n",
      "        2.3078, 2.2335, 2.2087, 2.2596, 2.0513, 2.2749, 2.2451, 2.1753, 2.0152,\n",
      "        2.0275, 2.0962, 2.1804, 2.1933, 2.3181, 2.1861, 2.3058, 2.2507, 1.9660,\n",
      "        2.2241, 2.1431, 2.0907, 3.3484, 2.1541, 2.2170, 2.2873, 2.4384, 2.2563,\n",
      "        2.3896, 2.2983, 2.2926, 2.3600, 2.2755, 2.2477, 2.2492, 2.3264, 1.9497,\n",
      "        2.2523, 2.6115, 2.1607, 2.1900, 2.3667, 2.3901, 2.3304, 2.3205, 2.1655,\n",
      "        2.1768, 2.0320, 2.0232, 2.1321, 2.0882, 2.1043, 2.4262, 2.4299, 2.3716,\n",
      "        2.2494, 2.3321, 2.1386, 1.9385, 2.3993, 2.1341, 2.1022, 2.0628, 2.3105,\n",
      "        2.0207, 2.1714, 2.2510, 2.2480, 2.1001, 2.2838, 2.1232, 2.0884, 2.3338,\n",
      "        2.0860, 2.2668, 2.0312, 2.3519, 1.9840, 2.0622, 2.3944, 2.2628, 2.1638,\n",
      "        2.0397, 1.9932, 2.2424, 2.2398, 2.3338, 2.0501, 2.3164, 2.3076, 2.1087,\n",
      "        2.1849, 2.2238, 2.1393, 2.1875, 2.1841, 2.3230, 2.8114, 2.3913, 2.1197,\n",
      "        1.9563, 2.1896, 2.0379, 2.4505, 2.4374, 2.3711, 2.1429, 2.0893, 2.2961,\n",
      "        2.1974, 2.2334, 2.1074, 2.2679, 2.5741, 2.1968, 2.2795, 2.2866, 2.3324,\n",
      "        2.1067, 2.1641, 2.1761, 2.3181, 2.2413, 2.3005, 2.0779, 2.0538, 2.1956,\n",
      "        2.0923, 2.0574, 2.2962, 2.2111, 2.1847, 2.1963, 2.1551, 2.3691, 2.2281,\n",
      "        2.3378, 2.0081, 2.0089, 2.0579, 2.3257, 1.9849, 1.9308, 2.3321, 2.3208,\n",
      "        2.1659, 2.3828, 2.1172, 2.2186, 2.0117], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 1.7043e-01, -8.2647e-01,  2.4838e-02,  ...,  4.7189e-02,\n",
      "         -8.3786e-02,  2.5851e-02],\n",
      "        [-3.5719e-01,  2.8952e-02,  8.5006e-02,  ...,  5.7942e-02,\n",
      "          2.0922e-01, -2.6888e-01],\n",
      "        [ 7.5063e-02,  1.2485e-01,  6.2840e-02,  ..., -5.7912e-02,\n",
      "          8.7211e-02,  5.9190e-02],\n",
      "        ...,\n",
      "        [ 6.2899e-03,  2.6713e-02, -3.9605e-02,  ...,  1.0640e-01,\n",
      "          7.5123e-04, -5.9767e-02],\n",
      "        [ 4.4162e-02,  4.6904e-03,  8.3011e-02,  ...,  1.4559e-01,\n",
      "          2.7426e-02, -3.2084e-02],\n",
      "        [ 1.7775e-04, -1.3213e-02, -1.0379e-02,  ..., -9.3273e-02,\n",
      "          1.4039e-01, -3.1797e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0285, -0.0048, -0.0054,  ...,  0.0129, -0.0562,  0.0302],\n",
      "        [ 0.1384,  0.0217, -0.0380,  ...,  0.0082, -0.0462,  0.0579],\n",
      "        [ 0.1178,  0.0019,  0.0343,  ...,  0.0018,  0.0262, -0.0057],\n",
      "        ...,\n",
      "        [ 0.0227, -0.0180,  0.0703,  ...,  0.0152,  0.0150, -0.0309],\n",
      "        [-0.0003,  0.0125,  0.0585,  ..., -0.1054, -0.0656,  0.0119],\n",
      "        [ 0.0313, -0.0175, -0.0365,  ...,  0.0937, -0.1237, -0.0196]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([0.5873, 0.9400, 0.8834, 1.0011, 0.8351, 1.6801, 0.8591, 1.3857, 0.7457,\n",
      "        0.6151, 0.8057, 1.1133, 1.2734, 0.9145, 1.2189, 1.8857, 2.3503, 1.6414,\n",
      "        1.3544, 0.8303, 2.1570, 0.5538, 1.0793, 1.0558, 0.8334, 0.7435, 1.4016,\n",
      "        1.6589, 1.0253, 2.0550, 1.1909, 2.2155, 1.3714, 1.6381, 1.6177, 1.6315,\n",
      "        1.5257, 0.9768, 1.3578, 1.1207, 1.3782, 0.7250, 0.9063, 0.7317, 0.9410,\n",
      "        1.7056, 1.5990, 1.0967, 0.5155, 0.8857, 0.7932, 0.7848, 1.0841, 1.2641,\n",
      "        0.8809, 1.1028, 0.6627, 0.8378, 1.0029, 1.2287, 1.3782, 0.4884, 1.3836,\n",
      "        1.1725, 1.1586, 0.8844, 0.9265, 1.6501, 1.8563, 0.6485, 1.6144, 1.1125,\n",
      "        1.7792, 1.4165, 1.4069, 0.7714, 0.4669, 1.7467, 1.6306, 1.4739, 1.1572,\n",
      "        1.2335, 0.9374, 1.2641, 0.6478, 1.1517, 1.5467, 1.6535, 1.6447, 1.1364,\n",
      "        1.5612, 1.3593, 1.6656, 1.8102, 1.0880, 1.5107, 0.5529, 0.5377, 0.6984,\n",
      "        0.8919, 0.6094, 1.2246, 2.0509, 1.5282, 0.6284, 0.5584, 0.5203, 1.3957,\n",
      "        1.0501, 1.1043, 1.2537, 1.6188, 0.4961, 0.4438, 0.6898, 0.5104, 0.6881,\n",
      "        1.0709, 1.0670, 0.9157, 0.4899, 0.5834, 0.6328, 0.6624, 1.2185, 0.5718,\n",
      "        1.4091, 1.2455, 0.8632, 0.6179, 1.5918, 1.5366, 1.0506, 0.8979, 1.0382,\n",
      "        0.7717, 0.3410, 0.8559, 1.3751, 0.9877, 1.1721, 1.2376, 1.0866, 0.7256,\n",
      "        1.8115, 1.2615, 1.5351, 1.6051, 2.2750, 0.4583, 1.5959, 1.8159, 1.5986,\n",
      "        1.2386, 1.2219, 0.5143, 0.4552, 2.5652, 1.4507, 0.9394, 0.7338, 0.8319,\n",
      "        1.0903, 1.1077, 1.3615, 1.5297, 1.5645, 0.7571, 1.0886, 1.0412, 0.8878,\n",
      "        1.3356, 1.8191, 1.3875, 0.7934, 0.7322, 1.2949, 0.9306, 1.2011, 1.9200,\n",
      "        1.4277, 1.2990, 0.9323, 1.6592, 0.7322, 0.8158, 1.0060, 1.9004, 1.2868,\n",
      "        1.2296, 1.4783, 1.2853, 0.5865, 1.1468, 0.8820, 1.1706, 1.2369, 0.6446,\n",
      "        1.0860, 1.6630, 0.4944, 0.9224, 1.2387, 0.9313, 0.7568, 1.0396, 1.7756,\n",
      "        0.9973, 0.4420, 0.6338, 1.4626, 0.9706, 0.8584, 0.7334, 0.6792, 0.8190,\n",
      "        0.4998, 0.5863, 0.3591, 1.1064, 1.1684, 1.3195, 0.7124, 0.6288, 3.4058,\n",
      "        2.2894, 2.3039, 2.3184, 1.4553, 2.1919, 1.5340, 1.7372, 3.1814, 2.2748,\n",
      "        2.9761, 1.6589, 2.8641, 1.8701, 2.1067, 2.2280, 1.1328, 0.8959, 0.5703,\n",
      "        1.4181, 1.8598, 0.8204, 1.1057, 1.2013, 0.6950, 1.1179, 1.4333, 1.1852,\n",
      "        0.5071, 1.8742, 1.0409, 0.8568, 0.5709, 1.1396, 1.4776, 1.2673, 1.6857,\n",
      "        0.5480, 1.7507, 0.8236, 1.0195, 0.9362, 0.5892, 1.5747, 0.7228, 1.8882,\n",
      "        1.9171, 0.8700, 1.3362, 1.5765, 1.6774, 1.3801, 1.2409, 0.7780, 1.7569,\n",
      "        1.2493, 0.9076, 1.0342, 2.0558, 1.9531, 2.2741, 2.4913, 1.1082, 1.1169,\n",
      "        1.6298, 0.7658, 1.6727, 1.8809, 1.2403, 0.7066, 1.6247, 1.0856, 0.8546,\n",
      "        1.2991, 0.5652, 0.9862, 1.1118, 1.9982, 1.8534, 1.1159, 0.5336, 0.5210,\n",
      "        1.3930, 0.4745, 0.4598, 2.1504, 1.5160, 1.1264, 1.4919, 1.9552, 1.1071,\n",
      "        1.4930, 2.3807, 0.5510, 0.8693, 1.0496], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.1005,  0.5498,  0.0729,  ..., -0.0110,  0.1209,  0.0658],\n",
      "        [ 0.0184,  0.0503,  0.0966,  ..., -0.0994, -0.3125, -0.1418],\n",
      "        [ 0.1038, -0.4246,  0.3924,  ...,  0.0704,  0.0173, -0.0551],\n",
      "        ...,\n",
      "        [-0.0880,  0.1564, -0.0328,  ...,  0.0017,  0.0048,  0.0033],\n",
      "        [ 0.2349,  0.0311, -0.0238,  ..., -0.0441, -0.0662,  0.0753],\n",
      "        [-0.1147,  0.0987, -0.0241,  ..., -0.0287,  0.1925, -0.0308]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0180, -0.0271, -0.0438,  ...,  0.0169,  0.0264, -0.0519],\n",
      "        [-0.0430,  0.0296,  0.2383,  ..., -0.0142,  0.0167,  0.0145],\n",
      "        [-0.0328,  0.0239, -0.0464,  ..., -0.1115, -0.0121, -0.0186],\n",
      "        ...,\n",
      "        [ 0.2671, -0.1343,  0.0173,  ...,  0.1614, -0.2601,  0.1452],\n",
      "        [ 0.0416, -0.0048, -0.0423,  ...,  0.0013,  0.0025, -0.0995],\n",
      "        [-0.0216,  0.0315, -0.0021,  ...,  0.0808, -0.0296, -0.0366]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([0.8374, 1.5018, 1.3041, 1.5074, 1.3031, 1.5585, 0.8238, 1.3315, 1.1663,\n",
      "        1.3236, 1.5947, 1.8838, 1.3975, 1.2249, 1.2498, 1.8339, 2.2672, 1.5248,\n",
      "        1.4410, 1.9168, 1.6767, 1.8155, 1.1106, 1.0726, 1.4798, 1.1436, 2.3019,\n",
      "        2.0788, 2.0536, 1.5654, 1.1589, 2.2396, 2.3073, 2.3499, 2.3821, 1.6036,\n",
      "        1.5135, 1.2445, 1.7083, 1.1409, 1.3000, 1.9088, 1.5593, 1.8481, 1.3570,\n",
      "        1.7538, 1.8334, 1.1688, 0.9453, 0.7923, 1.3227, 1.8655, 1.6307, 1.5398,\n",
      "        0.9651, 1.2580, 1.0628, 1.5225, 1.7023, 1.2249, 2.1230, 1.0524, 1.3691,\n",
      "        1.7108, 1.7749, 0.9035, 1.7066, 1.9496, 1.9017, 1.7468, 1.6053, 1.0325,\n",
      "        1.2983, 2.1550, 2.0419, 1.7701, 1.3376, 1.3285, 1.7744, 1.8624, 2.0150,\n",
      "        2.3612, 1.5541, 1.7043, 1.5866, 1.2589, 2.0176, 1.9571, 2.3139, 1.5499,\n",
      "        2.3821, 1.8515, 1.2197, 1.7504, 0.8956, 2.0813, 0.8289, 1.1110, 1.2757,\n",
      "        1.0507, 1.2258, 1.6359, 1.9388, 1.8491, 0.9566, 0.9820, 1.1050, 1.5810,\n",
      "        1.4832, 1.6291, 1.5713, 1.6667, 0.5607, 0.8297, 0.9082, 0.8456, 1.2877,\n",
      "        1.8089, 1.1470, 0.7896, 0.6228, 0.7809, 1.1584, 1.6024, 1.8576, 1.0482,\n",
      "        1.3274, 1.8177, 1.4362, 0.8596, 1.2995, 1.3695, 0.6723, 0.6998, 1.0542,\n",
      "        0.9279, 0.4507, 1.1520, 1.2370, 0.8146, 0.7387, 0.5565, 1.1019, 0.6311,\n",
      "        2.1095, 1.7319, 1.9390, 1.9829, 1.4765, 2.6327, 1.5222, 1.8818, 2.4252,\n",
      "        2.2102, 2.2935, 1.9216, 2.1528, 2.0348, 1.5985, 0.9358, 1.5996, 1.8172,\n",
      "        1.8319, 1.7025, 1.9583, 1.5395, 2.1588, 0.7071, 1.4212, 1.4276, 1.6181,\n",
      "        2.2830, 1.7858, 1.7429, 1.0917, 0.7814, 1.2101, 1.5572, 2.1317, 2.1498,\n",
      "        1.7333, 1.5078, 1.2501, 1.9016, 1.8095, 1.5969, 1.4775, 2.0020, 1.7182,\n",
      "        1.5114, 1.2332, 1.5002, 0.7958, 1.5418, 1.1868, 1.3188, 1.7840, 1.0916,\n",
      "        1.1084, 2.1938, 0.9388, 0.8873, 1.4611, 1.7027, 1.4112, 1.5773, 2.0078,\n",
      "        1.3213, 0.9271, 0.8252, 1.7045, 1.1069, 1.0183, 0.6630, 0.8054, 0.8501,\n",
      "        0.8252, 0.9793, 0.4845, 1.2222, 1.2250, 0.7868, 0.6276, 0.8069, 2.9451,\n",
      "        2.7186, 2.8895, 1.7407, 2.3580, 1.3964, 1.4032, 1.6708, 2.2550, 1.8002,\n",
      "        1.8418, 2.2657, 1.4594, 2.2007, 2.0315, 1.8336, 2.1152, 1.6711, 1.5109,\n",
      "        1.6622, 1.5138, 1.3808, 1.3711, 1.3025, 0.9266, 1.8843, 2.3109, 1.9331,\n",
      "        1.7377, 1.9275, 0.9689, 0.9301, 1.3997, 0.8373, 1.8372, 1.6347, 1.6297,\n",
      "        1.2762, 1.4147, 0.7269, 0.9910, 1.6774, 1.1875, 1.5988, 1.5300, 1.6326,\n",
      "        2.1069, 0.9015, 1.7949, 1.8477, 1.1526, 1.5241, 1.7778, 2.7731, 1.5229,\n",
      "        1.3803, 0.9781, 1.4339, 2.6973, 2.2279, 2.0353, 1.5313, 1.2138, 1.0883,\n",
      "        2.2136, 2.0958, 1.8546, 2.1536, 1.4123, 1.1453, 1.7430, 1.1563, 2.0697,\n",
      "        1.8661, 2.2181, 1.8846, 1.6379, 1.9485, 1.9344, 1.2116, 2.1902, 1.7793,\n",
      "        2.1231, 1.5778, 2.5312, 1.8458, 1.9622, 1.5202, 1.7185, 2.9070, 1.7154,\n",
      "        1.6991, 1.5724, 2.1885, 0.9060, 0.9609], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.1424, -0.0069,  0.2519,  ...,  0.0814, -0.1329, -0.0936],\n",
      "        [ 0.1260,  0.0035,  0.1406,  ...,  0.0060, -0.0241, -0.1546],\n",
      "        [ 0.0470,  0.0016,  0.0926,  ..., -0.0814, -0.0205, -0.0501],\n",
      "        ...,\n",
      "        [-0.0585,  0.0051, -0.0574,  ...,  0.0768, -0.0093, -0.0917],\n",
      "        [-0.0364, -0.0134,  0.0891,  ..., -0.1541,  0.0377, -0.0778],\n",
      "        [-0.1622,  0.0015, -0.0331,  ...,  0.0067, -0.0211,  0.1209]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0765,  0.0980,  0.1722,  ..., -0.0251, -0.0698, -0.1424],\n",
      "        [ 0.1300, -0.0205, -0.1334,  ..., -0.0760,  0.1092,  0.0228],\n",
      "        [ 0.0560,  0.0779, -0.1371,  ..., -0.1142,  0.0742, -0.0733],\n",
      "        ...,\n",
      "        [ 0.0053,  0.0530,  0.0346,  ..., -0.0580,  0.0288, -0.1748],\n",
      "        [-0.0745,  0.0186,  0.1499,  ..., -0.0532, -0.0884, -0.0369],\n",
      "        [ 0.2171,  0.1043,  0.0594,  ...,  0.0359, -0.0330,  0.0201]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.4927, 1.6146, 1.4119, 1.6148, 1.3547, 1.5420, 1.5957, 1.3482, 1.5070,\n",
      "        1.4946, 1.3155, 1.4255, 1.4958, 1.5160, 1.4856, 1.3993, 0.9258, 1.0924,\n",
      "        1.0338, 1.1353, 0.8695, 1.0913, 1.2008, 0.3927, 1.1997, 0.8729, 1.2249,\n",
      "        0.9813, 1.2411, 0.9379, 1.1826, 1.1731, 0.9171, 1.1147, 1.1085, 1.0160,\n",
      "        1.0529, 1.1061, 1.0714, 1.0326, 0.9978, 0.9595, 0.9963, 1.1957, 1.1058,\n",
      "        1.1943, 1.0503, 0.9583, 1.5632, 1.6116, 1.4257, 1.1160, 1.5282, 1.3789,\n",
      "        1.5325, 1.6005, 0.9923, 1.4162, 1.2519, 1.7454, 1.5428, 1.5050, 1.0535,\n",
      "        1.3630, 0.9217, 1.2765, 1.1505, 1.1329, 1.1758, 1.2591, 1.3913, 1.1450,\n",
      "        1.2662, 1.2610, 1.1588, 1.3159, 1.2937, 1.2024, 1.2959, 1.2083, 0.9873,\n",
      "        1.0030, 1.2463, 1.0519, 1.1947, 1.1835, 1.2502, 1.1807, 1.2851, 1.0200,\n",
      "        1.1974, 1.1204, 1.1151, 1.0728, 1.0995, 1.1339, 1.6850, 1.6453, 1.4477,\n",
      "        1.5283, 1.4608, 1.5123, 1.7606, 1.6790, 1.4536, 1.3615, 1.0539, 1.6766,\n",
      "        1.5956, 1.2666, 1.4661, 1.6749, 1.7290, 1.8016, 1.6522, 1.8008, 2.0479,\n",
      "        1.8339, 1.9653, 2.1335, 1.6599, 1.8508, 1.6515, 1.7320, 1.6060, 1.7662,\n",
      "        1.7140, 2.0657, 1.0041, 1.1502, 1.2057, 1.1104, 1.2182, 1.4887, 1.1868,\n",
      "        1.5332, 1.1300, 1.4410, 1.3467, 1.2089, 1.1034, 1.2099, 1.1175, 1.4389,\n",
      "        1.1561, 1.0273, 1.0741, 1.1234, 1.1185, 1.0851, 1.1877, 1.2212, 1.0896,\n",
      "        1.1967, 1.2134, 1.0944, 1.1203, 0.9993, 1.0941, 1.0678, 1.2287, 1.4793,\n",
      "        1.1623, 1.2790, 1.3448, 1.2633, 1.3038, 1.2589, 1.5259, 1.6493, 1.3567,\n",
      "        1.3565, 1.2454, 1.1212, 1.2549, 1.1834, 1.0260, 1.0332, 0.8878, 1.0279,\n",
      "        0.9775, 1.0704, 1.3009, 1.0478, 0.8605, 0.9796, 1.2201, 1.0061, 1.1037,\n",
      "        0.9611, 1.1367, 1.1465, 1.7780, 1.3971, 0.8716, 1.3058, 1.8646, 1.2762,\n",
      "        1.3038, 1.6715, 1.3641, 1.7771, 1.7013, 1.9484, 1.6952, 1.7031, 1.7976,\n",
      "        1.6869, 1.2911, 1.2696, 1.3963, 1.4774, 1.3635, 1.5648, 1.2879, 1.3984,\n",
      "        1.5977, 1.5329, 1.3327, 1.4316, 1.2458, 1.5834, 1.1738, 1.1879, 0.7350,\n",
      "        0.8385, 0.6717, 0.7792, 0.7465, 0.6887, 0.9211, 0.7858, 0.8819, 0.9442,\n",
      "        0.7440, 0.5943, 0.7240, 0.7652, 0.7081, 0.6787, 1.1885, 1.2643, 1.2546,\n",
      "        1.3681, 1.0384, 1.2648, 1.2786, 1.2049, 1.3213, 1.1544, 1.2183, 1.2704,\n",
      "        1.3843, 1.3557, 1.2026, 1.1787, 1.3437, 1.2421, 1.3807, 1.3408, 1.2601,\n",
      "        1.2891, 1.8066, 1.2268, 1.3248, 1.2082, 1.3135, 1.3192, 1.5398, 1.4319,\n",
      "        1.3836, 1.1218, 0.8959, 0.7929, 0.9015, 1.0226, 0.9853, 0.9635, 1.0151,\n",
      "        0.9750, 0.9321, 0.9203, 0.9476, 1.0982, 0.9120, 0.9769, 0.9376, 0.8773,\n",
      "        1.0452, 1.0148, 0.9932, 1.0126, 1.0655, 0.9017, 0.9699, 0.9783, 0.9918,\n",
      "        1.0670, 0.9244, 1.0514, 1.1607, 1.0265, 1.1332, 0.9821, 1.1537, 1.0887,\n",
      "        1.0461, 1.0413, 1.0952, 1.2016, 1.2325, 1.1638, 1.0600, 1.1822, 1.1641,\n",
      "        1.0915, 1.2743, 1.1293, 1.2005, 1.1988], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0240, -0.0543,  0.0521,  ..., -0.0150, -0.0552,  0.0442],\n",
      "        [ 0.0177, -0.0445, -0.1237,  ...,  0.0905,  0.0007,  0.0203],\n",
      "        [-0.0405,  0.0197,  0.0028,  ...,  0.0454,  0.0579,  0.0052],\n",
      "        ...,\n",
      "        [ 0.0658,  0.0374, -0.1228,  ...,  0.0194,  0.0028, -0.0704],\n",
      "        [-0.0010,  0.2788,  0.0367,  ..., -0.0402, -0.1133, -0.0133],\n",
      "        [-0.0667, -0.0574, -0.0855,  ...,  0.0544,  0.0049, -0.0901]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0671,  0.0215,  0.0105,  ...,  0.0306,  0.0409, -0.0089],\n",
      "        [ 0.0036, -0.0268, -0.0548,  ..., -0.0542, -0.1474, -0.0509],\n",
      "        [-0.0697, -0.0670, -0.0350,  ...,  0.0679,  0.0767,  0.0862],\n",
      "        ...,\n",
      "        [-0.0031,  0.0227, -0.0127,  ...,  0.0467,  0.0503, -0.0179],\n",
      "        [-0.0062, -0.0149,  0.0577,  ..., -0.1676,  0.1078, -0.0591],\n",
      "        [-0.0316, -0.0409, -0.0787,  ...,  0.0371,  0.1069, -0.0039]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.3747, 1.3894, 1.4854, 1.5076, 1.4498, 1.5789, 1.4099, 1.4072, 1.3064,\n",
      "        1.4274, 1.3434, 1.5063, 1.5148, 1.3795, 1.3653, 1.3729, 1.4341, 1.3506,\n",
      "        1.4613, 1.3473, 1.3978, 1.4063, 1.4204, 1.3573, 1.4028, 1.4448, 1.4522,\n",
      "        1.3809, 1.4116, 1.4559, 1.3596, 1.3025, 1.4531, 1.2708, 1.3587, 1.3878,\n",
      "        1.4148, 1.3787, 1.4404, 1.4551, 1.4135, 1.4298, 1.3495, 1.4463, 1.4323,\n",
      "        1.4360, 1.8061, 1.3503, 1.5649, 1.4131, 1.4355, 1.3738, 1.4393, 1.5773,\n",
      "        1.3801, 1.2495, 1.3841, 1.4282, 1.3481, 1.3407, 1.4928, 1.6891, 1.2522,\n",
      "        1.5080, 1.3808, 1.3823, 1.4502, 1.4289, 1.3966, 1.3128, 1.4076, 1.4644,\n",
      "        1.3594, 1.5224, 1.5123, 1.4181, 1.4212, 1.3225, 1.3818, 1.4714, 1.3919,\n",
      "        1.3776, 1.3128, 1.4218, 1.4323, 1.5548, 1.3659, 1.4452, 1.3946, 1.4262,\n",
      "        1.4827, 1.3974, 1.4582, 1.4218, 1.3798, 1.4249, 1.2987, 1.4325, 1.3801,\n",
      "        1.3422, 1.3673, 1.4486, 1.3536, 1.2743, 1.5162, 1.3325, 1.5177, 1.4820,\n",
      "        1.4218, 1.3542, 1.3831, 1.6063, 1.4634, 1.4684, 1.5416, 1.4143, 1.3137,\n",
      "        1.4473, 1.3444, 1.5344, 1.5863, 1.4146, 1.3642, 1.3577, 1.3317, 1.4611,\n",
      "        1.3587, 1.4405, 1.3654, 1.3447, 1.4065, 1.5712, 1.4418, 1.4660, 1.3712,\n",
      "        1.4646, 1.3927, 1.3188, 1.3750, 1.6459, 1.3480, 1.5365, 1.3741, 1.3423,\n",
      "        1.1471, 1.3893, 1.3614, 1.5097, 1.3817, 1.3700, 1.4585, 1.5309, 1.5796,\n",
      "        1.3792, 1.3830, 1.3127, 1.4157, 1.3674, 1.4844, 1.3998, 1.3517, 1.4317,\n",
      "        1.4065, 1.4833, 1.4850, 1.4964, 1.5006, 1.4470, 1.3830, 1.3691, 1.4570,\n",
      "        1.5999, 1.4686, 1.3846, 1.4037, 1.4579, 1.3285, 1.3512, 1.3857, 1.4283,\n",
      "        1.3920, 1.5174, 1.3553, 1.3516, 1.4557, 1.3244, 1.4279, 1.2325, 1.4418,\n",
      "        1.3500, 1.3720, 1.3385, 1.4989, 0.9617, 1.3869, 1.3330, 1.3944, 1.3058,\n",
      "        1.3582, 1.3430, 1.4276, 1.3860, 1.3119, 1.3375, 1.3986, 1.4417, 1.3726,\n",
      "        1.3996, 1.4161, 1.4537, 1.3882, 1.3350, 1.3058, 1.4176, 1.4913, 1.4282,\n",
      "        1.3890, 1.4206, 1.3563, 1.3812, 1.4005, 1.3811, 1.4505, 1.3248, 1.3475,\n",
      "        1.4196, 1.3802, 1.4666, 1.3961, 1.4665, 1.4254, 1.3331, 1.4709, 1.3260,\n",
      "        1.4457, 1.4008, 1.4648, 1.3595, 1.4141, 1.4384, 1.4431, 1.4605, 1.5257,\n",
      "        1.4278, 1.4500, 1.3984, 1.3878, 1.4334, 1.4737, 1.3175, 1.3484, 1.4413,\n",
      "        1.4654, 1.4015, 1.3200, 1.4629, 1.4865, 1.6980, 1.4436, 1.3686, 1.4155,\n",
      "        1.4526, 1.4338, 1.3436, 1.4876, 1.4125, 1.5029, 1.4164, 1.4158, 1.4855,\n",
      "        1.2872, 1.2392, 1.4545, 1.3489, 1.2877, 1.3852, 1.4283, 1.4458, 1.4448,\n",
      "        1.3937, 1.4512, 1.4746, 1.3522, 1.6366, 1.4459, 1.2592, 1.2784, 1.5182,\n",
      "        1.3845, 1.5055, 1.3924, 1.7714, 1.3866, 1.4244, 1.4894, 1.3473, 1.2799,\n",
      "        1.3888, 1.4305, 1.4076, 1.3896, 1.4325, 1.3978, 1.3426, 1.5426, 1.4078,\n",
      "        1.3793, 1.4331, 1.5409, 1.3797, 1.4806, 1.4996, 1.4277, 1.4412, 1.5105,\n",
      "        1.4543, 1.3805, 1.3937, 1.3924, 1.3659], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 1.7003e-01,  5.2951e-02, -1.2185e-01,  ...,  1.6899e-01,\n",
      "         -1.6760e-01,  6.4166e-01],\n",
      "        [ 9.1949e-02, -2.4223e-02, -9.0878e-02,  ...,  1.2673e-01,\n",
      "         -1.6266e-02,  7.7130e-02],\n",
      "        [-1.1728e-01, -5.2174e-02, -1.0652e-01,  ..., -2.3536e-01,\n",
      "         -2.0896e-02, -6.1848e-04],\n",
      "        ...,\n",
      "        [-1.5016e-02, -2.1608e-02,  4.3276e-02,  ..., -4.3280e-02,\n",
      "          1.7819e-01,  1.4624e-03],\n",
      "        [ 5.3831e-02,  8.2512e-02,  6.2805e-02,  ..., -4.7947e-02,\n",
      "         -1.5457e-01,  5.5668e-03],\n",
      "        [ 5.6542e-02, -1.4761e-01,  5.1521e-03,  ..., -9.4848e-02,\n",
      "          2.7665e-02,  1.6270e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0140,  0.0227,  0.0292,  ..., -0.0078,  0.0373, -0.0193],\n",
      "        [-0.1493, -0.1240, -0.0335,  ..., -0.0307,  0.0295,  0.0436],\n",
      "        [-0.1208, -0.1387, -0.0054,  ...,  0.0378, -0.0256,  0.0409],\n",
      "        ...,\n",
      "        [-0.0766, -0.0316, -0.1350,  ...,  0.0253, -0.0189,  0.1318],\n",
      "        [-0.0207,  0.1333,  0.1310,  ..., -0.0532, -0.0373, -0.0041],\n",
      "        [ 0.0695, -0.1402, -0.1067,  ..., -0.0060, -0.0811,  0.0301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.3228, 1.7167, 1.7892, 1.6899, 2.5000, 3.0245, 2.9065, 2.6829, 2.2675,\n",
      "        1.9312, 1.9584, 2.6223, 2.1600, 2.3743, 2.8331, 2.5622, 3.4464, 2.4802,\n",
      "        1.9317, 2.3686, 2.5087, 2.6261, 3.1150, 3.6424, 1.8733, 2.0167, 2.8550,\n",
      "        2.0079, 2.3863, 2.5467, 3.1784, 3.1810, 2.4409, 2.1004, 2.8212, 2.7785,\n",
      "        2.9947, 2.3737, 3.3397, 3.9800, 1.9109, 2.1063, 2.1877, 2.5391, 2.8875,\n",
      "        2.8831, 3.3345, 3.0384, 1.3548, 1.9214, 1.0094, 1.3006, 2.2433, 2.4186,\n",
      "        2.7794, 2.6735, 3.4520, 1.8977, 2.6700, 2.7093, 2.5843, 2.4909, 3.0235,\n",
      "        2.8959, 1.9297, 2.0115, 3.3876, 2.7099, 2.5087, 3.1259, 2.7278, 3.3231,\n",
      "        2.3439, 1.2477, 1.0489, 1.5469, 2.0124, 2.4396, 3.1604, 3.0250, 2.3145,\n",
      "        2.3602, 2.9603, 2.9110, 2.8074, 3.2007, 3.3319, 3.4903, 2.2887, 2.4577,\n",
      "        2.4752, 2.6465, 2.7445, 2.6685, 3.3151, 2.8944, 2.4961, 2.3645, 3.4088,\n",
      "        3.8269, 3.7414, 2.3287, 2.3481, 3.7498, 2.6456, 3.0459, 3.8318, 3.6100,\n",
      "        3.0284, 3.8170, 3.1901, 4.1504, 1.3845, 1.2867, 2.6492, 2.5096, 2.4507,\n",
      "        2.4978, 2.7566, 2.6403, 1.8373, 2.5612, 1.9942, 1.9798, 2.1954, 2.5927,\n",
      "        2.7389, 3.2072, 1.9173, 1.9458, 2.1176, 1.8394, 2.3602, 2.8897, 2.8006,\n",
      "        2.8387, 2.0002, 1.8972, 2.4731, 3.4814, 2.5468, 2.5005, 2.8950, 2.8635,\n",
      "        3.2978, 1.9449, 2.6062, 2.1491, 2.3243, 2.2584, 2.9874, 2.8292, 1.0420,\n",
      "        1.5749, 1.8214, 2.1740, 2.2250, 2.9403, 2.9848, 3.2168, 1.9528, 2.3546,\n",
      "        2.0247, 2.2130, 2.3638, 2.9385, 2.8415, 2.9179, 3.0707, 2.0531, 2.6329,\n",
      "        2.3713, 2.7022, 2.5385, 2.9941, 3.3606, 1.4401, 1.4543, 2.3839, 1.6381,\n",
      "        1.9803, 2.7022, 2.4293, 2.7150, 1.2429, 1.5818, 1.4417, 2.3670, 2.0308,\n",
      "        2.1258, 2.5203, 2.7178, 1.2981, 1.7122, 1.7755, 1.4587, 2.0367, 1.9014,\n",
      "        2.8426, 2.6285, 2.2243, 2.0749, 2.1893, 2.6212, 2.0866, 2.2924, 2.8065,\n",
      "        2.8627, 2.6159, 2.3010, 2.3303, 3.2447, 3.3884, 1.9804, 3.0135, 3.1311,\n",
      "        1.8345, 3.0779, 3.6628, 4.3906, 3.5270, 4.1559, 3.2852, 3.7826, 1.3692,\n",
      "        1.4752, 2.5798, 1.3959, 1.7982, 2.1479, 2.4733, 3.5554, 4.4083, 2.1676,\n",
      "        1.3069, 2.3187, 2.1601, 3.0743, 2.5531, 3.0497, 2.1873, 2.4900, 2.1923,\n",
      "        3.3045, 2.0328, 2.6299, 3.0715, 3.0616, 2.2864, 1.9831, 2.0289, 1.3874,\n",
      "        2.7274, 2.5959, 2.9740, 2.9868, 3.0506, 2.1440, 2.2682, 1.9939, 2.1247,\n",
      "        2.3379, 3.3555, 3.0993, 2.5158, 2.4700, 2.3264, 3.0261, 2.7810, 2.2113,\n",
      "        3.4599, 2.4841, 1.9812, 2.4335, 2.9191, 2.3438, 1.9682, 2.5484, 3.3898,\n",
      "        3.3511, 2.0106, 2.7205, 2.9787, 3.0968, 3.1368, 2.6210, 3.1475, 3.0286,\n",
      "        2.1818, 3.0290, 2.9021, 3.2458, 2.1815, 3.1650, 3.3631, 3.4817, 2.8906,\n",
      "        3.4028, 2.8311, 2.3375, 3.2507, 2.4482, 3.8870, 3.3289, 1.4857, 2.1173,\n",
      "        2.3755, 2.0315, 2.0787, 2.4021, 2.4472, 2.8881, 1.6336, 1.4086, 1.4457,\n",
      "        2.7128, 2.0880, 2.6069, 2.5944, 2.6837], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0261, -0.0791,  0.0779,  ..., -0.0908,  0.0643, -0.8742],\n",
      "        [-0.0095,  0.0145, -0.0314,  ..., -0.1198,  0.1143, -0.0831],\n",
      "        [ 0.0731,  0.0915, -0.0615,  ...,  0.0194, -0.1439, -0.0835],\n",
      "        ...,\n",
      "        [-0.0482,  0.0060, -0.0387,  ..., -0.0089,  0.1657, -0.0550],\n",
      "        [-0.0996, -0.0224,  0.0575,  ...,  0.1377,  0.0201,  0.0148],\n",
      "        [-0.0543, -0.1113,  0.1015,  ..., -0.0554,  0.0636, -0.0190]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0595,  0.0690, -0.0912,  ...,  0.1025, -0.0276,  0.0540],\n",
      "        [-0.1470,  0.1043, -0.1274,  ..., -0.1458, -0.0419, -0.0748],\n",
      "        [ 0.2249, -0.0237,  0.0069,  ...,  0.0293,  0.0029, -0.0273],\n",
      "        ...,\n",
      "        [ 0.1627, -0.0626,  0.0183,  ..., -0.0717, -0.0676, -0.0076],\n",
      "        [ 0.1088,  0.0133,  0.0367,  ...,  0.0800,  0.1615, -0.0504],\n",
      "        [-0.0880, -0.0830, -0.0804,  ..., -0.0791,  0.0564,  0.0468]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.9286, 1.6597, 1.4979, 3.0950, 1.3135, 1.9884, 2.4981, 2.6347, 1.9370,\n",
      "        2.2319, 2.5965, 1.2604, 2.9126, 2.0885, 2.8187, 2.0993, 1.6835, 2.0353,\n",
      "        2.7469, 1.7892, 1.7414, 1.7080, 3.2592, 3.1716, 1.9322, 2.2077, 1.6041,\n",
      "        1.6927, 3.4304, 3.6145, 3.4989, 3.0751, 2.6691, 1.9014, 2.4965, 2.6552,\n",
      "        2.2616, 1.9448, 2.1471, 2.9531, 1.6711, 2.2426, 2.5966, 2.2534, 2.1863,\n",
      "        2.2688, 2.4121, 2.3003, 2.6514, 1.6639, 1.4983, 1.9049, 2.9210, 1.3303,\n",
      "        2.4300, 2.5021, 1.5209, 2.2663, 1.7641, 1.4543, 1.2990, 3.0472, 2.6548,\n",
      "        2.4604, 1.7938, 1.7288, 1.3517, 1.2307, 1.4785, 1.7055, 2.6775, 2.0548,\n",
      "        1.9992, 1.5352, 1.6418, 2.5871, 1.7270, 2.8056, 2.5010, 2.0641, 2.4760,\n",
      "        2.5371, 2.7687, 2.3909, 2.3907, 2.4419, 3.2883, 3.1403, 1.7158, 2.2924,\n",
      "        2.2236, 2.5198, 2.2084, 2.2473, 3.1399, 2.6018, 2.4450, 2.6955, 3.1731,\n",
      "        3.2542, 2.8014, 3.6047, 3.4755, 3.3723, 2.8549, 3.3023, 3.0562, 3.1537,\n",
      "        2.9184, 1.5708, 2.2127, 3.5159, 1.2727, 2.1176, 1.5690, 1.4803, 1.4221,\n",
      "        2.8364, 2.7163, 2.7532, 1.6368, 1.5102, 1.9866, 2.4181, 2.6728, 1.4938,\n",
      "        2.6703, 2.2478, 2.1770, 2.2326, 1.8161, 3.1776, 1.6185, 1.5432, 2.6151,\n",
      "        2.5157, 1.9495, 1.8913, 2.9080, 1.5766, 1.7408, 3.0165, 2.5628, 2.7255,\n",
      "        2.0530, 2.2424, 1.5414, 1.6124, 1.3750, 3.0514, 2.5983, 2.6611, 1.3656,\n",
      "        1.5337, 2.7956, 1.6592, 3.0017, 1.6040, 2.7046, 2.2361, 2.1649, 2.5139,\n",
      "        2.9802, 3.0001, 3.2523, 1.8604, 3.2542, 3.0079, 2.2947, 2.0338, 1.6555,\n",
      "        1.8088, 1.7434, 3.2782, 3.3688, 3.0948, 1.2517, 1.4562, 1.4179, 2.6662,\n",
      "        1.2921, 1.4032, 2.2301, 2.1944, 1.6668, 1.8524, 2.1097, 1.1742, 1.5424,\n",
      "        2.3609, 2.4085, 1.9309, 1.9570, 1.8866, 1.5751, 3.1065, 1.4607, 1.4751,\n",
      "        2.6641, 2.7696, 2.4266, 2.2500, 1.4690, 1.3197, 2.9568, 3.0756, 2.5313,\n",
      "        2.8138, 1.8780, 2.7134, 3.3902, 3.1343, 2.8007, 3.1773, 2.5950, 4.0254,\n",
      "        2.3379, 2.0384, 2.2772, 2.7928, 2.7757, 1.8408, 3.9358, 3.9438, 1.3422,\n",
      "        1.9955, 1.2537, 2.1084, 2.7412, 2.6115, 2.2369, 2.0776, 1.2937, 1.6644,\n",
      "        1.2519, 1.2760, 1.2236, 1.5240, 2.2907, 2.0159, 2.3497, 2.1433, 1.9996,\n",
      "        1.6093, 3.1090, 1.6821, 2.7933, 2.8005, 2.0622, 2.2073, 2.0933, 2.1428,\n",
      "        1.3878, 2.3520, 2.6652, 2.8492, 2.4219, 2.2370, 2.2071, 2.1815, 2.8091,\n",
      "        2.1777, 3.0069, 2.5210, 2.8037, 2.3782, 2.2917, 2.2170, 1.9472, 2.0228,\n",
      "        2.9038, 2.3139, 1.7494, 2.7830, 2.7074, 2.4657, 3.2157, 2.1404, 3.1377,\n",
      "        3.2406, 2.0065, 2.3585, 2.4612, 2.5304, 1.8208, 2.0458, 3.2857, 3.0259,\n",
      "        2.7799, 2.9550, 2.6324, 2.9411, 3.7359, 2.2172, 3.3163, 3.2191, 2.3201,\n",
      "        2.7323, 2.6764, 2.2569, 1.7368, 2.4221, 3.0890, 3.4691, 1.7528, 1.9669,\n",
      "        1.2395, 2.7236, 2.6974, 1.9431, 2.4018, 2.0678, 1.7160, 1.5182, 2.6530,\n",
      "        1.3027, 1.1570, 2.1144, 2.2997, 2.4038], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0302,  0.0437,  0.1214,  ...,  0.0126,  0.0679, -0.0008],\n",
      "        [-0.1016,  0.0215, -0.0513,  ..., -0.0735, -0.0524, -0.0078],\n",
      "        [ 0.0997,  0.0311,  0.1339,  ...,  0.0097, -0.0245,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0551, -0.0566, -0.0324,  ..., -0.0236,  0.0844, -0.0093],\n",
      "        [ 0.0892,  0.2403, -0.0423,  ...,  0.1555,  0.0503, -0.0009],\n",
      "        [ 0.0319,  0.0144,  0.0019,  ..., -0.0038,  0.0024, -0.0006]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0196, -0.0285,  0.0052,  ...,  0.0950, -0.0904,  0.0049],\n",
      "        [-0.0085, -0.0143,  0.1247,  ..., -0.0727,  0.0395,  0.0079],\n",
      "        [-0.1056,  0.0185,  0.0115,  ...,  0.0196, -0.1344, -0.0975],\n",
      "        ...,\n",
      "        [-0.0363, -0.0705,  0.0230,  ...,  0.0657, -0.1139,  0.0757],\n",
      "        [-0.0149,  0.0176, -0.0516,  ..., -0.0228, -0.0085,  0.0473],\n",
      "        [ 0.0056,  0.1576, -0.1515,  ..., -0.0367, -0.0122, -0.0216]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.4993, 1.3476, 1.4469, 1.4356, 1.3921, 1.4253, 1.3475, 1.3992, 1.4736,\n",
      "        1.4392, 1.4102, 1.4343, 1.4128, 1.4302, 1.4738, 1.4424, 1.1952, 1.2019,\n",
      "        1.1851, 1.2134, 1.1942, 1.1493, 1.1944, 1.1528, 1.2793, 1.1387, 1.1779,\n",
      "        1.0983, 1.1763, 1.2063, 1.1155, 1.2548, 1.8259, 1.9658, 1.9917, 1.9290,\n",
      "        2.1091, 2.1928, 2.1768, 1.9590, 2.1324, 2.0345, 1.9358, 2.1147, 2.0026,\n",
      "        1.9803, 1.9682, 2.1186, 1.3952, 1.5002, 1.3788, 1.4128, 1.4860, 1.5012,\n",
      "        1.3699, 1.5264, 1.4839, 1.4494, 1.4857, 1.4571, 1.4977, 1.5067, 1.4853,\n",
      "        1.4155, 1.2766, 1.2417, 1.2617, 1.2971, 1.3214, 1.3286, 1.2780, 1.3287,\n",
      "        1.2603, 1.2176, 1.3094, 1.2633, 1.2080, 1.2647, 1.2592, 1.1700, 2.0403,\n",
      "        2.1187, 2.0816, 1.9920, 2.1230, 2.1298, 2.1993, 2.1632, 2.2220, 2.1032,\n",
      "        2.0196, 2.1157, 2.0299, 2.1863, 2.0824, 1.9219, 1.6350, 1.7297, 1.7399,\n",
      "        1.7099, 1.7632, 1.7662, 1.7406, 1.7121, 1.6636, 1.6426, 1.7014, 1.7518,\n",
      "        1.7105, 1.6557, 1.6358, 1.7484, 1.2066, 1.2424, 1.2631, 1.2280, 1.2606,\n",
      "        1.3033, 1.2010, 1.2022, 1.3080, 1.2521, 1.2003, 1.2689, 1.3170, 1.2624,\n",
      "        1.2587, 1.2285, 1.6081, 1.4722, 1.5109, 1.5398, 1.5213, 1.4310, 1.5557,\n",
      "        1.5547, 1.5511, 1.4765, 1.5584, 1.4371, 1.5441, 1.5903, 1.5369, 1.4705,\n",
      "        1.2628, 1.2742, 1.1872, 1.2943, 1.2327, 1.2563, 1.2880, 1.2513, 1.2882,\n",
      "        1.3168, 1.1640, 1.3027, 1.2704, 1.2639, 1.2473, 1.2138, 1.1424, 1.2293,\n",
      "        1.3036, 1.1335, 1.1833, 1.1583, 1.2376, 1.1815, 1.1947, 1.1560, 1.1682,\n",
      "        1.2242, 1.2274, 1.3154, 1.1884, 1.2220, 1.4578, 1.4571, 1.4496, 1.4295,\n",
      "        1.4834, 1.4492, 1.4315, 1.4714, 1.4800, 1.4078, 1.5120, 1.4859, 1.4432,\n",
      "        1.4355, 1.4302, 1.5152, 1.5692, 1.5594, 1.6091, 1.6369, 1.5520, 1.5656,\n",
      "        1.5105, 1.5801, 1.5916, 1.6341, 1.5665, 1.5994, 1.5867, 1.5837, 1.5863,\n",
      "        1.5473, 1.4517, 1.4992, 1.5409, 1.5012, 1.4251, 1.5205, 1.5316, 1.5059,\n",
      "        1.6549, 1.5527, 1.5018, 1.4358, 1.5477, 1.4191, 1.5175, 1.5237, 1.3556,\n",
      "        1.3576, 1.3311, 1.3826, 1.3560, 1.4141, 1.4361, 1.4116, 1.3973, 1.4189,\n",
      "        1.4681, 1.3850, 1.3540, 1.3955, 1.3962, 1.3742, 1.6136, 1.6316, 1.6667,\n",
      "        1.6941, 1.7123, 1.6411, 1.6649, 1.5852, 1.6681, 1.6766, 1.7108, 1.6858,\n",
      "        1.6217, 1.7520, 1.6430, 1.6982, 1.8724, 1.8083, 1.7239, 1.7069, 1.7412,\n",
      "        1.8341, 1.7948, 1.8121, 1.8127, 1.8227, 1.8570, 1.8260, 1.8708, 1.7828,\n",
      "        1.6911, 1.7559, 1.7640, 1.7519, 1.9516, 1.8071, 1.8303, 1.8054, 1.8920,\n",
      "        1.7751, 1.7910, 1.7499, 1.8676, 1.7230, 1.7257, 1.8327, 1.8592, 1.7925,\n",
      "        1.7741, 1.8035, 1.8036, 1.8015, 1.8489, 1.7842, 1.7836, 1.8231, 1.8144,\n",
      "        1.6899, 1.7658, 1.8264, 1.7925, 1.8126, 1.7412, 1.8792, 1.4864, 1.4154,\n",
      "        1.4228, 1.3774, 1.4762, 1.3867, 1.4033, 1.3672, 1.4363, 1.3737, 1.4069,\n",
      "        1.3530, 1.3883, 1.4635, 1.4446, 1.5020], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0217,  0.0229,  ...,  0.0115,  0.0520, -0.0495],\n",
      "        [ 0.0202, -0.0492,  0.0130,  ...,  0.0468, -0.0377,  0.0192],\n",
      "        [ 0.0090,  0.0069, -0.0064,  ..., -0.0227,  0.0403, -0.0175],\n",
      "        ...,\n",
      "        [-0.0451,  0.0256, -0.0389,  ...,  0.0549,  0.0195,  0.0403],\n",
      "        [-0.0086, -0.0007,  0.0650,  ...,  0.0418, -0.0590, -0.1459],\n",
      "        [ 0.0106,  0.0533,  0.0497,  ...,  0.0615,  0.1177,  0.0070]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0705, -0.0781,  0.0237,  ...,  0.0851,  0.1432,  0.0945],\n",
      "        [ 0.0221,  0.0467, -0.0008,  ...,  0.0623, -0.0340, -0.0129],\n",
      "        [ 0.0817,  0.0056,  0.0281,  ...,  0.0459, -0.1102, -0.0282],\n",
      "        ...,\n",
      "        [ 0.0303,  0.0699, -0.0239,  ..., -0.0942, -0.0153, -0.0297],\n",
      "        [ 0.0115,  0.0195,  0.0018,  ..., -0.0898,  0.0362,  0.0512],\n",
      "        [ 0.5918, -0.0390,  0.2050,  ..., -0.0053, -0.0623,  0.0609]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.5014, 1.3327, 1.4554, 1.5530, 1.5741, 1.3886, 1.4239, 1.3876, 1.3847,\n",
      "        1.5476, 1.4273, 1.5368, 1.4240, 1.3810, 1.4078, 1.5239, 1.5372, 1.5113,\n",
      "        1.3997, 1.4292, 1.5568, 1.3909, 1.5455, 1.4469, 1.4205, 1.5497, 1.4665,\n",
      "        1.4476, 1.5210, 1.5334, 1.4755, 1.4735, 1.4614, 1.4325, 1.4512, 1.5678,\n",
      "        1.6011, 1.5061, 1.5713, 1.5010, 1.4320, 1.4921, 1.7000, 1.5624, 1.4234,\n",
      "        1.3821, 1.3155, 1.5083, 1.5360, 1.5495, 1.4496, 1.4356, 1.4506, 1.4838,\n",
      "        1.4546, 1.5012, 1.4049, 1.4990, 1.4502, 1.4335, 1.5002, 1.7995, 1.4395,\n",
      "        1.5972, 1.5242, 1.4567, 1.4125, 1.4498, 1.4517, 1.5820, 1.4537, 1.6206,\n",
      "        1.5965, 1.5183, 1.4551, 1.3322, 1.4610, 1.4729, 1.4255, 1.4739, 1.5041,\n",
      "        1.5618, 1.4337, 1.5347, 1.5926, 1.5278, 1.4356, 1.3712, 1.3611, 1.5967,\n",
      "        1.4402, 1.5318, 1.5628, 1.4050, 1.5145, 1.4789, 1.2983, 1.5420, 1.6201,\n",
      "        1.3993, 1.4770, 1.3783, 1.4142, 1.3155, 1.4459, 1.5723, 1.4921, 1.4083,\n",
      "        1.6247, 1.4190, 1.4377, 1.4869, 1.5547, 1.5288, 1.4442, 1.4670, 1.6050,\n",
      "        1.5413, 1.4641, 1.4590, 1.5244, 1.5841, 1.4634, 1.4977, 1.4885, 1.3851,\n",
      "        1.3901, 1.4511, 1.4613, 1.6734, 1.4604, 1.5296, 1.4768, 1.4887, 1.4561,\n",
      "        1.2820, 1.5037, 1.3580, 1.5477, 1.3842, 1.4473, 1.5225, 1.4472, 1.4212,\n",
      "        1.4155, 1.4108, 1.3950, 1.4272, 1.4968, 1.4843, 1.7360, 1.5123, 1.4630,\n",
      "        1.3554, 1.4336, 1.4874, 1.5849, 1.5262, 1.5496, 1.5090, 1.3878, 1.4919,\n",
      "        1.4874, 1.4610, 1.4888, 1.5007, 1.4774, 1.4255, 1.4060, 1.4851, 1.4931,\n",
      "        1.4960, 1.5238, 1.5347, 1.5565, 1.4501, 1.5244, 1.4789, 1.3946, 1.4409,\n",
      "        1.4717, 1.3813, 1.6351, 1.5331, 1.4500, 1.4904, 1.5212, 1.3473, 1.3938,\n",
      "        1.4162, 1.3886, 1.4642, 2.2825, 1.4273, 1.4872, 1.4280, 1.4431, 1.4219,\n",
      "        1.4591, 1.4861, 1.4576, 1.4935, 1.3943, 1.4882, 1.4089, 1.4106, 1.5969,\n",
      "        1.4638, 1.5316, 1.3555, 1.5203, 1.3868, 1.4587, 1.4645, 1.4181, 1.5176,\n",
      "        1.4714, 1.4654, 1.4757, 1.5123, 1.4654, 1.5540, 1.5448, 1.4473, 1.3781,\n",
      "        1.3392, 1.4533, 1.4458, 1.4954, 1.5407, 1.5017, 1.4586, 1.4709, 1.4263,\n",
      "        1.4444, 1.5165, 1.4833, 1.3000, 1.4598, 1.5327, 1.5552, 1.3605, 1.5023,\n",
      "        1.5033, 1.4213, 1.3591, 1.3550, 1.4752, 1.4481, 1.4843, 1.4088, 1.5462,\n",
      "        1.4859, 1.5445, 1.4289, 1.5068, 1.5817, 1.3894, 1.3336, 1.5011, 1.4988,\n",
      "        1.4432, 1.4598, 1.5096, 1.6152, 1.5682, 1.5423, 1.5538, 1.4566, 1.4902,\n",
      "        1.4528, 1.4790, 1.6919, 1.4428, 1.5100, 1.5214, 1.4966, 1.4362, 1.4471,\n",
      "        1.4545, 1.4768, 1.5430, 1.3868, 1.5182, 1.4363, 1.4779, 1.4320, 1.4402,\n",
      "        1.2930, 1.5281, 1.5409, 1.5837, 1.3817, 1.4880, 1.4943, 1.5104, 1.5852,\n",
      "        1.4879, 1.4388, 1.4799, 1.4350, 1.5431, 1.3986, 1.3522, 1.4398, 1.4663,\n",
      "        1.5889, 1.5170, 1.4585, 1.3570, 1.4707, 1.5160, 1.4306, 1.5722, 1.4120,\n",
      "        1.5698, 1.5195, 1.4282, 1.4076, 2.1103], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.1913,  0.1439, -0.0587,  ...,  0.1206, -0.0412,  0.1710],\n",
      "        [ 0.1080, -0.1282,  0.0175,  ...,  0.0437,  0.0472, -0.0279],\n",
      "        [-0.0331, -0.0769, -0.0039,  ...,  0.0006, -0.1255, -0.0912],\n",
      "        ...,\n",
      "        [-0.1786, -0.0246, -0.0410,  ..., -0.0425,  0.0283, -0.0534],\n",
      "        [-0.0601, -0.1179, -0.1459,  ..., -0.0513, -0.0413,  0.0630],\n",
      "        [-0.1071,  0.0360,  0.0248,  ...,  0.0250,  0.0601,  0.0684]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0176, -0.0285,  0.0359,  ..., -0.1464,  0.1971, -0.1429],\n",
      "        [ 0.0213,  0.1127,  0.0043,  ..., -0.0422,  0.0240, -0.0810],\n",
      "        [ 0.0328, -0.0235,  0.0089,  ..., -0.0473,  0.0231,  0.0545],\n",
      "        ...,\n",
      "        [-0.1333, -0.1791,  0.1892,  ..., -0.0054,  0.0455, -0.0115],\n",
      "        [ 0.0381,  0.1481, -0.3332,  ...,  0.0029, -0.0540,  0.0920],\n",
      "        [-0.1313, -0.1393,  0.0490,  ..., -0.1403, -0.0163, -0.0613]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.6502, 2.6156, 2.0932, 1.2639, 2.4448, 4.1155, 4.0457, 4.0813, 1.6421,\n",
      "        1.7426, 2.3876, 2.2295, 1.8275, 3.9988, 3.8468, 3.9548, 1.6908, 1.9997,\n",
      "        2.5554, 2.4655, 2.4225, 2.8384, 2.4930, 2.8896, 2.1552, 2.0546, 1.9411,\n",
      "        1.9582, 1.9826, 2.1164, 2.6452, 3.0319, 2.0923, 1.9524, 1.9136, 1.3721,\n",
      "        1.9326, 2.9358, 2.5857, 2.5309, 2.3159, 1.6121, 1.7857, 2.2958, 2.6083,\n",
      "        1.9384, 2.4236, 2.3950, 1.2545, 1.4059, 2.0667, 1.9373, 1.6634, 2.5145,\n",
      "        2.3794, 2.6810, 1.7396, 2.3242, 1.9650, 1.8965, 1.9688, 1.9418, 2.2488,\n",
      "        2.6895, 2.1461, 2.1112, 1.7653, 2.0296, 2.1150, 2.2905, 2.7678, 3.8653,\n",
      "        2.3992, 2.2087, 2.1105, 1.9179, 1.7957, 2.3470, 2.9652, 2.7732, 1.9223,\n",
      "        2.1029, 2.0688, 1.9646, 2.0856, 2.3693, 2.5675, 2.0249, 1.8247, 1.6944,\n",
      "        2.2734, 2.1682, 2.0256, 2.0651, 2.6483, 2.8148, 2.7232, 2.0216, 2.3221,\n",
      "        2.3505, 2.1293, 2.4979, 2.7553, 2.7774, 1.6059, 2.2108, 1.7986, 2.3020,\n",
      "        2.6188, 3.3310, 2.7673, 3.0991, 3.1531, 1.7765, 2.4253, 2.0373, 2.1842,\n",
      "        2.8903, 2.4858, 2.5315, 1.7635, 2.2949, 1.6692, 2.4804, 2.7635, 2.4357,\n",
      "        2.8048, 2.8515, 1.2723, 1.8122, 1.7715, 2.0678, 2.2418, 2.5657, 2.2642,\n",
      "        2.8708, 2.3819, 1.7237, 2.2664, 1.7338, 1.9725, 2.1832, 2.4724, 3.1641,\n",
      "        2.2298, 1.5323, 1.7500, 2.3365, 2.7462, 2.5351, 2.5812, 2.5951, 1.8959,\n",
      "        1.4568, 1.8616, 1.7385, 2.0547, 2.0577, 2.5471, 2.6174, 1.9045, 2.0368,\n",
      "        2.1491, 2.2578, 2.0064, 1.7262, 2.6151, 2.8401, 1.8364, 2.0855, 2.4266,\n",
      "        2.0812, 2.6259, 2.4519, 2.6943, 1.9615, 2.0984, 2.4450, 1.9669, 2.6365,\n",
      "        3.3529, 3.3635, 2.8954, 3.0783, 2.4246, 2.1224, 2.4412, 2.2906, 2.2350,\n",
      "        2.7289, 2.7601, 2.9762, 1.9270, 1.6923, 1.9806, 1.6168, 2.1872, 3.1525,\n",
      "        2.7998, 2.9103, 2.6990, 2.2466, 2.1372, 2.7138, 2.8328, 2.3670, 2.5533,\n",
      "        2.6239, 2.3611, 2.1765, 2.2233, 2.1408, 1.9627, 2.6408, 2.8932, 2.7491,\n",
      "        2.0639, 2.1844, 2.0121, 2.2453, 2.3933, 2.6866, 2.9182, 2.9554, 1.2797,\n",
      "        1.4910, 2.3079, 1.6123, 2.3203, 1.7228, 2.2731, 2.2295, 1.6505, 1.6896,\n",
      "        1.3521, 2.0697, 1.6070, 2.3539, 2.1010, 2.3073, 2.1981, 1.8335, 1.8523,\n",
      "        1.8404, 1.7815, 1.8730, 2.4490, 2.6054, 1.9097, 1.7991, 2.0760, 2.4617,\n",
      "        2.1500, 2.2592, 2.4519, 2.3295, 1.9338, 2.3007, 2.7088, 2.2805, 2.1295,\n",
      "        2.6968, 2.8910, 3.2615, 2.6082, 2.0596, 1.8855, 2.2187, 2.5806, 3.1001,\n",
      "        3.1367, 3.6361, 2.1971, 2.1435, 2.1802, 2.0894, 1.8538, 2.2296, 3.7110,\n",
      "        3.8970, 2.2108, 2.2536, 2.6539, 2.3088, 2.0942, 2.0067, 3.6656, 3.6851,\n",
      "        2.9023, 1.8844, 1.6712, 2.2201, 2.2959, 2.3754, 2.8909, 2.8768, 1.7485,\n",
      "        1.7711, 2.1843, 1.9280, 1.9510, 2.7384, 2.7710, 3.0087, 2.0421, 1.9873,\n",
      "        2.2730, 2.8562, 2.3922, 2.3298, 3.2295, 2.8123, 2.2061, 2.0193, 2.1244,\n",
      "        1.9207, 2.8432, 2.7413, 2.7872, 2.3487], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0528,  0.0363, -0.1404,  ...,  0.1749, -0.0095,  0.2133],\n",
      "        [ 0.0687, -0.0108, -0.0267,  ...,  0.1657, -0.0969, -0.1916],\n",
      "        [-0.0832, -0.0244,  0.0423,  ..., -0.0532, -0.0036,  0.1286],\n",
      "        ...,\n",
      "        [ 0.0253,  0.2217,  0.1188,  ...,  0.0197, -0.0818, -0.0353],\n",
      "        [-0.0240,  0.1141,  0.0670,  ...,  0.0692,  0.0048, -0.1398],\n",
      "        [ 0.2032, -0.0093,  0.0479,  ..., -0.0212, -0.0887, -0.0002]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0136, -0.0452,  0.0064,  ...,  0.0516,  0.0515,  0.0320],\n",
      "        [ 0.0608,  0.0408, -0.0213,  ...,  0.0114, -0.2708, -0.0407],\n",
      "        [ 0.0469,  0.0192,  0.1265,  ..., -0.0443, -0.1521, -0.0255],\n",
      "        ...,\n",
      "        [-0.0878, -0.1330,  0.1468,  ..., -0.0283,  0.0971,  0.0644],\n",
      "        [-0.1082, -0.1558, -0.2830,  ...,  0.0050,  0.0208, -0.0580],\n",
      "        [ 0.0882, -0.0245,  0.0646,  ..., -0.0613,  0.1311, -0.1207]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.0801, 2.5454, 2.3534, 1.9563, 2.0947, 2.9690, 3.0816, 3.2038, 2.0320,\n",
      "        1.9005, 2.1902, 1.7012, 1.8005, 2.9080, 2.9956, 3.1893, 1.7702, 1.6986,\n",
      "        1.7789, 1.4597, 1.4505, 1.5532, 2.4262, 2.1200, 1.8313, 2.3842, 2.1493,\n",
      "        2.7785, 2.7044, 2.7936, 2.1872, 2.0606, 2.2343, 1.7282, 2.1142, 2.4230,\n",
      "        1.8947, 1.4490, 2.4256, 1.8532, 2.1351, 1.9019, 1.8645, 1.5370, 1.6852,\n",
      "        2.5597, 2.3226, 2.1420, 1.2325, 2.1807, 1.9795, 1.3838, 2.5246, 1.4686,\n",
      "        2.2704, 1.9798, 1.5304, 1.3496, 1.5547, 2.1629, 1.2913, 2.4373, 2.0808,\n",
      "        2.0001, 2.0338, 2.1033, 1.9229, 2.1926, 1.7963, 1.9357, 2.2573, 2.1702,\n",
      "        2.2783, 2.2245, 1.9967, 1.7335, 1.9597, 1.9497, 2.3237, 1.8741, 1.7868,\n",
      "        2.1621, 2.4480, 2.0349, 2.0122, 1.7537, 2.1366, 2.1363, 1.8818, 1.5387,\n",
      "        1.9156, 2.0270, 1.9629, 2.4083, 2.4379, 2.5233, 1.8670, 1.8546, 1.9981,\n",
      "        2.8820, 3.0257, 2.9527, 2.7151, 2.6997, 2.3930, 2.2379, 2.0177, 1.8048,\n",
      "        1.5508, 1.6605, 2.5677, 2.1661, 2.0660, 1.8363, 2.2414, 2.8375, 3.2821,\n",
      "        1.6042, 2.2681, 2.4918, 2.4985, 2.0704, 1.5898, 1.5874, 1.3801, 2.7167,\n",
      "        2.3488, 2.1238, 1.8422, 1.7717, 2.1960, 1.5201, 1.6356, 1.8005, 2.2272,\n",
      "        2.1135, 1.2367, 1.6991, 1.7485, 2.0175, 2.0073, 2.1669, 2.3764, 2.1196,\n",
      "        2.1707, 1.8195, 2.2821, 1.4093, 1.3516, 1.6107, 2.2630, 2.0711, 1.8645,\n",
      "        1.6004, 1.6279, 2.4121, 2.7284, 2.2204, 2.3359, 1.9434, 1.9885, 2.0685,\n",
      "        2.4383, 2.0437, 2.3073, 2.2920, 2.1596, 2.5379, 1.5089, 2.1324, 2.2579,\n",
      "        2.3212, 1.9888, 2.1639, 2.4237, 2.1530, 2.2684, 2.0758, 1.9215, 1.9331,\n",
      "        1.6264, 1.6005, 2.7913, 2.7354, 2.2101, 2.3566, 2.2811, 2.4981, 2.3413,\n",
      "        2.5897, 2.6791, 2.7004, 2.1357, 1.7810, 2.0047, 2.1804, 2.4103, 1.5509,\n",
      "        2.1535, 2.1462, 1.9084, 2.0515, 1.9552, 1.3726, 1.3420, 2.1230, 2.5501,\n",
      "        2.3498, 2.0701, 2.2193, 2.1227, 2.2995, 2.0020, 1.9695, 2.6695, 2.7156,\n",
      "        2.1324, 2.1671, 2.0408, 1.9931, 1.7616, 1.9403, 2.6549, 2.7555, 1.4491,\n",
      "        1.7870, 1.3653, 2.3537, 1.2332, 2.3115, 2.1162, 2.1013, 1.6064, 1.5327,\n",
      "        2.4257, 1.3044, 2.4427, 1.3136, 1.9825, 1.8853, 1.9633, 1.8179, 1.7905,\n",
      "        2.1321, 2.4386, 2.5614, 2.2665, 2.1441, 2.0938, 1.7983, 1.9854, 1.5365,\n",
      "        1.3738, 1.4349, 2.3596, 1.9332, 1.9685, 1.9769, 1.7733, 1.7968, 2.8031,\n",
      "        2.3921, 2.6747, 2.1273, 2.2021, 2.0743, 2.3725, 2.4030, 1.5839, 2.2402,\n",
      "        2.6566, 2.1743, 2.0850, 2.1494, 2.0353, 2.1392, 1.9327, 2.2130, 3.0331,\n",
      "        3.0326, 2.3201, 2.3914, 2.5229, 2.1936, 2.0664, 1.9892, 3.0062, 3.0446,\n",
      "        2.0394, 1.9070, 2.4598, 1.7389, 1.3756, 2.3826, 2.3162, 2.2542, 2.1030,\n",
      "        1.7690, 1.5512, 1.6548, 2.5122, 1.6617, 2.2071, 2.2569, 2.2578, 1.8992,\n",
      "        2.1725, 2.2870, 1.9433, 2.3382, 2.3189, 2.7745, 1.7905, 2.0525, 2.1216,\n",
      "        2.2870, 2.3020, 2.2691, 2.8605, 2.3341], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0283, -0.0195, -0.0466,  ...,  0.1461, -0.0745, -0.0679],\n",
      "        [-0.0280, -0.0835,  0.0401,  ..., -0.0004,  0.0150,  0.0179],\n",
      "        [-0.0676,  0.1491,  0.0185,  ..., -0.0324, -0.1147, -0.0558],\n",
      "        ...,\n",
      "        [-0.0194,  0.1029, -0.1721,  ..., -0.0643,  0.1024,  0.0681],\n",
      "        [-0.0606, -0.1340,  0.0140,  ..., -0.0180,  0.1068, -0.0306],\n",
      "        [ 0.1068, -0.0672, -0.0874,  ...,  0.0060, -0.0572, -0.0716]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.1007,  0.0420,  0.0728,  ..., -0.0412, -0.0170,  0.0073],\n",
      "        [ 0.0628, -0.2094, -0.0632,  ..., -0.0234, -0.0946,  0.1010],\n",
      "        [ 0.1231,  0.1951, -0.0433,  ..., -0.0022,  0.0793, -0.0088],\n",
      "        ...,\n",
      "        [ 0.0425,  0.0623, -0.0830,  ...,  0.0406,  0.0985,  0.0282],\n",
      "        [-0.1121,  0.0135,  0.0106,  ...,  0.0666, -0.0313, -0.0144],\n",
      "        [ 0.1298, -0.0779, -0.0233,  ..., -0.0352,  0.0530, -0.0361]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.5272, 1.6399, 1.5915, 1.5357, 1.6434, 1.6291, 1.5056, 1.5451, 1.5269,\n",
      "        1.5868, 1.5215, 1.5710, 1.5106, 1.5398, 1.5559, 1.5410, 1.1699, 1.2181,\n",
      "        1.2137, 1.1217, 1.2262, 1.1804, 1.2341, 1.1829, 1.2592, 1.1774, 1.2031,\n",
      "        1.1680, 1.1794, 1.1864, 1.1715, 1.2289, 1.6772, 1.4956, 1.6138, 1.6294,\n",
      "        1.6175, 1.5474, 1.6580, 1.6204, 1.6013, 1.5853, 1.6135, 1.6444, 1.6257,\n",
      "        1.6408, 1.6433, 1.6140, 1.2809, 1.1577, 1.2673, 1.2278, 1.1961, 1.2151,\n",
      "        1.2261, 1.2386, 1.2627, 1.2773, 1.1744, 1.2442, 1.2154, 1.2196, 1.2574,\n",
      "        1.2187, 1.8325, 1.8472, 1.8343, 1.7411, 1.8783, 1.8034, 1.8382, 1.9294,\n",
      "        1.8843, 1.9070, 1.8260, 1.9665, 1.8537, 1.9500, 1.8820, 1.9473, 1.7951,\n",
      "        1.7867, 1.8390, 1.7401, 1.7464, 1.9000, 1.7550, 1.7559, 1.8500, 1.6767,\n",
      "        1.7674, 1.8292, 1.7742, 1.8637, 1.8388, 1.8370, 1.6519, 1.7779, 1.6414,\n",
      "        1.6772, 1.7805, 1.6661, 1.6829, 1.7039, 1.7318, 1.6569, 1.6671, 1.6501,\n",
      "        1.6658, 1.7283, 1.6355, 1.6675, 1.6172, 1.5656, 1.6039, 1.5498, 1.5469,\n",
      "        1.5747, 1.6256, 1.5783, 1.5952, 1.5724, 1.5939, 1.5260, 1.6177, 1.5769,\n",
      "        1.5712, 1.6337, 1.2462, 1.1890, 1.2367, 1.2357, 1.2237, 1.1823, 1.1748,\n",
      "        1.1755, 1.2078, 1.1587, 1.1635, 1.1740, 1.2130, 1.1948, 1.1655, 1.2109,\n",
      "        1.5826, 1.6088, 1.5890, 1.5887, 1.5365, 1.5827, 1.5901, 1.5458, 1.5769,\n",
      "        1.6117, 1.6281, 1.6499, 1.5696, 1.6306, 1.6181, 1.6560, 1.7826, 1.8544,\n",
      "        1.7580, 1.8587, 1.8124, 1.7649, 1.7524, 1.8455, 1.7268, 1.8060, 1.6977,\n",
      "        1.8009, 1.6819, 1.8048, 1.6808, 1.7871, 1.7265, 1.7473, 1.6593, 1.6386,\n",
      "        1.7736, 1.7244, 1.7483, 1.8189, 1.7296, 1.7639, 1.7033, 1.6299, 1.7335,\n",
      "        1.6635, 1.7585, 1.6489, 1.5190, 1.5598, 1.4873, 1.5216, 1.5170, 1.5483,\n",
      "        1.5518, 1.5543, 1.5473, 1.5790, 1.6520, 1.5943, 1.6216, 1.5828, 1.5278,\n",
      "        1.5094, 1.7989, 1.8334, 1.7350, 1.7699, 1.8159, 1.8816, 1.7130, 1.7366,\n",
      "        1.6951, 1.8245, 1.8671, 1.7377, 1.7824, 1.7490, 1.8031, 1.7918, 1.3409,\n",
      "        1.3331, 1.2977, 1.3627, 1.3447, 1.4610, 1.3613, 1.3682, 1.4541, 1.4836,\n",
      "        1.3857, 1.3482, 1.3708, 1.3107, 1.3525, 1.3842, 1.6655, 1.6543, 1.6791,\n",
      "        1.6352, 1.5863, 1.6455, 1.6618, 1.6931, 1.6158, 1.6577, 1.6700, 1.6240,\n",
      "        1.7081, 1.6579, 1.7181, 1.6521, 1.2660, 1.2770, 1.2893, 1.2335, 1.2419,\n",
      "        1.2057, 1.2556, 1.2190, 1.2743, 1.2448, 1.2616, 1.1654, 1.2908, 1.2702,\n",
      "        1.2628, 1.2279, 1.8248, 1.6240, 1.6945, 1.6764, 1.6567, 1.6677, 1.6097,\n",
      "        1.8228, 1.5692, 1.6430, 1.7216, 1.7609, 1.7230, 1.6166, 1.7402, 1.7759,\n",
      "        1.3061, 1.2980, 1.2656, 1.2814, 1.2790, 1.3002, 1.3535, 1.2208, 1.2732,\n",
      "        1.3230, 1.2968, 1.3123, 1.2997, 1.3005, 1.2601, 1.3309, 1.4792, 1.5306,\n",
      "        1.5492, 1.4329, 1.5518, 1.5265, 1.4427, 1.4884, 1.5333, 1.4654, 1.5130,\n",
      "        1.4176, 1.4364, 1.5398, 1.4649, 1.6545], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0072,  0.0275, -0.0987,  ...,  0.0838, -0.0675,  0.2627],\n",
      "        [ 0.0284, -0.2144,  0.0263,  ...,  0.1146,  0.0788,  0.0401],\n",
      "        [ 0.1191, -0.0373, -0.0527,  ..., -0.0537, -0.0905,  0.1126],\n",
      "        ...,\n",
      "        [ 0.1226,  0.1020,  0.0064,  ..., -0.0435,  0.0303, -0.0754],\n",
      "        [-0.0416, -0.0597,  0.0929,  ..., -0.0577, -0.0103,  0.0725],\n",
      "        [ 0.1775, -0.0602,  0.0101,  ..., -0.0578,  0.1327,  0.0175]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0888,  0.0128,  0.0299,  ...,  0.0219,  0.0374, -0.0780],\n",
      "        [-0.0329,  0.1707,  0.1076,  ..., -0.0068, -0.0087, -0.1130],\n",
      "        [ 0.0619,  0.0029,  0.0427,  ...,  0.0916, -0.0434, -0.0564],\n",
      "        ...,\n",
      "        [ 0.0194, -0.1103, -0.0349,  ..., -0.1215,  0.0031,  0.1362],\n",
      "        [ 0.0195, -0.1059,  0.0562,  ..., -0.0463, -0.0015, -0.0162],\n",
      "        [ 0.0733, -0.1120, -0.0374,  ..., -0.0589,  0.0424,  0.0280]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.4852, 1.5863, 1.5307, 1.5088, 1.6193, 1.3944, 1.4274, 1.5202, 1.4342,\n",
      "        1.4310, 1.4320, 1.5326, 1.5587, 1.4011, 1.6347, 1.4332, 1.3336, 1.5348,\n",
      "        1.4366, 1.4599, 1.5344, 1.4167, 1.5995, 1.4376, 1.4715, 1.4841, 1.4711,\n",
      "        1.5340, 1.4947, 1.4911, 1.4454, 1.4229, 1.6320, 1.4771, 1.4091, 1.4460,\n",
      "        1.6202, 1.4149, 1.3964, 1.4694, 1.6347, 1.4984, 1.4732, 1.4960, 1.3767,\n",
      "        1.4153, 1.3913, 1.6091, 1.5136, 1.5012, 1.4560, 1.5075, 1.3090, 1.3775,\n",
      "        1.3879, 1.3591, 1.3411, 1.4378, 1.6290, 1.5532, 1.3855, 1.6763, 2.7814,\n",
      "        1.4581, 1.4423, 1.4788, 1.4278, 1.5659, 1.5388, 1.5121, 1.3352, 1.4405,\n",
      "        1.4037, 1.4332, 1.4897, 1.5940, 1.4385, 1.5158, 1.4677, 1.5055, 1.4844,\n",
      "        1.5357, 1.5033, 1.5741, 1.4117, 1.4569, 1.4445, 1.5497, 1.3004, 1.5096,\n",
      "        1.4159, 1.5419, 1.5935, 1.5185, 1.4364, 1.5835, 1.4850, 1.5966, 1.5393,\n",
      "        1.5207, 1.5843, 1.5451, 1.6172, 1.4659, 1.5571, 1.6062, 1.4274, 1.4081,\n",
      "        1.4243, 1.4254, 1.5863, 1.3596, 1.4392, 1.4486, 1.5461, 1.4971, 1.5840,\n",
      "        1.4750, 1.4543, 1.4336, 1.4925, 1.5821, 1.5370, 1.5233, 1.3452, 1.5704,\n",
      "        1.5224, 1.5331, 1.4827, 1.4917, 1.3546, 1.3926, 1.5097, 1.5815, 1.5642,\n",
      "        1.4026, 1.4565, 1.4945, 1.3433, 1.3841, 1.4233, 1.4171, 1.4768, 1.4137,\n",
      "        1.4062, 1.4882, 1.4657, 1.5479, 1.4730, 1.5519, 2.1133, 1.5775, 1.5598,\n",
      "        1.4972, 1.4767, 1.5169, 1.4652, 1.5629, 1.5577, 1.4844, 1.3515, 1.4537,\n",
      "        1.5758, 1.5780, 1.6583, 1.5675, 1.3802, 1.4450, 1.4097, 1.5219, 1.4319,\n",
      "        1.6426, 1.5335, 1.4466, 1.4913, 1.3297, 1.4979, 1.5517, 1.4700, 1.5537,\n",
      "        1.4411, 1.3917, 1.4599, 1.4890, 1.5446, 1.4041, 1.4665, 1.4746, 1.4919,\n",
      "        1.6231, 1.3502, 1.3560, 2.1453, 1.3052, 1.4609, 1.4697, 1.4984, 1.4280,\n",
      "        1.5222, 1.5612, 1.5971, 1.3091, 1.4373, 1.3624, 1.3663, 1.5522, 1.5311,\n",
      "        1.3408, 1.5385, 1.6565, 1.5588, 1.4876, 1.4312, 1.6053, 1.4554, 1.5336,\n",
      "        1.5547, 1.5143, 1.3529, 1.5455, 1.4272, 1.5759, 1.5304, 1.4447, 1.4745,\n",
      "        1.4382, 1.3927, 1.5012, 1.4370, 1.4487, 1.3536, 1.3955, 1.3746, 1.4724,\n",
      "        1.4584, 1.7232, 1.4524, 1.3445, 1.4818, 1.6252, 1.5196, 1.4486, 1.3228,\n",
      "        1.3826, 1.3533, 1.5325, 1.4179, 1.5299, 1.4783, 1.4135, 1.5139, 1.4146,\n",
      "        1.4979, 1.3969, 1.4388, 1.4577, 1.3766, 1.4539, 1.2180, 1.5059, 1.4594,\n",
      "        1.5009, 1.6209, 1.5234, 1.4064, 1.4583, 1.5368, 1.6806, 1.3966, 1.5973,\n",
      "        1.3467, 1.5388, 1.2857, 1.6064, 1.4255, 1.6169, 1.5345, 1.3505, 1.5718,\n",
      "        1.4807, 1.4838, 1.4700, 1.6023, 1.6147, 1.5629, 1.3989, 1.4161, 1.4603,\n",
      "        1.2450, 1.3561, 1.5340, 1.5493, 1.5026, 1.5171, 1.2936, 1.4623, 1.5910,\n",
      "        1.5688, 1.4752, 1.4966, 1.5027, 1.5506, 1.4831, 1.3553, 1.4801, 1.4337,\n",
      "        1.4599, 1.5498, 1.4339, 1.4534, 1.4642, 1.3374, 1.5551, 1.5341, 1.5673,\n",
      "        1.5014, 1.5767, 1.5708, 1.3815, 1.6792], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0974,  0.0594, -0.1145,  ..., -0.1115, -0.0819,  0.1815],\n",
      "        [ 0.1114, -0.1549, -0.0573,  ...,  0.1229, -0.0385, -0.0939],\n",
      "        [ 0.1785, -0.0444, -0.1038,  ..., -0.0657, -0.0241,  0.0197],\n",
      "        ...,\n",
      "        [-0.0357, -0.0714,  0.0728,  ...,  0.0254, -0.1044,  0.1380],\n",
      "        [-0.0301, -0.0758,  0.0531,  ..., -0.0502,  0.0368,  0.0036],\n",
      "        [-0.0124, -0.1155,  0.0185,  ..., -0.1466, -0.0767,  0.1884]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0701, -0.0195, -0.0521,  ..., -0.0232, -0.0614, -0.0142],\n",
      "        [-0.1042, -0.0104, -0.0218,  ..., -0.0150, -0.0675,  0.0020],\n",
      "        [ 0.0911, -0.0028, -0.1154,  ...,  0.0119,  0.0328,  0.0210],\n",
      "        ...,\n",
      "        [-0.1355, -0.0670,  0.0020,  ..., -0.0172,  0.0272,  0.0401],\n",
      "        [-0.0281, -0.0774, -0.1560,  ..., -0.0121,  0.1091, -0.0975],\n",
      "        [ 0.1272,  0.1299, -0.0124,  ...,  0.1130,  0.0355, -0.0015]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.6579, 1.4362, 1.4333, 1.3821, 1.4182, 2.3624, 2.0713, 2.5590, 1.8090,\n",
      "        1.4647, 1.3471, 1.3601, 1.5070, 1.2785, 2.5912, 2.0349, 1.1011, 1.1287,\n",
      "        1.4390, 1.9767, 2.4265, 4.6501, 4.2600, 3.4385, 1.0099, 1.2299, 1.2878,\n",
      "        2.0306, 2.6301, 4.4378, 4.2134, 3.6061, 1.4914, 1.5164, 1.7802, 1.7877,\n",
      "        1.8833, 1.5644, 2.2301, 1.8761, 1.4867, 1.5687, 2.1623, 1.8227, 1.8690,\n",
      "        1.7264, 2.2270, 2.2885, 1.5816, 1.9156, 1.9380, 1.8436, 2.0991, 1.7544,\n",
      "        2.3045, 2.3098, 1.5279, 1.9601, 1.8580, 1.9616, 2.2451, 2.5637, 2.5662,\n",
      "        2.3488, 2.1455, 1.7758, 2.3912, 2.2600, 2.2763, 2.1790, 2.7780, 2.3885,\n",
      "        1.4835, 1.8715, 2.1361, 2.0398, 2.0120, 2.5187, 2.6739, 2.4436, 2.2261,\n",
      "        1.8579, 1.4257, 1.7337, 1.6311, 2.0758, 2.3112, 2.2074, 2.1480, 1.8446,\n",
      "        1.7081, 1.6120, 2.4526, 2.4516, 2.1117, 2.2415, 1.2832, 1.4362, 1.4073,\n",
      "        1.3875, 1.4291, 2.3041, 1.9971, 2.5825, 1.4081, 1.5164, 1.3988, 1.2904,\n",
      "        1.4126, 1.4232, 2.0267, 2.1985, 2.0410, 1.6312, 1.5168, 1.5718, 1.2360,\n",
      "        2.0262, 2.1945, 2.1226, 1.9331, 1.3425, 1.3781, 1.5630, 2.0269, 1.5729,\n",
      "        1.9759, 2.5323, 1.8478, 1.3920, 1.4999, 1.4121, 1.4388, 2.0795, 1.9729,\n",
      "        2.5244, 1.4903, 1.6789, 1.5856, 1.4161, 1.9491, 1.6637, 2.0832, 2.4958,\n",
      "        1.7441, 1.6366, 2.1077, 1.9513, 2.2123, 1.8839, 2.4128, 2.1732, 2.4303,\n",
      "        1.9687, 1.7449, 2.2393, 1.7446, 2.4663, 2.5513, 2.6849, 1.9608, 1.7362,\n",
      "        2.0739, 2.0064, 2.4483, 2.5913, 2.9753, 2.8067, 1.6456, 1.9218, 1.9801,\n",
      "        2.2326, 2.0853, 2.5448, 2.7821, 2.7880, 2.0124, 1.5923, 1.7471, 2.1180,\n",
      "        1.7262, 1.9367, 2.3641, 2.5234, 1.7433, 1.7845, 1.9987, 1.8459, 1.7459,\n",
      "        1.7800, 2.2015, 2.4129, 1.5191, 1.5551, 1.4129, 1.4837, 1.7969, 2.2280,\n",
      "        2.0381, 2.7568, 1.6470, 1.5248, 1.7329, 1.6961, 1.7936, 1.7156, 1.9406,\n",
      "        2.3345, 1.1458, 1.1443, 1.4758, 1.7356, 2.5206, 3.6379, 3.7720, 3.2780,\n",
      "        0.8240, 1.1919, 1.2837, 2.0729, 2.2404, 3.7330, 3.5249, 3.3430, 1.6637,\n",
      "        1.9500, 1.9017, 2.0359, 1.9665, 2.1687, 2.4457, 2.3560, 1.9452, 1.8432,\n",
      "        1.9107, 2.0104, 1.9273, 1.9114, 2.1928, 2.4267, 1.8683, 1.4589, 1.7653,\n",
      "        1.8170, 1.9382, 1.6376, 2.3274, 2.0293, 1.4497, 1.7161, 1.8546, 1.7381,\n",
      "        1.9599, 1.6720, 2.3729, 2.3810, 1.3835, 1.3518, 1.7803, 1.7010, 1.6470,\n",
      "        1.7547, 2.4688, 2.1257, 1.3666, 1.5720, 1.7306, 1.7808, 1.6935, 1.9183,\n",
      "        2.0585, 2.0541, 1.4105, 1.4780, 1.8007, 1.6688, 1.6210, 1.8251, 2.2875,\n",
      "        2.3727, 1.4843, 1.3994, 1.4690, 1.6379, 1.5442, 1.7736, 2.5260, 2.5035,\n",
      "        1.8963, 1.7503, 1.7025, 1.4933, 1.6592, 1.8671, 2.3274, 2.2930, 1.6071,\n",
      "        1.4596, 1.6012, 1.8346, 1.5373, 1.9718, 2.2165, 2.4272, 1.5861, 1.1843,\n",
      "        1.2226, 1.4178, 1.3635, 1.8357, 2.0726, 2.3337, 1.4956, 1.3248, 1.4438,\n",
      "        1.6852, 1.6455, 1.7410, 1.9528, 2.0677], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.1011,  0.0472,  0.0762,  ...,  0.0725,  0.1155, -0.0312],\n",
      "        [ 0.0930, -0.0944, -0.0129,  ...,  0.0377,  0.0660, -0.0120],\n",
      "        [ 0.0839, -0.0321,  0.0353,  ...,  0.1766,  0.0375, -0.1734],\n",
      "        ...,\n",
      "        [-0.1094,  0.0767, -0.0775,  ..., -0.0416,  0.0441, -0.0375],\n",
      "        [ 0.0675, -0.0954, -0.0132,  ...,  0.0429,  0.0707,  0.0600],\n",
      "        [-0.0133,  0.0353, -0.0130,  ...,  0.0328,  0.0228, -0.0165]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-9.6924e-04,  1.4808e-02, -5.4056e-02,  ..., -5.9756e-05,\n",
      "          1.1257e-01,  6.8136e-02],\n",
      "        [-4.6217e-02, -7.3824e-03, -5.6782e-02,  ...,  5.3237e-02,\n",
      "          5.3616e-02,  6.6409e-02],\n",
      "        [-3.6919e-02,  9.3368e-02, -4.5654e-02,  ..., -1.5445e-01,\n",
      "         -8.1055e-03,  8.5169e-02],\n",
      "        ...,\n",
      "        [-2.3618e-04, -2.3092e-02,  2.8901e-02,  ...,  1.4425e-04,\n",
      "         -3.9764e-02, -4.2556e-02],\n",
      "        [-1.9380e-01, -2.0590e-02, -8.4090e-02,  ..., -3.4319e-02,\n",
      "         -9.5992e-02, -1.8172e-02],\n",
      "        [ 2.5444e-01, -2.3175e-02, -1.2144e-02,  ..., -8.6750e-03,\n",
      "          3.4835e-02, -1.6897e-02]], device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.6901, 1.4298, 1.7453, 1.6625, 1.6545, 1.2896, 1.8225, 1.7626, 1.8045,\n",
      "        1.7439, 1.6363, 1.7145, 1.6673, 2.3117, 1.6647, 1.7685, 1.1484, 1.3900,\n",
      "        1.3753, 2.0084, 1.9245, 3.2050, 2.9254, 2.7812, 1.1120, 0.9392, 1.4350,\n",
      "        1.8692, 2.2257, 3.0266, 3.0223, 2.7161, 1.4762, 1.5824, 1.9992, 1.8014,\n",
      "        1.9028, 2.2570, 1.9999, 1.9345, 1.4294, 1.4515, 1.8875, 1.8812, 1.8538,\n",
      "        1.6526, 2.1032, 2.1460, 1.6828, 2.0841, 1.8332, 1.7057, 1.8017, 2.2720,\n",
      "        2.2901, 2.0170, 1.4014, 1.6636, 1.9481, 1.9804, 1.8881, 1.5372, 2.1759,\n",
      "        2.0352, 1.8895, 1.7117, 2.0542, 2.0979, 1.8531, 2.0740, 2.4378, 2.1064,\n",
      "        1.7551, 1.9543, 2.2812, 2.0551, 1.9989, 1.8952, 2.5715, 2.3170, 1.7885,\n",
      "        1.8901, 1.7296, 1.7175, 1.9981, 1.9724, 2.0101, 2.0759, 2.2895, 1.8547,\n",
      "        1.9279, 1.8706, 1.3204, 1.4631, 2.0985, 1.9076, 1.4760, 1.5220, 1.7244,\n",
      "        1.9040, 1.4430, 1.2648, 1.8934, 1.5211, 1.3224, 1.7244, 1.6960, 1.5561,\n",
      "        1.7947, 1.9424, 1.8680, 1.6467, 1.8423, 1.6644, 1.7456, 1.6977, 1.9615,\n",
      "        1.4442, 1.9819, 2.0908, 2.0244, 1.7846, 1.7368, 1.8221, 1.3797, 2.2888,\n",
      "        2.1304, 1.7870, 1.8181, 1.6051, 1.6508, 1.6809, 1.8541, 1.3600, 1.9013,\n",
      "        1.8085, 1.4148, 1.6455, 1.8709, 1.7390, 1.3277, 2.0001, 2.0476, 1.8211,\n",
      "        2.0402, 1.7590, 1.9451, 2.0128, 1.7764, 2.1293, 1.9919, 1.9868, 1.5772,\n",
      "        1.7232, 1.7962, 1.9615, 2.0382, 1.8888, 2.2557, 2.2683, 1.9020, 1.8857,\n",
      "        1.8800, 2.0686, 2.0127, 1.9537, 2.7780, 2.6227, 1.6432, 1.7251, 2.1520,\n",
      "        2.0491, 2.0441, 2.2843, 2.5111, 2.3604, 1.7844, 1.4995, 1.9423, 1.9569,\n",
      "        1.9644, 2.0299, 2.4345, 2.4030, 1.8794, 1.8724, 1.7823, 2.0091, 1.7452,\n",
      "        1.7731, 2.1050, 2.2811, 1.7665, 1.6329, 1.7372, 1.8654, 1.5278, 1.2061,\n",
      "        1.9632, 1.8588, 1.9148, 1.7430, 1.5462, 1.3146, 1.3501, 2.2016, 1.9226,\n",
      "        1.9767, 1.1053, 1.2718, 1.5245, 1.9349, 2.1981, 2.7698, 2.8139, 2.5206,\n",
      "        0.8822, 1.1700, 1.1164, 1.7197, 1.8152, 2.8380, 2.5741, 2.6274, 1.7296,\n",
      "        1.8357, 1.9123, 1.8853, 1.6653, 1.7978, 2.2881, 2.2627, 1.7328, 1.8854,\n",
      "        1.8349, 2.0321, 1.8805, 1.7104, 1.9848, 2.1853, 1.7704, 1.5614, 1.7693,\n",
      "        1.8813, 1.8705, 1.9470, 2.1772, 1.9979, 1.3471, 1.6604, 1.8429, 1.7438,\n",
      "        1.9345, 1.8578, 2.2558, 2.3191, 1.2602, 1.5813, 1.8244, 1.8505, 1.7577,\n",
      "        1.8526, 1.9940, 2.1371, 1.5627, 1.4654, 1.7877, 1.8041, 1.9242, 1.7531,\n",
      "        2.5125, 1.9818, 1.2792, 1.3785, 1.7182, 1.7653, 1.6841, 1.7082, 2.1210,\n",
      "        1.8657, 1.0020, 1.3304, 1.6737, 1.9252, 1.7378, 1.7295, 1.9686, 1.9735,\n",
      "        1.8157, 1.7665, 1.7366, 1.7667, 1.7726, 1.7117, 2.3074, 2.0739, 1.6763,\n",
      "        1.6542, 1.8008, 1.8245, 1.7083, 1.6116, 2.1011, 2.2264, 1.4053, 1.5233,\n",
      "        1.4744, 1.3481, 1.8235, 1.2385, 1.4698, 1.5212, 1.7525, 1.4489, 1.4020,\n",
      "        1.2768, 1.1129, 1.2385, 1.6282, 1.5777], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.3337,  0.0604,  0.0433,  ..., -0.0721,  0.0114,  0.0046],\n",
      "        [ 0.0230,  0.1058,  0.0094,  ...,  0.0869, -0.0825,  0.0831],\n",
      "        [-0.0539, -0.0305, -0.3123,  ...,  0.0936, -0.0085,  0.0292],\n",
      "        ...,\n",
      "        [-0.1622, -0.0076, -0.0787,  ..., -0.1303, -0.1678, -0.0611],\n",
      "        [-0.0361, -0.0996,  0.0459,  ...,  0.0450,  0.0341, -0.0721],\n",
      "        [ 0.1475,  0.0459, -0.0538,  ...,  0.0524,  0.0190,  0.0563]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0893,  0.1474,  0.1481,  ..., -0.0069, -0.1107, -0.0561],\n",
      "        [-0.1553, -0.0772,  0.0110,  ...,  0.0783, -0.0595,  0.1390],\n",
      "        [-0.0306,  0.0049, -0.0071,  ...,  0.0093,  0.1346, -0.0018],\n",
      "        ...,\n",
      "        [-0.1086, -0.0581, -0.0051,  ...,  0.1052,  0.0394,  0.1085],\n",
      "        [-0.0931,  0.1065, -0.0347,  ..., -0.0974, -0.1386, -0.1102],\n",
      "        [-0.1270,  0.0642,  0.0119,  ...,  0.0213, -0.0573,  0.0164]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.9919, 1.7890, 1.7176, 1.8750, 1.9581, 1.7886, 1.7659, 1.8095, 1.7721,\n",
      "        1.9024, 1.7131, 1.8031, 1.7861, 1.8008, 1.9807, 1.8473, 1.5640, 1.4496,\n",
      "        1.5711, 1.4845, 1.5061, 1.4802, 1.4278, 1.4410, 1.4632, 1.4218, 1.4365,\n",
      "        1.4804, 1.5405, 1.4633, 1.4815, 1.4422, 1.7774, 1.8959, 1.8243, 1.7406,\n",
      "        1.8590, 1.7296, 1.8627, 1.6864, 1.8537, 1.9351, 1.7567, 1.7654, 1.7072,\n",
      "        1.7669, 1.8065, 1.8350, 1.7768, 1.7679, 2.2139, 1.9217, 1.8609, 1.8836,\n",
      "        1.8377, 1.8473, 1.9907, 1.9871, 1.7807, 1.8057, 2.0076, 2.1678, 1.8832,\n",
      "        1.8731, 2.1739, 2.0793, 1.7738, 2.3126, 1.8005, 2.3029, 1.7925, 1.9511,\n",
      "        1.9189, 1.9744, 1.9697, 1.9211, 1.8326, 1.9079, 2.0179, 1.9617, 1.7883,\n",
      "        1.9182, 1.7291, 1.7717, 1.7293, 1.7402, 1.7344, 1.7481, 1.8791, 1.7537,\n",
      "        1.7666, 1.6620, 1.7439, 1.7990, 1.7408, 1.7928, 1.7621, 1.7997, 1.8673,\n",
      "        1.8825, 1.8624, 1.8952, 1.7478, 1.9294, 1.9456, 1.8838, 1.9201, 1.8593,\n",
      "        1.9086, 1.9129, 1.8995, 1.8468, 1.8383, 1.8199, 1.7046, 1.8690, 1.8564,\n",
      "        1.8729, 1.7816, 1.8405, 1.7967, 1.8034, 1.8343, 1.7804, 1.8097, 1.7025,\n",
      "        1.7832, 1.8502, 1.8559, 1.8258, 1.7999, 1.6973, 1.8518, 1.7492, 1.7396,\n",
      "        1.9084, 1.7251, 1.8305, 1.7479, 1.7693, 1.7640, 1.8348, 1.9066, 1.7992,\n",
      "        1.6686, 1.6091, 1.7045, 1.6545, 1.7440, 1.6695, 1.7185, 1.6539, 1.6978,\n",
      "        1.6911, 1.7498, 1.7961, 1.7188, 1.7555, 1.6919, 1.7758, 1.8816, 1.8563,\n",
      "        1.8754, 1.8535, 2.0495, 2.0666, 1.8723, 1.8519, 1.9132, 1.8670, 1.9367,\n",
      "        1.9084, 1.9452, 1.8295, 1.9156, 1.9522, 1.9169, 2.0352, 1.8821, 1.9448,\n",
      "        2.0415, 1.8521, 1.9346, 1.9559, 1.9095, 1.9264, 1.9529, 1.8688, 1.9952,\n",
      "        1.9381, 1.9476, 1.9223, 1.3918, 1.4041, 1.3231, 1.4014, 1.2949, 1.3361,\n",
      "        1.4246, 1.3314, 1.3987, 1.3450, 1.5080, 1.4212, 1.4308, 1.4066, 1.3388,\n",
      "        1.3824, 1.5949, 1.5467, 1.4164, 1.5498, 1.4930, 1.5235, 1.5245, 1.4963,\n",
      "        1.4844, 1.3989, 1.4030, 1.5052, 1.4110, 1.4510, 1.4994, 1.4400, 2.1339,\n",
      "        1.9643, 2.0098, 1.9048, 2.1455, 1.9863, 1.8460, 1.9036, 1.8581, 1.9033,\n",
      "        1.8104, 2.1055, 2.1381, 1.9730, 1.9288, 1.8745, 1.7297, 1.7898, 1.7144,\n",
      "        1.7614, 1.7276, 1.7171, 1.8054, 1.7696, 1.7638, 1.7718, 1.8358, 1.6484,\n",
      "        1.7218, 1.7484, 1.7296, 1.8218, 1.8349, 1.7400, 1.8221, 1.7744, 1.7213,\n",
      "        1.6710, 1.7598, 1.8056, 1.7394, 1.7058, 1.7129, 1.8056, 1.8048, 1.7937,\n",
      "        1.6661, 1.7370, 1.8977, 1.9739, 1.8809, 2.0181, 1.8587, 1.8976, 1.9081,\n",
      "        1.8559, 1.7674, 1.8911, 1.8586, 1.8933, 1.9555, 2.0221, 1.9254, 1.9472,\n",
      "        1.9198, 1.8275, 1.8947, 1.8802, 1.9084, 2.0417, 1.8569, 1.9944, 1.8177,\n",
      "        1.9094, 1.9581, 1.8851, 1.9276, 1.9164, 1.8274, 1.9169, 1.7560, 1.7797,\n",
      "        1.8239, 1.6376, 2.0625, 1.7377, 1.6877, 1.8417, 1.8131, 1.7284, 1.8926,\n",
      "        1.6946, 1.7420, 1.7298, 1.7581, 1.8551], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.2649, -0.0890,  0.0675,  ..., -0.0717, -0.1811,  0.1666],\n",
      "        [-0.0697,  0.0303,  0.0303,  ..., -0.1414,  0.0624,  0.0833],\n",
      "        [-0.0070, -0.0207,  0.0639,  ...,  0.0318,  0.0641, -0.0440],\n",
      "        ...,\n",
      "        [-0.0555, -0.1256, -0.0432,  ..., -0.0047, -0.0023,  0.0331],\n",
      "        [ 0.0637, -0.1111,  0.0545,  ...,  0.0418,  0.0602, -0.0185],\n",
      "        [ 0.0140, -0.0386, -0.0045,  ...,  0.1037, -0.0839, -0.0737]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0072,  0.0051, -0.0387,  ...,  0.0063,  0.1297, -0.0130],\n",
      "        [-0.0630, -0.1436, -0.0281,  ...,  0.0100, -0.0442,  0.0720],\n",
      "        [ 0.0377, -0.0843,  0.0020,  ...,  0.0204, -0.1304,  0.0368],\n",
      "        ...,\n",
      "        [-0.0079,  0.0269,  0.0040,  ...,  0.0354,  0.0964, -0.0893],\n",
      "        [ 0.0157, -0.1492, -0.1592,  ..., -0.0159,  0.0884,  0.0334],\n",
      "        [-0.0033, -0.0794,  0.0264,  ...,  0.0229,  0.0525,  0.0873]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.5964, 1.7750, 1.6466, 1.8556, 1.7690, 1.6466, 1.6775, 1.5701, 1.7983,\n",
      "        1.8266, 1.3759, 1.5952, 1.7119, 1.6862, 1.7469, 1.6866, 1.8569, 1.7962,\n",
      "        1.6551, 1.8480, 1.8246, 1.6774, 1.7044, 1.5884, 1.6442, 1.5355, 1.5958,\n",
      "        1.7036, 1.7652, 1.8322, 1.6739, 1.4304, 1.6619, 1.4391, 1.7018, 1.7702,\n",
      "        1.6612, 1.6867, 1.5918, 1.6550, 1.6842, 1.6901, 1.7625, 1.6067, 1.7787,\n",
      "        1.5810, 1.6056, 1.5458, 1.6893, 1.7588, 1.7383, 1.7630, 1.4993, 1.6647,\n",
      "        1.5729, 1.6971, 1.5488, 1.7025, 1.7909, 1.6277, 1.7180, 1.7459, 5.9657,\n",
      "        1.7474, 1.7299, 1.7038, 1.5956, 1.7353, 1.7232, 1.8430, 1.7103, 1.8162,\n",
      "        1.5539, 1.7732, 1.6409, 1.7836, 1.7436, 1.6761, 1.7520, 1.7571, 1.5346,\n",
      "        1.6484, 1.6952, 1.6431, 1.5776, 1.6875, 1.6540, 1.7211, 1.4713, 1.8457,\n",
      "        1.8090, 1.6045, 1.9106, 1.6500, 1.7166, 1.7487, 1.7148, 1.8254, 1.7693,\n",
      "        1.8611, 1.8819, 1.6629, 1.7560, 1.6224, 1.7851, 1.8255, 1.5220, 1.6499,\n",
      "        1.7284, 1.6605, 1.6525, 1.6673, 1.6053, 1.5435, 1.8526, 1.5717, 1.6661,\n",
      "        1.6997, 1.5872, 1.8104, 1.6107, 1.7039, 1.7082, 1.7426, 1.6314, 1.7977,\n",
      "        1.6669, 1.6655, 1.5098, 1.7733, 1.5138, 1.8477, 1.7176, 1.7784, 1.7476,\n",
      "        1.6622, 1.7188, 1.6773, 1.5834, 1.7559, 1.5528, 1.6364, 1.7077, 1.6455,\n",
      "        1.6567, 1.8731, 1.6939, 1.6362, 1.6733, 1.6718, 1.9434, 1.7441, 1.9019,\n",
      "        1.7602, 1.8292, 1.6832, 1.5443, 1.7428, 1.9316, 1.6991, 1.6669, 1.7201,\n",
      "        1.6199, 1.6992, 1.7471, 1.6642, 1.7006, 1.7301, 1.7655, 1.7177, 1.5834,\n",
      "        1.7193, 1.6808, 1.6058, 1.7037, 1.7061, 1.5287, 1.7106, 1.7347, 1.6057,\n",
      "        1.6091, 1.6595, 1.6693, 1.7268, 1.6284, 1.6324, 1.5833, 1.6379, 1.6101,\n",
      "        1.6652, 1.6718, 1.6060, 2.5919, 1.6003, 1.4726, 1.6650, 1.6707, 1.7373,\n",
      "        1.6747, 1.8651, 1.7673, 1.7775, 1.7196, 1.5756, 1.8251, 1.6449, 1.7327,\n",
      "        1.3527, 1.6778, 1.6322, 1.6627, 1.6080, 1.6040, 1.7854, 1.6487, 1.8082,\n",
      "        1.6624, 1.8348, 1.4219, 1.5654, 1.5196, 1.6465, 1.6206, 1.7468, 1.7356,\n",
      "        1.6517, 1.6620, 1.5854, 1.6148, 1.6876, 1.6166, 1.5845, 1.6772, 1.7044,\n",
      "        1.5869, 1.6469, 1.6190, 1.5716, 1.7281, 1.7825, 1.5593, 1.6752, 1.6691,\n",
      "        1.6460, 1.5620, 1.8063, 1.6064, 1.6436, 1.7409, 1.7083, 1.4048, 1.6869,\n",
      "        1.7109, 1.5984, 1.6265, 1.7553, 1.7012, 1.5377, 1.4939, 1.7354, 1.6266,\n",
      "        1.6588, 1.7281, 1.6112, 1.6529, 1.7351, 1.7509, 2.0513, 1.5761, 1.6615,\n",
      "        1.5907, 1.7857, 1.6159, 1.6534, 1.8055, 1.6535, 1.7446, 1.5624, 1.7473,\n",
      "        1.8098, 1.6876, 1.6906, 1.6989, 1.7606, 1.7472, 1.7068, 1.8167, 1.6962,\n",
      "        1.4691, 1.6598, 1.6596, 1.8147, 1.7002, 1.7283, 1.4625, 1.7969, 1.7486,\n",
      "        1.6421, 1.5465, 1.8117, 1.7740, 1.6889, 1.6157, 1.5733, 1.6133, 1.6651,\n",
      "        1.7071, 1.6808, 1.5862, 1.6860, 1.6872, 1.4883, 1.6161, 1.6948, 1.7144,\n",
      "        1.7136, 1.7942, 1.6294, 1.6943, 1.6341], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0517,  0.0622,  0.1105,  ...,  0.0655, -0.0859,  0.0634],\n",
      "        [-0.1135,  0.1362, -0.0370,  ..., -0.1070, -0.1524,  0.0273],\n",
      "        [ 0.0609, -0.0011, -0.0382,  ...,  0.0540, -0.1280, -0.0369],\n",
      "        ...,\n",
      "        [ 0.0299,  0.0075, -0.0670,  ..., -0.0294, -0.0261,  0.0454],\n",
      "        [ 0.1140,  0.0563,  0.0533,  ...,  0.1218,  0.0815, -0.0206],\n",
      "        [ 0.0196, -0.0849, -0.1175,  ..., -0.0299,  0.0895, -0.0432]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0218,  0.0902, -0.0317,  ..., -0.0773,  0.0044, -0.0755],\n",
      "        [-0.0013,  0.0249,  0.0186,  ..., -0.1475, -0.0492,  0.0744],\n",
      "        [-0.0335,  0.0805, -0.0374,  ...,  0.0396,  0.0109, -0.0805],\n",
      "        ...,\n",
      "        [-0.0392, -0.0383,  0.0070,  ..., -0.0899,  0.0326,  0.0249],\n",
      "        [-0.0166, -0.0003,  0.1070,  ...,  0.0023, -0.0075, -0.0619],\n",
      "        [-0.0208, -0.0453, -0.0823,  ..., -0.1111,  0.1887,  0.0999]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.6337, 1.7600, 1.7629, 1.8963, 1.9098, 1.6751, 2.1733, 2.6076, 1.5449,\n",
      "        1.6958, 1.9653, 2.0703, 1.9918, 1.6585, 2.4516, 2.1484, 1.5309, 1.3538,\n",
      "        1.6992, 1.6709, 2.5066, 3.3846, 3.5920, 4.1216, 1.0307, 1.4845, 1.6685,\n",
      "        1.9913, 2.2828, 3.4988, 3.6315, 3.8500, 1.3643, 1.3228, 0.9992, 1.4191,\n",
      "        1.9344, 1.9588, 1.6406, 1.8346, 1.1080, 0.7372, 1.3827, 1.2633, 1.2873,\n",
      "        1.4144, 1.6797, 1.8822, 1.5576, 1.6729, 2.0475, 1.9566, 2.2402, 3.5156,\n",
      "        3.3188, 3.5732, 1.3067, 1.7685, 1.9725, 1.8790, 2.2742, 3.4703, 3.2260,\n",
      "        3.3209, 1.6189, 2.2401, 1.6556, 1.3670, 1.8052, 1.7063, 1.9953, 2.0377,\n",
      "        1.9500, 1.9010, 1.7561, 1.7332, 1.4086, 1.8552, 2.0625, 2.1352, 1.3984,\n",
      "        1.1396, 1.3981, 1.6107, 2.2101, 2.0423, 1.9024, 2.0720, 1.6050, 1.4876,\n",
      "        1.2246, 1.2394, 1.3747, 1.6754, 1.9947, 1.9357, 1.7249, 1.7370, 1.7914,\n",
      "        1.7440, 1.6500, 1.6080, 2.3578, 2.2440, 2.0404, 2.0412, 2.0147, 1.5548,\n",
      "        2.0393, 1.4602, 2.4878, 2.2049, 1.6869, 1.5122, 1.5080, 1.0083, 1.4087,\n",
      "        2.0937, 1.8564, 1.8915, 1.6557, 1.2708, 0.9796, 1.7265, 1.4343, 1.5847,\n",
      "        1.8911, 1.8790, 1.4347, 1.5057, 1.3525, 1.3807, 1.2588, 1.8227, 1.8048,\n",
      "        1.3318, 1.7523, 1.6641, 1.6454, 1.2975, 1.6354, 1.3294, 2.0041, 1.8899,\n",
      "        1.0600, 1.1079, 1.0697, 1.6158, 1.4061, 1.5240, 1.7901, 2.3184, 1.5918,\n",
      "        1.1358, 1.5190, 1.4117, 2.0765, 1.9414, 1.8248, 1.9604, 1.8536, 1.3922,\n",
      "        1.0155, 1.2763, 1.8027, 1.5505, 1.7617, 1.9545, 1.2159, 1.3947, 1.4961,\n",
      "        1.5048, 1.4424, 2.0258, 1.9112, 1.9610, 2.1253, 2.3290, 2.2206, 1.9620,\n",
      "        1.9712, 1.7326, 2.3938, 2.3578, 1.7861, 1.8462, 1.9090, 1.8549, 1.6395,\n",
      "        2.6132, 2.3810, 2.3269, 1.8619, 1.2375, 1.5966, 1.2859, 1.9249, 2.0485,\n",
      "        1.7793, 1.9945, 0.9056, 1.3242, 1.0726, 1.7650, 1.4618, 1.4797, 1.7451,\n",
      "        2.0096, 2.0175, 1.7332, 1.5400, 1.6013, 1.7008, 1.8693, 1.9584, 2.2831,\n",
      "        1.7529, 2.0045, 2.0085, 1.8197, 1.6432, 2.1935, 1.5392, 2.2392, 1.0377,\n",
      "        1.2115, 1.6351, 1.9476, 1.8446, 1.4346, 1.8432, 1.8279, 1.6863, 1.0471,\n",
      "        1.1405, 1.2439, 1.3783, 2.0083, 1.7855, 2.1968, 1.9842, 1.7413, 1.8590,\n",
      "        1.8665, 1.8045, 2.3448, 2.9954, 3.0288, 2.0846, 2.0311, 2.0887, 1.9372,\n",
      "        2.4821, 2.3633, 3.0243, 2.9675, 1.5588, 1.3154, 1.1663, 1.0990, 2.1888,\n",
      "        1.5730, 1.9676, 1.8778, 1.7880, 1.0257, 1.5217, 2.1233, 1.5294, 2.0089,\n",
      "        1.8003, 1.9225, 1.6347, 1.3483, 1.5454, 1.2217, 1.2806, 1.9207, 1.8523,\n",
      "        1.9190, 1.5480, 1.5318, 1.3424, 1.6983, 1.8449, 1.4883, 1.7564, 1.8995,\n",
      "        1.6189, 1.2694, 1.2178, 1.7529, 1.9763, 2.2447, 2.0339, 2.0146, 1.9455,\n",
      "        1.3152, 1.3260, 1.2605, 1.6739, 1.4883, 1.9536, 1.9307, 2.1591, 1.5101,\n",
      "        1.5884, 1.9242, 2.2654, 2.2242, 1.8935, 2.2635, 1.0623, 1.4864, 1.4776,\n",
      "        1.6304, 1.5762, 1.8336, 1.9985, 2.2520], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0448,  0.0019,  0.0529,  ..., -0.0895,  0.0385,  0.0121],\n",
      "        [ 0.1131, -0.0248, -0.0683,  ...,  0.0796,  0.1477, -0.0684],\n",
      "        [ 0.0132, -0.0822,  0.1646,  ..., -0.0319,  0.0211,  0.0143],\n",
      "        ...,\n",
      "        [ 0.0876,  0.0296, -0.0593,  ..., -0.0558,  0.0331,  0.0839],\n",
      "        [ 0.1090,  0.0368,  0.0678,  ..., -0.0397, -0.0424, -0.0278],\n",
      "        [-0.0228, -0.0715,  0.1695,  ..., -0.1912, -0.0186,  0.0759]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0146,  0.0669,  0.0628,  ...,  0.0699,  0.0609, -0.0460],\n",
      "        [-0.0259,  0.0440,  0.0093,  ..., -0.0763,  0.0814, -0.0719],\n",
      "        [ 0.0260,  0.0115,  0.0263,  ..., -0.0786, -0.0040, -0.0887],\n",
      "        ...,\n",
      "        [ 0.0031,  0.0294, -0.1311,  ...,  0.1427, -0.0770, -0.1167],\n",
      "        [ 0.0920,  0.1251,  0.0147,  ...,  0.0269, -0.0227, -0.1059],\n",
      "        [-0.2414, -0.0328, -0.0356,  ...,  0.0523,  0.0088, -0.0184]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.5527, 1.6836, 1.9057, 2.0700, 1.8872, 1.7274, 2.2482, 2.1215, 1.6831,\n",
      "        1.6709, 1.8061, 1.7953, 2.0014, 2.2334, 2.0840, 2.0100, 1.4847, 1.3700,\n",
      "        1.6244, 1.8292, 2.2017, 2.5366, 2.6763, 2.5882, 0.8686, 1.4008, 1.6317,\n",
      "        1.7958, 1.9868, 2.4663, 2.6693, 2.2738, 1.5796, 1.3381, 1.4306, 1.2349,\n",
      "        1.0504, 1.2101, 1.5936, 1.6932, 1.5338, 1.5388, 1.6129, 1.4604, 1.9296,\n",
      "        1.8910, 1.6164, 1.5873, 1.4555, 1.6198, 1.9103, 2.0822, 2.0350, 2.5993,\n",
      "        2.7536, 2.8014, 1.4121, 1.8084, 2.0293, 1.7922, 2.1079, 2.4175, 2.7038,\n",
      "        2.6871, 1.7676, 1.9740, 1.7641, 1.5678, 1.5016, 2.0393, 1.9101, 1.9341,\n",
      "        1.7609, 1.9777, 1.8762, 2.0067, 1.7348, 1.4864, 1.9480, 1.9633, 1.8620,\n",
      "        1.4972, 1.6819, 1.2940, 1.1250, 1.2779, 1.8183, 1.7357, 1.6343, 1.7543,\n",
      "        1.5193, 1.7725, 2.2930, 1.9039, 1.8066, 1.8178, 1.7647, 1.5868, 1.7468,\n",
      "        1.8785, 2.1126, 2.0701, 2.1605, 2.0891, 1.8717, 2.0131, 2.0067, 1.8450,\n",
      "        1.5731, 1.6982, 2.0890, 2.0686, 1.6830, 1.7705, 1.5698, 2.0105, 2.1196,\n",
      "        1.2902, 1.7660, 1.8228, 1.7187, 1.5760, 1.8091, 1.3199, 1.2591, 2.0989,\n",
      "        1.7923, 1.7583, 1.5653, 1.6532, 1.7333, 1.7522, 1.7245, 1.3768, 1.7222,\n",
      "        1.7858, 1.5699, 1.7473, 1.6486, 1.5443, 1.5633, 1.8391, 1.9007, 1.8051,\n",
      "        1.4656, 1.4234, 1.7060, 1.4061, 2.1120, 1.8643, 1.7289, 1.5457, 1.7553,\n",
      "        1.6197, 1.4914, 1.3728, 1.1500, 1.2303, 1.7243, 1.7132, 1.6175, 1.4258,\n",
      "        1.5289, 1.5401, 1.2048, 2.1726, 1.7051, 1.6495, 2.0435, 1.8500, 1.5929,\n",
      "        1.4828, 1.8095, 1.2305, 1.7223, 2.0698, 1.9618, 2.0583, 2.0588, 1.9041,\n",
      "        1.4948, 2.3093, 2.2549, 2.2688, 1.9004, 1.9927, 1.9676, 1.9637, 2.2409,\n",
      "        1.3802, 2.1645, 2.1262, 1.7710, 1.5278, 1.4636, 1.9029, 1.2497, 1.1409,\n",
      "        1.6232, 1.7810, 1.6648, 1.7164, 1.7614, 1.2102, 1.7844, 2.0994, 1.6078,\n",
      "        1.7682, 1.6891, 1.8583, 1.7302, 1.6331, 1.4540, 1.9424, 1.9737, 2.0331,\n",
      "        1.9947, 1.8180, 1.7956, 1.5962, 1.7092, 1.4826, 1.6836, 1.9929, 1.2224,\n",
      "        1.5670, 1.6229, 1.1581, 1.1049, 2.0730, 1.7103, 1.7277, 2.1365, 1.4394,\n",
      "        1.4532, 1.8331, 1.9981, 1.2285, 1.7264, 1.8582, 2.1642, 1.8281, 2.0565,\n",
      "        1.7690, 1.9134, 1.7711, 2.2791, 2.4941, 1.9043, 1.9575, 1.8916, 2.0040,\n",
      "        1.7748, 1.7361, 2.4646, 2.2605, 1.7605, 1.6842, 1.8603, 1.9928, 1.1350,\n",
      "        1.9951, 1.7126, 1.8104, 1.8551, 1.6262, 1.5254, 1.0824, 1.9858, 1.1804,\n",
      "        1.7378, 1.8603, 1.7102, 1.5969, 1.8054, 1.6991, 2.2189, 1.1911, 1.7671,\n",
      "        1.8433, 1.7125, 1.8152, 1.5446, 1.2737, 1.1487, 2.0897, 1.7754, 1.8121,\n",
      "        1.9613, 1.6838, 1.6091, 1.3819, 1.2131, 1.1501, 1.9194, 1.8058, 1.7156,\n",
      "        1.6409, 1.7919, 1.5859, 1.6545, 2.1121, 1.7657, 1.8743, 1.8266, 1.6771,\n",
      "        1.6245, 1.4244, 1.2024, 1.2840, 1.8309, 1.8192, 1.5122, 1.5164, 1.6434,\n",
      "        1.6908, 2.1068, 2.1800, 1.8020, 1.7915], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.1718, -0.0859,  0.0626,  ..., -0.0738, -0.0044, -0.1601],\n",
      "        [ 0.0806, -0.1421, -0.0522,  ..., -0.0081, -0.0670, -0.0989],\n",
      "        [ 0.0815,  0.2038,  0.0710,  ..., -0.1239,  0.1307, -0.0535],\n",
      "        ...,\n",
      "        [-0.0707, -0.0609,  0.0387,  ..., -0.0982, -0.1527,  0.0611],\n",
      "        [ 0.0489,  0.1235, -0.0628,  ..., -0.0616,  0.0903, -0.0107],\n",
      "        [-0.0358,  0.1172,  0.0172,  ..., -0.0789,  0.0425,  0.0145]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0233, -0.0073,  0.0339,  ..., -0.0433,  0.0200, -0.0178],\n",
      "        [-0.0347,  0.0100, -0.0310,  ..., -0.0817,  0.0731, -0.1374],\n",
      "        [-0.0671,  0.0209, -0.0325,  ..., -0.0387,  0.1345,  0.0349],\n",
      "        ...,\n",
      "        [ 0.1500,  0.0596,  0.0145,  ..., -0.0394, -0.0082, -0.0411],\n",
      "        [ 0.0905, -0.0880,  0.0774,  ...,  0.0236,  0.0559,  0.0292],\n",
      "        [-0.1805,  0.1048,  0.0474,  ..., -0.0185, -0.0503,  0.0819]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.0482, 2.2054, 2.1130, 2.1690, 2.1033, 2.1033, 2.1896, 2.2411, 2.2094,\n",
      "        2.2062, 2.1566, 2.0390, 2.2411, 2.3306, 2.1092, 2.0593, 1.9134, 2.0421,\n",
      "        1.9810, 1.9861, 1.9764, 2.0428, 1.9879, 2.0051, 1.9920, 1.9401, 1.9586,\n",
      "        2.0162, 1.9205, 1.9393, 2.0056, 1.9118, 2.2752, 2.0620, 2.2300, 2.0310,\n",
      "        2.2491, 2.1708, 2.2792, 2.1153, 2.1641, 2.1633, 2.1612, 2.3271, 2.1935,\n",
      "        2.2529, 2.1785, 2.1978, 2.1322, 2.0636, 2.0524, 2.0556, 2.1083, 2.0846,\n",
      "        2.0446, 2.1149, 2.0666, 2.0849, 2.0137, 2.1390, 2.1111, 2.1890, 2.1029,\n",
      "        2.0855, 2.0210, 2.1745, 2.0110, 2.1265, 2.2169, 2.1051, 2.0760, 2.1005,\n",
      "        2.0326, 2.1207, 2.1206, 2.0985, 2.1123, 2.0757, 2.0717, 2.0485, 2.1681,\n",
      "        2.1364, 2.1675, 2.1683, 2.0006, 2.1257, 2.0500, 1.9842, 2.2526, 2.0743,\n",
      "        2.0438, 1.9284, 2.1485, 1.9768, 2.2405, 2.0458, 2.2747, 2.0617, 1.9371,\n",
      "        2.0708, 2.1509, 2.0495, 2.0280, 2.0986, 2.1464, 2.1382, 2.0966, 2.1338,\n",
      "        2.1161, 2.0509, 2.1437, 2.0878, 2.1791, 2.1567, 2.1656, 2.1607, 2.1816,\n",
      "        2.1440, 2.0449, 2.1404, 2.2582, 2.2058, 2.1648, 2.1728, 2.0921, 2.1581,\n",
      "        2.0928, 2.1445, 2.2917, 2.3270, 2.3724, 2.2153, 2.4160, 2.3167, 2.2819,\n",
      "        2.3519, 2.2251, 2.3708, 2.1061, 2.4067, 2.2176, 2.3679, 2.2010, 2.4127,\n",
      "        1.9146, 2.0938, 2.1696, 2.2662, 1.9167, 2.0657, 2.1099, 2.0635, 1.9511,\n",
      "        2.1411, 2.0539, 2.0871, 2.1541, 2.1513, 1.9938, 2.1734, 1.9720, 2.0564,\n",
      "        2.1374, 2.1709, 2.0768, 2.0241, 2.0354, 2.0808, 2.0879, 2.0835, 2.0600,\n",
      "        2.0384, 2.1927, 2.0334, 2.0393, 2.1262, 2.0929, 2.1409, 2.1634, 2.1845,\n",
      "        2.2695, 2.1263, 2.1816, 2.2454, 2.1058, 2.3415, 2.1144, 2.1534, 2.3145,\n",
      "        2.1172, 2.0954, 2.1621, 1.9731, 1.9773, 2.0125, 2.0745, 2.0563, 2.1125,\n",
      "        2.0364, 1.9882, 2.0921, 1.9555, 2.1685, 2.0167, 2.0927, 2.0390, 2.0627,\n",
      "        1.9650, 2.1938, 2.0023, 2.1007, 2.1025, 2.1417, 2.1735, 2.0449, 2.1532,\n",
      "        2.1784, 2.3249, 2.2909, 2.2143, 2.1204, 2.1510, 2.2076, 2.2003, 1.9442,\n",
      "        1.9668, 2.0574, 1.9579, 1.9148, 2.0424, 2.0019, 1.8696, 2.0179, 1.9948,\n",
      "        2.0719, 1.9325, 1.9205, 2.0282, 2.0161, 1.9891, 2.2015, 2.1748, 2.1023,\n",
      "        2.3245, 2.1440, 2.3064, 2.0869, 1.7306, 2.1436, 2.3464, 2.0865, 2.1621,\n",
      "        2.1862, 2.1456, 2.2508, 2.0938, 2.1479, 2.1038, 2.1359, 2.0511, 2.0187,\n",
      "        2.1498, 2.0779, 2.0487, 2.0279, 2.1443, 1.9305, 2.0750, 2.1543, 2.0195,\n",
      "        2.0664, 2.0958, 1.9939, 2.0505, 2.1063, 2.0129, 2.0719, 2.1502, 2.2006,\n",
      "        2.1159, 2.1666, 2.0913, 2.0745, 2.0868, 2.0683, 1.9278, 1.9997, 2.1085,\n",
      "        2.0804, 2.0655, 2.1112, 2.1723, 2.0991, 2.1548, 2.1594, 2.0304, 2.0639,\n",
      "        2.0324, 2.1243, 2.0022, 2.0328, 2.1483, 1.9289, 2.0476, 1.9602, 1.7725,\n",
      "        1.8443, 1.7937, 1.7828, 1.9617, 1.7623, 1.8179, 1.7928, 1.8182, 1.8827,\n",
      "        1.8228, 1.8497, 1.8697, 1.7796, 1.8182], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0033,  0.2054,  0.0521,  ..., -0.0430, -0.0789, -0.0032],\n",
      "        [-0.0494,  0.0213,  0.0519,  ..., -0.0713, -0.0614, -0.0077],\n",
      "        [ 0.0576,  0.0239, -0.0174,  ..., -0.0183,  0.0689, -0.0190],\n",
      "        ...,\n",
      "        [-0.0003,  0.0031, -0.1341,  ..., -0.1379, -0.0011,  0.0655],\n",
      "        [ 0.0082,  0.0113, -0.0407,  ...,  0.0450,  0.0100,  0.0192],\n",
      "        [ 0.0840,  0.0197,  0.0107,  ..., -0.0154,  0.0061,  0.0379]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0058, -0.0679,  0.0396,  ..., -0.0469,  0.0028, -0.0857],\n",
      "        [ 0.0336,  0.1391, -0.1784,  ...,  0.0617,  0.1397, -0.0124],\n",
      "        [ 0.0215,  0.1162, -0.0030,  ..., -0.0232,  0.0618, -0.0355],\n",
      "        ...,\n",
      "        [-0.0154, -0.0004,  0.0499,  ..., -0.0895,  0.0407,  0.1043],\n",
      "        [-0.0195, -0.0594, -0.0106,  ...,  0.1714,  0.0258, -0.1175],\n",
      "        [-0.0417,  0.0827, -0.1193,  ..., -0.0509, -0.1264, -0.0498]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.0040, 1.9559, 1.9719, 2.1341, 2.1760, 1.9445, 2.0996, 1.9815, 2.0508,\n",
      "        1.9584, 1.8002, 1.9554, 1.9745, 1.8003, 2.1552, 2.0786, 1.9150, 1.9798,\n",
      "        1.8310, 1.8955, 2.1815, 1.9694, 1.9566, 2.0163, 1.8500, 1.9120, 1.8482,\n",
      "        2.0386, 2.1839, 2.1060, 1.9633, 1.6475, 2.1423, 1.8811, 1.9456, 1.8412,\n",
      "        1.8994, 1.8917, 1.8796, 2.1834, 1.9456, 2.0173, 2.0474, 2.0117, 2.0106,\n",
      "        2.0075, 2.9191, 1.9125, 1.9543, 1.9893, 2.0150, 1.9304, 1.9379, 1.8482,\n",
      "        1.9900, 1.9346, 2.1602, 2.1297, 2.3495, 2.0662, 1.8931, 1.9441, 4.8674,\n",
      "        1.8816, 2.0862, 2.1221, 2.1362, 2.0283, 1.8881, 2.0475, 1.9672, 1.9226,\n",
      "        1.8503, 2.2211, 1.9469, 2.1239, 1.9504, 1.9870, 1.9125, 1.7402, 1.8235,\n",
      "        1.9516, 1.8790, 2.1946, 1.8776, 1.8752, 1.7352, 2.0060, 1.7244, 2.0466,\n",
      "        2.1788, 1.9950, 2.2474, 1.9243, 1.9560, 2.0585, 1.9626, 2.1690, 2.1201,\n",
      "        2.1559, 2.0604, 2.1946, 1.9485, 2.0176, 1.9604, 2.0090, 2.0718, 2.0529,\n",
      "        1.8487, 1.7985, 2.2043, 2.1143, 2.0630, 1.9901, 2.1701, 1.9707, 2.0658,\n",
      "        1.9131, 1.9332, 2.2194, 2.0729, 1.9535, 2.1774, 2.0564, 1.8340, 2.1281,\n",
      "        1.9420, 1.9687, 1.9398, 1.9034, 1.8167, 2.1144, 2.1498, 1.9992, 2.1961,\n",
      "        1.9690, 1.8544, 2.0149, 1.8851, 1.9738, 1.9919, 1.8912, 2.3173, 2.1065,\n",
      "        1.9866, 1.9809, 1.9712, 2.1023, 1.9959, 1.8835, 2.1337, 2.0374, 1.9877,\n",
      "        1.9378, 2.1066, 1.9457, 1.8648, 1.9330, 2.1748, 1.9181, 1.9934, 2.0136,\n",
      "        2.1109, 1.9425, 2.0450, 2.1345, 2.1150, 2.0668, 2.0774, 1.9717, 1.9636,\n",
      "        1.7347, 2.0716, 2.0190, 1.8438, 1.8391, 1.8263, 2.0583, 2.3159, 1.9855,\n",
      "        1.8125, 1.9918, 2.0440, 2.0084, 2.0004, 1.9904, 1.9613, 2.0108, 1.9930,\n",
      "        1.9525, 2.0195, 1.8131, 2.7143, 2.0945, 1.9469, 2.0026, 1.9517, 1.8786,\n",
      "        2.0359, 1.9882, 1.9751, 2.0598, 1.9443, 2.1008, 1.9952, 2.0906, 2.0281,\n",
      "        1.7348, 1.9644, 1.9214, 1.8692, 2.1121, 2.0779, 2.1423, 2.2565, 2.0303,\n",
      "        1.8940, 2.0688, 1.6688, 1.8827, 1.8633, 2.0113, 1.9551, 2.1850, 2.1410,\n",
      "        2.0469, 2.0941, 2.0045, 1.9320, 2.0846, 1.8719, 1.7587, 2.0429, 1.8645,\n",
      "        2.0160, 2.0406, 1.8866, 1.8828, 2.0873, 2.0077, 1.8897, 1.8515, 2.0193,\n",
      "        2.0525, 2.0216, 1.8177, 2.0155, 2.0024, 1.9471, 1.9866, 1.9463, 1.9293,\n",
      "        2.0859, 1.7917, 1.9248, 1.9185, 1.9749, 1.7910, 1.7858, 2.0815, 2.1918,\n",
      "        2.1333, 2.0849, 1.9805, 1.9842, 2.0301, 2.0212, 2.5474, 2.0420, 1.9107,\n",
      "        1.9044, 2.1810, 1.8701, 2.0022, 1.9817, 1.9358, 2.1984, 1.8543, 1.9793,\n",
      "        2.0779, 1.9338, 1.9107, 2.0659, 2.0024, 2.0126, 2.0324, 2.0306, 1.8533,\n",
      "        1.9070, 1.8813, 1.8921, 2.1766, 1.9688, 2.1409, 1.7569, 2.0445, 2.1129,\n",
      "        1.9598, 1.8595, 1.9245, 2.0670, 2.0063, 2.1707, 1.9824, 2.1156, 1.9823,\n",
      "        2.0775, 1.7144, 1.8626, 2.1228, 2.0479, 1.8072, 1.7946, 2.1915, 2.1204,\n",
      "        1.9599, 2.0327, 1.9645, 2.1784, 1.9447], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 6.9096e-03,  9.0093e-03,  4.5108e-02,  ...,  3.1682e-02,\n",
      "          4.4049e-02,  1.1927e-03],\n",
      "        [ 3.6081e-02,  5.6901e-02,  4.8275e-02,  ..., -1.3207e-01,\n",
      "          1.5424e-01, -5.4517e-02],\n",
      "        [ 1.0428e-01, -3.7398e-02,  1.1350e-01,  ..., -1.7103e-02,\n",
      "         -6.5586e-02,  2.9993e-02],\n",
      "        ...,\n",
      "        [-3.9185e-02,  9.8685e-03,  1.2522e-01,  ..., -7.1830e-02,\n",
      "          1.2110e-04, -2.1157e-02],\n",
      "        [-4.6519e-02, -8.7204e-02,  7.1968e-02,  ...,  3.2845e-03,\n",
      "         -1.2215e-01,  7.9669e-02],\n",
      "        [ 1.8704e-01,  5.7959e-02, -6.8179e-02,  ...,  5.0360e-02,\n",
      "         -1.4938e-02,  5.5053e-02]], device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0467,  0.0254,  0.0032,  ...,  0.0329, -0.1292, -0.1989],\n",
      "        [-0.0130,  0.0239,  0.0279,  ..., -0.0043, -0.0127,  0.0392],\n",
      "        [-0.0999,  0.0127, -0.0081,  ..., -0.0279,  0.0270, -0.0032],\n",
      "        ...,\n",
      "        [-0.0325, -0.0455, -0.0247,  ..., -0.0798, -0.2549, -0.1018],\n",
      "        [-0.0014,  0.0118,  0.0049,  ...,  0.0606,  0.1390, -0.0190],\n",
      "        [-0.0111,  0.0297, -0.0052,  ..., -0.0325,  0.0013, -0.0695]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.5381, 1.3241, 1.3606, 1.3117, 1.5673, 1.7181, 1.6286, 1.6203, 1.5947,\n",
      "        1.4249, 1.1294, 1.3563, 1.4150, 1.3214, 1.6308, 1.5715, 1.5084, 1.3545,\n",
      "        1.0088, 1.3435, 1.6906, 1.4723, 1.8878, 1.9731, 1.0818, 1.0348, 1.3316,\n",
      "        1.1658, 1.1225, 1.9560, 2.0871, 1.8467, 1.0503, 1.1944, 1.1954, 1.6989,\n",
      "        2.1670, 1.5665, 1.8076, 2.5810, 2.0163, 1.5129, 1.6467, 1.4793, 1.4955,\n",
      "        2.1777, 1.9528, 1.9467, 1.6761, 1.7553, 1.7100, 1.5580, 1.4536, 1.5034,\n",
      "        1.8631, 1.9654, 1.6999, 1.7867, 1.5859, 1.7625, 1.7016, 1.5586, 1.8923,\n",
      "        1.9431, 1.1625, 1.6653, 1.4213, 1.9189, 2.0206, 2.0994, 2.3131, 2.0310,\n",
      "        1.2126, 1.2500, 1.5079, 2.0828, 2.0491, 1.6412, 2.2873, 2.1174, 1.7807,\n",
      "        1.3064, 1.6816, 1.0816, 1.8376, 1.7215, 1.3270, 1.4578, 1.6459, 1.7349,\n",
      "        1.3885, 1.8085, 1.1415, 1.1752, 1.3183, 1.3184, 1.1481, 0.9861, 1.0752,\n",
      "        1.1474, 1.4940, 1.4680, 1.4413, 1.5983, 1.1378, 0.9281, 1.2734, 1.3845,\n",
      "        1.2613, 1.5516, 1.4273, 1.4636, 1.5497, 1.6500, 1.9046, 1.7097, 1.8287,\n",
      "        1.6713, 2.2520, 2.3423, 1.5076, 1.6906, 1.7861, 1.8009, 1.5837, 1.9797,\n",
      "        2.2663, 2.3626, 2.0189, 1.7018, 1.6993, 1.5177, 1.6955, 2.2770, 1.9597,\n",
      "        1.9259, 1.6370, 1.7430, 1.6526, 1.3704, 1.7034, 1.3912, 1.8728, 1.9157,\n",
      "        1.9089, 1.2782, 1.6891, 1.2273, 1.2931, 1.4038, 1.6021, 2.1480, 1.2879,\n",
      "        1.2745, 1.1777, 1.8134, 1.9397, 2.0201, 1.7271, 1.7985, 1.4744, 1.4170,\n",
      "        1.6002, 1.7605, 1.5343, 2.2958, 1.8005, 1.8268, 1.6362, 1.4320, 1.6918,\n",
      "        1.7405, 1.8883, 1.4965, 1.6768, 1.9683, 0.7655, 1.3468, 1.5213, 1.4466,\n",
      "        2.0985, 2.0150, 2.4076, 2.0058, 1.3755, 1.1222, 1.2161, 1.6605, 1.9287,\n",
      "        2.0339, 2.3360, 1.9510, 1.6895, 1.2065, 1.6309, 1.8707, 1.8506, 1.8763,\n",
      "        1.6442, 1.9454, 1.2582, 1.4368, 1.0756, 1.2356, 1.2497, 1.3988, 1.6670,\n",
      "        1.6304, 1.0059, 1.3316, 1.2845, 1.2817, 1.2350, 1.2023, 1.7394, 1.7847,\n",
      "        1.2201, 1.1940, 1.2930, 1.3842, 1.1869, 1.7422, 1.9143, 1.8138, 1.4444,\n",
      "        1.3211, 1.7328, 1.7142, 1.2688, 1.3226, 1.6830, 1.6361, 1.6761, 1.4869,\n",
      "        1.0866, 1.1001, 1.6807, 1.9232, 1.5747, 1.8789, 0.7083, 1.5451, 1.4541,\n",
      "        1.6945, 2.1873, 1.7500, 2.1163, 2.2851, 1.3602, 1.1812, 1.4561, 1.5435,\n",
      "        2.2452, 1.7591, 2.2909, 2.3954, 1.9507, 1.9902, 1.9600, 1.6377, 1.3966,\n",
      "        2.1447, 1.8187, 2.0263, 1.8509, 1.8431, 1.8020, 1.3191, 1.7604, 1.5306,\n",
      "        1.9956, 1.9023, 1.8232, 1.9189, 1.9032, 1.8481, 1.4953, 2.1604, 2.1009,\n",
      "        2.0643, 1.7075, 1.7333, 1.9907, 1.9706, 1.7323, 1.5755, 2.2104, 2.0081,\n",
      "        1.2970, 1.3091, 1.3988, 1.2809, 1.3336, 1.1335, 2.0048, 2.0325, 1.2376,\n",
      "        1.2608, 1.2689, 1.3746, 1.2606, 1.5977, 1.5886, 1.3858, 0.7806, 0.7912,\n",
      "        0.8362, 1.1981, 1.9292, 2.0808, 2.0394, 1.9664, 1.7727, 1.4186, 1.5101,\n",
      "        1.5409, 1.7831, 2.0739, 1.9813, 1.6855], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.0232,  0.0128,  0.0074,  ...,  0.0509,  0.0133, -0.0386],\n",
      "        [-0.0883,  0.0237, -0.0643,  ...,  0.0992, -0.0433,  0.0486],\n",
      "        [-0.0292, -0.0248, -0.0834,  ..., -0.0787,  0.1826, -0.1204],\n",
      "        ...,\n",
      "        [ 0.0445, -0.0735,  0.0530,  ...,  0.0662, -0.0577, -0.0873],\n",
      "        [ 0.0491, -0.0735, -0.0313,  ...,  0.0187,  0.0054, -0.0281],\n",
      "        [-0.0102, -0.0021,  0.0063,  ..., -0.0803,  0.0854, -0.0696]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-0.0805,  0.0673,  0.0426,  ..., -0.0207,  0.0048,  0.0317],\n",
      "        [-0.0556,  0.0072,  0.0265,  ...,  0.0056,  0.0443, -0.0146],\n",
      "        [-0.0879, -0.0427, -0.0578,  ...,  0.1052,  0.0684, -0.0826],\n",
      "        ...,\n",
      "        [-0.0928, -0.0110,  0.0696,  ..., -0.1158,  0.0105, -0.0457],\n",
      "        [ 0.0775, -0.0020,  0.0036,  ...,  0.0620,  0.0231, -0.0015],\n",
      "        [-0.0437, -0.0561,  0.0451,  ...,  0.0499,  0.0158,  0.0429]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([1.4120, 1.2885, 1.4003, 1.4359, 1.2012, 1.1708, 1.5482, 1.5687, 1.4945,\n",
      "        1.4145, 1.3142, 1.3258, 1.4146, 1.8676, 1.5420, 1.5307, 1.1158, 1.5155,\n",
      "        1.0557, 1.2026, 1.3733, 1.3775, 1.5734, 1.4394, 1.2882, 1.0482, 1.5250,\n",
      "        1.2323, 1.2005, 1.3294, 1.7931, 1.6491, 1.5698, 1.5052, 1.5440, 1.3188,\n",
      "        1.2876, 2.0958, 1.7369, 1.8793, 1.4784, 1.4597, 1.4362, 1.7059, 2.0755,\n",
      "        1.3089, 1.8648, 1.8775, 1.6831, 1.7312, 1.7440, 1.5016, 1.5092, 1.4459,\n",
      "        1.7224, 1.8467, 1.6303, 1.8358, 1.7772, 1.7181, 1.4353, 2.0451, 1.7525,\n",
      "        1.9263, 1.1927, 1.5767, 1.5799, 1.7375, 1.5925, 1.3498, 1.8650, 1.7685,\n",
      "        1.2294, 1.2721, 1.6534, 1.7710, 1.4073, 1.6553, 1.6974, 1.8732, 1.8131,\n",
      "        1.4575, 1.6273, 1.5673, 0.9564, 1.0141, 1.3880, 1.4738, 1.5901, 1.6948,\n",
      "        1.4297, 1.2684, 1.5858, 1.3848, 1.3454, 1.3128, 1.0242, 1.2420, 1.2504,\n",
      "        1.3265, 1.0533, 1.0974, 1.3863, 1.4605, 1.2098, 1.1468, 1.3285, 1.0935,\n",
      "        1.1326, 1.2770, 1.3693, 1.4441, 1.4769, 1.5894, 1.9289, 1.7987, 1.7325,\n",
      "        1.7949, 1.9599, 1.8069, 1.5693, 1.7317, 1.7642, 1.8238, 1.7539, 1.6203,\n",
      "        1.9929, 1.8863, 1.9249, 1.7468, 1.8039, 1.6011, 1.2425, 1.1449, 1.8424,\n",
      "        1.8389, 1.7078, 1.8506, 1.7749, 1.5125, 1.4091, 2.3307, 1.7605, 1.7613,\n",
      "        1.8183, 1.4697, 1.3213, 1.8501, 1.8321, 1.9360, 1.5376, 1.7318, 1.4935,\n",
      "        1.4940, 1.5666, 1.1238, 1.0510, 1.2036, 1.6913, 1.6986, 1.5926, 1.5324,\n",
      "        1.4486, 1.3799, 1.6664, 1.1669, 1.6408, 1.7325, 1.5452, 1.6070, 1.4553,\n",
      "        1.4355, 1.1947, 1.9876, 1.6986, 1.7164, 1.5539, 1.0645, 1.3167, 1.4890,\n",
      "        1.6279, 1.8737, 1.9548, 1.8404, 0.8375, 1.5047, 1.3291, 1.3541, 1.7082,\n",
      "        1.8385, 1.9628, 1.8327, 1.7749, 1.4125, 1.3953, 1.1282, 1.0333, 1.1730,\n",
      "        1.6001, 1.7618, 1.3852, 1.5273, 1.5377, 1.5882, 1.9737, 1.8627, 1.6450,\n",
      "        1.5467, 0.8895, 0.9862, 1.0044, 1.3033, 1.1541, 1.2802, 1.4819, 1.4580,\n",
      "        1.0285, 1.1637, 1.3768, 1.2034, 1.1819, 1.2230, 1.7715, 1.3497, 1.5540,\n",
      "        1.4253, 1.6287, 1.0641, 1.8440, 1.9160, 1.6406, 1.5991, 1.7898, 1.5941,\n",
      "        1.3458, 1.7170, 1.0052, 1.0840, 1.5320, 1.6331, 1.5172, 1.5524, 1.5585,\n",
      "        1.6600, 1.6216, 1.7651, 1.9542, 1.9153, 1.1167, 0.9682, 1.1384, 1.2487,\n",
      "        1.8588, 1.7127, 2.0839, 1.9503, 1.9314, 1.9193, 1.9716, 1.7119, 1.9399,\n",
      "        1.2639, 1.6187, 1.7545, 1.7953, 1.8933, 1.8136, 1.6014, 1.4758, 2.1887,\n",
      "        1.8906, 1.7025, 1.7860, 1.9090, 1.9333, 1.9106, 1.6137, 1.4672, 1.9806,\n",
      "        1.8559, 1.6878, 1.6937, 1.8634, 1.8420, 1.5637, 2.0973, 2.0649, 1.9817,\n",
      "        0.9981, 0.9282, 1.1798, 1.1234, 1.3285, 1.2654, 1.6858, 1.4549, 0.8565,\n",
      "        1.0240, 1.2684, 1.3303, 1.1138, 1.1891, 1.4737, 1.4395, 1.1795, 1.5165,\n",
      "        1.6334, 1.6215, 1.6741, 1.8065, 1.8570, 1.8657, 1.4551, 1.1416, 1.0379,\n",
      "        1.5223, 1.7389, 1.8636, 1.8447, 1.7293], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.initial.weight Parameter containing:\n",
      "tensor([[-0.1136,  0.1394,  0.0808,  ...,  0.1340, -0.0458,  0.1404],\n",
      "        [ 0.0126,  0.1288, -0.1405,  ..., -0.2244, -0.0030, -0.0677],\n",
      "        [ 0.0645, -0.0071, -0.0293,  ..., -0.0629,  0.0924,  0.0253],\n",
      "        ...,\n",
      "        [-0.0759, -0.1033,  0.0147,  ...,  0.0384, -0.0330,  0.0149],\n",
      "        [-0.1311,  0.0460,  0.0167,  ..., -0.0687, -0.0850,  0.0366],\n",
      "        [ 0.2005, -0.0571,  0.1022,  ..., -0.0224,  0.1006, -0.0177]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.initial.weight Parameter containing:\n",
      "tensor([[-7.7648e-02, -6.2766e-03, -5.0111e-03,  ..., -4.1308e-03,\n",
      "          3.2643e-05, -2.3375e-02],\n",
      "        [ 6.6529e-02,  4.9720e-02,  5.4404e-02,  ..., -7.5455e-02,\n",
      "         -1.8741e-01, -2.4146e-02],\n",
      "        [ 3.5613e-02, -1.5745e-02,  6.3330e-02,  ..., -5.0738e-02,\n",
      "         -7.5435e-02, -6.5039e-02],\n",
      "        ...,\n",
      "        [ 1.5077e-03, -3.9175e-02, -2.5339e-02,  ..., -1.6529e-01,\n",
      "         -3.0051e-02,  7.5463e-02],\n",
      "        [ 3.3585e-02,  1.4387e-01,  1.9341e-02,  ..., -1.6217e-01,\n",
      "          2.7490e-02, -7.6189e-02],\n",
      "        [-1.2541e-01,  6.7943e-02,  1.3112e-02,  ..., -1.1398e-01,\n",
      "         -6.8334e-02,  3.3773e-02]], device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.0459, 2.0872, 2.0457, 2.0850, 2.3455, 2.2168, 2.0853, 1.9858, 2.1007,\n",
      "        2.1373, 2.1752, 2.0311, 2.1220, 2.1631, 2.0555, 2.1820, 2.2886, 2.2388,\n",
      "        2.2620, 2.3479, 2.2422, 2.1314, 2.3345, 2.4123, 2.2920, 2.2968, 2.2117,\n",
      "        2.1382, 2.2910, 2.1872, 2.2931, 2.2569, 2.0402, 1.9761, 2.1097, 2.0977,\n",
      "        1.9650, 2.0153, 2.0831, 2.0870, 2.0587, 2.0829, 1.9905, 2.0311, 2.0403,\n",
      "        2.0348, 2.0150, 1.9851, 2.1885, 2.2447, 2.2228, 2.3064, 1.8946, 2.1378,\n",
      "        2.2134, 2.0713, 2.0203, 2.1733, 2.2562, 2.2405, 2.2758, 2.0138, 2.3362,\n",
      "        2.2121, 2.2693, 2.3481, 2.1181, 2.1267, 2.4583, 2.1993, 2.2224, 2.1675,\n",
      "        2.2083, 2.2359, 2.2512, 2.1663, 2.2373, 2.1530, 2.1574, 2.2369, 2.1073,\n",
      "        2.1448, 2.2113, 2.1324, 2.2754, 2.1430, 2.1214, 2.1327, 2.2852, 2.0837,\n",
      "        2.1750, 2.1983, 2.1430, 2.2045, 2.0853, 2.0761, 2.3358, 2.0172, 2.2436,\n",
      "        2.1088, 2.2148, 2.1317, 2.0736, 2.1319, 2.1974, 2.1190, 2.2420, 2.2013,\n",
      "        2.3971, 2.2047, 2.1358, 2.1751, 2.1783, 2.2657, 2.0836, 2.1345, 2.0512,\n",
      "        2.1883, 2.2334, 2.2579, 2.5340, 2.1016, 2.1879, 2.2572, 2.0334, 2.1877,\n",
      "        2.2357, 2.2800, 2.2116, 2.0817, 2.2148, 2.2371, 2.2617, 2.2827, 2.0812,\n",
      "        2.2271, 2.1301, 2.2714, 2.1242, 2.3077, 2.2771, 2.2564, 2.1113, 2.1436,\n",
      "        2.0739, 2.1041, 1.8338, 1.9468, 2.0473, 2.0995, 2.1161, 1.9709, 2.2373,\n",
      "        2.1643, 2.0425, 2.1024, 2.0105, 2.1975, 2.0431, 2.0336, 2.1611, 2.0961,\n",
      "        2.0551, 2.0954, 2.1029, 2.1169, 2.1227, 2.0961, 2.0525, 2.0018, 2.1081,\n",
      "        2.1327, 2.0566, 1.9604, 2.0457, 2.1595, 1.9850, 1.8274, 2.0199, 1.9642,\n",
      "        1.8366, 1.9036, 1.9247, 1.9568, 2.0136, 2.0041, 1.9606, 1.8684, 1.9361,\n",
      "        1.9552, 1.9374, 1.8698, 2.0734, 1.8850, 2.0125, 2.1891, 2.1604, 2.0338,\n",
      "        2.0337, 1.9774, 2.1750, 2.2208, 2.0105, 2.1371, 2.1542, 2.0531, 1.8514,\n",
      "        2.1638, 2.5341, 2.5410, 2.5105, 2.5094, 2.5290, 2.9251, 2.5418, 2.2712,\n",
      "        2.1183, 2.3824, 2.2876, 2.2975, 2.2318, 2.4948, 2.3729, 2.3485, 2.1484,\n",
      "        2.1590, 2.2384, 2.1686, 2.1431, 1.9848, 2.1943, 2.1450, 2.2909, 2.0974,\n",
      "        2.1466, 2.1537, 2.0876, 2.1705, 2.1065, 1.9446, 2.0186, 2.0394, 1.9057,\n",
      "        1.9337, 1.9500, 1.9467, 1.9051, 1.9885, 1.9479, 1.9157, 1.9688, 1.9108,\n",
      "        1.8772, 1.8621, 1.9732, 1.9708, 2.2738, 2.2149, 1.9944, 2.2762, 2.1991,\n",
      "        2.1562, 2.0752, 2.2249, 2.2091, 2.2391, 2.1431, 2.1852, 2.2309, 2.1233,\n",
      "        2.1733, 2.0893, 2.2208, 2.1557, 2.1793, 2.2890, 2.1551, 2.2233, 2.2100,\n",
      "        2.1162, 2.2384, 2.2032, 2.1036, 2.0494, 2.0176, 2.3342, 2.1241, 2.2456,\n",
      "        3.5250, 2.7166, 2.6705, 3.1644, 2.3545, 2.8128, 2.5481, 2.6732, 3.5540,\n",
      "        2.5471, 2.7344, 2.6662, 3.0300, 2.7949, 2.7925, 2.7367, 2.1109, 1.9796,\n",
      "        1.9016, 1.8760, 1.9274, 1.9137, 1.9299, 2.1599, 1.9690, 1.9874, 1.9013,\n",
      "        1.9380, 2.1150, 1.9397, 1.9751, 1.9484], device='cuda:1',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.initial.weight Parameter containing:\n",
      "tensor([[ 0.0963, -0.0767,  0.0546,  ..., -0.0051, -0.0175, -0.0335],\n",
      "        [ 0.0110,  0.0416,  0.0049,  ..., -0.0334,  0.0195,  0.0284],\n",
      "        [-0.0472,  0.0821, -0.0153,  ..., -0.0571,  0.0504, -0.0197],\n",
      "        ...,\n",
      "        [-0.0540, -0.0311,  0.0638,  ...,  0.0519,  0.0094, -0.2150],\n",
      "        [ 0.0354,  0.0642, -0.0220,  ..., -0.0137,  0.2641,  0.0333],\n",
      "        [-0.0912, -0.0097, -0.0008,  ...,  0.0239, -0.0368,  0.0488]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.initial.weight Parameter containing:\n",
      "tensor([[ 0.0470,  0.0703,  0.1437,  ..., -0.0295, -0.0685, -0.0047],\n",
      "        [-0.0250, -0.0849,  0.1145,  ...,  0.0571, -0.0512,  0.0459],\n",
      "        [ 0.0214,  0.0659, -0.1185,  ..., -0.0499, -0.0136, -0.0093],\n",
      "        ...,\n",
      "        [-0.0645,  0.0273, -0.0748,  ...,  0.1247, -0.0636, -0.0256],\n",
      "        [ 0.0135,  0.0323,  0.0477,  ..., -0.0681, -0.0011,  0.0257],\n",
      "        [ 0.0150, -0.0669,  0.0234,  ...,  0.0196,  0.0166, -0.1589]],\n",
      "       device='cuda:1', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.initial Parameter containing:\n",
      "tensor([2.2006, 2.0657, 2.1802, 2.2342, 2.5547, 2.2691, 2.1380, 2.3179, 2.2961,\n",
      "        2.1777, 2.1096, 2.4494, 2.5215, 2.0670, 2.3622, 2.1919, 2.2548, 2.1264,\n",
      "        2.4321, 2.0805, 2.4589, 2.4542, 2.0407, 2.1860, 2.1341, 1.9930, 2.2346,\n",
      "        2.2173, 2.4632, 2.2399, 2.1041, 2.0852, 2.4457, 2.1679, 2.1058, 2.0830,\n",
      "        2.1188, 2.2574, 2.3751, 2.1786, 2.1387, 2.3536, 2.3232, 2.2077, 2.3385,\n",
      "        2.1442, 3.1453, 1.9829, 2.3005, 2.5675, 2.2675, 2.2296, 2.0703, 2.3497,\n",
      "        2.0743, 2.1796, 2.2047, 2.3915, 2.6277, 2.4032, 2.3249, 2.1075, 6.9186,\n",
      "        2.1943, 2.2000, 2.1898, 2.3786, 2.6556, 2.3278, 2.1597, 2.2228, 2.1402,\n",
      "        2.0080, 2.3187, 2.1237, 2.3826, 2.2431, 2.4328, 1.9139, 1.9923, 2.0537,\n",
      "        2.1299, 2.3023, 2.2714, 2.0234, 2.1668, 2.4934, 2.4340, 1.9738, 2.2208,\n",
      "        2.1318, 2.0382, 2.4105, 2.2387, 2.2245, 2.2876, 2.1124, 2.2899, 2.1746,\n",
      "        2.2922, 2.3415, 2.2709, 2.4396, 2.4025, 2.3144, 2.3605, 2.1049, 2.2974,\n",
      "        2.0244, 2.3103, 2.2656, 2.2653, 2.2157, 2.1713, 2.4520, 2.1407, 2.2505,\n",
      "        2.3006, 2.4731, 2.5070, 2.4000, 2.2931, 2.2897, 2.2192, 2.0109, 2.5255,\n",
      "        2.1196, 2.1500, 2.2468, 2.3480, 2.1511, 2.0098, 2.1829, 2.1692, 2.4278,\n",
      "        2.1292, 2.2237, 2.4373, 2.1596, 2.3053, 2.1168, 2.1038, 2.2343, 2.3608,\n",
      "        2.3900, 2.2983, 2.1589, 2.3392, 2.1760, 2.2740, 2.4360, 2.2611, 2.3880,\n",
      "        2.1533, 2.2852, 2.2903, 2.0797, 2.3667, 2.2895, 2.1738, 2.0741, 2.2175,\n",
      "        2.3321, 2.3705, 2.3487, 2.3343, 2.3052, 2.1094, 2.3291, 2.1580, 2.3033,\n",
      "        2.3078, 2.2335, 2.2087, 2.2596, 2.0513, 2.2749, 2.2451, 2.1753, 2.0152,\n",
      "        2.0275, 2.0962, 2.1804, 2.1933, 2.3181, 2.1861, 2.3058, 2.2507, 1.9660,\n",
      "        2.2241, 2.1431, 2.0907, 3.3484, 2.1541, 2.2170, 2.2873, 2.4384, 2.2563,\n",
      "        2.3896, 2.2983, 2.2926, 2.3600, 2.2755, 2.2477, 2.2492, 2.3264, 1.9497,\n",
      "        2.2523, 2.6115, 2.1607, 2.1900, 2.3667, 2.3901, 2.3304, 2.3205, 2.1655,\n",
      "        2.1768, 2.0320, 2.0232, 2.1321, 2.0882, 2.1043, 2.4262, 2.4299, 2.3716,\n",
      "        2.2494, 2.3321, 2.1386, 1.9385, 2.3993, 2.1341, 2.1022, 2.0628, 2.3105,\n",
      "        2.0207, 2.1714, 2.2510, 2.2480, 2.1001, 2.2838, 2.1232, 2.0884, 2.3338,\n",
      "        2.0860, 2.2668, 2.0312, 2.3519, 1.9840, 2.0622, 2.3944, 2.2628, 2.1638,\n",
      "        2.0397, 1.9932, 2.2424, 2.2398, 2.3338, 2.0501, 2.3164, 2.3076, 2.1087,\n",
      "        2.1849, 2.2238, 2.1393, 2.1875, 2.1841, 2.3230, 2.8114, 2.3913, 2.1197,\n",
      "        1.9563, 2.1896, 2.0379, 2.4505, 2.4374, 2.3711, 2.1429, 2.0893, 2.2961,\n",
      "        2.1974, 2.2334, 2.1074, 2.2679, 2.5741, 2.1968, 2.2795, 2.2866, 2.3324,\n",
      "        2.1067, 2.1641, 2.1761, 2.3181, 2.2413, 2.3005, 2.0779, 2.0538, 2.1956,\n",
      "        2.0923, 2.0574, 2.2962, 2.2111, 2.1847, 2.1963, 2.1551, 2.3691, 2.2281,\n",
      "        2.3378, 2.0081, 2.0089, 2.0579, 2.3257, 1.9849, 1.9308, 2.3321, 2.3208,\n",
      "        2.1659, 2.3828, 2.1172, 2.2186, 2.0117], device='cuda:1',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in a.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1699, -0.8279,  0.0260,  ...,  0.0469, -0.0843,  0.0254],\n",
      "        [-0.3562,  0.0308,  0.0865,  ...,  0.0591,  0.2071, -0.2667],\n",
      "        [ 0.0750,  0.1241,  0.0656,  ..., -0.0567,  0.0863,  0.0597],\n",
      "        ...,\n",
      "        [-0.0034, -0.1483, -0.1708,  ..., -0.0852, -0.1563, -0.0351],\n",
      "        [ 0.0784, -0.0335,  0.0649,  ...,  0.0123, -0.1528, -0.1552],\n",
      "        [ 0.0145,  0.0106, -0.0876,  ...,  0.0228,  0.0360, -0.0452]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0273, -0.0062, -0.0049,  ...,  0.0532, -0.0117,  0.0456],\n",
      "        [ 0.1398,  0.0231, -0.0385,  ...,  0.0212,  0.0046,  0.0588],\n",
      "        [ 0.1207,  0.0045,  0.0326,  ...,  0.0863,  0.0458, -0.0689],\n",
      "        ...,\n",
      "        [ 0.0235, -0.0163,  0.0695,  ...,  0.0183,  0.0525, -0.0219],\n",
      "        [-0.0018,  0.0108,  0.0602,  ..., -0.0264,  0.1311,  0.0807],\n",
      "        [ 0.0329, -0.0148, -0.0398,  ..., -0.1342,  0.0230,  0.0035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5853, 0.9414, 0.8864, 1.0005, 0.8341, 1.6792, 0.8581, 1.3855, 0.7455,\n",
      "        0.6171, 0.8099, 1.1151, 1.2725, 0.9141, 1.2154, 1.8849, 2.3473, 1.6398,\n",
      "        1.3562, 0.8318, 2.1569, 0.5517, 1.0800, 1.0548, 0.8301, 0.7455, 1.4008,\n",
      "        1.6584, 1.0272, 2.0568, 1.1916, 2.2181, 1.3737, 1.6398, 1.6193, 1.6319,\n",
      "        1.5264, 0.9735, 1.3574, 1.1211, 1.3794, 0.7246, 0.9086, 0.7327, 0.9418,\n",
      "        1.7050, 1.5984, 1.0973, 0.5182, 0.8853, 0.7954, 0.7849, 1.0845, 1.2646,\n",
      "        0.8790, 1.1035, 0.6633, 0.8367, 1.0042, 1.2310, 1.3798, 0.4869, 1.3834,\n",
      "        1.1706, 1.1599, 0.8857, 0.9255, 1.6514, 1.8569, 0.6478, 1.6139, 1.1124,\n",
      "        1.7787, 1.4179, 1.4084, 0.7719, 0.4653, 1.7452, 1.6296, 1.4729, 1.1574,\n",
      "        1.2324, 0.9352, 1.2625, 0.6487, 1.1540, 1.5481, 1.6548, 1.6443, 1.1342,\n",
      "        1.5609, 1.3580, 1.6658, 1.8118, 1.0891, 1.5124, 0.5507, 0.5390, 0.6970,\n",
      "        0.8906, 0.6116, 1.2291, 2.0492, 1.5262, 0.6264, 0.5575, 0.5198, 1.3949,\n",
      "        1.0530, 1.1037, 1.2533, 1.6176, 0.4951, 0.4427, 0.6902, 0.5102, 0.6893,\n",
      "        1.0745, 1.0676, 0.9169, 0.4903, 0.5838, 0.6322, 0.6646, 1.2191, 0.5728,\n",
      "        1.4091, 1.2412, 0.8634, 0.6194, 1.5913, 1.5369, 1.0518, 0.8959, 1.0386,\n",
      "        0.7692, 0.3437, 0.8577, 1.3768, 0.9904, 1.1698, 1.2362, 1.0852, 0.7233,\n",
      "        1.8126, 1.2617, 1.5355, 1.6066, 2.2761, 0.4591, 1.5947, 1.8179, 1.6002,\n",
      "        1.2395, 1.2208, 0.5141, 0.4554, 2.5645, 1.4504, 0.9397, 0.7354, 0.8315,\n",
      "        1.0938, 1.1105, 1.3614, 1.5309, 1.5623, 0.7575, 1.0908, 1.0442, 0.8914,\n",
      "        1.3379, 1.8194, 1.3875, 0.7927, 0.7337, 1.2977, 0.9300, 1.2044, 1.9222,\n",
      "        1.4305, 1.3003, 0.9287, 1.6573, 0.7350, 0.8158, 1.0075, 1.9005, 1.2888,\n",
      "        1.2273, 1.4752, 1.2818, 0.5897, 1.1480, 0.8825, 1.1704, 1.2369, 0.6458,\n",
      "        1.0872, 1.6619, 0.4945, 0.9241, 1.2399, 0.9323, 0.7577, 1.0398, 1.7767,\n",
      "        0.9969, 0.4415, 0.6359, 1.4633, 0.9700, 0.8584, 0.7347, 0.6810, 0.8217,\n",
      "        0.5012, 0.5886, 0.3603, 1.1078, 1.1681, 1.3178, 0.7116, 0.6306, 3.4055,\n",
      "        2.2885, 2.3034, 2.3188, 1.4570, 2.1923, 1.5341, 1.7381, 3.1816, 2.2742,\n",
      "        2.9759, 1.6584, 2.8653, 1.8699, 2.1077, 2.2289, 1.1329, 0.8952, 0.5713,\n",
      "        1.4181, 1.8599, 0.8214, 1.1055, 1.2004, 0.6950, 1.1168, 1.4317, 1.1842,\n",
      "        0.5058, 1.8753, 1.0431, 0.8566, 0.5702, 1.1380, 1.4764, 1.2667, 1.6866,\n",
      "        0.5480, 1.7491, 0.8235, 1.0170, 0.9379, 0.5901, 1.5741, 0.7238, 1.8894,\n",
      "        1.9161, 0.8679, 1.3350, 1.5763, 1.6763, 1.3810, 1.2394, 0.7780, 1.7551,\n",
      "        1.2473, 0.9061, 1.0333, 2.0561, 1.9544, 2.2742, 2.4918, 1.1108, 1.1175,\n",
      "        1.6286, 0.7643, 1.6727, 1.8822, 1.2415, 0.7075, 1.6238, 1.0862, 0.8523,\n",
      "        1.2977, 0.5647, 0.9873, 1.1112, 1.9990, 1.8551, 1.1154, 0.5343, 0.5222,\n",
      "        1.3942, 0.4759, 0.4580, 2.1503, 1.5154, 1.1260, 1.4921, 1.9562, 1.1058,\n",
      "        1.4934, 2.3807, 0.5520, 0.8688, 1.0518], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0983,  0.5507,  0.0746,  ..., -0.0111,  0.1217,  0.0652],\n",
      "        [ 0.0198,  0.0509,  0.0957,  ..., -0.0973, -0.3128, -0.1401],\n",
      "        [ 0.1018, -0.4264,  0.3920,  ...,  0.0732,  0.0144, -0.0555],\n",
      "        ...,\n",
      "        [-0.0891, -0.0386, -0.0503,  ..., -0.0899,  0.2477,  0.2061],\n",
      "        [ 0.0940,  0.0337, -0.0276,  ..., -0.1335, -0.0683,  0.0346],\n",
      "        [ 0.0818,  0.2765, -0.0031,  ...,  0.0455, -0.0677, -0.2436]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0168, -0.0270, -0.0437,  ...,  0.1121, -0.0504, -0.0876],\n",
      "        [-0.0428,  0.0307,  0.2387,  ...,  0.0859, -0.0233,  0.0573],\n",
      "        [-0.0325,  0.0235, -0.0456,  ..., -0.0668, -0.0114,  0.2030],\n",
      "        ...,\n",
      "        [ 0.2680, -0.1350,  0.0185,  ...,  0.1096,  0.0893, -0.1141],\n",
      "        [ 0.0432, -0.0036, -0.0414,  ...,  0.0023,  0.0168, -0.0585],\n",
      "        [-0.0194,  0.0331, -0.0021,  ...,  0.0606, -0.0588,  0.1131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8392, 1.5019, 1.3078, 1.5079, 1.3025, 1.5585, 0.8220, 1.3313, 1.1638,\n",
      "        1.3262, 1.5978, 1.8851, 1.3966, 1.2226, 1.2462, 1.8329, 2.2638, 1.5260,\n",
      "        1.4429, 1.9183, 1.6768, 1.8161, 1.1113, 1.0717, 1.4786, 1.1437, 2.3015,\n",
      "        2.0781, 2.0550, 1.5666, 1.1595, 2.2420, 2.3097, 2.3510, 2.3838, 1.6049,\n",
      "        1.5131, 1.2421, 1.7080, 1.1408, 1.3006, 1.9111, 1.5604, 1.8477, 1.3583,\n",
      "        1.7531, 1.8328, 1.1693, 0.9461, 0.7927, 1.3253, 1.8668, 1.6322, 1.5397,\n",
      "        0.9627, 1.2589, 1.0646, 1.5216, 1.7032, 1.2262, 2.1231, 1.0534, 1.3696,\n",
      "        1.7091, 1.7756, 0.9021, 1.7070, 1.9496, 1.9010, 1.7491, 1.6047, 1.0320,\n",
      "        1.2980, 2.1570, 2.0436, 1.7722, 1.3395, 1.3266, 1.7732, 1.8616, 2.0157,\n",
      "        2.3580, 1.5554, 1.7025, 1.5890, 1.2600, 2.0191, 1.9583, 2.3127, 1.5514,\n",
      "        2.3803, 1.8509, 1.2195, 1.7522, 0.8964, 2.0830, 0.8266, 1.1107, 1.2753,\n",
      "        1.0481, 1.2297, 1.6393, 1.9367, 1.8472, 0.9549, 0.9827, 1.1031, 1.5806,\n",
      "        1.4853, 1.6312, 1.5712, 1.6653, 0.5601, 0.8297, 0.9081, 0.8465, 1.2861,\n",
      "        1.8130, 1.1477, 0.7905, 0.6225, 0.7807, 1.1590, 1.6046, 1.8592, 1.0478,\n",
      "        1.3277, 1.8135, 1.4369, 0.8611, 1.3014, 1.3713, 0.6711, 0.6987, 1.0542,\n",
      "        0.9251, 0.4482, 1.1542, 1.2361, 0.8154, 0.7378, 0.5549, 1.1003, 0.6287,\n",
      "        2.1110, 1.7296, 1.9410, 1.9848, 1.4768, 2.6344, 1.5212, 1.8838, 2.4259,\n",
      "        2.2121, 2.2918, 1.9224, 2.1539, 2.0339, 1.5981, 0.9361, 1.6011, 1.8197,\n",
      "        1.8355, 1.7057, 1.9585, 1.5404, 2.1567, 0.7077, 1.4238, 1.4275, 1.6206,\n",
      "        2.2843, 1.7853, 1.7433, 1.0910, 0.7828, 1.2124, 1.5569, 2.1363, 2.1490,\n",
      "        1.7376, 1.5103, 1.2466, 1.8997, 1.8123, 1.5968, 1.4765, 2.0046, 1.7172,\n",
      "        1.5096, 1.2301, 1.4968, 0.8001, 1.5426, 1.1875, 1.3194, 1.7850, 1.0918,\n",
      "        1.1095, 2.1928, 0.9384, 0.8893, 1.4626, 1.7024, 1.4097, 1.5778, 2.0078,\n",
      "        1.3213, 0.9281, 0.8281, 1.7055, 1.1076, 1.0169, 0.6628, 0.8071, 0.8527,\n",
      "        0.8261, 0.9799, 0.4856, 1.2225, 1.2257, 0.7850, 0.6270, 0.8081, 2.9451,\n",
      "        2.7178, 2.8894, 1.7407, 2.3582, 1.3965, 1.4035, 1.6724, 2.2536, 1.7997,\n",
      "        1.8414, 2.2658, 1.4603, 2.2006, 2.0325, 1.8343, 2.1152, 1.6703, 1.5110,\n",
      "        1.6628, 1.5135, 1.3825, 1.3710, 1.3022, 0.9267, 1.8836, 2.3095, 1.9317,\n",
      "        1.7390, 1.9284, 0.9710, 0.9299, 1.3975, 0.8381, 1.8367, 1.6382, 1.6303,\n",
      "        1.2757, 1.4130, 0.7267, 0.9907, 1.6766, 1.1864, 1.5973, 1.5308, 1.6346,\n",
      "        2.1062, 0.8994, 1.7928, 1.8450, 1.1499, 1.5268, 1.7757, 2.7709, 1.5211,\n",
      "        1.3783, 0.9778, 1.4357, 2.6976, 2.2279, 2.0355, 1.5333, 1.2152, 1.0892,\n",
      "        2.2129, 2.0966, 1.8543, 2.1541, 1.4145, 1.1462, 1.7422, 1.1569, 2.0677,\n",
      "        1.8638, 2.2184, 1.8875, 1.6365, 1.9494, 1.9363, 1.2111, 2.1896, 1.7809,\n",
      "        2.1253, 1.5785, 2.5326, 1.8456, 1.9618, 1.5199, 1.7201, 2.9075, 1.7146,\n",
      "        1.6995, 1.5715, 2.1892, 0.9054, 0.9631], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1435, -0.0052,  0.2564,  ...,  0.0818, -0.1329, -0.0929],\n",
      "        [ 0.1267,  0.0044,  0.1405,  ...,  0.0048, -0.0224, -0.1540],\n",
      "        [ 0.0457,  0.0006,  0.0886,  ..., -0.0794, -0.0209, -0.0535],\n",
      "        ...,\n",
      "        [-0.0140,  0.0056,  0.1073,  ..., -0.0442, -0.0528, -0.0796],\n",
      "        [ 0.1323, -0.0084, -0.0060,  ...,  0.0288, -0.0053,  0.1176],\n",
      "        [ 0.0237, -0.0080,  0.1673,  ...,  0.1739, -0.1087,  0.0090]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0796,  0.0986,  0.1738,  ...,  0.0372,  0.0837, -0.0044],\n",
      "        [ 0.1285, -0.0206, -0.1360,  ..., -0.0217, -0.2396,  0.0991],\n",
      "        [ 0.0575,  0.0792, -0.1378,  ..., -0.0274,  0.1571, -0.2062],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0528,  0.0342,  ..., -0.0391,  0.0607, -0.0446],\n",
      "        [-0.0733,  0.0174,  0.1513,  ...,  0.0083, -0.1267, -0.0529],\n",
      "        [ 0.2143,  0.1047,  0.0596,  ...,  0.0273,  0.0736,  0.1619]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4917, 1.6185, 1.4137, 1.6170, 1.3530, 1.5407, 1.5942, 1.3468, 1.5060,\n",
      "        1.4935, 1.3167, 1.4252, 1.4963, 1.5173, 1.4869, 1.3957, 0.9246, 1.0934,\n",
      "        1.0331, 1.1334, 0.8693, 1.0941, 1.2022, 0.3912, 1.2021, 0.8737, 1.2234,\n",
      "        0.9823, 1.2429, 0.9371, 1.1835, 1.1728, 0.9154, 1.1116, 1.1121, 1.0157,\n",
      "        1.0529, 1.1077, 1.0750, 1.0342, 0.9951, 0.9604, 0.9966, 1.1954, 1.1085,\n",
      "        1.1945, 1.0490, 0.9612, 1.5618, 1.6146, 1.4254, 1.1164, 1.5294, 1.3780,\n",
      "        1.5317, 1.5991, 0.9939, 1.4163, 1.2540, 1.7454, 1.5451, 1.5020, 1.0513,\n",
      "        1.3620, 0.9185, 1.2775, 1.1574, 1.1358, 1.1757, 1.2605, 1.3937, 1.1459,\n",
      "        1.2644, 1.2660, 1.1582, 1.3192, 1.2963, 1.2039, 1.2946, 1.2063, 0.9913,\n",
      "        1.0043, 1.2472, 1.0516, 1.1947, 1.1830, 1.2502, 1.1816, 1.2841, 1.0190,\n",
      "        1.1961, 1.1195, 1.1155, 1.0742, 1.0973, 1.1344, 1.6846, 1.6441, 1.4500,\n",
      "        1.5267, 1.4659, 1.5084, 1.7593, 1.6825, 1.4565, 1.3629, 1.0535, 1.6753,\n",
      "        1.5996, 1.2679, 1.4663, 1.6751, 1.7296, 1.8034, 1.6548, 1.7982, 2.0474,\n",
      "        1.8326, 1.9631, 2.1344, 1.6607, 1.8523, 1.6492, 1.7315, 1.6057, 1.7653,\n",
      "        1.7109, 2.0691, 1.0070, 1.1540, 1.2071, 1.1142, 1.2202, 1.4893, 1.1888,\n",
      "        1.5323, 1.1306, 1.4408, 1.3467, 1.2089, 1.1028, 1.2117, 1.1165, 1.4369,\n",
      "        1.1537, 1.0274, 1.0762, 1.1233, 1.1200, 1.0833, 1.1910, 1.2220, 1.0911,\n",
      "        1.1992, 1.2143, 1.0937, 1.1224, 0.9989, 1.0956, 1.0685, 1.2307, 1.4808,\n",
      "        1.1657, 1.2746, 1.3468, 1.2639, 1.3008, 1.2622, 1.5240, 1.6527, 1.3566,\n",
      "        1.3567, 1.2450, 1.1234, 1.2562, 1.1868, 1.0280, 1.0338, 0.8857, 1.0276,\n",
      "        0.9761, 1.0706, 1.3064, 1.0495, 0.8612, 0.9804, 1.2206, 1.0050, 1.1042,\n",
      "        0.9609, 1.1378, 1.1455, 1.7731, 1.4016, 0.8720, 1.3047, 1.8650, 1.2752,\n",
      "        1.3031, 1.6741, 1.3652, 1.7766, 1.7024, 1.9491, 1.6957, 1.7040, 1.7962,\n",
      "        1.6843, 1.2912, 1.2714, 1.3934, 1.4783, 1.3656, 1.5633, 1.2917, 1.4029,\n",
      "        1.5967, 1.5338, 1.3340, 1.4297, 1.2472, 1.5835, 1.1728, 1.1883, 0.7338,\n",
      "        0.8380, 0.6725, 0.7765, 0.7458, 0.6886, 0.9215, 0.7849, 0.8845, 0.9460,\n",
      "        0.7414, 0.5960, 0.7254, 0.7640, 0.7100, 0.6799, 1.1931, 1.2631, 1.2553,\n",
      "        1.3665, 1.0376, 1.2633, 1.2751, 1.2068, 1.3215, 1.1548, 1.2156, 1.2706,\n",
      "        1.3866, 1.3554, 1.2047, 1.1761, 1.3438, 1.2454, 1.3817, 1.3412, 1.2593,\n",
      "        1.2886, 1.8079, 1.2284, 1.3260, 1.2047, 1.3143, 1.3199, 1.5424, 1.4314,\n",
      "        1.3844, 1.1220, 0.8966, 0.7924, 0.9035, 1.0229, 0.9823, 0.9605, 1.0148,\n",
      "        0.9744, 0.9330, 0.9196, 0.9483, 1.0967, 0.9123, 0.9746, 0.9372, 0.8767,\n",
      "        1.0440, 1.0145, 0.9922, 1.0112, 1.0669, 0.9030, 0.9701, 0.9799, 0.9938,\n",
      "        1.0708, 0.9241, 1.0546, 1.1600, 1.0257, 1.1353, 0.9803, 1.1513, 1.0881,\n",
      "        1.0447, 1.0441, 1.0934, 1.2000, 1.2321, 1.1657, 1.0623, 1.1826, 1.1684,\n",
      "        1.0922, 1.2743, 1.1286, 1.2012, 1.1993], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 2.2875e-02, -5.7616e-02,  5.3130e-02,  ..., -1.5022e-02,\n",
      "         -5.6132e-02,  4.2808e-02],\n",
      "        [ 1.7193e-02, -4.6617e-02, -1.2302e-01,  ...,  9.1113e-02,\n",
      "          1.4257e-03,  1.8496e-02],\n",
      "        [-4.1950e-02,  1.8176e-02,  3.2392e-03,  ...,  4.5562e-02,\n",
      "          5.6913e-02,  3.7032e-03],\n",
      "        ...,\n",
      "        [ 3.2912e-02, -1.2532e-01,  5.4360e-02,  ...,  2.1749e-02,\n",
      "          2.1646e-02, -8.4060e-02],\n",
      "        [ 1.1246e-01,  1.1954e-04,  8.8178e-02,  ..., -2.4415e-02,\n",
      "         -3.7357e-02,  3.6012e-02],\n",
      "        [-7.4992e-03,  2.0190e-02, -7.8832e-03,  ...,  3.2634e-02,\n",
      "          1.2502e-01, -8.3001e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0694,  0.0207,  0.0094,  ..., -0.0309,  0.0413, -0.1416],\n",
      "        [ 0.0015, -0.0281, -0.0550,  ..., -0.0552, -0.1333,  0.0577],\n",
      "        [-0.0678, -0.0670, -0.0332,  ...,  0.0564,  0.0964,  0.0413],\n",
      "        ...,\n",
      "        [-0.0034,  0.0211, -0.0138,  ...,  0.0476,  0.0222,  0.0774],\n",
      "        [-0.0069, -0.0140,  0.0580,  ...,  0.0576,  0.0310, -0.0124],\n",
      "        [-0.0300, -0.0429, -0.0785,  ...,  0.0153,  0.0679, -0.0098]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3740, 1.3904, 1.4838, 1.5118, 1.4510, 1.5785, 1.4105, 1.4067, 1.3052,\n",
      "        1.4298, 1.3426, 1.5070, 1.5158, 1.3758, 1.3639, 1.3709, 1.4356, 1.3511,\n",
      "        1.4590, 1.3494, 1.4009, 1.4041, 1.4205, 1.3598, 1.3991, 1.4443, 1.4524,\n",
      "        1.3815, 1.4113, 1.4558, 1.3600, 1.3032, 1.4515, 1.2735, 1.3580, 1.3907,\n",
      "        1.4180, 1.3804, 1.4393, 1.4540, 1.4143, 1.4283, 1.3486, 1.4449, 1.4344,\n",
      "        1.4374, 1.8067, 1.3505, 1.5645, 1.4160, 1.4332, 1.3737, 1.4381, 1.5773,\n",
      "        1.3828, 1.2475, 1.3874, 1.4301, 1.3509, 1.3399, 1.4914, 1.6885, 1.2521,\n",
      "        1.5101, 1.3818, 1.3831, 1.4515, 1.4315, 1.3969, 1.3142, 1.4090, 1.4638,\n",
      "        1.3627, 1.5256, 1.5117, 1.4152, 1.4218, 1.3243, 1.3828, 1.4735, 1.3924,\n",
      "        1.3767, 1.3131, 1.4211, 1.4310, 1.5536, 1.3673, 1.4457, 1.3947, 1.4270,\n",
      "        1.4839, 1.3977, 1.4594, 1.4249, 1.3820, 1.4246, 1.3019, 1.4367, 1.3813,\n",
      "        1.3407, 1.3682, 1.4494, 1.3550, 1.2743, 1.5165, 1.3337, 1.5171, 1.4821,\n",
      "        1.4211, 1.3537, 1.3815, 1.6056, 1.4653, 1.4690, 1.5439, 1.4124, 1.3118,\n",
      "        1.4480, 1.3472, 1.5352, 1.5861, 1.4152, 1.3639, 1.3605, 1.3323, 1.4614,\n",
      "        1.3607, 1.4409, 1.3664, 1.3438, 1.4065, 1.5738, 1.4407, 1.4703, 1.3721,\n",
      "        1.4661, 1.3906, 1.3201, 1.3794, 1.6478, 1.3449, 1.5379, 1.3763, 1.3410,\n",
      "        1.1465, 1.3874, 1.3620, 1.5130, 1.3825, 1.3696, 1.4624, 1.5309, 1.5773,\n",
      "        1.3766, 1.3823, 1.3124, 1.4153, 1.3662, 1.4840, 1.3986, 1.3518, 1.4303,\n",
      "        1.4070, 1.4805, 1.4840, 1.4982, 1.4995, 1.4460, 1.3829, 1.3684, 1.4587,\n",
      "        1.6012, 1.4714, 1.3866, 1.4046, 1.4590, 1.3298, 1.3535, 1.3849, 1.4275,\n",
      "        1.3954, 1.5177, 1.3541, 1.3493, 1.4538, 1.3263, 1.4283, 1.2313, 1.4423,\n",
      "        1.3513, 1.3759, 1.3375, 1.4987, 0.9611, 1.3892, 1.3322, 1.3945, 1.3051,\n",
      "        1.3567, 1.3461, 1.4279, 1.3839, 1.3114, 1.3412, 1.3983, 1.4422, 1.3717,\n",
      "        1.4004, 1.4139, 1.4557, 1.3899, 1.3335, 1.3043, 1.4171, 1.4907, 1.4302,\n",
      "        1.3905, 1.4190, 1.3560, 1.3773, 1.3991, 1.3786, 1.4501, 1.3242, 1.3477,\n",
      "        1.4172, 1.3803, 1.4688, 1.3984, 1.4666, 1.4248, 1.3366, 1.4687, 1.3257,\n",
      "        1.4461, 1.4017, 1.4657, 1.3568, 1.4132, 1.4396, 1.4444, 1.4564, 1.5247,\n",
      "        1.4280, 1.4539, 1.3982, 1.3897, 1.4348, 1.4736, 1.3185, 1.3491, 1.4413,\n",
      "        1.4655, 1.4000, 1.3226, 1.4614, 1.4832, 1.6965, 1.4447, 1.3706, 1.4175,\n",
      "        1.4554, 1.4346, 1.3463, 1.4888, 1.4108, 1.5053, 1.4166, 1.4168, 1.4874,\n",
      "        1.2874, 1.2381, 1.4579, 1.3490, 1.2877, 1.3832, 1.4297, 1.4457, 1.4464,\n",
      "        1.3924, 1.4519, 1.4723, 1.3501, 1.6392, 1.4451, 1.2619, 1.2780, 1.5177,\n",
      "        1.3808, 1.5079, 1.3931, 1.7730, 1.3874, 1.4245, 1.4911, 1.3472, 1.2795,\n",
      "        1.3893, 1.4300, 1.4069, 1.3913, 1.4318, 1.3971, 1.3429, 1.5421, 1.4095,\n",
      "        1.3809, 1.4325, 1.5401, 1.3790, 1.4808, 1.4995, 1.4290, 1.4436, 1.5071,\n",
      "        1.4553, 1.3814, 1.3954, 1.3907, 1.3718], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.6870e-01,  4.9902e-02, -1.2189e-01,  ...,  1.6759e-01,\n",
      "         -1.6534e-01,  6.3929e-01],\n",
      "        [ 8.9402e-02, -2.6805e-02, -9.0301e-02,  ...,  1.2442e-01,\n",
      "         -1.6093e-02,  7.5032e-02],\n",
      "        [-1.1900e-01, -5.2743e-02, -1.0721e-01,  ..., -2.3550e-01,\n",
      "         -2.1219e-02, -5.0626e-04],\n",
      "        ...,\n",
      "        [-5.7376e-02, -2.9079e-02, -1.2264e-01,  ..., -1.8853e-02,\n",
      "         -9.9525e-02,  5.4936e-02],\n",
      "        [-7.9351e-02, -9.3222e-02, -8.0281e-02,  ...,  1.0874e-01,\n",
      "         -1.5452e-01, -1.4049e-01],\n",
      "        [-4.1216e-02, -1.4729e-01, -1.3642e-01,  ..., -1.2539e-01,\n",
      "          3.1789e-03,  3.3047e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0141,  0.0241,  0.0289,  ...,  0.0098,  0.0666, -0.0135],\n",
      "        [-0.1490, -0.1216, -0.0346,  ..., -0.0460, -0.0623,  0.0192],\n",
      "        [-0.1204, -0.1405, -0.0062,  ...,  0.0264, -0.0182,  0.0078],\n",
      "        ...,\n",
      "        [-0.0759, -0.0302, -0.1345,  ...,  0.0959, -0.0223,  0.0074],\n",
      "        [-0.0206,  0.1339,  0.1310,  ..., -0.0788, -0.0802,  0.0098],\n",
      "        [ 0.0677, -0.1423, -0.1050,  ...,  0.0419,  0.1962, -0.0726]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3245, 1.7168, 1.7899, 1.6921, 2.5019, 3.0230, 2.9051, 2.6821, 2.2656,\n",
      "        1.9286, 1.9575, 2.6198, 2.1558, 2.3735, 2.8304, 2.5617, 3.4486, 2.4797,\n",
      "        1.9333, 2.3668, 2.5104, 2.6295, 3.1140, 3.6420, 1.8723, 2.0190, 2.8540,\n",
      "        2.0065, 2.3818, 2.5469, 3.1762, 3.1797, 2.4396, 2.1003, 2.8242, 2.7793,\n",
      "        2.9950, 2.3750, 3.3404, 3.9783, 1.9111, 2.1084, 2.1892, 2.5384, 2.8881,\n",
      "        2.8819, 3.3324, 3.0388, 1.3544, 1.9157, 1.0063, 1.3015, 2.2458, 2.4231,\n",
      "        2.7791, 2.6768, 3.4506, 1.8947, 2.6747, 2.7112, 2.5856, 2.4897, 3.0258,\n",
      "        2.8980, 1.9294, 2.0109, 3.3894, 2.7113, 2.5089, 3.1272, 2.7267, 3.3233,\n",
      "        2.3448, 1.2482, 1.0486, 1.5450, 2.0146, 2.4396, 3.1620, 3.0219, 2.3152,\n",
      "        2.3613, 2.9629, 2.9116, 2.8080, 3.2015, 3.3321, 3.4925, 2.2904, 2.4564,\n",
      "        2.4738, 2.6459, 2.7453, 2.6686, 3.3170, 2.8940, 2.4965, 2.3668, 3.4119,\n",
      "        3.8276, 3.7419, 2.3307, 2.3495, 3.7480, 2.6472, 3.0456, 3.8322, 3.6121,\n",
      "        3.0308, 3.8170, 3.1908, 4.1504, 1.3837, 1.2890, 2.6511, 2.5087, 2.4521,\n",
      "        2.5008, 2.7571, 2.6419, 1.8360, 2.5589, 1.9935, 1.9783, 2.1956, 2.5941,\n",
      "        2.7387, 3.2058, 1.9193, 1.9477, 2.1161, 1.8403, 2.3576, 2.8870, 2.8029,\n",
      "        2.8380, 2.0006, 1.8999, 2.4727, 3.4790, 2.5432, 2.4968, 2.8966, 2.8671,\n",
      "        3.2983, 1.9448, 2.6039, 2.1472, 2.3245, 2.2576, 2.9870, 2.8286, 1.0430,\n",
      "        1.5752, 1.8245, 2.1747, 2.2242, 2.9386, 2.9885, 3.2160, 1.9532, 2.3568,\n",
      "        2.0262, 2.2121, 2.3628, 2.9375, 2.8399, 2.9173, 3.0711, 2.0559, 2.6335,\n",
      "        2.3709, 2.7033, 2.5364, 2.9938, 3.3630, 1.4400, 1.4544, 2.3855, 1.6382,\n",
      "        1.9818, 2.7049, 2.4330, 2.7141, 1.2462, 1.5827, 1.4435, 2.3673, 2.0342,\n",
      "        2.1272, 2.5182, 2.7164, 1.2996, 1.7125, 1.7743, 1.4574, 2.0389, 1.9021,\n",
      "        2.8402, 2.6258, 2.2244, 2.0748, 2.1919, 2.6215, 2.0894, 2.2945, 2.8064,\n",
      "        2.8616, 2.6161, 2.3002, 2.3308, 3.2465, 3.3884, 1.9806, 3.0118, 3.1338,\n",
      "        1.8333, 3.0788, 3.6652, 4.3917, 3.5267, 4.1543, 3.2870, 3.7847, 1.3667,\n",
      "        1.4765, 2.5817, 1.3997, 1.8004, 2.1469, 2.4749, 3.5545, 4.4102, 2.1699,\n",
      "        1.3076, 2.3206, 2.1613, 3.0763, 2.5509, 3.0494, 2.1864, 2.4883, 2.1916,\n",
      "        3.3076, 2.0339, 2.6320, 3.0719, 3.0617, 2.2879, 1.9822, 2.0287, 1.3856,\n",
      "        2.7286, 2.5970, 2.9732, 2.9859, 3.0516, 2.1417, 2.2701, 1.9949, 2.1251,\n",
      "        2.3400, 3.3546, 3.1013, 2.5145, 2.4692, 2.3275, 3.0263, 2.7831, 2.2138,\n",
      "        3.4609, 2.4852, 1.9811, 2.4358, 2.9205, 2.3416, 1.9662, 2.5458, 3.3893,\n",
      "        3.3530, 2.0105, 2.7228, 2.9802, 3.0973, 3.1343, 2.6227, 3.1460, 3.0294,\n",
      "        2.1830, 3.0292, 2.9034, 3.2462, 2.1824, 3.1652, 3.3615, 3.4801, 2.8929,\n",
      "        3.4013, 2.8332, 2.3360, 3.2497, 2.4488, 3.8892, 3.3294, 1.4866, 2.1185,\n",
      "        2.3769, 2.0340, 2.0800, 2.3999, 2.4460, 2.8901, 1.6317, 1.4096, 1.4471,\n",
      "        2.7143, 2.0882, 2.6064, 2.5952, 2.6835], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0254, -0.0781,  0.0762,  ..., -0.0876,  0.0627, -0.8724],\n",
      "        [-0.0080,  0.0157, -0.0311,  ..., -0.1192,  0.1119, -0.0826],\n",
      "        [ 0.0734,  0.0899, -0.0601,  ...,  0.0186, -0.1439, -0.0848],\n",
      "        ...,\n",
      "        [-0.0558, -0.0730,  0.0440,  ..., -0.1581, -0.1455,  0.0570],\n",
      "        [ 0.0727, -0.0508, -0.0124,  ...,  0.0745,  0.0575, -0.0876],\n",
      "        [-0.0860,  0.0428,  0.0784,  ..., -0.0787,  0.0608,  0.0376]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0593,  0.0664, -0.0933,  ...,  0.1563,  0.0474,  0.0165],\n",
      "        [-0.1472,  0.1030, -0.1273,  ...,  0.0431,  0.0448,  0.0223],\n",
      "        [ 0.2234, -0.0241,  0.0058,  ...,  0.0161,  0.0079, -0.0367],\n",
      "        ...,\n",
      "        [ 0.1645, -0.0652,  0.0206,  ..., -0.0526, -0.0232, -0.0255],\n",
      "        [ 0.1108,  0.0107,  0.0389,  ...,  0.0542,  0.0508, -0.0125],\n",
      "        [-0.0878, -0.0834, -0.0796,  ...,  0.0009,  0.0354, -0.0484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9300, 1.6594, 1.4977, 3.0943, 1.3151, 1.9870, 2.4968, 2.6338, 1.9344,\n",
      "        2.2295, 2.5972, 1.2599, 2.9110, 2.0880, 2.8159, 2.0986, 1.6854, 2.0358,\n",
      "        2.7459, 1.7867, 1.7394, 1.7103, 3.2583, 3.1709, 1.9329, 2.2107, 1.6039,\n",
      "        1.6938, 3.4331, 3.6160, 3.4965, 3.0764, 2.6695, 1.9023, 2.4984, 2.6565,\n",
      "        2.2615, 1.9451, 2.1478, 2.9513, 1.6700, 2.2433, 2.5989, 2.2520, 2.1870,\n",
      "        2.2697, 2.4098, 2.3007, 2.6485, 1.6586, 1.4965, 1.9027, 2.9195, 1.3350,\n",
      "        2.4299, 2.5055, 1.5202, 2.2669, 1.7688, 1.4574, 1.3019, 3.0451, 2.6566,\n",
      "        2.4623, 1.7950, 1.7287, 1.3536, 1.2326, 1.4801, 1.7067, 2.6765, 2.0548,\n",
      "        1.9986, 1.5348, 1.6435, 2.5852, 1.7279, 2.8056, 2.5023, 2.0610, 2.4779,\n",
      "        2.5367, 2.7675, 2.3908, 2.3919, 2.4414, 3.2886, 3.1418, 1.7161, 2.2924,\n",
      "        2.2264, 2.5200, 2.2085, 2.2493, 3.1418, 2.6015, 2.4477, 2.6953, 3.1755,\n",
      "        3.2549, 2.8033, 3.6047, 3.4769, 3.3705, 2.8548, 3.3037, 3.0578, 3.1547,\n",
      "        2.9186, 1.5723, 2.2129, 3.5157, 1.2725, 2.1193, 1.5701, 1.4799, 1.4237,\n",
      "        2.8391, 2.7166, 2.7548, 1.6344, 1.5087, 1.9889, 2.4190, 2.6733, 1.4955,\n",
      "        2.6702, 2.2463, 2.1765, 2.2354, 1.8153, 3.1809, 1.6192, 1.5404, 2.6178,\n",
      "        2.5148, 1.9515, 1.8926, 2.9066, 1.5735, 1.7372, 3.0129, 2.5642, 2.7293,\n",
      "        2.0537, 2.2480, 1.5403, 1.6119, 1.3753, 3.0500, 2.5977, 2.6608, 1.3663,\n",
      "        1.5327, 2.7925, 1.6589, 2.9992, 1.6027, 2.7083, 2.2353, 2.1649, 2.5114,\n",
      "        2.9816, 3.0002, 3.2522, 1.8592, 3.2529, 3.0077, 2.2953, 2.0367, 1.6566,\n",
      "        1.8081, 1.7440, 3.2761, 3.3680, 3.0972, 1.2509, 1.4573, 1.4197, 2.6669,\n",
      "        1.2936, 1.4062, 2.2339, 2.1945, 1.6679, 1.8526, 2.1087, 1.1746, 1.5446,\n",
      "        2.3620, 2.4067, 1.9295, 1.9584, 1.8850, 1.5768, 3.1045, 1.4623, 1.4757,\n",
      "        2.6626, 2.7667, 2.4256, 2.2508, 1.4692, 1.3203, 2.9583, 3.0773, 2.5310,\n",
      "        2.8126, 1.8787, 2.7114, 3.3920, 3.1376, 2.8005, 3.1771, 2.5933, 4.0284,\n",
      "        2.3364, 2.0400, 2.2781, 2.7932, 2.7753, 1.8393, 3.9381, 3.9458, 1.3425,\n",
      "        1.9988, 1.2551, 2.1113, 2.7417, 2.6113, 2.2380, 2.0766, 1.2947, 1.6646,\n",
      "        1.2516, 1.2782, 1.2255, 1.5256, 2.2886, 2.0154, 2.3515, 2.1413, 1.9988,\n",
      "        1.6107, 3.1144, 1.6842, 2.7934, 2.8004, 2.0608, 2.2068, 2.0929, 2.1452,\n",
      "        1.3882, 2.3526, 2.6642, 2.8483, 2.4208, 2.2372, 2.2080, 2.1844, 2.8083,\n",
      "        2.1781, 3.0061, 2.5230, 2.8054, 2.3766, 2.2933, 2.2169, 1.9492, 2.0266,\n",
      "        2.9048, 2.3151, 1.7494, 2.7849, 2.7082, 2.4644, 3.2154, 2.1386, 3.1371,\n",
      "        3.2422, 2.0058, 2.3608, 2.4633, 2.5312, 1.8178, 2.0440, 3.2848, 3.0267,\n",
      "        2.7804, 2.9552, 2.6353, 2.9414, 3.7376, 2.2175, 3.3149, 3.2174, 2.3222,\n",
      "        2.7310, 2.6773, 2.2565, 1.7349, 2.4219, 3.0909, 3.4695, 1.7518, 1.9688,\n",
      "        1.2404, 2.7240, 2.6984, 1.9412, 2.4008, 2.0697, 1.7143, 1.5165, 2.6549,\n",
      "        1.3040, 1.1566, 2.1132, 2.3006, 2.4035], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0298,  0.0448,  0.1184,  ...,  0.0129,  0.0675, -0.0011],\n",
      "        [-0.0980,  0.0223, -0.0524,  ..., -0.0727, -0.0518, -0.0085],\n",
      "        [ 0.0984,  0.0308,  0.1346,  ...,  0.0088, -0.0262,  0.0029],\n",
      "        ...,\n",
      "        [-0.0525, -0.1194,  0.0279,  ..., -0.0296,  0.0422,  0.0077],\n",
      "        [ 0.0403,  0.1124,  0.0035,  ..., -0.0498,  0.0148,  0.0041],\n",
      "        [ 0.0406,  0.0608,  0.0054,  ...,  0.0125,  0.0080, -0.0073]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0197, -0.0262,  0.0052,  ...,  0.0081, -0.0378, -0.0087],\n",
      "        [-0.0079, -0.0146,  0.1244,  ...,  0.0421, -0.0523,  0.0365],\n",
      "        [-0.1055,  0.0199,  0.0096,  ...,  0.0417, -0.0194,  0.0826],\n",
      "        ...,\n",
      "        [-0.0373, -0.0710,  0.0228,  ...,  0.0516,  0.1481, -0.0235],\n",
      "        [-0.0164,  0.0163, -0.0548,  ...,  0.0427,  0.0267,  0.0363],\n",
      "        [ 0.0066,  0.1573, -0.1508,  ..., -0.0057, -0.0282, -0.1634]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4980, 1.3459, 1.4473, 1.4358, 1.3913, 1.4258, 1.3466, 1.3956, 1.4740,\n",
      "        1.4381, 1.4112, 1.4339, 1.4113, 1.4300, 1.4728, 1.4411, 1.1943, 1.2046,\n",
      "        1.1835, 1.2129, 1.1931, 1.1500, 1.1957, 1.1494, 1.2792, 1.1394, 1.1760,\n",
      "        1.0981, 1.1760, 1.2063, 1.1164, 1.2555, 1.8288, 1.9685, 1.9895, 1.9297,\n",
      "        2.1100, 2.1907, 2.1788, 1.9598, 2.1310, 2.0346, 1.9360, 2.1141, 1.9997,\n",
      "        1.9785, 1.9648, 2.1194, 1.3960, 1.4999, 1.3750, 1.4157, 1.4814, 1.5020,\n",
      "        1.3688, 1.5273, 1.4835, 1.4501, 1.4853, 1.4578, 1.4939, 1.5067, 1.4857,\n",
      "        1.4171, 1.2774, 1.2409, 1.2623, 1.2956, 1.3209, 1.3307, 1.2779, 1.3276,\n",
      "        1.2602, 1.2174, 1.3099, 1.2621, 1.2084, 1.2641, 1.2588, 1.1698, 2.0404,\n",
      "        2.1187, 2.0826, 1.9912, 2.1269, 2.1338, 2.1966, 2.1638, 2.2216, 2.1042,\n",
      "        2.0221, 2.1139, 2.0291, 2.1846, 2.0846, 1.9232, 1.6377, 1.7291, 1.7390,\n",
      "        1.7090, 1.7661, 1.7659, 1.7432, 1.7142, 1.6639, 1.6418, 1.7013, 1.7508,\n",
      "        1.7101, 1.6589, 1.6387, 1.7484, 1.2062, 1.2426, 1.2586, 1.2244, 1.2592,\n",
      "        1.3000, 1.1969, 1.2025, 1.3070, 1.2519, 1.2038, 1.2704, 1.3148, 1.2617,\n",
      "        1.2592, 1.2307, 1.6067, 1.4729, 1.5090, 1.5377, 1.5189, 1.4314, 1.5557,\n",
      "        1.5550, 1.5509, 1.4745, 1.5585, 1.4381, 1.5450, 1.5911, 1.5353, 1.4696,\n",
      "        1.2607, 1.2757, 1.1858, 1.2939, 1.2295, 1.2569, 1.2915, 1.2513, 1.2909,\n",
      "        1.3173, 1.1631, 1.3012, 1.2709, 1.2663, 1.2420, 1.2154, 1.1433, 1.2282,\n",
      "        1.2999, 1.1342, 1.1834, 1.1620, 1.2391, 1.1815, 1.1959, 1.1553, 1.1661,\n",
      "        1.2237, 1.2253, 1.3122, 1.1865, 1.2240, 1.4584, 1.4568, 1.4475, 1.4286,\n",
      "        1.4856, 1.4470, 1.4348, 1.4726, 1.4814, 1.4087, 1.5119, 1.4882, 1.4415,\n",
      "        1.4326, 1.4308, 1.5150, 1.5702, 1.5611, 1.6107, 1.6394, 1.5555, 1.5646,\n",
      "        1.5111, 1.5860, 1.5927, 1.6342, 1.5680, 1.5987, 1.5860, 1.5844, 1.5843,\n",
      "        1.5492, 1.4515, 1.4995, 1.5403, 1.5012, 1.4267, 1.5219, 1.5294, 1.5072,\n",
      "        1.6551, 1.5566, 1.4999, 1.4341, 1.5469, 1.4206, 1.5184, 1.5231, 1.3572,\n",
      "        1.3593, 1.3325, 1.3854, 1.3571, 1.4159, 1.4355, 1.4141, 1.3958, 1.4192,\n",
      "        1.4696, 1.3837, 1.3532, 1.3965, 1.3966, 1.3738, 1.6144, 1.6316, 1.6704,\n",
      "        1.6964, 1.7117, 1.6431, 1.6655, 1.5849, 1.6665, 1.6786, 1.7127, 1.6865,\n",
      "        1.6185, 1.7515, 1.6403, 1.6968, 1.8723, 1.8087, 1.7256, 1.7091, 1.7462,\n",
      "        1.8303, 1.7958, 1.8147, 1.8122, 1.8251, 1.8571, 1.8262, 1.8690, 1.7861,\n",
      "        1.6899, 1.7560, 1.7648, 1.7503, 1.9527, 1.8088, 1.8292, 1.8069, 1.8912,\n",
      "        1.7744, 1.7893, 1.7516, 1.8652, 1.7254, 1.7266, 1.8316, 1.8588, 1.7919,\n",
      "        1.7756, 1.8021, 1.8023, 1.8010, 1.8481, 1.7843, 1.7818, 1.8256, 1.8149,\n",
      "        1.6881, 1.7637, 1.8266, 1.7904, 1.8124, 1.7440, 1.8805, 1.4873, 1.4176,\n",
      "        1.4244, 1.3782, 1.4729, 1.3896, 1.4025, 1.3679, 1.4368, 1.3750, 1.4059,\n",
      "        1.3538, 1.3856, 1.4647, 1.4448, 1.5029], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0222,  0.0224,  ...,  0.0101,  0.0533, -0.0518],\n",
      "        [ 0.0227, -0.0480,  0.0123,  ...,  0.0455, -0.0377,  0.0202],\n",
      "        [ 0.0099,  0.0050, -0.0066,  ..., -0.0239,  0.0389, -0.0145],\n",
      "        ...,\n",
      "        [-0.0525, -0.0258,  0.0031,  ..., -0.0879,  0.0053,  0.0270],\n",
      "        [ 0.0043,  0.0049,  0.0244,  ...,  0.0091,  0.0857, -0.0087],\n",
      "        [ 0.0048, -0.0286, -0.0033,  ..., -0.0186,  0.0499, -0.0678]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0699, -0.0754,  0.0219,  ...,  0.0680, -0.0979, -0.0380],\n",
      "        [ 0.0222,  0.0479, -0.0024,  ...,  0.0479, -0.0308,  0.0669],\n",
      "        [ 0.0789,  0.0041,  0.0304,  ..., -0.0284,  0.0059, -0.0273],\n",
      "        ...,\n",
      "        [ 0.0269,  0.0711, -0.0231,  ...,  0.0981,  0.0372,  0.0509],\n",
      "        [ 0.0127,  0.0197,  0.0026,  ...,  0.0688, -0.0037, -0.0093],\n",
      "        [ 0.5940, -0.0378,  0.2032,  ...,  0.0810,  0.0980,  0.1319]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5017, 1.3348, 1.4557, 1.5515, 1.5730, 1.3890, 1.4217, 1.3872, 1.3844,\n",
      "        1.5491, 1.4278, 1.5350, 1.4248, 1.3817, 1.4057, 1.5236, 1.5381, 1.5122,\n",
      "        1.3979, 1.4319, 1.5571, 1.3928, 1.5441, 1.4488, 1.4208, 1.5494, 1.4653,\n",
      "        1.4478, 1.5200, 1.5324, 1.4738, 1.4753, 1.4613, 1.4356, 1.4540, 1.5696,\n",
      "        1.5998, 1.5053, 1.5695, 1.5046, 1.4315, 1.4927, 1.7001, 1.5625, 1.4244,\n",
      "        1.3825, 1.3145, 1.5076, 1.5389, 1.5490, 1.4521, 1.4387, 1.4519, 1.4844,\n",
      "        1.4565, 1.4971, 1.4035, 1.5016, 1.4489, 1.4327, 1.5005, 1.8010, 1.4406,\n",
      "        1.5980, 1.5243, 1.4550, 1.4112, 1.4501, 1.4538, 1.5796, 1.4548, 1.6206,\n",
      "        1.5958, 1.5160, 1.4537, 1.3340, 1.4609, 1.4703, 1.4276, 1.4735, 1.5058,\n",
      "        1.5631, 1.4336, 1.5360, 1.5922, 1.5253, 1.4387, 1.3703, 1.3609, 1.5956,\n",
      "        1.4412, 1.5321, 1.5645, 1.4062, 1.5134, 1.4783, 1.2997, 1.5408, 1.6227,\n",
      "        1.3970, 1.4782, 1.3789, 1.4151, 1.3146, 1.4456, 1.5722, 1.4935, 1.4081,\n",
      "        1.6215, 1.4184, 1.4402, 1.4856, 1.5544, 1.5291, 1.4436, 1.4682, 1.6035,\n",
      "        1.5418, 1.4652, 1.4597, 1.5233, 1.5839, 1.4628, 1.4946, 1.4858, 1.3861,\n",
      "        1.3928, 1.4512, 1.4620, 1.6742, 1.4592, 1.5305, 1.4766, 1.4869, 1.4565,\n",
      "        1.2850, 1.5033, 1.3575, 1.5451, 1.3872, 1.4509, 1.5233, 1.4467, 1.4184,\n",
      "        1.4185, 1.4106, 1.3943, 1.4288, 1.4935, 1.4826, 1.7352, 1.5121, 1.4623,\n",
      "        1.3521, 1.4324, 1.4864, 1.5864, 1.5271, 1.5507, 1.5103, 1.3882, 1.4910,\n",
      "        1.4856, 1.4616, 1.4890, 1.5005, 1.4750, 1.4226, 1.4069, 1.4865, 1.4920,\n",
      "        1.4959, 1.5237, 1.5382, 1.5554, 1.4507, 1.5220, 1.4799, 1.3936, 1.4417,\n",
      "        1.4713, 1.3843, 1.6368, 1.5296, 1.4537, 1.4887, 1.5235, 1.3477, 1.3948,\n",
      "        1.4150, 1.3907, 1.4642, 2.2830, 1.4287, 1.4877, 1.4324, 1.4421, 1.4250,\n",
      "        1.4568, 1.4854, 1.4586, 1.4920, 1.3931, 1.4875, 1.4075, 1.4117, 1.5979,\n",
      "        1.4611, 1.5325, 1.3526, 1.5216, 1.3862, 1.4600, 1.4651, 1.4202, 1.5174,\n",
      "        1.4706, 1.4642, 1.4751, 1.5125, 1.4629, 1.5553, 1.5431, 1.4479, 1.3771,\n",
      "        1.3374, 1.4546, 1.4461, 1.4929, 1.5388, 1.5058, 1.4566, 1.4710, 1.4280,\n",
      "        1.4451, 1.5151, 1.4833, 1.2989, 1.4596, 1.5329, 1.5531, 1.3593, 1.5034,\n",
      "        1.5055, 1.4199, 1.3569, 1.3553, 1.4771, 1.4486, 1.4853, 1.4104, 1.5438,\n",
      "        1.4865, 1.5443, 1.4281, 1.5061, 1.5819, 1.3899, 1.3341, 1.5003, 1.4953,\n",
      "        1.4423, 1.4588, 1.5079, 1.6188, 1.5684, 1.5438, 1.5535, 1.4586, 1.4856,\n",
      "        1.4503, 1.4786, 1.6876, 1.4415, 1.5083, 1.5201, 1.4955, 1.4365, 1.4477,\n",
      "        1.4546, 1.4780, 1.5449, 1.3886, 1.5185, 1.4341, 1.4793, 1.4304, 1.4436,\n",
      "        1.2968, 1.5297, 1.5393, 1.5855, 1.3822, 1.4893, 1.4967, 1.5099, 1.5831,\n",
      "        1.4870, 1.4372, 1.4805, 1.4345, 1.5428, 1.4000, 1.3519, 1.4385, 1.4646,\n",
      "        1.5905, 1.5174, 1.4593, 1.3578, 1.4713, 1.5128, 1.4325, 1.5718, 1.4143,\n",
      "        1.5713, 1.5181, 1.4250, 1.4072, 2.1116], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1897,  0.1444, -0.0601,  ...,  0.1208, -0.0411,  0.1693],\n",
      "        [ 0.1106, -0.1279,  0.0166,  ...,  0.0435,  0.0459, -0.0275],\n",
      "        [-0.0351, -0.0751, -0.0027,  ...,  0.0003, -0.1235, -0.0907],\n",
      "        ...,\n",
      "        [ 0.0315, -0.0510,  0.0915,  ..., -0.0235,  0.0670,  0.0321],\n",
      "        [-0.0569,  0.0056,  0.0759,  ..., -0.1676, -0.0737, -0.0191],\n",
      "        [-0.1071, -0.0135,  0.1023,  ...,  0.0200,  0.1014,  0.0301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-1.9415e-02, -3.0802e-02,  3.7045e-02,  ...,  1.2388e-01,\n",
      "          5.8074e-02,  3.7667e-02],\n",
      "        [ 2.2984e-02,  1.1339e-01,  3.0278e-03,  ...,  9.6700e-02,\n",
      "          8.4788e-02,  1.1016e-01],\n",
      "        [ 2.8255e-02, -2.6010e-02,  9.8865e-03,  ..., -1.2348e-02,\n",
      "         -4.1980e-02,  1.8962e-02],\n",
      "        ...,\n",
      "        [-1.3478e-01, -1.8110e-01,  1.9137e-01,  ...,  2.2181e-02,\n",
      "         -8.2965e-02, -1.8213e-02],\n",
      "        [ 3.8945e-02,  1.4990e-01, -3.3312e-01,  ...,  1.3039e-01,\n",
      "          1.4722e-01, -6.6580e-02],\n",
      "        [-1.3467e-01, -1.4184e-01,  4.9619e-02,  ...,  3.7615e-02,\n",
      "          7.1831e-02,  2.6942e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6495, 2.6144, 2.0923, 1.2663, 2.4449, 4.1128, 4.0464, 4.0822, 1.6418,\n",
      "        1.7398, 2.3901, 2.2299, 1.8270, 4.0006, 3.8445, 3.9535, 1.6899, 1.9972,\n",
      "        2.5525, 2.4626, 2.4197, 2.8368, 2.4949, 2.8939, 2.1522, 2.0523, 1.9412,\n",
      "        1.9591, 1.9815, 2.1155, 2.6463, 3.0366, 2.0924, 1.9519, 1.9135, 1.3717,\n",
      "        1.9336, 2.9369, 2.5860, 2.5326, 2.3171, 1.6125, 1.7838, 2.2947, 2.6071,\n",
      "        1.9390, 2.4219, 2.3957, 1.2522, 1.4052, 2.0674, 1.9370, 1.6648, 2.5130,\n",
      "        2.3807, 2.6832, 1.7377, 2.3225, 1.9634, 1.8986, 1.9682, 1.9425, 2.2501,\n",
      "        2.6913, 2.1484, 2.1123, 1.7639, 2.0286, 2.1186, 2.2937, 2.7662, 3.8656,\n",
      "        2.4026, 2.2081, 2.1102, 1.9174, 1.7959, 2.3483, 2.9658, 2.7738, 1.9227,\n",
      "        2.1042, 2.0690, 1.9647, 2.0851, 2.3690, 2.5679, 2.0241, 1.8270, 1.6958,\n",
      "        2.2732, 2.1671, 2.0249, 2.0650, 2.6504, 2.8148, 2.7256, 2.0201, 2.3212,\n",
      "        2.3504, 2.1281, 2.4978, 2.7551, 2.7805, 1.6057, 2.2123, 1.8004, 2.3013,\n",
      "        2.6184, 3.3338, 2.7687, 3.1015, 3.1543, 1.7773, 2.4207, 2.0360, 2.1830,\n",
      "        2.8891, 2.4851, 2.5311, 1.7617, 2.2906, 1.6699, 2.4805, 2.7635, 2.4349,\n",
      "        2.8071, 2.8553, 1.2718, 1.8109, 1.7713, 2.0664, 2.2396, 2.5660, 2.2657,\n",
      "        2.8725, 2.3810, 1.7232, 2.2653, 1.7358, 1.9742, 2.1835, 2.4735, 3.1659,\n",
      "        2.2275, 1.5316, 1.7490, 2.3351, 2.7477, 2.5349, 2.5800, 2.5983, 1.8945,\n",
      "        1.4549, 1.8616, 1.7386, 2.0547, 2.0583, 2.5462, 2.6193, 1.9031, 2.0368,\n",
      "        2.1479, 2.2566, 2.0079, 1.7242, 2.6139, 2.8391, 1.8368, 2.0845, 2.4292,\n",
      "        2.0815, 2.6271, 2.4523, 2.6958, 1.9641, 2.0994, 2.4425, 1.9681, 2.6334,\n",
      "        3.3529, 3.3632, 2.8961, 3.0801, 2.4249, 2.1216, 2.4391, 2.2884, 2.2374,\n",
      "        2.7303, 2.7596, 2.9757, 1.9266, 1.6893, 1.9801, 1.6159, 2.1873, 3.1525,\n",
      "        2.7997, 2.9109, 2.6979, 2.2458, 2.1388, 2.7122, 2.8330, 2.3681, 2.5511,\n",
      "        2.6233, 2.3600, 2.1750, 2.2240, 2.1401, 1.9611, 2.6368, 2.8951, 2.7490,\n",
      "        2.0636, 2.1809, 2.0105, 2.2464, 2.3964, 2.6847, 2.9179, 2.9543, 1.2775,\n",
      "        1.4900, 2.3056, 1.6133, 2.3192, 1.7255, 2.2733, 2.2277, 1.6507, 1.6870,\n",
      "        1.3529, 2.0694, 1.6068, 2.3528, 2.1011, 2.3101, 2.1997, 1.8327, 1.8519,\n",
      "        1.8385, 1.7826, 1.8731, 2.4475, 2.6061, 1.9085, 1.7990, 2.0775, 2.4626,\n",
      "        2.1533, 2.2617, 2.4516, 2.3303, 1.9336, 2.2999, 2.7084, 2.2813, 2.1295,\n",
      "        2.7011, 2.8912, 3.2627, 2.6091, 2.0597, 1.8857, 2.2190, 2.5810, 3.0999,\n",
      "        3.1366, 3.6367, 2.1977, 2.1439, 2.1812, 2.0905, 1.8529, 2.2315, 3.7129,\n",
      "        3.8967, 2.2103, 2.2557, 2.6557, 2.3096, 2.0927, 2.0087, 3.6656, 3.6867,\n",
      "        2.9027, 1.8847, 1.6723, 2.2194, 2.2961, 2.3742, 2.8923, 2.8787, 1.7463,\n",
      "        1.7686, 2.1835, 1.9271, 1.9526, 2.7359, 2.7723, 3.0096, 2.0436, 1.9872,\n",
      "        2.2731, 2.8552, 2.3956, 2.3333, 3.2278, 2.8162, 2.2047, 2.0217, 2.1248,\n",
      "        1.9222, 2.8409, 2.7430, 2.7870, 2.3523], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0500,  0.0367, -0.1410,  ...,  0.1743, -0.0096,  0.2134],\n",
      "        [ 0.0673, -0.0120, -0.0263,  ...,  0.1661, -0.0988, -0.1892],\n",
      "        [-0.0827, -0.0236,  0.0409,  ..., -0.0563, -0.0041,  0.1286],\n",
      "        ...,\n",
      "        [ 0.0416, -0.1141, -0.0563,  ..., -0.0755, -0.1019, -0.0351],\n",
      "        [-0.0106,  0.0886,  0.1031,  ..., -0.1101,  0.0183, -0.0582],\n",
      "        [ 0.2231, -0.0744,  0.1027,  ..., -0.0649, -0.0113, -0.0082]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141, -0.0454,  0.0057,  ..., -0.0188,  0.0038,  0.0191],\n",
      "        [ 0.0571,  0.0436, -0.0234,  ...,  0.0247, -0.1909, -0.2112],\n",
      "        [ 0.0445,  0.0218,  0.1265,  ..., -0.0322, -0.0156, -0.1175],\n",
      "        ...,\n",
      "        [-0.0849, -0.1355,  0.1477,  ...,  0.0361,  0.0411,  0.1307],\n",
      "        [-0.1064, -0.1564, -0.2834,  ..., -0.1471, -0.0404,  0.2403],\n",
      "        [ 0.0911, -0.0271,  0.0647,  ...,  0.0427,  0.2251,  0.1965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0793, 2.5442, 2.3540, 1.9580, 2.0947, 2.9668, 3.0822, 3.2043, 2.0321,\n",
      "        1.8978, 2.1931, 1.7006, 1.8006, 2.9096, 2.9934, 3.1883, 1.7706, 1.6959,\n",
      "        1.7769, 1.4557, 1.4466, 1.5502, 2.4281, 2.1244, 1.8274, 2.3831, 2.1472,\n",
      "        2.7797, 2.7033, 2.7936, 2.1874, 2.0652, 2.2348, 1.7275, 2.1146, 2.4233,\n",
      "        1.8946, 1.4516, 2.4257, 1.8548, 2.1352, 1.9024, 1.8633, 1.5362, 1.6844,\n",
      "        2.5594, 2.3211, 2.1430, 1.2303, 2.1816, 1.9798, 1.3834, 2.5260, 1.4670,\n",
      "        2.2720, 1.9820, 1.5280, 1.3478, 1.5541, 2.1650, 1.2909, 2.4384, 2.0820,\n",
      "        2.0018, 2.0357, 2.1039, 1.9218, 2.1927, 1.7991, 1.9380, 2.2556, 2.1697,\n",
      "        2.2817, 2.2250, 1.9968, 1.7323, 1.9625, 1.9532, 2.3242, 1.8753, 1.7879,\n",
      "        2.1644, 2.4467, 2.0333, 2.0129, 1.7532, 2.1370, 2.1353, 1.8837, 1.5389,\n",
      "        1.9164, 2.0265, 1.9623, 2.4082, 2.4402, 2.5236, 1.8690, 1.8526, 1.9993,\n",
      "        2.8850, 3.0240, 2.9510, 2.7143, 2.7029, 2.3952, 2.2387, 2.0162, 1.8027,\n",
      "        1.5500, 1.6638, 2.5698, 2.1683, 2.0655, 1.8332, 2.2394, 2.8401, 3.2795,\n",
      "        1.6035, 2.2678, 2.4907, 2.4999, 2.0698, 1.5869, 1.5862, 1.3818, 2.7155,\n",
      "        2.3508, 2.1278, 1.8412, 1.7707, 2.1987, 1.5185, 1.6337, 1.8006, 2.2282,\n",
      "        2.1152, 1.2382, 1.7002, 1.7470, 2.0186, 2.0072, 2.1672, 2.3772, 2.1212,\n",
      "        2.1688, 1.8210, 2.2809, 1.4070, 1.3524, 1.6107, 2.2615, 2.0743, 1.8620,\n",
      "        1.5988, 1.6275, 2.4134, 2.7282, 2.2205, 2.3348, 1.9452, 1.9873, 2.0682,\n",
      "        2.4391, 2.0440, 2.3092, 2.2915, 2.1585, 2.5369, 1.5104, 2.1322, 2.2597,\n",
      "        2.3189, 1.9889, 2.1624, 2.4252, 2.1554, 2.2700, 2.0741, 1.9190, 1.9291,\n",
      "        1.6265, 1.6005, 2.7915, 2.7372, 2.2100, 2.3545, 2.2805, 2.4966, 2.3441,\n",
      "        2.5899, 2.6791, 2.6998, 2.1339, 1.7781, 2.0061, 2.1823, 2.4102, 1.5506,\n",
      "        2.1535, 2.1468, 1.9080, 2.0525, 1.9541, 1.3706, 1.3432, 2.1246, 2.5483,\n",
      "        2.3490, 2.0671, 2.2180, 2.1230, 2.3011, 2.0029, 1.9655, 2.6712, 2.7159,\n",
      "        2.1323, 2.1638, 2.0403, 1.9926, 1.7621, 1.9377, 2.6548, 2.7543, 1.4470,\n",
      "        1.7856, 1.3633, 2.3535, 1.2309, 2.3134, 2.1170, 2.1004, 1.6081, 1.5316,\n",
      "        2.4258, 1.3032, 2.4427, 1.3130, 1.9826, 1.8881, 1.9640, 1.8179, 1.7906,\n",
      "        2.1343, 2.4397, 2.5613, 2.2650, 2.1448, 2.0950, 1.7971, 1.9852, 1.5358,\n",
      "        1.3759, 1.4370, 2.3595, 1.9340, 1.9675, 1.9765, 1.7728, 1.7976, 2.8042,\n",
      "        2.3958, 2.6748, 2.1281, 2.2027, 2.0739, 2.3718, 2.4037, 1.5837, 2.2386,\n",
      "        2.6574, 2.1748, 2.0852, 2.1502, 2.0357, 2.1396, 1.9317, 2.2147, 3.0343,\n",
      "        3.0321, 2.3202, 2.3938, 2.5252, 2.1948, 2.0657, 1.9904, 3.0063, 3.0462,\n",
      "        2.0403, 1.9066, 2.4615, 1.7370, 1.3764, 2.3821, 2.3170, 2.2560, 2.1020,\n",
      "        1.7692, 1.5508, 1.6553, 2.5132, 1.6593, 2.2081, 2.2577, 2.2588, 1.8993,\n",
      "        2.1746, 2.2868, 1.9393, 2.3400, 2.3158, 2.7784, 1.7887, 2.0533, 2.1199,\n",
      "        2.2876, 2.3033, 2.2717, 2.8608, 2.3375], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0288, -0.0201, -0.0456,  ...,  0.1469, -0.0759, -0.0684],\n",
      "        [-0.0304, -0.0814,  0.0367,  ..., -0.0041,  0.0160,  0.0169],\n",
      "        [-0.0677,  0.1499,  0.0201,  ..., -0.0365, -0.1146, -0.0570],\n",
      "        ...,\n",
      "        [-0.0368, -0.0433,  0.0684,  ..., -0.0629, -0.0465,  0.0074],\n",
      "        [-0.0696, -0.0594,  0.0368,  ...,  0.0569, -0.0507,  0.0033],\n",
      "        [-0.0476,  0.0658, -0.0835,  ..., -0.0380,  0.0564, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.1009,  0.0422,  0.0744,  ..., -0.0690,  0.0039,  0.0293],\n",
      "        [ 0.0631, -0.2107, -0.0640,  ..., -0.0489, -0.0206,  0.0946],\n",
      "        [ 0.1240,  0.1947, -0.0428,  ...,  0.0176,  0.2068,  0.0217],\n",
      "        ...,\n",
      "        [ 0.0440,  0.0606, -0.0811,  ...,  0.0092,  0.0140, -0.0459],\n",
      "        [-0.1132,  0.0147,  0.0120,  ..., -0.0290,  0.0403,  0.0715],\n",
      "        [ 0.1267, -0.0766, -0.0246,  ..., -0.0502, -0.0462,  0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5237, 1.6380, 1.5909, 1.5373, 1.6410, 1.6303, 1.5066, 1.5455, 1.5264,\n",
      "        1.5900, 1.5219, 1.5666, 1.5115, 1.5389, 1.5558, 1.5440, 1.1689, 1.2181,\n",
      "        1.2120, 1.1211, 1.2291, 1.1781, 1.2347, 1.1810, 1.2587, 1.1774, 1.2028,\n",
      "        1.1693, 1.1764, 1.1862, 1.1704, 1.2264, 1.6736, 1.4945, 1.6123, 1.6292,\n",
      "        1.6166, 1.5468, 1.6582, 1.6193, 1.6017, 1.5828, 1.6127, 1.6441, 1.6247,\n",
      "        1.6412, 1.6417, 1.6128, 1.2785, 1.1570, 1.2661, 1.2270, 1.1938, 1.2148,\n",
      "        1.2249, 1.2382, 1.2601, 1.2769, 1.1751, 1.2436, 1.2156, 1.2205, 1.2555,\n",
      "        1.2170, 1.8320, 1.8457, 1.8351, 1.7394, 1.8790, 1.8049, 1.8405, 1.9301,\n",
      "        1.8848, 1.9098, 1.8283, 1.9672, 1.8562, 1.9477, 1.8806, 1.9459, 1.7929,\n",
      "        1.7863, 1.8406, 1.7410, 1.7440, 1.8983, 1.7523, 1.7559, 1.8501, 1.6784,\n",
      "        1.7680, 1.8315, 1.7797, 1.8618, 1.8402, 1.8362, 1.6506, 1.7759, 1.6428,\n",
      "        1.6796, 1.7780, 1.6657, 1.6824, 1.7052, 1.7319, 1.6564, 1.6668, 1.6512,\n",
      "        1.6643, 1.7300, 1.6339, 1.6688, 1.6180, 1.5663, 1.6039, 1.5467, 1.5501,\n",
      "        1.5765, 1.6244, 1.5800, 1.5934, 1.5740, 1.5941, 1.5254, 1.6167, 1.5758,\n",
      "        1.5669, 1.6327, 1.2452, 1.1876, 1.2376, 1.2357, 1.2244, 1.1834, 1.1787,\n",
      "        1.1754, 1.2060, 1.1574, 1.1616, 1.1757, 1.2127, 1.1926, 1.1651, 1.2091,\n",
      "        1.5834, 1.6092, 1.5880, 1.5917, 1.5367, 1.5804, 1.5876, 1.5490, 1.5769,\n",
      "        1.6101, 1.6322, 1.6483, 1.5669, 1.6292, 1.6192, 1.6547, 1.7856, 1.8543,\n",
      "        1.7574, 1.8590, 1.8113, 1.7640, 1.7554, 1.8429, 1.7273, 1.8047, 1.6980,\n",
      "        1.8044, 1.6846, 1.8038, 1.6839, 1.7869, 1.7251, 1.7426, 1.6602, 1.6381,\n",
      "        1.7714, 1.7228, 1.7506, 1.8162, 1.7294, 1.7619, 1.7026, 1.6313, 1.7297,\n",
      "        1.6644, 1.7589, 1.6497, 1.5183, 1.5592, 1.4837, 1.5197, 1.5177, 1.5479,\n",
      "        1.5514, 1.5530, 1.5441, 1.5787, 1.6513, 1.5932, 1.6224, 1.5820, 1.5263,\n",
      "        1.5076, 1.8011, 1.8360, 1.7349, 1.7691, 1.8147, 1.8794, 1.7126, 1.7388,\n",
      "        1.6939, 1.8265, 1.8655, 1.7353, 1.7816, 1.7472, 1.8009, 1.7903, 1.3405,\n",
      "        1.3327, 1.2976, 1.3638, 1.3436, 1.4620, 1.3607, 1.3682, 1.4538, 1.4819,\n",
      "        1.3863, 1.3462, 1.3702, 1.3104, 1.3549, 1.3847, 1.6643, 1.6504, 1.6803,\n",
      "        1.6373, 1.5873, 1.6462, 1.6596, 1.6927, 1.6154, 1.6603, 1.6730, 1.6267,\n",
      "        1.7073, 1.6567, 1.7175, 1.6536, 1.2674, 1.2777, 1.2885, 1.2327, 1.2414,\n",
      "        1.2075, 1.2551, 1.2171, 1.2736, 1.2409, 1.2649, 1.1671, 1.2889, 1.2685,\n",
      "        1.2630, 1.2291, 1.8232, 1.6227, 1.6922, 1.6783, 1.6564, 1.6695, 1.6106,\n",
      "        1.8201, 1.5714, 1.6413, 1.7176, 1.7646, 1.7243, 1.6156, 1.7404, 1.7740,\n",
      "        1.3033, 1.2989, 1.2630, 1.2814, 1.2807, 1.3029, 1.3530, 1.2194, 1.2728,\n",
      "        1.3257, 1.2967, 1.3116, 1.2969, 1.2997, 1.2598, 1.3316, 1.4795, 1.5311,\n",
      "        1.5504, 1.4317, 1.5504, 1.5279, 1.4412, 1.4876, 1.5327, 1.4673, 1.5108,\n",
      "        1.4187, 1.4377, 1.5406, 1.4656, 1.6510], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0107,  0.0294, -0.0998,  ...,  0.0848, -0.0658,  0.2637],\n",
      "        [ 0.0298, -0.2141,  0.0243,  ...,  0.1150,  0.0791,  0.0398],\n",
      "        [ 0.1201, -0.0364, -0.0535,  ..., -0.0532, -0.0891,  0.1107],\n",
      "        ...,\n",
      "        [-0.0448,  0.1025,  0.0266,  ..., -0.0527, -0.0337, -0.0475],\n",
      "        [ 0.0427,  0.0838, -0.0343,  ..., -0.0642, -0.0347, -0.0093],\n",
      "        [-0.0318, -0.0439,  0.0419,  ...,  0.1552, -0.1432,  0.0283]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0852,  0.0132,  0.0308,  ..., -0.0494, -0.0430,  0.1691],\n",
      "        [-0.0361,  0.1662,  0.1051,  ..., -0.0820,  0.0795, -0.1699],\n",
      "        [ 0.0602,  0.0065,  0.0425,  ...,  0.0193, -0.0457,  0.0358],\n",
      "        ...,\n",
      "        [ 0.0220, -0.1076, -0.0351,  ...,  0.0743,  0.1189, -0.0309],\n",
      "        [ 0.0220, -0.1038,  0.0549,  ...,  0.0006, -0.0522,  0.0158],\n",
      "        [ 0.0709, -0.1168, -0.0378,  ..., -0.2090, -0.0591,  0.0829]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4847, 1.5854, 1.5281, 1.5112, 1.6205, 1.3933, 1.4293, 1.5220, 1.4344,\n",
      "        1.4322, 1.4330, 1.5356, 1.5584, 1.4024, 1.6372, 1.4345, 1.3345, 1.5364,\n",
      "        1.4364, 1.4626, 1.5345, 1.4153, 1.6026, 1.4375, 1.4696, 1.4832, 1.4734,\n",
      "        1.5345, 1.4947, 1.4880, 1.4454, 1.4259, 1.6303, 1.4785, 1.4093, 1.4467,\n",
      "        1.6177, 1.4142, 1.3959, 1.4692, 1.6344, 1.4974, 1.4720, 1.4923, 1.3762,\n",
      "        1.4122, 1.3920, 1.6090, 1.5128, 1.5003, 1.4581, 1.5089, 1.3121, 1.3747,\n",
      "        1.3892, 1.3606, 1.3440, 1.4343, 1.6283, 1.5520, 1.3893, 1.6759, 2.7844,\n",
      "        1.4561, 1.4414, 1.4795, 1.4250, 1.5668, 1.5379, 1.5108, 1.3348, 1.4412,\n",
      "        1.4062, 1.4302, 1.4888, 1.5939, 1.4416, 1.5165, 1.4689, 1.5040, 1.4802,\n",
      "        1.5338, 1.5007, 1.5733, 1.4101, 1.4544, 1.4441, 1.5479, 1.2997, 1.5089,\n",
      "        1.4162, 1.5413, 1.5944, 1.5202, 1.4379, 1.5832, 1.4857, 1.5958, 1.5407,\n",
      "        1.5192, 1.5830, 1.5442, 1.6174, 1.4657, 1.5545, 1.6074, 1.4277, 1.4065,\n",
      "        1.4201, 1.4251, 1.5887, 1.3608, 1.4386, 1.4492, 1.5470, 1.4966, 1.5836,\n",
      "        1.4753, 1.4542, 1.4340, 1.4909, 1.5836, 1.5379, 1.5224, 1.3440, 1.5710,\n",
      "        1.5226, 1.5317, 1.4819, 1.4899, 1.3567, 1.3921, 1.5082, 1.5816, 1.5643,\n",
      "        1.4018, 1.4542, 1.4932, 1.3423, 1.3792, 1.4234, 1.4151, 1.4774, 1.4146,\n",
      "        1.4055, 1.4890, 1.4656, 1.5486, 1.4744, 1.5527, 2.1154, 1.5793, 1.5607,\n",
      "        1.4953, 1.4764, 1.5145, 1.4657, 1.5617, 1.5575, 1.4825, 1.3502, 1.4525,\n",
      "        1.5726, 1.5779, 1.6583, 1.5692, 1.3804, 1.4436, 1.4081, 1.5196, 1.4325,\n",
      "        1.6387, 1.5334, 1.4450, 1.4886, 1.3273, 1.4974, 1.5533, 1.4687, 1.5539,\n",
      "        1.4422, 1.3911, 1.4604, 1.4915, 1.5441, 1.4027, 1.4639, 1.4745, 1.4918,\n",
      "        1.6241, 1.3499, 1.3536, 2.1478, 1.3057, 1.4600, 1.4698, 1.4978, 1.4253,\n",
      "        1.5231, 1.5591, 1.5982, 1.3092, 1.4334, 1.3613, 1.3678, 1.5513, 1.5309,\n",
      "        1.3400, 1.5387, 1.6581, 1.5580, 1.4863, 1.4336, 1.6080, 1.4570, 1.5346,\n",
      "        1.5540, 1.5155, 1.3515, 1.5483, 1.4277, 1.5748, 1.5334, 1.4417, 1.4742,\n",
      "        1.4410, 1.3977, 1.4992, 1.4367, 1.4513, 1.3513, 1.3961, 1.3730, 1.4724,\n",
      "        1.4606, 1.7224, 1.4556, 1.3458, 1.4811, 1.6254, 1.5191, 1.4462, 1.3226,\n",
      "        1.3853, 1.3522, 1.5353, 1.4177, 1.5309, 1.4781, 1.4179, 1.5149, 1.4159,\n",
      "        1.4979, 1.3967, 1.4373, 1.4593, 1.3772, 1.4508, 1.2212, 1.5037, 1.4556,\n",
      "        1.5011, 1.6198, 1.5268, 1.4054, 1.4593, 1.5355, 1.6802, 1.3931, 1.5964,\n",
      "        1.3429, 1.5394, 1.2852, 1.6051, 1.4286, 1.6176, 1.5341, 1.3497, 1.5703,\n",
      "        1.4810, 1.4809, 1.4689, 1.6007, 1.6133, 1.5608, 1.3981, 1.4171, 1.4585,\n",
      "        1.2427, 1.3542, 1.5318, 1.5489, 1.5062, 1.5158, 1.2936, 1.4616, 1.5901,\n",
      "        1.5646, 1.4796, 1.4929, 1.5041, 1.5503, 1.4835, 1.3523, 1.4823, 1.4329,\n",
      "        1.4578, 1.5483, 1.4357, 1.4534, 1.4646, 1.3408, 1.5548, 1.5341, 1.5688,\n",
      "        1.5005, 1.5777, 1.5705, 1.3821, 1.6774], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0948,  0.0598, -0.1138,  ..., -0.1099, -0.0847,  0.1816],\n",
      "        [ 0.1103, -0.1547, -0.0565,  ...,  0.1228, -0.0392, -0.0935],\n",
      "        [ 0.1795, -0.0441, -0.1028,  ..., -0.0675, -0.0260,  0.0195],\n",
      "        ...,\n",
      "        [-0.0545, -0.0109,  0.0828,  ..., -0.0864,  0.1112,  0.0080],\n",
      "        [ 0.0687,  0.0949,  0.0336,  ...,  0.0302, -0.0092, -0.0079],\n",
      "        [ 0.0093,  0.0754, -0.1319,  ...,  0.0140,  0.0235,  0.0426]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0682, -0.0174, -0.0538,  ..., -0.0117,  0.0471,  0.0644],\n",
      "        [-0.1031, -0.0088, -0.0234,  ..., -0.0370,  0.0062, -0.0071],\n",
      "        [ 0.0909, -0.0037, -0.1149,  ...,  0.0268, -0.0139,  0.0319],\n",
      "        ...,\n",
      "        [-0.1392, -0.0707,  0.0049,  ...,  0.0538, -0.0339,  0.0174],\n",
      "        [-0.0306, -0.0796, -0.1533,  ...,  0.0644, -0.0339, -0.0305],\n",
      "        [ 0.1289,  0.1313, -0.0153,  ..., -0.0077, -0.0347, -0.0208]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6552, 1.4334, 1.4333, 1.3825, 1.4168, 2.3624, 2.0710, 2.5610, 1.8070,\n",
      "        1.4649, 1.3482, 1.3601, 1.5074, 1.2783, 2.5928, 2.0364, 1.1000, 1.1304,\n",
      "        1.4394, 1.9744, 2.4227, 4.6512, 4.2595, 3.4382, 1.0111, 1.2298, 1.2887,\n",
      "        2.0301, 2.6273, 4.4372, 4.2133, 3.6077, 1.4863, 1.5198, 1.7810, 1.7904,\n",
      "        1.8810, 1.5664, 2.2300, 1.8756, 1.4852, 1.5696, 2.1647, 1.8271, 1.8696,\n",
      "        1.7276, 2.2294, 2.2891, 1.5805, 1.9161, 1.9421, 1.8423, 2.0988, 1.7557,\n",
      "        2.3040, 2.3112, 1.5298, 1.9603, 1.8596, 1.9622, 2.2476, 2.5642, 2.5681,\n",
      "        2.3505, 2.1444, 1.7776, 2.3910, 2.2577, 2.2775, 2.1773, 2.7792, 2.3904,\n",
      "        1.4821, 1.8719, 2.1337, 2.0356, 2.0097, 2.5207, 2.6754, 2.4400, 2.2276,\n",
      "        1.8581, 1.4236, 1.7332, 1.6358, 2.0797, 2.3105, 2.2091, 2.1493, 1.8424,\n",
      "        1.7077, 1.6115, 2.4484, 2.4536, 2.1115, 2.2423, 1.2833, 1.4388, 1.4090,\n",
      "        1.3854, 1.4278, 2.3062, 1.9912, 2.5803, 1.4052, 1.5144, 1.4000, 1.2931,\n",
      "        1.4113, 1.4254, 2.0248, 2.1956, 2.0416, 1.6295, 1.5176, 1.5757, 1.2364,\n",
      "        2.0242, 2.1945, 2.1233, 1.9329, 1.3427, 1.3791, 1.5646, 2.0294, 1.5717,\n",
      "        1.9742, 2.5319, 1.8474, 1.3917, 1.4993, 1.4128, 1.4397, 2.0798, 1.9736,\n",
      "        2.5238, 1.4915, 1.6802, 1.5838, 1.4165, 1.9504, 1.6635, 2.0834, 2.4967,\n",
      "        1.7435, 1.6351, 2.1095, 1.9533, 2.2135, 1.8848, 2.4154, 2.1751, 2.4308,\n",
      "        1.9679, 1.7467, 2.2406, 1.7439, 2.4684, 2.5535, 2.6872, 1.9586, 1.7359,\n",
      "        2.0742, 2.0076, 2.4434, 2.5920, 2.9749, 2.8062, 1.6446, 1.9224, 1.9818,\n",
      "        2.2306, 2.0848, 2.5432, 2.7834, 2.7860, 2.0137, 1.5935, 1.7468, 2.1187,\n",
      "        1.7261, 1.9407, 2.3628, 2.5211, 1.7424, 1.7850, 2.0001, 1.8485, 1.7472,\n",
      "        1.7776, 2.2005, 2.4120, 1.5175, 1.5551, 1.4119, 1.4831, 1.8009, 2.2293,\n",
      "        2.0382, 2.7584, 1.6488, 1.5253, 1.7321, 1.6944, 1.7926, 1.7162, 1.9397,\n",
      "        2.3349, 1.1478, 1.1457, 1.4768, 1.7365, 2.5167, 3.6357, 3.7704, 3.2782,\n",
      "        0.8232, 1.1941, 1.2854, 2.0714, 2.2399, 3.7320, 3.5234, 3.3439, 1.6643,\n",
      "        1.9483, 1.9040, 2.0350, 1.9639, 2.1692, 2.4454, 2.3579, 1.9448, 1.8434,\n",
      "        1.9090, 2.0098, 1.9273, 1.9125, 2.1907, 2.4258, 1.8691, 1.4603, 1.7656,\n",
      "        1.8131, 1.9404, 1.6375, 2.3291, 2.0268, 1.4493, 1.7165, 1.8529, 1.7386,\n",
      "        1.9589, 1.6711, 2.3684, 2.3801, 1.3841, 1.3492, 1.7807, 1.7038, 1.6484,\n",
      "        1.7553, 2.4681, 2.1278, 1.3672, 1.5706, 1.7322, 1.7799, 1.6928, 1.9181,\n",
      "        2.0598, 2.0556, 1.4114, 1.4788, 1.8002, 1.6682, 1.6220, 1.8274, 2.2851,\n",
      "        2.3713, 1.4840, 1.4003, 1.4672, 1.6401, 1.5482, 1.7767, 2.5248, 2.5006,\n",
      "        1.8956, 1.7507, 1.7005, 1.4926, 1.6595, 1.8658, 2.3257, 2.2957, 1.6053,\n",
      "        1.4602, 1.6022, 1.8346, 1.5352, 1.9717, 2.2161, 2.4296, 1.5886, 1.1851,\n",
      "        1.2203, 1.4174, 1.3647, 1.8373, 2.0729, 2.3338, 1.4978, 1.3226, 1.4443,\n",
      "        1.6841, 1.6486, 1.7464, 1.9548, 2.0678], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1013,  0.0492,  0.0783,  ...,  0.0711,  0.1158, -0.0316],\n",
      "        [ 0.0913, -0.0953, -0.0129,  ...,  0.0360,  0.0673, -0.0089],\n",
      "        [ 0.0834, -0.0333,  0.0348,  ...,  0.1749,  0.0360, -0.1717],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0785,  0.0348,  ..., -0.0383,  0.0202, -0.0574],\n",
      "        [-0.0494,  0.1110, -0.0394,  ..., -0.0033, -0.0134, -0.0389],\n",
      "        [ 0.1508, -0.1824, -0.0611,  ...,  0.0301,  0.0138, -0.0965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-2.2536e-03,  1.6058e-02, -5.2478e-02,  ...,  2.3720e-02,\n",
      "          4.3420e-02,  3.8817e-02],\n",
      "        [-4.3882e-02, -9.3136e-03, -5.4000e-02,  ..., -2.1725e-02,\n",
      "         -5.7059e-03,  2.7946e-02],\n",
      "        [-3.8626e-02,  9.5391e-02, -4.3173e-02,  ...,  2.4294e-02,\n",
      "          3.1823e-02,  1.7045e-02],\n",
      "        ...,\n",
      "        [ 1.5301e-03, -2.2620e-02,  2.9410e-02,  ...,  7.5472e-02,\n",
      "         -1.1191e-02, -2.1986e-02],\n",
      "        [-1.9202e-01, -2.1342e-02, -8.4856e-02,  ...,  3.0970e-02,\n",
      "         -2.9097e-05, -5.5118e-02],\n",
      "        [ 2.5200e-01, -2.2403e-02, -1.2297e-02,  ...,  2.3491e-02,\n",
      "         -7.7852e-03, -2.0245e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6889, 1.4271, 1.7430, 1.6628, 1.6534, 1.2899, 1.8223, 1.7646, 1.8019,\n",
      "        1.7449, 1.6390, 1.7138, 1.6659, 2.3107, 1.6661, 1.7704, 1.1473, 1.3915,\n",
      "        1.3758, 2.0068, 1.9256, 3.2060, 2.9245, 2.7808, 1.1138, 0.9388, 1.4358,\n",
      "        1.8676, 2.2222, 3.0254, 3.0221, 2.7178, 1.4727, 1.5844, 1.9981, 1.8033,\n",
      "        1.9005, 2.2584, 1.9988, 1.9343, 1.4264, 1.4540, 1.8902, 1.8841, 1.8554,\n",
      "        1.6541, 2.1052, 2.1458, 1.6813, 2.0848, 1.8382, 1.7060, 1.8021, 2.2735,\n",
      "        2.2901, 2.0180, 1.4035, 1.6622, 1.9498, 1.9802, 1.8883, 1.5377, 2.1773,\n",
      "        2.0368, 1.8880, 1.7127, 2.0528, 2.0940, 1.8528, 2.0759, 2.4393, 2.1081,\n",
      "        1.7533, 1.9557, 2.2812, 2.0530, 2.0005, 1.8960, 2.5726, 2.3136, 1.7890,\n",
      "        1.8889, 1.7278, 1.7179, 2.0000, 1.9749, 2.0090, 2.0776, 2.2909, 1.8528,\n",
      "        1.9292, 1.8684, 1.3181, 1.4653, 2.0984, 1.9083, 1.4736, 1.5217, 1.7262,\n",
      "        1.9033, 1.4419, 1.2665, 1.8880, 1.5187, 1.3202, 1.7243, 1.6979, 1.5558,\n",
      "        1.7915, 1.9450, 1.8671, 1.6432, 1.8415, 1.6661, 1.7453, 1.7020, 1.9645,\n",
      "        1.4424, 1.9818, 2.0917, 2.0265, 1.7816, 1.7390, 1.8201, 1.3807, 2.2870,\n",
      "        2.1284, 1.7866, 1.8202, 1.6078, 1.6488, 1.6817, 1.8529, 1.3601, 1.9022,\n",
      "        1.8078, 1.4124, 1.6446, 1.8692, 1.7385, 1.3284, 1.9982, 2.0476, 1.8216,\n",
      "        2.0397, 1.7566, 1.9442, 2.0140, 1.7768, 2.1294, 1.9944, 1.9886, 1.5791,\n",
      "        1.7224, 1.7992, 1.9633, 2.0390, 1.8915, 2.2578, 2.2700, 1.9004, 1.8885,\n",
      "        1.8815, 2.0661, 2.0135, 1.9570, 2.7777, 2.6218, 1.6407, 1.7246, 2.1530,\n",
      "        2.0494, 2.0387, 2.2814, 2.5121, 2.3586, 1.7866, 1.5010, 1.9422, 1.9570,\n",
      "        1.9640, 2.0323, 2.4325, 2.4010, 1.8795, 1.8733, 1.7822, 2.0121, 1.7464,\n",
      "        1.7709, 2.1043, 2.2803, 1.7662, 1.6333, 1.7373, 1.8647, 1.5324, 1.2077,\n",
      "        1.9634, 1.8604, 1.9159, 1.7432, 1.5445, 1.3137, 1.3502, 2.2007, 1.9211,\n",
      "        1.9771, 1.1066, 1.2730, 1.5260, 1.9362, 2.1939, 2.7678, 2.8119, 2.5205,\n",
      "        0.8824, 1.1725, 1.1172, 1.7185, 1.8117, 2.8365, 2.5728, 2.6282, 1.7312,\n",
      "        1.8346, 1.9135, 1.8828, 1.6636, 1.7983, 2.2877, 2.2648, 1.7312, 1.8848,\n",
      "        1.8329, 2.0321, 1.8790, 1.7116, 1.9831, 2.1841, 1.7704, 1.5616, 1.7671,\n",
      "        1.8773, 1.8690, 1.9465, 2.1781, 1.9954, 1.3467, 1.6612, 1.8440, 1.7460,\n",
      "        1.9353, 1.8570, 2.2530, 2.3184, 1.2619, 1.5797, 1.8240, 1.8519, 1.7561,\n",
      "        1.8521, 1.9934, 2.1386, 1.5619, 1.4646, 1.7888, 1.8055, 1.9259, 1.7539,\n",
      "        2.5143, 1.9833, 1.2787, 1.3773, 1.7145, 1.7669, 1.6839, 1.7099, 2.1194,\n",
      "        1.8643, 1.0018, 1.3337, 1.6767, 1.9264, 1.7412, 1.7326, 1.9674, 1.9704,\n",
      "        1.8140, 1.7674, 1.7366, 1.7669, 1.7734, 1.7101, 2.3058, 2.0764, 1.6758,\n",
      "        1.6545, 1.7996, 1.8239, 1.7054, 1.6117, 2.1010, 2.2289, 1.4085, 1.5210,\n",
      "        1.4740, 1.3472, 1.8246, 1.2400, 1.4690, 1.5208, 1.7546, 1.4491, 1.4017,\n",
      "        1.2756, 1.1156, 1.2434, 1.6302, 1.5776], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.3299,  0.0627,  0.0467,  ..., -0.0733,  0.0088,  0.0062],\n",
      "        [ 0.0204,  0.1075,  0.0079,  ...,  0.0863, -0.0849,  0.0835],\n",
      "        [-0.0494, -0.0335, -0.3117,  ...,  0.0966, -0.0074,  0.0293],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0343, -0.0824,  ..., -0.0599,  0.1646,  0.0819],\n",
      "        [-0.0434, -0.0323,  0.0117,  ...,  0.0464, -0.1104,  0.1330],\n",
      "        [ 0.0307,  0.0402, -0.0509,  ..., -0.0433,  0.0360,  0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0920,  0.1429,  0.1498,  ...,  0.0309,  0.2395, -0.0015],\n",
      "        [-0.1530, -0.0796,  0.0126,  ..., -0.0116,  0.0841, -0.0048],\n",
      "        [-0.0298,  0.0040, -0.0071,  ...,  0.1056,  0.0821,  0.0290],\n",
      "        ...,\n",
      "        [-0.1107, -0.0566, -0.0015,  ..., -0.0197, -0.0874,  0.0378],\n",
      "        [-0.0951,  0.1062, -0.0328,  ..., -0.0955, -0.0743, -0.0265],\n",
      "        [-0.1280,  0.0642,  0.0136,  ...,  0.0097, -0.1531, -0.1130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9910, 1.7859, 1.7181, 1.8752, 1.9573, 1.7906, 1.7669, 1.8068, 1.7691,\n",
      "        1.9023, 1.7144, 1.8018, 1.7877, 1.8022, 1.9825, 1.8459, 1.5629, 1.4503,\n",
      "        1.5721, 1.4834, 1.5047, 1.4777, 1.4249, 1.4403, 1.4609, 1.4187, 1.4352,\n",
      "        1.4818, 1.5416, 1.4619, 1.4833, 1.4405, 1.7743, 1.8952, 1.8282, 1.7398,\n",
      "        1.8591, 1.7324, 1.8634, 1.6844, 1.8550, 1.9357, 1.7557, 1.7681, 1.7066,\n",
      "        1.7661, 1.8067, 1.8372, 1.7756, 1.7685, 2.2151, 1.9220, 1.8615, 1.8839,\n",
      "        1.8420, 1.8436, 1.9874, 1.9857, 1.7812, 1.8052, 2.0072, 2.1669, 1.8810,\n",
      "        1.8712, 2.1752, 2.0793, 1.7751, 2.3139, 1.8010, 2.3051, 1.7924, 1.9537,\n",
      "        1.9208, 1.9720, 1.9665, 1.9238, 1.8328, 1.9078, 2.0223, 1.9621, 1.7919,\n",
      "        1.9148, 1.7307, 1.7685, 1.7281, 1.7405, 1.7336, 1.7506, 1.8781, 1.7557,\n",
      "        1.7685, 1.6618, 1.7410, 1.7996, 1.7391, 1.7953, 1.7625, 1.8009, 1.8637,\n",
      "        1.8832, 1.8602, 1.8920, 1.7451, 1.9282, 1.9451, 1.8833, 1.9162, 1.8563,\n",
      "        1.9071, 1.9149, 1.9026, 1.8479, 1.8409, 1.8171, 1.7042, 1.8710, 1.8583,\n",
      "        1.8760, 1.7786, 1.8398, 1.7976, 1.8047, 1.8330, 1.7827, 1.8125, 1.7038,\n",
      "        1.7855, 1.8503, 1.8570, 1.8230, 1.7993, 1.6955, 1.8503, 1.7523, 1.7433,\n",
      "        1.9098, 1.7231, 1.8333, 1.7466, 1.7698, 1.7607, 1.8340, 1.9074, 1.8020,\n",
      "        1.6698, 1.6084, 1.7089, 1.6531, 1.7456, 1.6656, 1.7196, 1.6576, 1.7034,\n",
      "        1.6911, 1.7480, 1.7944, 1.7231, 1.7571, 1.6898, 1.7747, 1.8855, 1.8563,\n",
      "        1.8743, 1.8496, 2.0436, 2.0675, 1.8706, 1.8486, 1.9140, 1.8655, 1.9353,\n",
      "        1.9089, 1.9487, 1.8241, 1.9167, 1.9539, 1.9157, 2.0385, 1.8803, 1.9441,\n",
      "        2.0407, 1.8543, 1.9363, 1.9550, 1.9103, 1.9249, 1.9537, 1.8689, 1.9965,\n",
      "        1.9404, 1.9471, 1.9209, 1.3920, 1.4053, 1.3236, 1.4026, 1.2973, 1.3371,\n",
      "        1.4223, 1.3296, 1.3952, 1.3427, 1.5071, 1.4203, 1.4299, 1.4072, 1.3403,\n",
      "        1.3813, 1.5942, 1.5461, 1.4136, 1.5487, 1.4926, 1.5218, 1.5217, 1.4937,\n",
      "        1.4831, 1.3992, 1.4020, 1.5040, 1.4087, 1.4507, 1.4973, 1.4373, 2.1330,\n",
      "        1.9655, 2.0132, 1.9046, 2.1477, 1.9870, 1.8481, 1.9034, 1.8565, 1.9019,\n",
      "        1.8099, 2.1112, 2.1387, 1.9774, 1.9240, 1.8725, 1.7300, 1.7895, 1.7128,\n",
      "        1.7626, 1.7251, 1.7140, 1.8034, 1.7685, 1.7610, 1.7741, 1.8405, 1.6437,\n",
      "        1.7217, 1.7438, 1.7286, 1.8240, 1.8331, 1.7401, 1.8251, 1.7745, 1.7199,\n",
      "        1.6723, 1.7602, 1.8064, 1.7394, 1.7077, 1.7129, 1.8042, 1.8022, 1.7928,\n",
      "        1.6652, 1.7366, 1.8956, 1.9773, 1.8821, 2.0162, 1.8606, 1.8993, 1.9118,\n",
      "        1.8570, 1.7689, 1.8904, 1.8602, 1.8927, 1.9566, 2.0228, 1.9236, 1.9475,\n",
      "        1.9208, 1.8269, 1.8951, 1.8767, 1.9128, 2.0383, 1.8567, 1.9909, 1.8191,\n",
      "        1.9111, 1.9571, 1.8844, 1.9293, 1.9161, 1.8280, 1.9185, 1.7560, 1.7776,\n",
      "        1.8200, 1.6379, 2.0614, 1.7395, 1.6892, 1.8435, 1.8097, 1.7288, 1.8897,\n",
      "        1.6953, 1.7442, 1.7315, 1.7611, 1.8561], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.2656, -0.0917,  0.0655,  ..., -0.0740, -0.1863,  0.1651],\n",
      "        [-0.0688,  0.0296,  0.0274,  ..., -0.1426,  0.0641,  0.0861],\n",
      "        [-0.0090, -0.0202,  0.0656,  ...,  0.0323,  0.0653, -0.0444],\n",
      "        ...,\n",
      "        [-0.0234,  0.0651,  0.1240,  ..., -0.0122,  0.1006, -0.0528],\n",
      "        [ 0.0212, -0.0394, -0.0730,  ..., -0.0338, -0.0896, -0.0396],\n",
      "        [-0.1534, -0.0358,  0.0791,  ...,  0.0288,  0.0590, -0.0004]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.0284e-02,  3.0753e-03, -3.8465e-02,  ...,  5.6222e-02,\n",
      "          8.0556e-02,  4.7121e-02],\n",
      "        [-6.1613e-02, -1.4256e-01, -2.7052e-02,  ..., -1.4203e-02,\n",
      "         -1.4489e-02,  8.0996e-02],\n",
      "        [ 3.5126e-02, -8.7188e-02,  3.3745e-03,  ...,  5.1317e-02,\n",
      "         -5.1557e-02, -1.4658e-02],\n",
      "        ...,\n",
      "        [-1.0158e-02,  2.5906e-02,  6.1291e-03,  ..., -7.0126e-02,\n",
      "          6.1601e-02,  1.5138e-04],\n",
      "        [ 1.0797e-02, -1.5308e-01, -1.5853e-01,  ...,  1.2545e-01,\n",
      "         -9.3582e-02, -5.8525e-02],\n",
      "        [-1.5178e-03, -7.7993e-02,  2.2972e-02,  ..., -6.3244e-03,\n",
      "         -2.8605e-02,  1.5632e-01]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5955, 1.7774, 1.6411, 1.8565, 1.7713, 1.6490, 1.6777, 1.5707, 1.7964,\n",
      "        1.8246, 1.3758, 1.5962, 1.7137, 1.6887, 1.7496, 1.6856, 1.8570, 1.7961,\n",
      "        1.6562, 1.8484, 1.8222, 1.6783, 1.7030, 1.5851, 1.6452, 1.5345, 1.5934,\n",
      "        1.7014, 1.7638, 1.8334, 1.6760, 1.4294, 1.6589, 1.4353, 1.6987, 1.7667,\n",
      "        1.6623, 1.6831, 1.5934, 1.6539, 1.6826, 1.6921, 1.7623, 1.6043, 1.7830,\n",
      "        1.5819, 1.6015, 1.5426, 1.6916, 1.7561, 1.7366, 1.7634, 1.4993, 1.6625,\n",
      "        1.5742, 1.6977, 1.5489, 1.7028, 1.7906, 1.6264, 1.7208, 1.7474, 5.9713,\n",
      "        1.7458, 1.7300, 1.7038, 1.5947, 1.7354, 1.7214, 1.8418, 1.7113, 1.8144,\n",
      "        1.5569, 1.7767, 1.6403, 1.7845, 1.7405, 1.6785, 1.7531, 1.7531, 1.5310,\n",
      "        1.6492, 1.6948, 1.6424, 1.5773, 1.6855, 1.6548, 1.7195, 1.4733, 1.8451,\n",
      "        1.8094, 1.6022, 1.9097, 1.6485, 1.7164, 1.7464, 1.7125, 1.8256, 1.7713,\n",
      "        1.8616, 1.8821, 1.6654, 1.7550, 1.6193, 1.7847, 1.8291, 1.5213, 1.6521,\n",
      "        1.7260, 1.6569, 1.6522, 1.6711, 1.6025, 1.5468, 1.8519, 1.5736, 1.6658,\n",
      "        1.6979, 1.5867, 1.8101, 1.6108, 1.7021, 1.7064, 1.7419, 1.6317, 1.7966,\n",
      "        1.6672, 1.6655, 1.5100, 1.7696, 1.5108, 1.8478, 1.7174, 1.7817, 1.7482,\n",
      "        1.6621, 1.7180, 1.6757, 1.5812, 1.7535, 1.5516, 1.6415, 1.7088, 1.6439,\n",
      "        1.6568, 1.8728, 1.6958, 1.6390, 1.6714, 1.6700, 1.9461, 1.7414, 1.9013,\n",
      "        1.7572, 1.8281, 1.6806, 1.5423, 1.7415, 1.9297, 1.6975, 1.6655, 1.7206,\n",
      "        1.6213, 1.7000, 1.7465, 1.6649, 1.6983, 1.7315, 1.7622, 1.7193, 1.5828,\n",
      "        1.7184, 1.6800, 1.6066, 1.7018, 1.7054, 1.5293, 1.7079, 1.7337, 1.6082,\n",
      "        1.6103, 1.6586, 1.6661, 1.7270, 1.6324, 1.6296, 1.5839, 1.6382, 1.6109,\n",
      "        1.6628, 1.6732, 1.6020, 2.5911, 1.5976, 1.4699, 1.6618, 1.6716, 1.7346,\n",
      "        1.6751, 1.8661, 1.7660, 1.7797, 1.7193, 1.5756, 1.8248, 1.6417, 1.7320,\n",
      "        1.3543, 1.6818, 1.6336, 1.6644, 1.6058, 1.6006, 1.7863, 1.6508, 1.8074,\n",
      "        1.6615, 1.8319, 1.4196, 1.5631, 1.5192, 1.6476, 1.6225, 1.7474, 1.7357,\n",
      "        1.6482, 1.6599, 1.5845, 1.6166, 1.6875, 1.6143, 1.5869, 1.6801, 1.7017,\n",
      "        1.5875, 1.6446, 1.6187, 1.5729, 1.7308, 1.7823, 1.5599, 1.6769, 1.6657,\n",
      "        1.6443, 1.5579, 1.8099, 1.6075, 1.6483, 1.7434, 1.7089, 1.4043, 1.6873,\n",
      "        1.7122, 1.5987, 1.6250, 1.7545, 1.7020, 1.5411, 1.4943, 1.7379, 1.6249,\n",
      "        1.6571, 1.7278, 1.6129, 1.6526, 1.7350, 1.7500, 2.0525, 1.5756, 1.6635,\n",
      "        1.5913, 1.7879, 1.6143, 1.6523, 1.8038, 1.6544, 1.7428, 1.5644, 1.7499,\n",
      "        1.8054, 1.6897, 1.6909, 1.6986, 1.7602, 1.7452, 1.7076, 1.8191, 1.6954,\n",
      "        1.4708, 1.6595, 1.6592, 1.8116, 1.7024, 1.7261, 1.4623, 1.7947, 1.7470,\n",
      "        1.6415, 1.5469, 1.8144, 1.7742, 1.6891, 1.6152, 1.5693, 1.6135, 1.6632,\n",
      "        1.7055, 1.6798, 1.5891, 1.6839, 1.6884, 1.4870, 1.6161, 1.6982, 1.7105,\n",
      "        1.7106, 1.7921, 1.6304, 1.6943, 1.6345], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0533,  0.0625,  0.1111,  ...,  0.0668, -0.0847,  0.0618],\n",
      "        [-0.1108,  0.1348, -0.0355,  ..., -0.1056, -0.1544,  0.0290],\n",
      "        [ 0.0574, -0.0030, -0.0396,  ...,  0.0564, -0.1296, -0.0356],\n",
      "        ...,\n",
      "        [-0.0012,  0.0894,  0.1241,  ..., -0.0101, -0.0140, -0.0335],\n",
      "        [ 0.0125,  0.1294,  0.0398,  ...,  0.1606, -0.0676, -0.0041],\n",
      "        [ 0.0019,  0.0132, -0.0200,  ..., -0.0212,  0.0234,  0.0203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0204,  0.0916, -0.0315,  ..., -0.0114, -0.0107,  0.0267],\n",
      "        [-0.0031,  0.0262,  0.0189,  ...,  0.0338, -0.0096, -0.0121],\n",
      "        [-0.0315,  0.0781, -0.0404,  ...,  0.1034, -0.0275,  0.0202],\n",
      "        ...,\n",
      "        [-0.0392, -0.0384,  0.0073,  ..., -0.0457, -0.0576,  0.0726],\n",
      "        [-0.0173,  0.0006,  0.1081,  ..., -0.0750, -0.0350,  0.1080],\n",
      "        [-0.0203, -0.0470, -0.0835,  ...,  0.1310,  0.0393,  0.0021]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6348, 1.7580, 1.7600, 1.8987, 1.9073, 1.6733, 2.1742, 2.6053, 1.5479,\n",
      "        1.6980, 1.9652, 2.0713, 1.9909, 1.6580, 2.4569, 2.1450, 1.5354, 1.3555,\n",
      "        1.7000, 1.6704, 2.5079, 3.3870, 3.5893, 4.1186, 1.0323, 1.4894, 1.6665,\n",
      "        1.9888, 2.2814, 3.4984, 3.6289, 3.8487, 1.3646, 1.3216, 0.9955, 1.4157,\n",
      "        1.9318, 1.9584, 1.6417, 1.8326, 1.1083, 0.7401, 1.3810, 1.2650, 1.2892,\n",
      "        1.4124, 1.6810, 1.8815, 1.5584, 1.6734, 2.0461, 1.9555, 2.2413, 3.5153,\n",
      "        3.3176, 3.5692, 1.3070, 1.7736, 1.9741, 1.8774, 2.2711, 3.4681, 3.2261,\n",
      "        3.3209, 1.6160, 2.2388, 1.6578, 1.3719, 1.8077, 1.7066, 1.9959, 2.0373,\n",
      "        1.9473, 1.8989, 1.7558, 1.7326, 1.4083, 1.8565, 2.0642, 2.1348, 1.3966,\n",
      "        1.1396, 1.3960, 1.6090, 2.2096, 2.0403, 1.9043, 2.0704, 1.6033, 1.4882,\n",
      "        1.2290, 1.2406, 1.3760, 1.6762, 1.9929, 1.9341, 1.7229, 1.7364, 1.7947,\n",
      "        1.7437, 1.6513, 1.6086, 2.3612, 2.2471, 2.0393, 2.0386, 2.0181, 1.5519,\n",
      "        2.0385, 1.4618, 2.4900, 2.2070, 1.6887, 1.5130, 1.5078, 1.0071, 1.4091,\n",
      "        2.0931, 1.8544, 1.8918, 1.6560, 1.2705, 0.9788, 1.7273, 1.4331, 1.5869,\n",
      "        1.8908, 1.8766, 1.4347, 1.5045, 1.3517, 1.3812, 1.2570, 1.8235, 1.8054,\n",
      "        1.3317, 1.7539, 1.6626, 1.6450, 1.2972, 1.6370, 1.3301, 2.0044, 1.8913,\n",
      "        1.0565, 1.1074, 1.0726, 1.6159, 1.4091, 1.5235, 1.7917, 2.3176, 1.5910,\n",
      "        1.1359, 1.5162, 1.4123, 2.0761, 1.9423, 1.8269, 1.9580, 1.8525, 1.3937,\n",
      "        1.0158, 1.2760, 1.8018, 1.5525, 1.7616, 1.9545, 1.2163, 1.3962, 1.4957,\n",
      "        1.5052, 1.4415, 2.0264, 1.9120, 1.9618, 2.1249, 2.3317, 2.2198, 1.9617,\n",
      "        1.9704, 1.7355, 2.3967, 2.3548, 1.7853, 1.8450, 1.9076, 1.8547, 1.6410,\n",
      "        2.6102, 2.3785, 2.3255, 1.8591, 1.2332, 1.5919, 1.2847, 1.9284, 2.0518,\n",
      "        1.7799, 1.9947, 0.9066, 1.3266, 1.0756, 1.7669, 1.4615, 1.4817, 1.7480,\n",
      "        2.0130, 2.0156, 1.7369, 1.5396, 1.5977, 1.7004, 1.8698, 1.9593, 2.2848,\n",
      "        1.7541, 2.0033, 2.0069, 1.8192, 1.6435, 2.1934, 1.5391, 2.2392, 1.0392,\n",
      "        1.2093, 1.6377, 1.9484, 1.8447, 1.4341, 1.8449, 1.8253, 1.6858, 1.0493,\n",
      "        1.1410, 1.2426, 1.3798, 2.0106, 1.7852, 2.1957, 1.9850, 1.7412, 1.8574,\n",
      "        1.8661, 1.8031, 2.3466, 2.9997, 3.0253, 2.0872, 2.0299, 2.0917, 1.9352,\n",
      "        2.4777, 2.3653, 3.0240, 2.9702, 1.5602, 1.3140, 1.1655, 1.0991, 2.1892,\n",
      "        1.5726, 1.9698, 1.8761, 1.7897, 1.0264, 1.5203, 2.1242, 1.5288, 2.0092,\n",
      "        1.7991, 1.9230, 1.6340, 1.3481, 1.5435, 1.2237, 1.2811, 1.9219, 1.8515,\n",
      "        1.9168, 1.5493, 1.5348, 1.3427, 1.6986, 1.8454, 1.4895, 1.7584, 1.8982,\n",
      "        1.6191, 1.2692, 1.2172, 1.7493, 1.9760, 2.2467, 2.0327, 2.0167, 1.9440,\n",
      "        1.3115, 1.3252, 1.2610, 1.6757, 1.4898, 1.9572, 1.9318, 2.1588, 1.5117,\n",
      "        1.5889, 1.9241, 2.2671, 2.2252, 1.8919, 2.2643, 1.0607, 1.4866, 1.4783,\n",
      "        1.6324, 1.5742, 1.8355, 1.9991, 2.2522], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0421,  0.0021,  0.0544,  ..., -0.0900,  0.0365,  0.0097],\n",
      "        [ 0.1137, -0.0258, -0.0686,  ...,  0.0793,  0.1475, -0.0688],\n",
      "        [ 0.0153, -0.0830,  0.1688,  ..., -0.0317,  0.0222,  0.0120],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0593, -0.0321,  ..., -0.0472,  0.0927, -0.0035],\n",
      "        [-0.0721,  0.0438,  0.0404,  ...,  0.1253, -0.0052, -0.0427],\n",
      "        [-0.0041,  0.0302,  0.0469,  ..., -0.0585, -0.0741, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141,  0.0679,  0.0643,  ..., -0.0474, -0.0354, -0.0297],\n",
      "        [-0.0238,  0.0461,  0.0108,  ..., -0.0805, -0.0284,  0.0932],\n",
      "        [ 0.0274,  0.0133,  0.0273,  ...,  0.0278, -0.0944, -0.0970],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0287, -0.1295,  ...,  0.1143, -0.0046,  0.0036],\n",
      "        [ 0.0929,  0.1265,  0.0153,  ..., -0.1550,  0.0848, -0.0352],\n",
      "        [-0.2411, -0.0324, -0.0354,  ...,  0.0552,  0.0189, -0.0306]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5536, 1.6816, 1.9042, 2.0716, 1.8858, 1.7257, 2.2499, 2.1195, 1.6863,\n",
      "        1.6731, 1.8040, 1.7963, 1.9987, 2.2336, 2.0888, 2.0066, 1.4894, 1.3723,\n",
      "        1.6265, 1.8277, 2.2012, 2.5397, 2.6738, 2.5855, 0.8681, 1.4053, 1.6286,\n",
      "        1.7931, 1.9907, 2.4652, 2.6671, 2.2722, 1.5785, 1.3352, 1.4237, 1.2357,\n",
      "        1.0469, 1.2093, 1.5947, 1.6913, 1.5363, 1.5405, 1.6118, 1.4568, 1.9306,\n",
      "        1.8896, 1.6171, 1.5869, 1.4570, 1.6241, 1.9119, 2.0824, 2.0316, 2.5976,\n",
      "        2.7515, 2.7973, 1.4126, 1.8107, 2.0275, 1.7897, 2.1093, 2.4162, 2.7039,\n",
      "        2.6874, 1.7641, 1.9721, 1.7654, 1.5692, 1.5022, 2.0413, 1.9107, 1.9334,\n",
      "        1.7602, 1.9761, 1.8772, 2.0069, 1.7361, 1.4872, 1.9491, 1.9623, 1.8603,\n",
      "        1.4981, 1.6829, 1.2947, 1.1256, 1.2755, 1.8204, 1.7343, 1.6326, 1.7542,\n",
      "        1.5185, 1.7682, 2.2894, 1.9047, 1.8048, 1.8163, 1.7625, 1.5840, 1.7491,\n",
      "        1.8754, 2.1086, 2.0706, 2.1636, 2.0922, 1.8712, 2.0121, 2.0101, 1.8445,\n",
      "        1.5731, 1.7000, 2.0915, 2.0706, 1.6826, 1.7709, 1.5696, 2.0106, 2.1213,\n",
      "        1.2891, 1.7646, 1.8229, 1.7205, 1.5759, 1.8092, 1.3200, 1.2581, 2.1016,\n",
      "        1.7918, 1.7562, 1.5662, 1.6547, 1.7322, 1.7514, 1.7226, 1.3767, 1.7225,\n",
      "        1.7857, 1.5723, 1.7444, 1.6497, 1.5467, 1.5653, 1.8414, 1.9007, 1.8063,\n",
      "        1.4640, 1.4229, 1.7060, 1.4072, 2.1158, 1.8636, 1.7304, 1.5449, 1.7545,\n",
      "        1.6200, 1.4899, 1.3729, 1.1498, 1.2309, 1.7266, 1.7108, 1.6178, 1.4281,\n",
      "        1.5278, 1.5410, 1.2036, 2.1763, 1.7051, 1.6497, 2.0421, 1.8508, 1.5932,\n",
      "        1.4825, 1.8101, 1.2307, 1.7226, 2.0706, 1.9593, 2.0603, 2.0549, 1.9018,\n",
      "        1.4942, 2.3109, 2.2576, 2.2654, 1.9011, 1.9925, 1.9706, 1.9669, 2.2415,\n",
      "        1.3770, 2.1620, 2.1251, 1.7700, 1.5246, 1.4617, 1.9063, 1.2533, 1.1436,\n",
      "        1.6237, 1.7813, 1.6624, 1.7156, 1.7583, 1.2114, 1.7833, 2.1020, 1.6106,\n",
      "        1.7715, 1.6870, 1.8602, 1.7306, 1.6342, 1.4546, 1.9430, 1.9743, 2.0345,\n",
      "        1.9950, 1.8160, 1.7939, 1.5938, 1.7092, 1.4822, 1.6834, 1.9933, 1.2194,\n",
      "        1.5691, 1.6256, 1.1583, 1.1055, 2.0742, 1.7112, 1.7251, 2.1369, 1.4342,\n",
      "        1.4548, 1.8314, 1.9962, 1.2296, 1.7255, 1.8570, 2.1658, 1.8280, 2.0589,\n",
      "        1.7702, 1.9085, 1.7693, 2.2831, 2.4908, 1.9063, 1.9562, 1.8910, 2.0011,\n",
      "        1.7704, 1.7396, 2.4638, 2.2630, 1.7626, 1.6832, 1.8605, 1.9925, 1.1350,\n",
      "        1.9958, 1.7153, 1.8086, 1.8552, 1.6254, 1.5218, 1.0843, 1.9864, 1.1807,\n",
      "        1.7365, 1.8613, 1.7112, 1.6000, 1.8047, 1.6983, 2.2184, 1.1924, 1.7662,\n",
      "        1.8420, 1.7115, 1.8174, 1.5433, 1.2747, 1.1495, 2.0921, 1.7776, 1.8112,\n",
      "        1.9601, 1.6811, 1.6068, 1.3809, 1.2137, 1.1520, 1.9185, 1.8082, 1.7144,\n",
      "        1.6394, 1.7922, 1.5821, 1.6538, 2.1146, 1.7697, 1.8755, 1.8261, 1.6772,\n",
      "        1.6253, 1.4242, 1.2031, 1.2849, 1.8295, 1.8200, 1.5124, 1.5177, 1.6446,\n",
      "        1.6937, 2.1035, 2.1817, 1.8018, 1.7916], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1715, -0.0842,  0.0623,  ..., -0.0756, -0.0031, -0.1606],\n",
      "        [ 0.0817, -0.1410, -0.0496,  ..., -0.0089, -0.0691, -0.1001],\n",
      "        [ 0.0820,  0.2034,  0.0725,  ..., -0.1266,  0.1318, -0.0540],\n",
      "        ...,\n",
      "        [-0.0061, -0.0810, -0.0924,  ...,  0.0680,  0.0238,  0.0275],\n",
      "        [ 0.1704,  0.0787,  0.0281,  ..., -0.0454, -0.0855, -0.1911],\n",
      "        [-0.2309, -0.0218, -0.0763,  ...,  0.1687,  0.1254, -0.0525]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0212, -0.0068,  0.0331,  ..., -0.0212,  0.0122, -0.0030],\n",
      "        [-0.0332,  0.0130, -0.0306,  ..., -0.0494, -0.0249, -0.0276],\n",
      "        [-0.0666,  0.0183, -0.0298,  ..., -0.0298, -0.0221, -0.0577],\n",
      "        ...,\n",
      "        [ 0.1483,  0.0577,  0.0132,  ...,  0.0972,  0.1014,  0.0527],\n",
      "        [ 0.0901, -0.0900,  0.0795,  ..., -0.0050,  0.0649, -0.0170],\n",
      "        [-0.1823,  0.1025,  0.0497,  ..., -0.0007,  0.0304, -0.0460]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0495, 2.2048, 2.1120, 2.1693, 2.1060, 2.1021, 2.1868, 2.2377, 2.2128,\n",
      "        2.2043, 2.1598, 2.0417, 2.2390, 2.3311, 2.1067, 2.0563, 1.9171, 2.0404,\n",
      "        1.9799, 1.9873, 1.9744, 2.0415, 1.9896, 2.0045, 1.9913, 1.9401, 1.9581,\n",
      "        2.0183, 1.9174, 1.9384, 2.0045, 1.9150, 2.2766, 2.0638, 2.2254, 2.0308,\n",
      "        2.2499, 2.1712, 2.2782, 2.1155, 2.1624, 2.1638, 2.1577, 2.3273, 2.1918,\n",
      "        2.2545, 2.1759, 2.1963, 2.1347, 2.0653, 2.0541, 2.0570, 2.1073, 2.0817,\n",
      "        2.0449, 2.1140, 2.0666, 2.0831, 2.0142, 2.1385, 2.1106, 2.1883, 2.1007,\n",
      "        2.0872, 2.0205, 2.1767, 2.0134, 2.1241, 2.2175, 2.1041, 2.0752, 2.1008,\n",
      "        2.0314, 2.1187, 2.1195, 2.1005, 2.1170, 2.0785, 2.0752, 2.0460, 2.1669,\n",
      "        2.1353, 2.1647, 2.1658, 2.0014, 2.1286, 2.0516, 1.9847, 2.2515, 2.0725,\n",
      "        2.0402, 1.9245, 2.1495, 1.9749, 2.2374, 2.0468, 2.2730, 2.0603, 1.9387,\n",
      "        2.0699, 2.1515, 2.0536, 2.0301, 2.0969, 2.1480, 2.1383, 2.0986, 2.1326,\n",
      "        2.1154, 2.0511, 2.1418, 2.0887, 2.1814, 2.1556, 2.1672, 2.1582, 2.1819,\n",
      "        2.1460, 2.0467, 2.1432, 2.2585, 2.2070, 2.1642, 2.1787, 2.0933, 2.1587,\n",
      "        2.0944, 2.1431, 2.2903, 2.3285, 2.3718, 2.2128, 2.4165, 2.3178, 2.2839,\n",
      "        2.3518, 2.2228, 2.3738, 2.1087, 2.4056, 2.2179, 2.3715, 2.2003, 2.4125,\n",
      "        1.9146, 2.0916, 2.1712, 2.2652, 1.9171, 2.0636, 2.1150, 2.0638, 1.9520,\n",
      "        2.1379, 2.0530, 2.0866, 2.1546, 2.1500, 1.9942, 2.1712, 1.9708, 2.0548,\n",
      "        2.1380, 2.1717, 2.0747, 2.0236, 2.0393, 2.0816, 2.0894, 2.0867, 2.0580,\n",
      "        2.0386, 2.1921, 2.0320, 2.0395, 2.1253, 2.0965, 2.1400, 2.1609, 2.1872,\n",
      "        2.2712, 2.1265, 2.1808, 2.2425, 2.1028, 2.3416, 2.1107, 2.1525, 2.3117,\n",
      "        2.1140, 2.0935, 2.1631, 1.9732, 1.9822, 2.0123, 2.0756, 2.0586, 2.1165,\n",
      "        2.0382, 1.9871, 2.0904, 1.9565, 2.1668, 2.0140, 2.0896, 2.0407, 2.0649,\n",
      "        1.9673, 2.1920, 2.0060, 2.1001, 2.0976, 2.1416, 2.1732, 2.0442, 2.1553,\n",
      "        2.1796, 2.3227, 2.2859, 2.2147, 2.1200, 2.1530, 2.2035, 2.2011, 1.9450,\n",
      "        1.9687, 2.0582, 1.9553, 1.9166, 2.0437, 2.0069, 1.8648, 2.0188, 1.9949,\n",
      "        2.0728, 1.9337, 1.9205, 2.0275, 2.0149, 1.9903, 2.1992, 2.1705, 2.1031,\n",
      "        2.3225, 2.1443, 2.3063, 2.0864, 1.7365, 2.1415, 2.3493, 2.0829, 2.1643,\n",
      "        2.1815, 2.1439, 2.2507, 2.0906, 2.1452, 2.1071, 2.1356, 2.0505, 2.0215,\n",
      "        2.1501, 2.0770, 2.0448, 2.0266, 2.1436, 1.9286, 2.0773, 2.1543, 2.0217,\n",
      "        2.0664, 2.0946, 1.9948, 2.0498, 2.1070, 2.0138, 2.0716, 2.1482, 2.2025,\n",
      "        2.1179, 2.1650, 2.0878, 2.0716, 2.0906, 2.0693, 1.9299, 1.9989, 2.1120,\n",
      "        2.0810, 2.0685, 2.1059, 2.1710, 2.0967, 2.1544, 2.1586, 2.0304, 2.0635,\n",
      "        2.0298, 2.1248, 2.0035, 2.0301, 2.1508, 1.9285, 2.0460, 1.9622, 1.7745,\n",
      "        1.8431, 1.7964, 1.7838, 1.9629, 1.7669, 1.8178, 1.7955, 1.8170, 1.8814,\n",
      "        1.8242, 1.8466, 1.8700, 1.7794, 1.8211], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 8.8324e-05,  2.0355e-01,  5.2049e-02,  ..., -4.1894e-02,\n",
      "         -7.8651e-02, -3.7493e-03],\n",
      "        [-4.9087e-02,  2.3672e-02,  5.3354e-02,  ..., -6.8793e-02,\n",
      "         -6.4622e-02, -9.6315e-03],\n",
      "        [ 5.7368e-02,  2.3672e-02, -1.5432e-02,  ..., -1.9427e-02,\n",
      "          6.9371e-02, -1.9224e-02],\n",
      "        ...,\n",
      "        [ 1.0113e-01, -3.3027e-02, -7.1275e-02,  ...,  4.1312e-02,\n",
      "         -1.3080e-02,  3.2657e-02],\n",
      "        [-6.8477e-03,  7.8059e-02,  8.9468e-03,  ...,  1.3031e-02,\n",
      "          2.7940e-02, -8.5010e-02],\n",
      "        [-2.0587e-02, -1.0060e-02, -8.8134e-02,  ...,  5.5471e-02,\n",
      "          2.6415e-02, -2.2054e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0090, -0.0695,  0.0390,  ..., -0.0564, -0.0452,  0.0496],\n",
      "        [ 0.0317,  0.1405, -0.1798,  ..., -0.0167, -0.0521, -0.0025],\n",
      "        [ 0.0198,  0.1143, -0.0069,  ..., -0.0293, -0.1098,  0.0112],\n",
      "        ...,\n",
      "        [-0.0177,  0.0038,  0.0465,  ...,  0.0580,  0.0385, -0.0033],\n",
      "        [-0.0204, -0.0618, -0.0123,  ..., -0.0552, -0.0003, -0.0503],\n",
      "        [-0.0456,  0.0811, -0.1216,  ...,  0.0442, -0.0663, -0.0061]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0055, 1.9557, 1.9704, 2.1324, 2.1731, 1.9505, 2.0974, 1.9793, 2.0461,\n",
      "        1.9577, 1.8013, 1.9536, 1.9740, 1.8011, 2.1541, 2.0805, 1.9135, 1.9814,\n",
      "        1.8274, 1.8958, 2.1822, 1.9660, 1.9524, 2.0149, 1.8457, 1.9096, 1.8501,\n",
      "        2.0354, 2.1854, 2.1081, 1.9647, 1.6484, 2.1421, 1.8790, 1.9481, 1.8430,\n",
      "        1.8972, 1.8930, 1.8771, 2.1814, 1.9473, 2.0182, 2.0468, 2.0100, 2.0133,\n",
      "        2.0035, 2.9223, 1.9116, 1.9517, 1.9880, 2.0135, 1.9278, 1.9356, 1.8451,\n",
      "        1.9900, 1.9375, 2.1638, 2.1308, 2.3529, 2.0649, 1.8935, 1.9455, 4.8716,\n",
      "        1.8821, 2.0843, 2.1245, 2.1351, 2.0274, 1.8840, 2.0495, 1.9643, 1.9253,\n",
      "        1.8507, 2.2194, 1.9492, 2.1264, 1.9499, 1.9875, 1.9122, 1.7431, 1.8252,\n",
      "        1.9525, 1.8802, 2.1915, 1.8790, 1.8753, 1.7320, 2.0096, 1.7238, 2.0467,\n",
      "        2.1768, 1.9937, 2.2495, 1.9270, 1.9573, 2.0583, 1.9605, 2.1692, 2.1222,\n",
      "        2.1558, 2.0593, 2.1935, 1.9482, 2.0171, 1.9617, 2.0094, 2.0740, 2.0508,\n",
      "        1.8458, 1.7966, 2.2040, 2.1126, 2.0663, 1.9918, 2.1703, 1.9680, 2.0676,\n",
      "        1.9163, 1.9293, 2.2184, 2.0688, 1.9553, 2.1763, 2.0546, 1.8338, 2.1322,\n",
      "        1.9444, 1.9715, 1.9406, 1.9044, 1.8161, 2.1147, 2.1509, 2.0006, 2.1967,\n",
      "        1.9711, 1.8554, 2.0119, 1.8858, 1.9758, 1.9901, 1.8955, 2.3196, 2.1101,\n",
      "        1.9873, 1.9814, 1.9739, 2.1033, 1.9955, 1.8833, 2.1361, 2.0371, 1.9852,\n",
      "        1.9350, 2.1060, 1.9405, 1.8593, 1.9374, 2.1718, 1.9165, 1.9951, 2.0135,\n",
      "        2.1132, 1.9389, 2.0485, 2.1346, 2.1194, 2.0623, 2.0735, 1.9688, 1.9620,\n",
      "        1.7365, 2.0689, 2.0171, 1.8450, 1.8389, 1.8285, 2.0580, 2.3147, 1.9832,\n",
      "        1.8122, 1.9919, 2.0453, 2.0067, 1.9990, 1.9914, 1.9614, 2.0083, 1.9909,\n",
      "        1.9493, 2.0185, 1.8169, 2.7180, 2.0940, 1.9456, 2.0006, 1.9487, 1.8783,\n",
      "        2.0361, 1.9904, 1.9744, 2.0577, 1.9463, 2.1006, 1.9936, 2.0922, 2.0284,\n",
      "        1.7363, 1.9648, 1.9240, 1.8725, 2.1117, 2.0740, 2.1431, 2.2522, 2.0319,\n",
      "        1.8962, 2.0656, 1.6674, 1.8803, 1.8633, 2.0120, 1.9570, 2.1855, 2.1416,\n",
      "        2.0462, 2.0886, 2.0041, 1.9346, 2.0826, 1.8729, 1.7602, 2.0428, 1.8642,\n",
      "        2.0166, 2.0407, 1.8838, 1.8824, 2.0865, 2.0063, 1.8915, 1.8490, 2.0172,\n",
      "        2.0545, 2.0240, 1.8180, 2.0169, 2.0037, 1.9446, 1.9863, 1.9482, 1.9289,\n",
      "        2.0845, 1.7917, 1.9263, 1.9173, 1.9728, 1.7890, 1.7846, 2.0817, 2.1906,\n",
      "        2.1338, 2.0852, 1.9841, 1.9849, 2.0283, 2.0234, 2.5428, 2.0396, 1.9104,\n",
      "        1.9049, 2.1833, 1.8690, 2.0071, 1.9823, 1.9348, 2.1952, 1.8549, 1.9779,\n",
      "        2.0817, 1.9344, 1.9139, 2.0625, 2.0004, 2.0117, 2.0327, 2.0320, 1.8527,\n",
      "        1.9051, 1.8836, 1.8895, 2.1753, 1.9715, 2.1408, 1.7530, 2.0435, 2.1139,\n",
      "        1.9584, 1.8592, 1.9205, 2.0673, 2.0071, 2.1712, 1.9832, 2.1144, 1.9808,\n",
      "        2.0780, 1.7150, 1.8597, 2.1245, 2.0491, 1.8093, 1.7969, 2.1929, 2.1235,\n",
      "        1.9569, 2.0337, 1.9640, 2.1792, 1.9460], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0073,  0.0102,  0.0462,  ...,  0.0315,  0.0441,  0.0013],\n",
      "        [ 0.0333,  0.0555,  0.0503,  ..., -0.1330,  0.1519, -0.0530],\n",
      "        [ 0.1017, -0.0370,  0.1119,  ..., -0.0176, -0.0636,  0.0295],\n",
      "        ...,\n",
      "        [ 0.0825, -0.0387, -0.0156,  ..., -0.0756,  0.1528, -0.0153],\n",
      "        [-0.1052,  0.0206, -0.0937,  ...,  0.0618, -0.0624, -0.0180],\n",
      "        [ 0.0490,  0.0565, -0.0574,  ..., -0.0635, -0.0364,  0.0447]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0461,  0.0262,  0.0022,  ..., -0.0061,  0.0090, -0.0286],\n",
      "        [-0.0102,  0.0213,  0.0306,  ..., -0.0180, -0.0194, -0.0525],\n",
      "        [-0.1038,  0.0173, -0.0120,  ..., -0.0310,  0.0195,  0.0772],\n",
      "        ...,\n",
      "        [-0.0309, -0.0474, -0.0235,  ..., -0.0311,  0.0514, -0.0992],\n",
      "        [-0.0016,  0.0119,  0.0048,  ...,  0.0858, -0.0234,  0.0073],\n",
      "        [-0.0076,  0.0255, -0.0022,  ...,  0.0099, -0.0004,  0.0467]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5368, 1.3225, 1.3641, 1.3156, 1.5666, 1.7158, 1.6279, 1.6214, 1.5950,\n",
      "        1.4265, 1.1316, 1.3534, 1.4139, 1.3207, 1.6320, 1.5699, 1.5069, 1.3517,\n",
      "        1.0071, 1.3447, 1.6910, 1.4730, 1.8885, 1.9736, 1.0800, 1.0347, 1.3305,\n",
      "        1.1702, 1.1228, 1.9557, 2.0854, 1.8484, 1.0515, 1.1953, 1.1974, 1.6993,\n",
      "        2.1686, 1.5645, 1.8078, 2.5822, 2.0143, 1.5149, 1.6445, 1.4798, 1.4966,\n",
      "        2.1797, 1.9514, 1.9484, 1.6748, 1.7555, 1.7091, 1.5560, 1.4525, 1.5003,\n",
      "        1.8639, 1.9671, 1.7003, 1.7868, 1.5899, 1.7639, 1.7015, 1.5566, 1.8937,\n",
      "        1.9436, 1.1612, 1.6655, 1.4198, 1.9186, 2.0157, 2.1018, 2.3118, 2.0316,\n",
      "        1.2119, 1.2488, 1.5115, 2.0779, 2.0502, 1.6387, 2.2858, 2.1165, 1.7823,\n",
      "        1.3058, 1.6804, 1.0797, 1.8355, 1.7230, 1.3282, 1.4588, 1.6455, 1.7354,\n",
      "        1.3877, 1.8082, 1.1443, 1.1742, 1.3140, 1.3171, 1.1484, 0.9841, 1.0700,\n",
      "        1.1459, 1.4953, 1.4697, 1.4394, 1.5985, 1.1349, 0.9326, 1.2745, 1.3835,\n",
      "        1.2613, 1.5509, 1.4289, 1.4626, 1.5476, 1.6488, 1.9058, 1.7081, 1.8283,\n",
      "        1.6734, 2.2480, 2.3427, 1.5089, 1.6913, 1.7863, 1.8000, 1.5815, 1.9787,\n",
      "        2.2678, 2.3637, 2.0180, 1.7015, 1.6983, 1.5177, 1.6967, 2.2796, 1.9591,\n",
      "        1.9252, 1.6387, 1.7445, 1.6532, 1.3730, 1.7039, 1.3887, 1.8727, 1.9169,\n",
      "        1.9107, 1.2786, 1.6920, 1.2270, 1.2931, 1.4013, 1.6026, 2.1462, 1.2905,\n",
      "        1.2760, 1.1794, 1.8154, 1.9418, 2.0207, 1.7277, 1.7967, 1.4730, 1.4159,\n",
      "        1.5970, 1.7577, 1.5353, 2.2971, 1.8043, 1.8251, 1.6383, 1.4362, 1.6954,\n",
      "        1.7452, 1.8870, 1.4961, 1.6804, 1.9680, 0.7657, 1.3476, 1.5195, 1.4450,\n",
      "        2.0980, 2.0155, 2.4045, 2.0078, 1.3796, 1.1228, 1.2158, 1.6612, 1.9276,\n",
      "        2.0324, 2.3394, 1.9491, 1.6883, 1.2101, 1.6318, 1.8729, 1.8518, 1.8774,\n",
      "        1.6447, 1.9473, 1.2597, 1.4344, 1.0772, 1.2358, 1.2471, 1.3985, 1.6674,\n",
      "        1.6312, 1.0052, 1.3318, 1.2853, 1.2822, 1.2366, 1.2028, 1.7407, 1.7831,\n",
      "        1.2223, 1.1921, 1.2935, 1.3818, 1.1879, 1.7481, 1.9144, 1.8125, 1.4456,\n",
      "        1.3235, 1.7326, 1.7165, 1.2700, 1.3193, 1.6807, 1.6340, 1.6753, 1.4890,\n",
      "        1.0858, 1.0995, 1.6785, 1.9240, 1.5711, 1.8783, 0.7041, 1.5450, 1.4543,\n",
      "        1.6946, 2.1882, 1.7497, 2.1172, 2.2867, 1.3589, 1.1821, 1.4575, 1.5449,\n",
      "        2.2424, 1.7564, 2.2913, 2.3966, 1.9503, 1.9917, 1.9599, 1.6376, 1.3974,\n",
      "        2.1447, 1.8159, 2.0288, 1.8508, 1.8440, 1.8045, 1.3187, 1.7615, 1.5278,\n",
      "        1.9940, 1.9049, 1.8235, 1.9191, 1.9034, 1.8488, 1.4945, 2.1589, 2.1034,\n",
      "        2.0661, 1.7055, 1.7328, 1.9938, 1.9696, 1.7322, 1.5763, 2.2130, 2.0086,\n",
      "        1.2941, 1.3094, 1.3938, 1.2779, 1.3335, 1.1334, 2.0082, 2.0317, 1.2359,\n",
      "        1.2616, 1.2682, 1.3733, 1.2583, 1.5971, 1.5906, 1.3886, 0.7830, 0.7899,\n",
      "        0.8347, 1.1984, 1.9244, 2.0819, 2.0385, 1.9636, 1.7728, 1.4158, 1.5102,\n",
      "        1.5398, 1.7841, 2.0747, 1.9819, 1.6824], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0215,  0.0150,  0.0091,  ...,  0.0519,  0.0153, -0.0416],\n",
      "        [-0.0860,  0.0242, -0.0667,  ...,  0.1001, -0.0442,  0.0452],\n",
      "        [-0.0295, -0.0235, -0.0832,  ..., -0.0746,  0.1778, -0.1176],\n",
      "        ...,\n",
      "        [ 0.0532, -0.1309,  0.0575,  ...,  0.0162,  0.0261, -0.0626],\n",
      "        [-0.0855, -0.1336,  0.0984,  ..., -0.1099,  0.0492, -0.0313],\n",
      "        [-0.0704, -0.1061,  0.0029,  ...,  0.0134,  0.0346, -0.0062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0783,  0.0690,  0.0403,  ...,  0.0509,  0.0027, -0.0378],\n",
      "        [-0.0576,  0.0044,  0.0273,  ..., -0.0060,  0.0540, -0.0076],\n",
      "        [-0.0898, -0.0435, -0.0597,  ...,  0.0474,  0.0171, -0.0014],\n",
      "        ...,\n",
      "        [-0.0914, -0.0074,  0.0715,  ..., -0.0711, -0.0640, -0.0445],\n",
      "        [ 0.0785, -0.0027,  0.0033,  ..., -0.0626,  0.0321, -0.0602],\n",
      "        [-0.0413, -0.0573,  0.0439,  ...,  0.0396,  0.0621, -0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4116, 1.2869, 1.4040, 1.4386, 1.1972, 1.1681, 1.5470, 1.5688, 1.4929,\n",
      "        1.4154, 1.3172, 1.3301, 1.4167, 1.8665, 1.5428, 1.5289, 1.1147, 1.5148,\n",
      "        1.0565, 1.2031, 1.3732, 1.3777, 1.5738, 1.4400, 1.2858, 1.0459, 1.5223,\n",
      "        1.2367, 1.2028, 1.3299, 1.7910, 1.6500, 1.5675, 1.5081, 1.5430, 1.3184,\n",
      "        1.2879, 2.0940, 1.7373, 1.8801, 1.4800, 1.4605, 1.4369, 1.7065, 2.0743,\n",
      "        1.3102, 1.8638, 1.8796, 1.6820, 1.7304, 1.7476, 1.5021, 1.5088, 1.4430,\n",
      "        1.7225, 1.8481, 1.6303, 1.8364, 1.7775, 1.7184, 1.4339, 2.0440, 1.7543,\n",
      "        1.9274, 1.1919, 1.5750, 1.5811, 1.7357, 1.5912, 1.3472, 1.8635, 1.7683,\n",
      "        1.2279, 1.2720, 1.6522, 1.7685, 1.4027, 1.6574, 1.6959, 1.8719, 1.8123,\n",
      "        1.4573, 1.6272, 1.5664, 0.9547, 1.0160, 1.3885, 1.4747, 1.5920, 1.6941,\n",
      "        1.4292, 1.2681, 1.5888, 1.3839, 1.3415, 1.3113, 1.0193, 1.2440, 1.2518,\n",
      "        1.3264, 1.0544, 1.0992, 1.3848, 1.4612, 1.2096, 1.1463, 1.3280, 1.0917,\n",
      "        1.1314, 1.2770, 1.3709, 1.4434, 1.4747, 1.5877, 1.9298, 1.7948, 1.7309,\n",
      "        1.7952, 1.9564, 1.8071, 1.5686, 1.7315, 1.7642, 1.8259, 1.7535, 1.6207,\n",
      "        1.9943, 1.8868, 1.9234, 1.7467, 1.8033, 1.6020, 1.2421, 1.1475, 1.8421,\n",
      "        1.8383, 1.7106, 1.8518, 1.7749, 1.5160, 1.4104, 2.3289, 1.7604, 1.7623,\n",
      "        1.8187, 1.4724, 1.3239, 1.8488, 1.8319, 1.9336, 1.5368, 1.7297, 1.4966,\n",
      "        1.4935, 1.5658, 1.1262, 1.0515, 1.2040, 1.6920, 1.6968, 1.5927, 1.5324,\n",
      "        1.4527, 1.3844, 1.6690, 1.1685, 1.6447, 1.7316, 1.5451, 1.6106, 1.4540,\n",
      "        1.4350, 1.1940, 1.9879, 1.7021, 1.7158, 1.5575, 1.0656, 1.3163, 1.4897,\n",
      "        1.6273, 1.8750, 1.9520, 1.8415, 0.8389, 1.5049, 1.3271, 1.3535, 1.7081,\n",
      "        1.8360, 1.9656, 1.8309, 1.7724, 1.4108, 1.3970, 1.1305, 1.0311, 1.1721,\n",
      "        1.6011, 1.7636, 1.3853, 1.5272, 1.5339, 1.5868, 1.9726, 1.8663, 1.6444,\n",
      "        1.5473, 0.8886, 0.9861, 1.0051, 1.3035, 1.1549, 1.2811, 1.4831, 1.4566,\n",
      "        1.0308, 1.1621, 1.3777, 1.2034, 1.1838, 1.2269, 1.7718, 1.3476, 1.5553,\n",
      "        1.4274, 1.6282, 1.0651, 1.8447, 1.9128, 1.6386, 1.5973, 1.7890, 1.5965,\n",
      "        1.3439, 1.7148, 1.0031, 1.0843, 1.5286, 1.6319, 1.5151, 1.5535, 1.5597,\n",
      "        1.6597, 1.6226, 1.7640, 1.9543, 1.9166, 1.1147, 0.9685, 1.1389, 1.2499,\n",
      "        1.8571, 1.7111, 2.0848, 1.9509, 1.9312, 1.9202, 1.9729, 1.7157, 1.9367,\n",
      "        1.2632, 1.6160, 1.7558, 1.7952, 1.8941, 1.8133, 1.5970, 1.4772, 2.1867,\n",
      "        1.8879, 1.7051, 1.7840, 1.9081, 1.9374, 1.9089, 1.6112, 1.4665, 1.9838,\n",
      "        1.8579, 1.6893, 1.6945, 1.8617, 1.8429, 1.5650, 2.0964, 2.0671, 1.9823,\n",
      "        0.9965, 0.9309, 1.1761, 1.1208, 1.3269, 1.2626, 1.6881, 1.4547, 0.8535,\n",
      "        1.0226, 1.2652, 1.3274, 1.1132, 1.1905, 1.4763, 1.4421, 1.1819, 1.5136,\n",
      "        1.6333, 1.6202, 1.6708, 1.8065, 1.8563, 1.8628, 1.4555, 1.1405, 1.0376,\n",
      "        1.5225, 1.7374, 1.8638, 1.8448, 1.7264], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1150,  0.1392,  0.0754,  ...,  0.1355, -0.0420,  0.1357],\n",
      "        [ 0.0094,  0.1272, -0.1467,  ..., -0.2259,  0.0004, -0.0703],\n",
      "        [ 0.0644, -0.0123, -0.0300,  ..., -0.0635,  0.0930,  0.0264],\n",
      "        ...,\n",
      "        [-0.0049, -0.0458, -0.2506,  ..., -0.0136, -0.0258, -0.0807],\n",
      "        [-0.1246, -0.0504,  0.1183,  ..., -0.0251, -0.0156,  0.0329],\n",
      "        [ 0.0340, -0.1423, -0.0712,  ..., -0.0710,  0.0840, -0.1588]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0761, -0.0069, -0.0053,  ..., -0.0090, -0.1210, -0.0851],\n",
      "        [ 0.0666,  0.0496,  0.0566,  ...,  0.0779, -0.0479,  0.0591],\n",
      "        [ 0.0380, -0.0183,  0.0653,  ..., -0.0602, -0.0586,  0.0154],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0399, -0.0262,  ..., -0.0599,  0.1558, -0.1293],\n",
      "        [ 0.0307,  0.1448,  0.0184,  ...,  0.0054,  0.0137,  0.0574],\n",
      "        [-0.1246,  0.0679,  0.0135,  ...,  0.0936,  0.0072, -0.0776]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0442, 2.0853, 2.0471, 2.0901, 2.3486, 2.2211, 2.0851, 1.9905, 2.1028,\n",
      "        2.1358, 2.1735, 2.0286, 2.1258, 2.1664, 2.0550, 2.1849, 2.2911, 2.2386,\n",
      "        2.2585, 2.3480, 2.2424, 2.1293, 2.3298, 2.4110, 2.2936, 2.2989, 2.2159,\n",
      "        2.1354, 2.2864, 2.1896, 2.2933, 2.2576, 2.0436, 1.9744, 2.1105, 2.0963,\n",
      "        1.9682, 2.0158, 2.0849, 2.0885, 2.0617, 2.0828, 1.9925, 2.0324, 2.0425,\n",
      "        2.0312, 2.0150, 1.9847, 2.1889, 2.2438, 2.2250, 2.3072, 1.8944, 2.1396,\n",
      "        2.2150, 2.0761, 2.0216, 2.1733, 2.2568, 2.2428, 2.2746, 2.0169, 2.3392,\n",
      "        2.2099, 2.2647, 2.3458, 2.1150, 2.1306, 2.4575, 2.2004, 2.2174, 2.1677,\n",
      "        2.2060, 2.2350, 2.2470, 2.1667, 2.2380, 2.1511, 2.1586, 2.2338, 2.1038,\n",
      "        2.1432, 2.2130, 2.1341, 2.2741, 2.1409, 2.1226, 2.1318, 2.2851, 2.0852,\n",
      "        2.1748, 2.2007, 2.1417, 2.1999, 2.0864, 2.0789, 2.3388, 2.0160, 2.2409,\n",
      "        2.1115, 2.2180, 2.1312, 2.0715, 2.1348, 2.1959, 2.1153, 2.2379, 2.2033,\n",
      "        2.3930, 2.2008, 2.1358, 2.1759, 2.1748, 2.2629, 2.0810, 2.1339, 2.0520,\n",
      "        2.1913, 2.2353, 2.2570, 2.5342, 2.1002, 2.1875, 2.2594, 2.0362, 2.1932,\n",
      "        2.2333, 2.2806, 2.2092, 2.0789, 2.2171, 2.2364, 2.2626, 2.2793, 2.0792,\n",
      "        2.2267, 2.1276, 2.2702, 2.1248, 2.3065, 2.2737, 2.2561, 2.1163, 2.1424,\n",
      "        2.0718, 2.1055, 1.8319, 1.9468, 2.0511, 2.1004, 2.1169, 1.9696, 2.2386,\n",
      "        2.1631, 2.0404, 2.0989, 2.0110, 2.2007, 2.0452, 2.0343, 2.1601, 2.1030,\n",
      "        2.0542, 2.0920, 2.1077, 2.1149, 2.1249, 2.1001, 2.0537, 2.0069, 2.1051,\n",
      "        2.1377, 2.0574, 1.9637, 2.0481, 2.1575, 1.9833, 1.8287, 2.0211, 1.9624,\n",
      "        1.8355, 1.9022, 1.9274, 1.9562, 2.0097, 2.0032, 1.9591, 1.8686, 1.9321,\n",
      "        1.9571, 1.9369, 1.8656, 2.0766, 1.8880, 2.0142, 2.1909, 2.1615, 2.0364,\n",
      "        2.0369, 1.9778, 2.1755, 2.2215, 2.0100, 2.1371, 2.1592, 2.0551, 1.8469,\n",
      "        2.1685, 2.5375, 2.5440, 2.5067, 2.5049, 2.5299, 2.9297, 2.5395, 2.2732,\n",
      "        2.1209, 2.3802, 2.2883, 2.2952, 2.2291, 2.4940, 2.3730, 2.3485, 2.1469,\n",
      "        2.1571, 2.2396, 2.1710, 2.1478, 1.9849, 2.1935, 2.1474, 2.2911, 2.0965,\n",
      "        2.1456, 2.1554, 2.0864, 2.1692, 2.1087, 1.9452, 2.0201, 2.0361, 1.9028,\n",
      "        1.9347, 1.9515, 1.9447, 1.9039, 1.9894, 1.9496, 1.9135, 1.9692, 1.9135,\n",
      "        1.8777, 1.8615, 1.9766, 1.9654, 2.2727, 2.2128, 1.9967, 2.2745, 2.1988,\n",
      "        2.1535, 2.0757, 2.2224, 2.2121, 2.2413, 2.1406, 2.1887, 2.2318, 2.1231,\n",
      "        2.1738, 2.0930, 2.2212, 2.1576, 2.1767, 2.2876, 2.1582, 2.2211, 2.2092,\n",
      "        2.1175, 2.2415, 2.2014, 2.1048, 2.0464, 2.0210, 2.3349, 2.1261, 2.2483,\n",
      "        3.5255, 2.7152, 2.6703, 3.1695, 2.3523, 2.8169, 2.5482, 2.6783, 3.5530,\n",
      "        2.5467, 2.7356, 2.6696, 3.0308, 2.7923, 2.7934, 2.7396, 2.1105, 1.9799,\n",
      "        1.8996, 1.8759, 1.9248, 1.9094, 1.9289, 2.1587, 1.9692, 1.9847, 1.9027,\n",
      "        1.9387, 2.1132, 1.9424, 1.9731, 1.9452], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0915, -0.0714,  0.0588,  ..., -0.0032, -0.0169, -0.0294],\n",
      "        [ 0.0116,  0.0402,  0.0047,  ..., -0.0320,  0.0208,  0.0288],\n",
      "        [-0.0471,  0.0851, -0.0107,  ..., -0.0535,  0.0484, -0.0171],\n",
      "        ...,\n",
      "        [ 0.0645, -0.1493,  0.0411,  ...,  0.0696,  0.0132, -0.0090],\n",
      "        [-0.0267,  0.0851,  0.0993,  ..., -0.0058,  0.0118, -0.0687],\n",
      "        [-0.0289,  0.1402,  0.0259,  ..., -0.0161, -0.0446,  0.0596]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0415,  0.0729,  0.1431,  ..., -0.0753, -0.1106, -0.0613],\n",
      "        [-0.0254, -0.0852,  0.1126,  ...,  0.0349, -0.0273, -0.0711],\n",
      "        [ 0.0217,  0.0666, -0.1168,  ..., -0.0975,  0.0970,  0.0671],\n",
      "        ...,\n",
      "        [-0.0635,  0.0283, -0.0731,  ...,  0.0655, -0.1097,  0.0682],\n",
      "        [ 0.0096,  0.0344,  0.0452,  ...,  0.1059,  0.1194,  0.0186],\n",
      "        [ 0.0120, -0.0659,  0.0237,  ...,  0.0144,  0.0119, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2001, 2.0649, 2.1826, 2.2333, 2.5525, 2.2693, 2.1388, 2.3175, 2.2945,\n",
      "        2.1789, 2.1131, 2.4515, 2.5238, 2.0640, 2.3606, 2.1918, 2.2525, 2.1239,\n",
      "        2.4282, 2.0793, 2.4574, 2.4551, 2.0395, 2.1859, 2.1356, 1.9936, 2.2356,\n",
      "        2.2176, 2.4658, 2.2351, 2.1030, 2.0881, 2.4467, 2.1637, 2.1030, 2.0851,\n",
      "        2.1193, 2.2560, 2.3777, 2.1805, 2.1419, 2.3535, 2.3202, 2.2070, 2.3381,\n",
      "        2.1412, 3.1431, 1.9852, 2.2996, 2.5638, 2.2664, 2.2294, 2.0731, 2.3491,\n",
      "        2.0709, 2.1836, 2.2054, 2.3917, 2.6282, 2.4052, 2.3250, 2.1080, 6.9229,\n",
      "        2.1975, 2.1994, 2.1904, 2.3762, 2.6559, 2.3245, 2.1622, 2.2235, 2.1452,\n",
      "        2.0118, 2.3210, 2.1238, 2.3784, 2.2433, 2.4320, 1.9149, 1.9942, 2.0483,\n",
      "        2.1285, 2.3058, 2.2698, 2.0258, 2.1690, 2.4940, 2.4309, 1.9704, 2.2228,\n",
      "        2.1342, 2.0366, 2.4102, 2.2417, 2.2248, 2.2922, 2.1154, 2.2886, 2.1748,\n",
      "        2.2911, 2.3384, 2.2690, 2.4402, 2.4001, 2.3141, 2.3590, 2.1077, 2.2956,\n",
      "        2.0209, 2.3123, 2.2632, 2.2647, 2.2199, 2.1725, 2.4502, 2.1415, 2.2513,\n",
      "        2.3008, 2.4727, 2.5089, 2.4013, 2.2929, 2.2879, 2.2217, 2.0046, 2.5290,\n",
      "        2.1169, 2.1518, 2.2462, 2.3462, 2.1483, 2.0092, 2.1823, 2.1726, 2.4301,\n",
      "        2.1273, 2.2263, 2.4388, 2.1580, 2.3052, 2.1156, 2.1031, 2.2378, 2.3579,\n",
      "        2.3933, 2.2964, 2.1586, 2.3353, 2.1749, 2.2731, 2.4370, 2.2586, 2.3892,\n",
      "        2.1547, 2.2835, 2.2933, 2.0832, 2.3654, 2.2919, 2.1714, 2.0760, 2.2150,\n",
      "        2.3346, 2.3733, 2.3468, 2.3351, 2.3019, 2.1078, 2.3303, 2.1595, 2.3057,\n",
      "        2.3066, 2.2320, 2.2088, 2.2607, 2.0505, 2.2779, 2.2448, 2.1731, 2.0139,\n",
      "        2.0261, 2.0946, 2.1817, 2.1949, 2.3202, 2.1889, 2.3096, 2.2494, 1.9641,\n",
      "        2.2257, 2.1471, 2.0857, 3.3503, 2.1548, 2.2182, 2.2905, 2.4367, 2.2555,\n",
      "        2.3901, 2.2950, 2.2943, 2.3602, 2.2790, 2.2419, 2.2514, 2.3281, 1.9495,\n",
      "        2.2518, 2.6105, 2.1610, 2.1901, 2.3634, 2.3917, 2.3293, 2.3204, 2.1613,\n",
      "        2.1776, 2.0311, 2.0213, 2.1334, 2.0871, 2.1079, 2.4267, 2.4329, 2.3723,\n",
      "        2.2466, 2.3325, 2.1380, 1.9406, 2.3966, 2.1306, 2.1025, 2.0609, 2.3101,\n",
      "        2.0160, 2.1705, 2.2525, 2.2484, 2.0993, 2.2830, 2.1215, 2.0874, 2.3337,\n",
      "        2.0839, 2.2686, 2.0305, 2.3525, 1.9844, 2.0573, 2.3921, 2.2612, 2.1623,\n",
      "        2.0428, 1.9955, 2.2409, 2.2393, 2.3304, 2.0546, 2.3170, 2.3081, 2.1124,\n",
      "        2.1868, 2.2249, 2.1425, 2.1851, 2.1879, 2.3203, 2.8144, 2.3924, 2.1185,\n",
      "        1.9596, 2.1890, 2.0378, 2.4455, 2.4396, 2.3690, 2.1453, 2.0890, 2.2988,\n",
      "        2.1969, 2.2344, 2.1081, 2.2689, 2.5746, 2.1945, 2.2806, 2.2878, 2.3380,\n",
      "        2.1081, 2.1647, 2.1794, 2.3166, 2.2429, 2.3029, 2.0794, 2.0539, 2.1944,\n",
      "        2.0975, 2.0556, 2.2989, 2.2117, 2.1806, 2.1987, 2.1547, 2.3704, 2.2298,\n",
      "        2.3404, 2.0101, 2.0088, 2.0561, 2.3239, 1.9865, 1.9307, 2.3306, 2.3229,\n",
      "        2.1678, 2.3807, 2.1174, 2.2179, 2.0103], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in light_mod.model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.classifier.dense.original_module.weight\n",
      "base_model.model.classifier.dense.original_module.bias\n",
      "base_model.model.classifier.dense.modules_to_save.default.weight\n",
      "base_model.model.classifier.dense.modules_to_save.default.bias\n",
      "base_model.model.classifier.out_proj.original_module.weight\n",
      "base_model.model.classifier.out_proj.original_module.bias\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.weight\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in light_mod.model.named_parameters():\n",
    "    if \"lora\" in name or \"classifier\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | PeftModel | 8.2 M \n",
      "------------------------------------\n",
      "356 K     Trainable params\n",
      "7.8 M     Non-trainable params\n",
      "8.2 M     Total params\n",
      "32.790    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d06d9e6e2e400684c83c5d2ba8d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:213: You called `self.log('Val_MCC', ...)` in your `on_validation_epoch_end` but the value needs to be floating to be reduced. Converting it to torch.float32. You can silence this warning by converting the value to floating point yourself. If you don't intend to reduce the value (for instance when logging the global step or epoch) then you can use `self.logger.log_metrics({'Val_MCC': ...})` instead.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No negative samples in targets, false positive value should be meaningless. Returning zero tensor in false positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples found in target, recall is undefined. Setting recall to one for all thresholds.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff1776405334b089086c5aed072132b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df78fc9d7ae5450081a842ad5340006b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved. New best score: -0.170\n",
      "Epoch 0, global step 47: 'Val_MCC' reached -0.17045 (best -0.17045), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=0-Val_MCC=-0.17.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef5c7b37148c469ca3de097216e897fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.439 >= min_delta = 0.005. New best score: 0.269\n",
      "Epoch 1, global step 94: 'Val_MCC' reached 0.26857 (best 0.26857), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=1-Val_MCC=0.27.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f89399c58624a94b6ab5c526b6900d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 141: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61a1d988eac4a61af1ba9e522e49415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.037 >= min_delta = 0.005. New best score: 0.306\n",
      "Epoch 3, global step 188: 'Val_MCC' reached 0.30551 (best 0.30551), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=3-Val_MCC=0.31.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da268b20003b4b72b506bc85173c7e7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.033 >= min_delta = 0.005. New best score: 0.338\n",
      "Epoch 4, global step 235: 'Val_MCC' reached 0.33806 (best 0.33806), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=4-Val_MCC=0.34.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b69a18d9f14c71bed7ef4d810c69a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.100 >= min_delta = 0.005. New best score: 0.438\n",
      "Epoch 5, global step 282: 'Val_MCC' reached 0.43819 (best 0.43819), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=5-Val_MCC=0.44.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3039f5bf2e054ab3b5197a52609db68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 329: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9d58fff41584ecf9c259158f7600927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.069 >= min_delta = 0.005. New best score: 0.507\n",
      "Epoch 7, global step 376: 'Val_MCC' reached 0.50709 (best 0.50709), saving model to 'model_checkpoint/lightning_logs/version_1/checkpoints/epoch=7-Val_MCC=0.51.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a40c0d83e2497d8b2b05cdb40df27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 423: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b37d142979fb48e4aad936b9ac2fa87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 470: 'Val_MCC' was not in top 1\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = ft.Trainer(callbacks=[checkpoint_callback, early_callback], default_root_dir=train_config.model_checkpoint_dir,\n",
    "                          fast_dev_run=bool(train_config.debug_mode_sample), max_epochs=10, \n",
    "                          max_time=train_config.max_time, precision=train_config.precision,\n",
    "                          accumulate_grad_batches=train_config.accumulate_grad_batches)\n",
    "\n",
    "#tuner = Tuner(trainer)\n",
    "#lr_finder = tuner.lr_find(light_mod, data_module, min_lr=1e-6, max_lr=1, num_training=1000, mode=\"exponential\", early_stop_threshold=4)\n",
    "\n",
    "trainer.fit(model=light_mod, datamodule=data_module)\n",
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(peft_model: str, llm_config,\n",
    "                 use_adapter: str=\"initial\", adapters: dict[str, str] | None=None):\n",
    "    device = \"auto\" if llm_config.device == \"cuda\" else llm_config.device\n",
    "    model = AutoPeftModel.from_pretrained(peft_model, adapter_name=\"initial\", \n",
    "                                                                   low_cpu_mem_usage=True, device_map=device,\n",
    "                                                                   torch_dtype=llm_config.dtype)                                                                \n",
    "    if adapters:\n",
    "        for key, value in adapters.items():\n",
    "            model.load_adapter(value, adapter_name=key)\n",
    "    model.set_adapter(use_adapter)\n",
    "    model.merge_adapter()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ft.TransformerModule.load_from_checkpoint(best_model_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.prepare_data()\n",
    "data_module.setup(\"fit\")\n",
    "inputs = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[20, 15, 15,  ...,  1,  1,  1],\n",
      "        [20, 15, 15,  ...,  1,  1,  1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([0, 1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in inputs:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "a = load_adapter(\"model\", ft.LLMConfig(), use_adapter=\"initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6370, -0.5562],\n",
       "        [ 0.3029, -0.2428]], device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForSequenceClassification(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 320, padding_idx=1)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 320, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-5): 6 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=320, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 320 (cuda:0)])\n",
       "                  )\n",
       "                  (key): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=320, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 320 (cuda:0)])\n",
       "                  )\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=320, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 320 (cuda:0)])\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (rotary_embeddings): RotaryEmbedding()\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=320, out_features=320, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.05, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=320, out_features=16, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=16, out_features=320, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ParameterDict(  (default): Parameter containing: [torch.cuda.FloatTensor of size 320 (cuda:0)])\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=1280, out_features=320, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (classifier): EsmClassificationHead(\n",
       "        (dense): ModulesToSaveWrapper(\n",
       "          (original_module): Linear(in_features=320, out_features=320, bias=True)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Linear(in_features=320, out_features=320, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (out_proj): ModulesToSaveWrapper(\n",
       "          (original_module): Linear(in_features=320, out_features=2, bias=True)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Linear(in_features=320, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6370, -0.5562],\n",
       "        [ 0.3029, -0.2428]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits # stochastic dorpout if not eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6370, -0.5562],\n",
       "        [ 0.3029, -0.2428]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
