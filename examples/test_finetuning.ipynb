{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to finetune HuggingFace models on text data of any size and format with custom splitting (not random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to handle text data of any size and format with custom split because random splitting is not recommended for protein sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "from datasets import Dataset, DatasetDict\n",
    "from BioML.utilities import split_methods\n",
    "from BioML.deep.embeddings import LLMConfig, TokenizeFasta\n",
    "from BioML.deep.utils import set_seed\n",
    "from peft import get_peft_model, LoraConfig\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/text_classification.ipynb\n",
    "## https://github.com/huggingface/notebooks/blob/main/examples/protein_language_modeling-tf.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to label the target values as labels so Trainer can recognize it.\n",
    "Dataset can actually be used for any usecases with large   files it doesn't depend on transformers  \n",
    "Although you would need to use PyTorch Dataloader to transform it into batches (but it only returns inputs ids and attention masks will it also return labels?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "y = np.random.randint(0, 2, size=len(b))\n",
    "dataset = b.add_column(\"labels\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TokenizeFasta()\n",
    "tokens = tok.tokenize(\"../data/whole_sequence.fasta\", ([\"labels\", y],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom spliting with indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = split_methods.ClusterSpliter(\"../data/resultsDB_clu.tsv\")\n",
    "train, test = cluster.train_test_split(range(len(dataset)), groups=dataset[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(range(len(dataset)), stratify=dataset[\"labels\"], test_size=0.2) # random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = DatasetDict({\"train\":dataset.select(train), \"test\":dataset.select(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, validation = cluster.train_test_split(range(len(new[\"train\"])), groups=new[\"train\"][\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_2 = DatasetDict({\"train\":new[\"train\"].select(train_), \"test\":dataset.select(test), \"validation\": new[\"train\"].select(validation)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the protein language models model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def model_init(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2) # torch_dtype=torch.bfloat16 to load in bfloat16 which is accepted by CPUs unlike float16\n",
    "\n",
    "def model_init2(): # 0 or 1 parameters ( the trial hyperparameters)\n",
    "\treturn AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", low_cpu_mem_usage=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f68842954984e87abc3735b7d39e729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef3b4ce46174a68bbf041a21f15b511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new[\"train\"] = new[\"train\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)\n",
    "new[\"test\"] = new[\"test\"].map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 8e-5\n",
    "bs = 1\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se use cpu to False whe you wan to use GPUs (it will automatically use GPUs), when f16 is True it will only use GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.2, lr_scheduler_type='cosine', fp16=False if device==\"cpu\" else True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.01, report_to=['mlflow'],\n",
    "    load_best_model_at_end=True, metric_for_best_model=\"matthews_correlation\", \n",
    "    save_total_limit=2, save_strategy=\"epoch\", seed=3242342, gradient_accumulation_steps=4, use_cpu=True if device==\"cpu\" else False) \n",
    "\n",
    "## The warmup step together with cosine learning rate scheduler turns to onecycle learning rate scheduler\n",
    "## weight decay for the Adam (AdamW) -> this is fast.Ai does\n",
    "## fp16 is half precision -> mixed training (using fp32 and fp16)\n",
    "## save_total_limit to 3 -> so only 3 models will be saved\n",
    "## each 500 steps will be saved a model\n",
    "## Save the report to mlflow\n",
    "# How to evaluate mlflow?\n",
    "# LR finder does not give reliable results for Transformers models https://github.com/huggingface/transformers/issues/16013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model using several evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlflow\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evaluate'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own function as an evaluation metric -> then you have to retun as an dict  \n",
    "Or you can use the evaluate library from hugging face to load different functions: [evaluate](https://huggingface.co/docs/evaluate/a_quick_tour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_metrics(eval_pred):\n",
    "    metrics = [\"accuracy\", \"f1\", \"matthews_correlation\", \"precision\", \"recall\"]\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    loaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "    results = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "               for metric in metrics}\n",
    "\n",
    "    # the predictions from the models are logits (it also returns the labels, \n",
    "    # it also returns loss, attentions and hidden state but that is the classification model, for evalaution Trainer will only \n",
    "    # return logits and labels)\n",
    "    return results\n",
    "\n",
    "def compute_regression_metrics(eval_pred):\n",
    "\tmetrics = [\"mse\", \"mae\"]\n",
    "\tlogits, labels = eval_pred\n",
    "\tpredictions = logits\n",
    "\tloaded = {metric:evaluate.load(metric) for metric in metrics}\n",
    "\tresults = {metric: loaded[metric].compute(predictions=predictions, references=labels)[metric] \n",
    "\t\t\t   for metric in metrics}\n",
    "\tresults[\"r2\"] = evaluate.load(\"r_squared\").compute(predictions=predictions, references=labels)\n",
    "\tresults[\"rmse\"] = loaded[\"mse\"].compute(predictions=predictions, references=labels, squared=False)[\"mse\"]\n",
    "\treturn results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        print(inputs)\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.compute_loss(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, args, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, compute_metrics=compute_classification_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters learning rate and batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-2, log=True),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8, 16]),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_classification_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_matthews_correlation\"]\n",
    "\n",
    "def compute_regression_objective(metrics: dict[str, float]) -> tuple[float, float]:\n",
    "\treturn metrics[\"eval_loss\"], metrics[\"eval_r2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "model_init should have 0 or 1 argument.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# we need to pass tokenized datasets\u001b[39;49;00m\n\u001b[0;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_classification_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStoppingCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:389\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;241m=\u001b[39m model_init\n\u001b[1;32m--> 389\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_model_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    391\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\trainer.py:1457\u001b[0m, in \u001b[0;36mTrainer.call_model_init\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init(trial)\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should have 0 or 1 argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_init should not return None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: model_init should have 0 or 1 argument."
     ]
    }
   ],
   "source": [
    "trainer = Trainer(None, args, model_init=model_init2, train_dataset=new['train'], eval_dataset=new['test'], # we need to pass tokenized datasets\n",
    "                  tokenizer=tokenizer, \n",
    "                  compute_metrics=compute_classification_metrics, \n",
    "                  callbacks=[EarlyStoppingCallback(early_stopping_patience=2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:27:53,211] A new study created in RDB with name: no-name-305c8b7d-b552-4cd6-b1d9-2e327a424486\n",
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bff178a0fc74c9b9ffc182ef25172a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fd50fa88104f34832ff3bb22d0fcc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6917587518692017, 'eval_accuracy': 0.5666666666666667, 'eval_f1': 0.0, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_runtime': 18.5898, 'eval_samples_per_second': 1.614, 'eval_steps_per_second': 0.807, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ddca6ef5484552b6fbd08fc014c515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7241240739822388, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 17.8967, 'eval_samples_per_second': 1.676, 'eval_steps_per_second': 0.838, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d8f83bc982433cbba6a578c3844d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7805582880973816, 'eval_accuracy': 0.5, 'eval_f1': 0.4444444444444444, 'eval_matthews_correlation': -0.008988968316207744, 'eval_precision': 0.42857142857142855, 'eval_recall': 0.46153846153846156, 'eval_runtime': 18.3333, 'eval_samples_per_second': 1.636, 'eval_steps_per_second': 0.818, 'epoch': 2.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-05 18:36:15,996] Trial 0 finished with values: [0.7805582880973816, -0.008988968316207744] and parameters: {'learning_rate': 7.813286994811102e-05, 'gradient_accumulation_steps': 4}. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 501.8248, 'train_samples_per_second': 0.933, 'train_steps_per_second': 0.231, 'train_loss': 0.6711522244859016, 'epoch': 2.97}\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "\n",
    "best_trials = trainer.hyperparameter_search(\n",
    "\tdirection=[\"minimize\", \"maximize\"],\n",
    "\tbackend=\"optuna\",\n",
    "\thp_space=optuna_hp_space,\n",
    "\tn_trials=1,\n",
    "\tcompute_objective=compute_classification_objective,\n",
    "    storage='sqlite:///my_optuna_studies.db',\n",
    "    load_if_exists=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.learning_rate = best_trials[0].hyperparameters[\"learning_rate\"]\n",
    "trainer.args.gradient_accumulation_steps = best_trials[0].hyperparameters[\"gradient_accumulation_steps\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3914d1c5b36242109903c985c86c6faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/116 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29f5a46314b493cb66386ad59cc7861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7221236824989319, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.508, 'eval_samples_per_second': 0.952, 'eval_steps_per_second': 0.476, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e19a75be81b47138b51a3809d41f9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7303746938705444, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2142, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.481, 'epoch': 1.98}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b5477268b8416aae340e6bca02f7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7445932030677795, 'eval_accuracy': 0.5333333333333333, 'eval_f1': 0.6956521739130436, 'eval_matthews_correlation': 0.0, 'eval_precision': 0.5333333333333333, 'eval_recall': 1.0, 'eval_runtime': 31.2265, 'eval_samples_per_second': 0.961, 'eval_steps_per_second': 0.48, 'epoch': 2.97}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b70848b104408281ab0169afb06ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.753971517086029, 'eval_accuracy': 0.6, 'eval_f1': 0.7272727272727273, 'eval_matthews_correlation': 0.2857142857142857, 'eval_precision': 0.5714285714285714, 'eval_recall': 1.0, 'eval_runtime': 31.5365, 'eval_samples_per_second': 0.951, 'eval_steps_per_second': 0.476, 'epoch': 3.97}\n",
      "{'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=116, training_loss=0.6006371070598734, metrics={'train_runtime': 618.7594, 'train_samples_per_second': 0.756, 'train_steps_per_second': 0.187, 'train_loss': 0.6006371070598734, 'epoch': 3.97})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that we get the same results by evaluating the results once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for hyperparameters like the learning rate which is the most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well it is actually batch size and learning rate -> smaller batch sizes tend to work better than large batch sizes -> but learning rate is affected by batch as well -> higher abtch need higher learning rate.\n",
    "\n",
    "Fix everything else and tune the learning rate -> learning rate finder doesn'0t seem to work very well for transformers?  \n",
    "But teh idea of learning rate finder is just test different learning rates -> so I cannot test them?\n",
    "\n",
    "Ktrains: A wrapper to do many tasks and has a learning rate finder: [ktrains](https://github.com/amaiya/ktrain)\n",
    "\n",
    "Use pytorch lightning perhaps: [pytorch_lighningt_huggingface](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a552ef6cc74743f4b55e8d0d0bfe2327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3b6497c18947b4a880757af2b28b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/44.5G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbigscience/T0pp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\modeling_utils.py:3190\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   3175\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m   3176\u001b[0m     cached_file_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3177\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_dir,\n\u001b[0;32m   3178\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_download\u001b[39m\u001b[38;5;124m\"\u001b[39m: force_download,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3188\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m: commit_hash,\n\u001b[0;32m   3189\u001b[0m     }\n\u001b[1;32m-> 3190\u001b[0m     resolved_archive_file \u001b[38;5;241m=\u001b[39m cached_file(pretrained_model_name_or_path, filename, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcached_file_kwargs)\n\u001b[0;32m   3192\u001b[0m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[0;32m   3193\u001b[0m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[0;32m   3194\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename \u001b[38;5;241m==\u001b[39m _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[0;32m   3195\u001b[0m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\transformers\\utils\\hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:119\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    117\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:1492\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, headers, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[0;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1490\u001b[0m             _check_disk_space(expected_size, local_dir)\n\u001b[1;32m-> 1492\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\huggingface_hub\\file_download.py:535\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    533\u001b[0m new_resume_size \u001b[38;5;241m=\u001b[39m resume_size\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m r\u001b[38;5;241m.\u001b[39miter_content(chunk_size\u001b[38;5;241m=\u001b[39mDOWNLOAD_CHUNK_SIZE):\n\u001b[0;32m    536\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    537\u001b[0m             progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\requests\\models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:1043\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1043\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1045\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[0;32m   1046\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:935\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m amt:\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer\u001b[38;5;241m.\u001b[39mget(amt)\n\u001b[1;32m--> 935\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    937\u001b[0m flush_decoder \u001b[38;5;241m=\u001b[39m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data)\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:862\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    859\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_catcher():\n\u001b[1;32m--> 862\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fp_closed \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Platform-specific: Buggy versions of Python.\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;66;03m# Close the connection when no data is returned\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;66;03m# not properly close the connection in all cases. There is\u001b[39;00m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# no harm in redundantly calling close.\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\site-packages\\urllib3\\response.py:845\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[1;34m(self, amt, read1)\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1(amt) \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread1()\n\u001b[0;32m    843\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m amt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\http\\client.py:466\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    464\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 466\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    468\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\ruite\\miniforge3\\envs\\bioml\\lib\\ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model = AutoModel.from_pretrained(\"bigscience/T0pp\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter efficient fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from torch.optim import AdamW, Optimizer\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from lightning import LightningModule, LightningDataModule\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "from lightning import Trainer\n",
    "from torchmetrics.functional.classification import (\n",
    "    accuracy,\n",
    "    f1_score,\n",
    "    precision,\n",
    "    recall,\n",
    "    auroc,\n",
    "    average_precision,\n",
    "    cohen_kappa,\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef\n",
    ") \n",
    "\n",
    "from torchmetrics.functional.regression import (\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    pearson_corrcoef,\n",
    "    kendall_rank_corrcoef,\n",
    "    r2_score,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_log_error)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "from BioML.deep.train_config import LLMConfig\n",
    "from BioML.utilities import split_methods as split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                     target: torch.tensor, num_classes: int=2, threshold: float=0.5):\n",
    "    task = \"binary\" if num_classes == 2 else \"multiclass\"\n",
    "    metrics = {\n",
    "                f\"{split}_Loss\": loss,\n",
    "                f\"{split}_Acc\": accuracy(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_F1\":f1_score(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Precision\": precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Recall\": recall(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    task=task,\n",
    "                    num_classes=num_classes,\n",
    "                    average=\"weighted\"\n",
    "                ),\n",
    "                f\"{split}_MCC\": matthews_corrcoef(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    threshold=threshold,\n",
    "                    task=task,\n",
    "                ),\n",
    "                f\"{split}_Confusion_Matrix\": confusion_matrix(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    normalize=\"true\",\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                ),\n",
    "                f\"{split}_AUROC\": auroc(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    thresholds=None,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Average_Precision\": average_precision(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    average=\"weighted\",\n",
    "                ),\n",
    "                f\"{split}_Cohen_Kappa\": cohen_kappa(\n",
    "                    preds=preds,\n",
    "                    target=target,\n",
    "                    num_classes=num_classes,\n",
    "                    task=task,\n",
    "                    threshold=threshold,\n",
    "                )}\n",
    "    return metrics\n",
    "\n",
    "def calculate_regression_metrics(split: str, loss: torch.tensor, preds: torch.tensor, \n",
    "                                 target: torch.tensor):\n",
    "    metrics = {f\"{split}_Loss\": loss,\n",
    "                f\"{split}_MAE\": mean_absolute_error(preds, target),\n",
    "                f\"{split}_MSE\": mean_squared_error(preds, target),\n",
    "                f\"{split}_RMSE\": mean_squared_error(preds, target, squared=False),\n",
    "                f\"{split}_R2\": r2_score(preds, target),\n",
    "                f\"{split}_Pearson\": pearson_corrcoef(preds, target),\n",
    "                f\"{split}_Kendall\": kendall_rank_corrcoef(preds, target),\n",
    "                f\"{split}_MAPE\": mean_absolute_percentage_error(preds, target),\n",
    "                f\"{split}_MSLE\": mean_squared_log_error(preds, target)}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_values = (8, 16, 32, 64, 128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "def test(x, y):\n",
    "    return x + y\n",
    "class Test:\n",
    "    def __init__(self):\n",
    "        self.x = test\n",
    "        self.tes_ = partial(self.x, y=2)\n",
    "    def __call__(self, x):\n",
    "        return self.tes_(x)\n",
    "\n",
    "t = Test()\n",
    "t(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.esm.encoder.layer.0.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.0.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.1.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.2.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.3.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.4.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.attention.output.dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.base_layer',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_dropout.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_A.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_B.default',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_A',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dense.lora_embedding_B',\n",
       " 'base_model.model.esm.encoder.layer.5.output.dropout']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_target_module_names_for_peft(model, filter_=\"key\"):\n",
    "    if isinstance(filter_, str):\n",
    "        filter_ = [filter_] # if it is a string, convert it to a list\n",
    "    module_names = []\n",
    "    for num, (name, module) in enumerate(model.named_modules()):\n",
    "        n = name.split(\".\")\n",
    "        if filter_ and set(n).intersection(filter_):\n",
    "            module_names.append(name)\n",
    "        elif not filter_:\n",
    "            module_names.append(name)\n",
    "    return module_names\n",
    "\n",
    "names = get_target_module_names_for_peft(model, filter_=\"output\")\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training, replace_lora_weights_loftq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=16, lora_dropout=0.1, \n",
    "                         target_modules=\"all-linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 285,144 || all params: 8,125,907 || trainable%: 3.509072894878073\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from BioML.deep import finetuning as ft\n",
    "from BioML.deep.utils import load_adapter\n",
    "from datasets import Dataset\n",
    "from Bio import SeqIO\n",
    "from safetensors import SafetensorError\n",
    "import torch\n",
    "from peft import replace_lora_weights_loftq, AutoPeftModelForSequenceClassification, AutoPeftModel\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"../data/esterase_labels.csv\"\n",
    "lab = pd.read_csv(label, index_col=0)\n",
    "split_config = ft.SplitConfig()\n",
    "llm_config = ft.LLMConfig()\n",
    "train_config = ft.TrainConfig(2, batch_size=2, max_epochs=1, lora_rank=8, optimize=\"Val_MCC\")\n",
    "fasta_file = \"../data/whole_sequence.fasta\"\n",
    "label_regre = np.array(list(map(float, range(len(lab)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'classification'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_config.target_modules = ['query', 'key', 'value', 'attention.output.dense']\n",
    "train_config.objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ft.TokenizeFasta(llm_config)\n",
    "data = tokenizer.tokenize(fasta_file, add_columns=[(\"labels\", lab.to_numpy().flatten())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 233,922 || all params: 8,074,685 || trainable%: 2.8970\n"
     ]
    }
   ],
   "source": [
    "splitter = ft.PrepareSplit(split_config.cluster_file, split_config.shuffle, split_config.random_seed, \n",
    "                            split_config.splitting_strategy, \n",
    "                            split_config.num_split, False)\n",
    "data_module = ft.DataModule(splitter, fasta_file, lab.to_numpy().flatten(), llm_config, train_config.batch_size)\n",
    "peft = ft.PreparePEFT(train_config, llm_config, \"pissa\")\n",
    "model = peft.prepare_model()\n",
    "light_mod = ft.TransformerModule(model, train_config, lr=1e-3)\n",
    "\n",
    "filename = f\"{{epoch}}-{{{train_config.optimize}:.2f}}\"\n",
    "checkpoint_callback = ft.ModelCheckpoint(filename=filename, monitor=train_config.optimize, \n",
    "                                              mode=train_config.optimize_mode, verbose=True, save_top_k=1)\n",
    "early_callback = ft.EarlyStopping(monitor=train_config.optimize, min_delta=train_config.min_delta, \n",
    "                                       patience=train_config.patience, verbose=True, mode=train_config.optimize_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 233,922 || all params: 8,074,685 || trainable%: 2.8970\n"
     ]
    }
   ],
   "source": [
    "peft = ft.PreparePEFT(train_config, llm_config)\n",
    "model2 = peft.prepare_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.classifier.dense.modules_to_save.default.weight\n",
      "base_model.model.classifier.dense.modules_to_save.default.bias\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.weight\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if params.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1699, -0.8279,  0.0260,  ...,  0.0469, -0.0843,  0.0254],\n",
      "        [-0.3562,  0.0308,  0.0865,  ...,  0.0591,  0.2071, -0.2667],\n",
      "        [ 0.0750,  0.1241,  0.0656,  ..., -0.0567,  0.0863,  0.0597],\n",
      "        ...,\n",
      "        [-0.0034, -0.1483, -0.1708,  ..., -0.0852, -0.1563, -0.0351],\n",
      "        [ 0.0784, -0.0335,  0.0649,  ...,  0.0123, -0.1528, -0.1552],\n",
      "        [ 0.0145,  0.0106, -0.0876,  ...,  0.0228,  0.0360, -0.0452]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0273, -0.0062, -0.0049,  ...,  0.0532, -0.0117,  0.0456],\n",
      "        [ 0.1398,  0.0231, -0.0385,  ...,  0.0212,  0.0046,  0.0588],\n",
      "        [ 0.1207,  0.0045,  0.0326,  ...,  0.0863,  0.0458, -0.0689],\n",
      "        ...,\n",
      "        [ 0.0235, -0.0163,  0.0695,  ...,  0.0183,  0.0525, -0.0219],\n",
      "        [-0.0018,  0.0108,  0.0602,  ..., -0.0264,  0.1311,  0.0807],\n",
      "        [ 0.0329, -0.0148, -0.0398,  ..., -0.1342,  0.0230,  0.0035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5853, 0.9414, 0.8864, 1.0005, 0.8341, 1.6792, 0.8581, 1.3855, 0.7455,\n",
      "        0.6171, 0.8099, 1.1151, 1.2725, 0.9141, 1.2154, 1.8849, 2.3473, 1.6398,\n",
      "        1.3562, 0.8318, 2.1569, 0.5517, 1.0800, 1.0548, 0.8301, 0.7455, 1.4008,\n",
      "        1.6584, 1.0272, 2.0568, 1.1916, 2.2181, 1.3737, 1.6398, 1.6193, 1.6319,\n",
      "        1.5264, 0.9735, 1.3574, 1.1211, 1.3794, 0.7246, 0.9086, 0.7327, 0.9418,\n",
      "        1.7050, 1.5984, 1.0973, 0.5182, 0.8853, 0.7954, 0.7849, 1.0845, 1.2646,\n",
      "        0.8790, 1.1035, 0.6633, 0.8367, 1.0042, 1.2310, 1.3798, 0.4869, 1.3834,\n",
      "        1.1706, 1.1599, 0.8857, 0.9255, 1.6514, 1.8569, 0.6478, 1.6139, 1.1124,\n",
      "        1.7787, 1.4179, 1.4084, 0.7719, 0.4653, 1.7452, 1.6296, 1.4729, 1.1574,\n",
      "        1.2324, 0.9352, 1.2625, 0.6487, 1.1540, 1.5481, 1.6548, 1.6443, 1.1342,\n",
      "        1.5609, 1.3580, 1.6658, 1.8118, 1.0891, 1.5124, 0.5507, 0.5390, 0.6970,\n",
      "        0.8906, 0.6116, 1.2291, 2.0492, 1.5262, 0.6264, 0.5575, 0.5198, 1.3949,\n",
      "        1.0530, 1.1037, 1.2533, 1.6176, 0.4951, 0.4427, 0.6902, 0.5102, 0.6893,\n",
      "        1.0745, 1.0676, 0.9169, 0.4903, 0.5838, 0.6322, 0.6646, 1.2191, 0.5728,\n",
      "        1.4091, 1.2412, 0.8634, 0.6194, 1.5913, 1.5369, 1.0518, 0.8959, 1.0386,\n",
      "        0.7692, 0.3437, 0.8577, 1.3768, 0.9904, 1.1698, 1.2362, 1.0852, 0.7233,\n",
      "        1.8126, 1.2617, 1.5355, 1.6066, 2.2761, 0.4591, 1.5947, 1.8179, 1.6002,\n",
      "        1.2395, 1.2208, 0.5141, 0.4554, 2.5645, 1.4504, 0.9397, 0.7354, 0.8315,\n",
      "        1.0938, 1.1105, 1.3614, 1.5309, 1.5623, 0.7575, 1.0908, 1.0442, 0.8914,\n",
      "        1.3379, 1.8194, 1.3875, 0.7927, 0.7337, 1.2977, 0.9300, 1.2044, 1.9222,\n",
      "        1.4305, 1.3003, 0.9287, 1.6573, 0.7350, 0.8158, 1.0075, 1.9005, 1.2888,\n",
      "        1.2273, 1.4752, 1.2818, 0.5897, 1.1480, 0.8825, 1.1704, 1.2369, 0.6458,\n",
      "        1.0872, 1.6619, 0.4945, 0.9241, 1.2399, 0.9323, 0.7577, 1.0398, 1.7767,\n",
      "        0.9969, 0.4415, 0.6359, 1.4633, 0.9700, 0.8584, 0.7347, 0.6810, 0.8217,\n",
      "        0.5012, 0.5886, 0.3603, 1.1078, 1.1681, 1.3178, 0.7116, 0.6306, 3.4055,\n",
      "        2.2885, 2.3034, 2.3188, 1.4570, 2.1923, 1.5341, 1.7381, 3.1816, 2.2742,\n",
      "        2.9759, 1.6584, 2.8653, 1.8699, 2.1077, 2.2289, 1.1329, 0.8952, 0.5713,\n",
      "        1.4181, 1.8599, 0.8214, 1.1055, 1.2004, 0.6950, 1.1168, 1.4317, 1.1842,\n",
      "        0.5058, 1.8753, 1.0431, 0.8566, 0.5702, 1.1380, 1.4764, 1.2667, 1.6866,\n",
      "        0.5480, 1.7491, 0.8235, 1.0170, 0.9379, 0.5901, 1.5741, 0.7238, 1.8894,\n",
      "        1.9161, 0.8679, 1.3350, 1.5763, 1.6763, 1.3810, 1.2394, 0.7780, 1.7551,\n",
      "        1.2473, 0.9061, 1.0333, 2.0561, 1.9544, 2.2742, 2.4918, 1.1108, 1.1175,\n",
      "        1.6286, 0.7643, 1.6727, 1.8822, 1.2415, 0.7075, 1.6238, 1.0862, 0.8523,\n",
      "        1.2977, 0.5647, 0.9873, 1.1112, 1.9990, 1.8551, 1.1154, 0.5343, 0.5222,\n",
      "        1.3942, 0.4759, 0.4580, 2.1503, 1.5154, 1.1260, 1.4921, 1.9562, 1.1058,\n",
      "        1.4934, 2.3807, 0.5520, 0.8688, 1.0518], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0983,  0.5507,  0.0746,  ..., -0.0111,  0.1217,  0.0652],\n",
      "        [ 0.0198,  0.0509,  0.0957,  ..., -0.0973, -0.3128, -0.1401],\n",
      "        [ 0.1018, -0.4264,  0.3920,  ...,  0.0732,  0.0144, -0.0555],\n",
      "        ...,\n",
      "        [-0.0891, -0.0386, -0.0503,  ..., -0.0899,  0.2477,  0.2061],\n",
      "        [ 0.0940,  0.0337, -0.0276,  ..., -0.1335, -0.0683,  0.0346],\n",
      "        [ 0.0818,  0.2765, -0.0031,  ...,  0.0455, -0.0677, -0.2436]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0168, -0.0270, -0.0437,  ...,  0.1121, -0.0504, -0.0876],\n",
      "        [-0.0428,  0.0307,  0.2387,  ...,  0.0859, -0.0233,  0.0573],\n",
      "        [-0.0325,  0.0235, -0.0456,  ..., -0.0668, -0.0114,  0.2030],\n",
      "        ...,\n",
      "        [ 0.2680, -0.1350,  0.0185,  ...,  0.1096,  0.0893, -0.1141],\n",
      "        [ 0.0432, -0.0036, -0.0414,  ...,  0.0023,  0.0168, -0.0585],\n",
      "        [-0.0194,  0.0331, -0.0021,  ...,  0.0606, -0.0588,  0.1131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8392, 1.5019, 1.3078, 1.5079, 1.3025, 1.5585, 0.8220, 1.3313, 1.1638,\n",
      "        1.3262, 1.5978, 1.8851, 1.3966, 1.2226, 1.2462, 1.8329, 2.2638, 1.5260,\n",
      "        1.4429, 1.9183, 1.6768, 1.8161, 1.1113, 1.0717, 1.4786, 1.1437, 2.3015,\n",
      "        2.0781, 2.0550, 1.5666, 1.1595, 2.2420, 2.3097, 2.3510, 2.3838, 1.6049,\n",
      "        1.5131, 1.2421, 1.7080, 1.1408, 1.3006, 1.9111, 1.5604, 1.8477, 1.3583,\n",
      "        1.7531, 1.8328, 1.1693, 0.9461, 0.7927, 1.3253, 1.8668, 1.6322, 1.5397,\n",
      "        0.9627, 1.2589, 1.0646, 1.5216, 1.7032, 1.2262, 2.1231, 1.0534, 1.3696,\n",
      "        1.7091, 1.7756, 0.9021, 1.7070, 1.9496, 1.9010, 1.7491, 1.6047, 1.0320,\n",
      "        1.2980, 2.1570, 2.0436, 1.7722, 1.3395, 1.3266, 1.7732, 1.8616, 2.0157,\n",
      "        2.3580, 1.5554, 1.7025, 1.5890, 1.2600, 2.0191, 1.9583, 2.3127, 1.5514,\n",
      "        2.3803, 1.8509, 1.2195, 1.7522, 0.8964, 2.0830, 0.8266, 1.1107, 1.2753,\n",
      "        1.0481, 1.2297, 1.6393, 1.9367, 1.8472, 0.9549, 0.9827, 1.1031, 1.5806,\n",
      "        1.4853, 1.6312, 1.5712, 1.6653, 0.5601, 0.8297, 0.9081, 0.8465, 1.2861,\n",
      "        1.8130, 1.1477, 0.7905, 0.6225, 0.7807, 1.1590, 1.6046, 1.8592, 1.0478,\n",
      "        1.3277, 1.8135, 1.4369, 0.8611, 1.3014, 1.3713, 0.6711, 0.6987, 1.0542,\n",
      "        0.9251, 0.4482, 1.1542, 1.2361, 0.8154, 0.7378, 0.5549, 1.1003, 0.6287,\n",
      "        2.1110, 1.7296, 1.9410, 1.9848, 1.4768, 2.6344, 1.5212, 1.8838, 2.4259,\n",
      "        2.2121, 2.2918, 1.9224, 2.1539, 2.0339, 1.5981, 0.9361, 1.6011, 1.8197,\n",
      "        1.8355, 1.7057, 1.9585, 1.5404, 2.1567, 0.7077, 1.4238, 1.4275, 1.6206,\n",
      "        2.2843, 1.7853, 1.7433, 1.0910, 0.7828, 1.2124, 1.5569, 2.1363, 2.1490,\n",
      "        1.7376, 1.5103, 1.2466, 1.8997, 1.8123, 1.5968, 1.4765, 2.0046, 1.7172,\n",
      "        1.5096, 1.2301, 1.4968, 0.8001, 1.5426, 1.1875, 1.3194, 1.7850, 1.0918,\n",
      "        1.1095, 2.1928, 0.9384, 0.8893, 1.4626, 1.7024, 1.4097, 1.5778, 2.0078,\n",
      "        1.3213, 0.9281, 0.8281, 1.7055, 1.1076, 1.0169, 0.6628, 0.8071, 0.8527,\n",
      "        0.8261, 0.9799, 0.4856, 1.2225, 1.2257, 0.7850, 0.6270, 0.8081, 2.9451,\n",
      "        2.7178, 2.8894, 1.7407, 2.3582, 1.3965, 1.4035, 1.6724, 2.2536, 1.7997,\n",
      "        1.8414, 2.2658, 1.4603, 2.2006, 2.0325, 1.8343, 2.1152, 1.6703, 1.5110,\n",
      "        1.6628, 1.5135, 1.3825, 1.3710, 1.3022, 0.9267, 1.8836, 2.3095, 1.9317,\n",
      "        1.7390, 1.9284, 0.9710, 0.9299, 1.3975, 0.8381, 1.8367, 1.6382, 1.6303,\n",
      "        1.2757, 1.4130, 0.7267, 0.9907, 1.6766, 1.1864, 1.5973, 1.5308, 1.6346,\n",
      "        2.1062, 0.8994, 1.7928, 1.8450, 1.1499, 1.5268, 1.7757, 2.7709, 1.5211,\n",
      "        1.3783, 0.9778, 1.4357, 2.6976, 2.2279, 2.0355, 1.5333, 1.2152, 1.0892,\n",
      "        2.2129, 2.0966, 1.8543, 2.1541, 1.4145, 1.1462, 1.7422, 1.1569, 2.0677,\n",
      "        1.8638, 2.2184, 1.8875, 1.6365, 1.9494, 1.9363, 1.2111, 2.1896, 1.7809,\n",
      "        2.1253, 1.5785, 2.5326, 1.8456, 1.9618, 1.5199, 1.7201, 2.9075, 1.7146,\n",
      "        1.6995, 1.5715, 2.1892, 0.9054, 0.9631], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1435, -0.0052,  0.2564,  ...,  0.0818, -0.1329, -0.0929],\n",
      "        [ 0.1267,  0.0044,  0.1405,  ...,  0.0048, -0.0224, -0.1540],\n",
      "        [ 0.0457,  0.0006,  0.0886,  ..., -0.0794, -0.0209, -0.0535],\n",
      "        ...,\n",
      "        [-0.0140,  0.0056,  0.1073,  ..., -0.0442, -0.0528, -0.0796],\n",
      "        [ 0.1323, -0.0084, -0.0060,  ...,  0.0288, -0.0053,  0.1176],\n",
      "        [ 0.0237, -0.0080,  0.1673,  ...,  0.1739, -0.1087,  0.0090]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0796,  0.0986,  0.1738,  ...,  0.0372,  0.0837, -0.0044],\n",
      "        [ 0.1285, -0.0206, -0.1360,  ..., -0.0217, -0.2396,  0.0991],\n",
      "        [ 0.0575,  0.0792, -0.1378,  ..., -0.0274,  0.1571, -0.2062],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0528,  0.0342,  ..., -0.0391,  0.0607, -0.0446],\n",
      "        [-0.0733,  0.0174,  0.1513,  ...,  0.0083, -0.1267, -0.0529],\n",
      "        [ 0.2143,  0.1047,  0.0596,  ...,  0.0273,  0.0736,  0.1619]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4917, 1.6185, 1.4137, 1.6170, 1.3530, 1.5407, 1.5942, 1.3468, 1.5060,\n",
      "        1.4935, 1.3167, 1.4252, 1.4963, 1.5173, 1.4869, 1.3957, 0.9246, 1.0934,\n",
      "        1.0331, 1.1334, 0.8693, 1.0941, 1.2022, 0.3912, 1.2021, 0.8737, 1.2234,\n",
      "        0.9823, 1.2429, 0.9371, 1.1835, 1.1728, 0.9154, 1.1116, 1.1121, 1.0157,\n",
      "        1.0529, 1.1077, 1.0750, 1.0342, 0.9951, 0.9604, 0.9966, 1.1954, 1.1085,\n",
      "        1.1945, 1.0490, 0.9612, 1.5618, 1.6146, 1.4254, 1.1164, 1.5294, 1.3780,\n",
      "        1.5317, 1.5991, 0.9939, 1.4163, 1.2540, 1.7454, 1.5451, 1.5020, 1.0513,\n",
      "        1.3620, 0.9185, 1.2775, 1.1574, 1.1358, 1.1757, 1.2605, 1.3937, 1.1459,\n",
      "        1.2644, 1.2660, 1.1582, 1.3192, 1.2963, 1.2039, 1.2946, 1.2063, 0.9913,\n",
      "        1.0043, 1.2472, 1.0516, 1.1947, 1.1830, 1.2502, 1.1816, 1.2841, 1.0190,\n",
      "        1.1961, 1.1195, 1.1155, 1.0742, 1.0973, 1.1344, 1.6846, 1.6441, 1.4500,\n",
      "        1.5267, 1.4659, 1.5084, 1.7593, 1.6825, 1.4565, 1.3629, 1.0535, 1.6753,\n",
      "        1.5996, 1.2679, 1.4663, 1.6751, 1.7296, 1.8034, 1.6548, 1.7982, 2.0474,\n",
      "        1.8326, 1.9631, 2.1344, 1.6607, 1.8523, 1.6492, 1.7315, 1.6057, 1.7653,\n",
      "        1.7109, 2.0691, 1.0070, 1.1540, 1.2071, 1.1142, 1.2202, 1.4893, 1.1888,\n",
      "        1.5323, 1.1306, 1.4408, 1.3467, 1.2089, 1.1028, 1.2117, 1.1165, 1.4369,\n",
      "        1.1537, 1.0274, 1.0762, 1.1233, 1.1200, 1.0833, 1.1910, 1.2220, 1.0911,\n",
      "        1.1992, 1.2143, 1.0937, 1.1224, 0.9989, 1.0956, 1.0685, 1.2307, 1.4808,\n",
      "        1.1657, 1.2746, 1.3468, 1.2639, 1.3008, 1.2622, 1.5240, 1.6527, 1.3566,\n",
      "        1.3567, 1.2450, 1.1234, 1.2562, 1.1868, 1.0280, 1.0338, 0.8857, 1.0276,\n",
      "        0.9761, 1.0706, 1.3064, 1.0495, 0.8612, 0.9804, 1.2206, 1.0050, 1.1042,\n",
      "        0.9609, 1.1378, 1.1455, 1.7731, 1.4016, 0.8720, 1.3047, 1.8650, 1.2752,\n",
      "        1.3031, 1.6741, 1.3652, 1.7766, 1.7024, 1.9491, 1.6957, 1.7040, 1.7962,\n",
      "        1.6843, 1.2912, 1.2714, 1.3934, 1.4783, 1.3656, 1.5633, 1.2917, 1.4029,\n",
      "        1.5967, 1.5338, 1.3340, 1.4297, 1.2472, 1.5835, 1.1728, 1.1883, 0.7338,\n",
      "        0.8380, 0.6725, 0.7765, 0.7458, 0.6886, 0.9215, 0.7849, 0.8845, 0.9460,\n",
      "        0.7414, 0.5960, 0.7254, 0.7640, 0.7100, 0.6799, 1.1931, 1.2631, 1.2553,\n",
      "        1.3665, 1.0376, 1.2633, 1.2751, 1.2068, 1.3215, 1.1548, 1.2156, 1.2706,\n",
      "        1.3866, 1.3554, 1.2047, 1.1761, 1.3438, 1.2454, 1.3817, 1.3412, 1.2593,\n",
      "        1.2886, 1.8079, 1.2284, 1.3260, 1.2047, 1.3143, 1.3199, 1.5424, 1.4314,\n",
      "        1.3844, 1.1220, 0.8966, 0.7924, 0.9035, 1.0229, 0.9823, 0.9605, 1.0148,\n",
      "        0.9744, 0.9330, 0.9196, 0.9483, 1.0967, 0.9123, 0.9746, 0.9372, 0.8767,\n",
      "        1.0440, 1.0145, 0.9922, 1.0112, 1.0669, 0.9030, 0.9701, 0.9799, 0.9938,\n",
      "        1.0708, 0.9241, 1.0546, 1.1600, 1.0257, 1.1353, 0.9803, 1.1513, 1.0881,\n",
      "        1.0447, 1.0441, 1.0934, 1.2000, 1.2321, 1.1657, 1.0623, 1.1826, 1.1684,\n",
      "        1.0922, 1.2743, 1.1286, 1.2012, 1.1993], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 2.2875e-02, -5.7616e-02,  5.3130e-02,  ..., -1.5022e-02,\n",
      "         -5.6132e-02,  4.2808e-02],\n",
      "        [ 1.7193e-02, -4.6617e-02, -1.2302e-01,  ...,  9.1113e-02,\n",
      "          1.4257e-03,  1.8496e-02],\n",
      "        [-4.1950e-02,  1.8176e-02,  3.2392e-03,  ...,  4.5562e-02,\n",
      "          5.6913e-02,  3.7032e-03],\n",
      "        ...,\n",
      "        [ 3.2912e-02, -1.2532e-01,  5.4360e-02,  ...,  2.1749e-02,\n",
      "          2.1646e-02, -8.4060e-02],\n",
      "        [ 1.1246e-01,  1.1954e-04,  8.8178e-02,  ..., -2.4415e-02,\n",
      "         -3.7357e-02,  3.6012e-02],\n",
      "        [-7.4992e-03,  2.0190e-02, -7.8832e-03,  ...,  3.2634e-02,\n",
      "          1.2502e-01, -8.3001e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0694,  0.0207,  0.0094,  ..., -0.0309,  0.0413, -0.1416],\n",
      "        [ 0.0015, -0.0281, -0.0550,  ..., -0.0552, -0.1333,  0.0577],\n",
      "        [-0.0678, -0.0670, -0.0332,  ...,  0.0564,  0.0964,  0.0413],\n",
      "        ...,\n",
      "        [-0.0034,  0.0211, -0.0138,  ...,  0.0476,  0.0222,  0.0774],\n",
      "        [-0.0069, -0.0140,  0.0580,  ...,  0.0576,  0.0310, -0.0124],\n",
      "        [-0.0300, -0.0429, -0.0785,  ...,  0.0153,  0.0679, -0.0098]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3740, 1.3904, 1.4838, 1.5118, 1.4510, 1.5785, 1.4105, 1.4067, 1.3052,\n",
      "        1.4298, 1.3426, 1.5070, 1.5158, 1.3758, 1.3639, 1.3709, 1.4356, 1.3511,\n",
      "        1.4590, 1.3494, 1.4009, 1.4041, 1.4205, 1.3598, 1.3991, 1.4443, 1.4524,\n",
      "        1.3815, 1.4113, 1.4558, 1.3600, 1.3032, 1.4515, 1.2735, 1.3580, 1.3907,\n",
      "        1.4180, 1.3804, 1.4393, 1.4540, 1.4143, 1.4283, 1.3486, 1.4449, 1.4344,\n",
      "        1.4374, 1.8067, 1.3505, 1.5645, 1.4160, 1.4332, 1.3737, 1.4381, 1.5773,\n",
      "        1.3828, 1.2475, 1.3874, 1.4301, 1.3509, 1.3399, 1.4914, 1.6885, 1.2521,\n",
      "        1.5101, 1.3818, 1.3831, 1.4515, 1.4315, 1.3969, 1.3142, 1.4090, 1.4638,\n",
      "        1.3627, 1.5256, 1.5117, 1.4152, 1.4218, 1.3243, 1.3828, 1.4735, 1.3924,\n",
      "        1.3767, 1.3131, 1.4211, 1.4310, 1.5536, 1.3673, 1.4457, 1.3947, 1.4270,\n",
      "        1.4839, 1.3977, 1.4594, 1.4249, 1.3820, 1.4246, 1.3019, 1.4367, 1.3813,\n",
      "        1.3407, 1.3682, 1.4494, 1.3550, 1.2743, 1.5165, 1.3337, 1.5171, 1.4821,\n",
      "        1.4211, 1.3537, 1.3815, 1.6056, 1.4653, 1.4690, 1.5439, 1.4124, 1.3118,\n",
      "        1.4480, 1.3472, 1.5352, 1.5861, 1.4152, 1.3639, 1.3605, 1.3323, 1.4614,\n",
      "        1.3607, 1.4409, 1.3664, 1.3438, 1.4065, 1.5738, 1.4407, 1.4703, 1.3721,\n",
      "        1.4661, 1.3906, 1.3201, 1.3794, 1.6478, 1.3449, 1.5379, 1.3763, 1.3410,\n",
      "        1.1465, 1.3874, 1.3620, 1.5130, 1.3825, 1.3696, 1.4624, 1.5309, 1.5773,\n",
      "        1.3766, 1.3823, 1.3124, 1.4153, 1.3662, 1.4840, 1.3986, 1.3518, 1.4303,\n",
      "        1.4070, 1.4805, 1.4840, 1.4982, 1.4995, 1.4460, 1.3829, 1.3684, 1.4587,\n",
      "        1.6012, 1.4714, 1.3866, 1.4046, 1.4590, 1.3298, 1.3535, 1.3849, 1.4275,\n",
      "        1.3954, 1.5177, 1.3541, 1.3493, 1.4538, 1.3263, 1.4283, 1.2313, 1.4423,\n",
      "        1.3513, 1.3759, 1.3375, 1.4987, 0.9611, 1.3892, 1.3322, 1.3945, 1.3051,\n",
      "        1.3567, 1.3461, 1.4279, 1.3839, 1.3114, 1.3412, 1.3983, 1.4422, 1.3717,\n",
      "        1.4004, 1.4139, 1.4557, 1.3899, 1.3335, 1.3043, 1.4171, 1.4907, 1.4302,\n",
      "        1.3905, 1.4190, 1.3560, 1.3773, 1.3991, 1.3786, 1.4501, 1.3242, 1.3477,\n",
      "        1.4172, 1.3803, 1.4688, 1.3984, 1.4666, 1.4248, 1.3366, 1.4687, 1.3257,\n",
      "        1.4461, 1.4017, 1.4657, 1.3568, 1.4132, 1.4396, 1.4444, 1.4564, 1.5247,\n",
      "        1.4280, 1.4539, 1.3982, 1.3897, 1.4348, 1.4736, 1.3185, 1.3491, 1.4413,\n",
      "        1.4655, 1.4000, 1.3226, 1.4614, 1.4832, 1.6965, 1.4447, 1.3706, 1.4175,\n",
      "        1.4554, 1.4346, 1.3463, 1.4888, 1.4108, 1.5053, 1.4166, 1.4168, 1.4874,\n",
      "        1.2874, 1.2381, 1.4579, 1.3490, 1.2877, 1.3832, 1.4297, 1.4457, 1.4464,\n",
      "        1.3924, 1.4519, 1.4723, 1.3501, 1.6392, 1.4451, 1.2619, 1.2780, 1.5177,\n",
      "        1.3808, 1.5079, 1.3931, 1.7730, 1.3874, 1.4245, 1.4911, 1.3472, 1.2795,\n",
      "        1.3893, 1.4300, 1.4069, 1.3913, 1.4318, 1.3971, 1.3429, 1.5421, 1.4095,\n",
      "        1.3809, 1.4325, 1.5401, 1.3790, 1.4808, 1.4995, 1.4290, 1.4436, 1.5071,\n",
      "        1.4553, 1.3814, 1.3954, 1.3907, 1.3718], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.6870e-01,  4.9902e-02, -1.2189e-01,  ...,  1.6759e-01,\n",
      "         -1.6534e-01,  6.3929e-01],\n",
      "        [ 8.9402e-02, -2.6805e-02, -9.0301e-02,  ...,  1.2442e-01,\n",
      "         -1.6093e-02,  7.5032e-02],\n",
      "        [-1.1900e-01, -5.2743e-02, -1.0721e-01,  ..., -2.3550e-01,\n",
      "         -2.1219e-02, -5.0626e-04],\n",
      "        ...,\n",
      "        [-5.7376e-02, -2.9079e-02, -1.2264e-01,  ..., -1.8853e-02,\n",
      "         -9.9525e-02,  5.4936e-02],\n",
      "        [-7.9351e-02, -9.3222e-02, -8.0281e-02,  ...,  1.0874e-01,\n",
      "         -1.5452e-01, -1.4049e-01],\n",
      "        [-4.1216e-02, -1.4729e-01, -1.3642e-01,  ..., -1.2539e-01,\n",
      "          3.1789e-03,  3.3047e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0141,  0.0241,  0.0289,  ...,  0.0098,  0.0666, -0.0135],\n",
      "        [-0.1490, -0.1216, -0.0346,  ..., -0.0460, -0.0623,  0.0192],\n",
      "        [-0.1204, -0.1405, -0.0062,  ...,  0.0264, -0.0182,  0.0078],\n",
      "        ...,\n",
      "        [-0.0759, -0.0302, -0.1345,  ...,  0.0959, -0.0223,  0.0074],\n",
      "        [-0.0206,  0.1339,  0.1310,  ..., -0.0788, -0.0802,  0.0098],\n",
      "        [ 0.0677, -0.1423, -0.1050,  ...,  0.0419,  0.1962, -0.0726]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3245, 1.7168, 1.7899, 1.6921, 2.5019, 3.0230, 2.9051, 2.6821, 2.2656,\n",
      "        1.9286, 1.9575, 2.6198, 2.1558, 2.3735, 2.8304, 2.5617, 3.4486, 2.4797,\n",
      "        1.9333, 2.3668, 2.5104, 2.6295, 3.1140, 3.6420, 1.8723, 2.0190, 2.8540,\n",
      "        2.0065, 2.3818, 2.5469, 3.1762, 3.1797, 2.4396, 2.1003, 2.8242, 2.7793,\n",
      "        2.9950, 2.3750, 3.3404, 3.9783, 1.9111, 2.1084, 2.1892, 2.5384, 2.8881,\n",
      "        2.8819, 3.3324, 3.0388, 1.3544, 1.9157, 1.0063, 1.3015, 2.2458, 2.4231,\n",
      "        2.7791, 2.6768, 3.4506, 1.8947, 2.6747, 2.7112, 2.5856, 2.4897, 3.0258,\n",
      "        2.8980, 1.9294, 2.0109, 3.3894, 2.7113, 2.5089, 3.1272, 2.7267, 3.3233,\n",
      "        2.3448, 1.2482, 1.0486, 1.5450, 2.0146, 2.4396, 3.1620, 3.0219, 2.3152,\n",
      "        2.3613, 2.9629, 2.9116, 2.8080, 3.2015, 3.3321, 3.4925, 2.2904, 2.4564,\n",
      "        2.4738, 2.6459, 2.7453, 2.6686, 3.3170, 2.8940, 2.4965, 2.3668, 3.4119,\n",
      "        3.8276, 3.7419, 2.3307, 2.3495, 3.7480, 2.6472, 3.0456, 3.8322, 3.6121,\n",
      "        3.0308, 3.8170, 3.1908, 4.1504, 1.3837, 1.2890, 2.6511, 2.5087, 2.4521,\n",
      "        2.5008, 2.7571, 2.6419, 1.8360, 2.5589, 1.9935, 1.9783, 2.1956, 2.5941,\n",
      "        2.7387, 3.2058, 1.9193, 1.9477, 2.1161, 1.8403, 2.3576, 2.8870, 2.8029,\n",
      "        2.8380, 2.0006, 1.8999, 2.4727, 3.4790, 2.5432, 2.4968, 2.8966, 2.8671,\n",
      "        3.2983, 1.9448, 2.6039, 2.1472, 2.3245, 2.2576, 2.9870, 2.8286, 1.0430,\n",
      "        1.5752, 1.8245, 2.1747, 2.2242, 2.9386, 2.9885, 3.2160, 1.9532, 2.3568,\n",
      "        2.0262, 2.2121, 2.3628, 2.9375, 2.8399, 2.9173, 3.0711, 2.0559, 2.6335,\n",
      "        2.3709, 2.7033, 2.5364, 2.9938, 3.3630, 1.4400, 1.4544, 2.3855, 1.6382,\n",
      "        1.9818, 2.7049, 2.4330, 2.7141, 1.2462, 1.5827, 1.4435, 2.3673, 2.0342,\n",
      "        2.1272, 2.5182, 2.7164, 1.2996, 1.7125, 1.7743, 1.4574, 2.0389, 1.9021,\n",
      "        2.8402, 2.6258, 2.2244, 2.0748, 2.1919, 2.6215, 2.0894, 2.2945, 2.8064,\n",
      "        2.8616, 2.6161, 2.3002, 2.3308, 3.2465, 3.3884, 1.9806, 3.0118, 3.1338,\n",
      "        1.8333, 3.0788, 3.6652, 4.3917, 3.5267, 4.1543, 3.2870, 3.7847, 1.3667,\n",
      "        1.4765, 2.5817, 1.3997, 1.8004, 2.1469, 2.4749, 3.5545, 4.4102, 2.1699,\n",
      "        1.3076, 2.3206, 2.1613, 3.0763, 2.5509, 3.0494, 2.1864, 2.4883, 2.1916,\n",
      "        3.3076, 2.0339, 2.6320, 3.0719, 3.0617, 2.2879, 1.9822, 2.0287, 1.3856,\n",
      "        2.7286, 2.5970, 2.9732, 2.9859, 3.0516, 2.1417, 2.2701, 1.9949, 2.1251,\n",
      "        2.3400, 3.3546, 3.1013, 2.5145, 2.4692, 2.3275, 3.0263, 2.7831, 2.2138,\n",
      "        3.4609, 2.4852, 1.9811, 2.4358, 2.9205, 2.3416, 1.9662, 2.5458, 3.3893,\n",
      "        3.3530, 2.0105, 2.7228, 2.9802, 3.0973, 3.1343, 2.6227, 3.1460, 3.0294,\n",
      "        2.1830, 3.0292, 2.9034, 3.2462, 2.1824, 3.1652, 3.3615, 3.4801, 2.8929,\n",
      "        3.4013, 2.8332, 2.3360, 3.2497, 2.4488, 3.8892, 3.3294, 1.4866, 2.1185,\n",
      "        2.3769, 2.0340, 2.0800, 2.3999, 2.4460, 2.8901, 1.6317, 1.4096, 1.4471,\n",
      "        2.7143, 2.0882, 2.6064, 2.5952, 2.6835], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0254, -0.0781,  0.0762,  ..., -0.0876,  0.0627, -0.8724],\n",
      "        [-0.0080,  0.0157, -0.0311,  ..., -0.1192,  0.1119, -0.0826],\n",
      "        [ 0.0734,  0.0899, -0.0601,  ...,  0.0186, -0.1439, -0.0848],\n",
      "        ...,\n",
      "        [-0.0558, -0.0730,  0.0440,  ..., -0.1581, -0.1455,  0.0570],\n",
      "        [ 0.0727, -0.0508, -0.0124,  ...,  0.0745,  0.0575, -0.0876],\n",
      "        [-0.0860,  0.0428,  0.0784,  ..., -0.0787,  0.0608,  0.0376]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0593,  0.0664, -0.0933,  ...,  0.1563,  0.0474,  0.0165],\n",
      "        [-0.1472,  0.1030, -0.1273,  ...,  0.0431,  0.0448,  0.0223],\n",
      "        [ 0.2234, -0.0241,  0.0058,  ...,  0.0161,  0.0079, -0.0367],\n",
      "        ...,\n",
      "        [ 0.1645, -0.0652,  0.0206,  ..., -0.0526, -0.0232, -0.0255],\n",
      "        [ 0.1108,  0.0107,  0.0389,  ...,  0.0542,  0.0508, -0.0125],\n",
      "        [-0.0878, -0.0834, -0.0796,  ...,  0.0009,  0.0354, -0.0484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9300, 1.6594, 1.4977, 3.0943, 1.3151, 1.9870, 2.4968, 2.6338, 1.9344,\n",
      "        2.2295, 2.5972, 1.2599, 2.9110, 2.0880, 2.8159, 2.0986, 1.6854, 2.0358,\n",
      "        2.7459, 1.7867, 1.7394, 1.7103, 3.2583, 3.1709, 1.9329, 2.2107, 1.6039,\n",
      "        1.6938, 3.4331, 3.6160, 3.4965, 3.0764, 2.6695, 1.9023, 2.4984, 2.6565,\n",
      "        2.2615, 1.9451, 2.1478, 2.9513, 1.6700, 2.2433, 2.5989, 2.2520, 2.1870,\n",
      "        2.2697, 2.4098, 2.3007, 2.6485, 1.6586, 1.4965, 1.9027, 2.9195, 1.3350,\n",
      "        2.4299, 2.5055, 1.5202, 2.2669, 1.7688, 1.4574, 1.3019, 3.0451, 2.6566,\n",
      "        2.4623, 1.7950, 1.7287, 1.3536, 1.2326, 1.4801, 1.7067, 2.6765, 2.0548,\n",
      "        1.9986, 1.5348, 1.6435, 2.5852, 1.7279, 2.8056, 2.5023, 2.0610, 2.4779,\n",
      "        2.5367, 2.7675, 2.3908, 2.3919, 2.4414, 3.2886, 3.1418, 1.7161, 2.2924,\n",
      "        2.2264, 2.5200, 2.2085, 2.2493, 3.1418, 2.6015, 2.4477, 2.6953, 3.1755,\n",
      "        3.2549, 2.8033, 3.6047, 3.4769, 3.3705, 2.8548, 3.3037, 3.0578, 3.1547,\n",
      "        2.9186, 1.5723, 2.2129, 3.5157, 1.2725, 2.1193, 1.5701, 1.4799, 1.4237,\n",
      "        2.8391, 2.7166, 2.7548, 1.6344, 1.5087, 1.9889, 2.4190, 2.6733, 1.4955,\n",
      "        2.6702, 2.2463, 2.1765, 2.2354, 1.8153, 3.1809, 1.6192, 1.5404, 2.6178,\n",
      "        2.5148, 1.9515, 1.8926, 2.9066, 1.5735, 1.7372, 3.0129, 2.5642, 2.7293,\n",
      "        2.0537, 2.2480, 1.5403, 1.6119, 1.3753, 3.0500, 2.5977, 2.6608, 1.3663,\n",
      "        1.5327, 2.7925, 1.6589, 2.9992, 1.6027, 2.7083, 2.2353, 2.1649, 2.5114,\n",
      "        2.9816, 3.0002, 3.2522, 1.8592, 3.2529, 3.0077, 2.2953, 2.0367, 1.6566,\n",
      "        1.8081, 1.7440, 3.2761, 3.3680, 3.0972, 1.2509, 1.4573, 1.4197, 2.6669,\n",
      "        1.2936, 1.4062, 2.2339, 2.1945, 1.6679, 1.8526, 2.1087, 1.1746, 1.5446,\n",
      "        2.3620, 2.4067, 1.9295, 1.9584, 1.8850, 1.5768, 3.1045, 1.4623, 1.4757,\n",
      "        2.6626, 2.7667, 2.4256, 2.2508, 1.4692, 1.3203, 2.9583, 3.0773, 2.5310,\n",
      "        2.8126, 1.8787, 2.7114, 3.3920, 3.1376, 2.8005, 3.1771, 2.5933, 4.0284,\n",
      "        2.3364, 2.0400, 2.2781, 2.7932, 2.7753, 1.8393, 3.9381, 3.9458, 1.3425,\n",
      "        1.9988, 1.2551, 2.1113, 2.7417, 2.6113, 2.2380, 2.0766, 1.2947, 1.6646,\n",
      "        1.2516, 1.2782, 1.2255, 1.5256, 2.2886, 2.0154, 2.3515, 2.1413, 1.9988,\n",
      "        1.6107, 3.1144, 1.6842, 2.7934, 2.8004, 2.0608, 2.2068, 2.0929, 2.1452,\n",
      "        1.3882, 2.3526, 2.6642, 2.8483, 2.4208, 2.2372, 2.2080, 2.1844, 2.8083,\n",
      "        2.1781, 3.0061, 2.5230, 2.8054, 2.3766, 2.2933, 2.2169, 1.9492, 2.0266,\n",
      "        2.9048, 2.3151, 1.7494, 2.7849, 2.7082, 2.4644, 3.2154, 2.1386, 3.1371,\n",
      "        3.2422, 2.0058, 2.3608, 2.4633, 2.5312, 1.8178, 2.0440, 3.2848, 3.0267,\n",
      "        2.7804, 2.9552, 2.6353, 2.9414, 3.7376, 2.2175, 3.3149, 3.2174, 2.3222,\n",
      "        2.7310, 2.6773, 2.2565, 1.7349, 2.4219, 3.0909, 3.4695, 1.7518, 1.9688,\n",
      "        1.2404, 2.7240, 2.6984, 1.9412, 2.4008, 2.0697, 1.7143, 1.5165, 2.6549,\n",
      "        1.3040, 1.1566, 2.1132, 2.3006, 2.4035], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0298,  0.0448,  0.1184,  ...,  0.0129,  0.0675, -0.0011],\n",
      "        [-0.0980,  0.0223, -0.0524,  ..., -0.0727, -0.0518, -0.0085],\n",
      "        [ 0.0984,  0.0308,  0.1346,  ...,  0.0088, -0.0262,  0.0029],\n",
      "        ...,\n",
      "        [-0.0525, -0.1194,  0.0279,  ..., -0.0296,  0.0422,  0.0077],\n",
      "        [ 0.0403,  0.1124,  0.0035,  ..., -0.0498,  0.0148,  0.0041],\n",
      "        [ 0.0406,  0.0608,  0.0054,  ...,  0.0125,  0.0080, -0.0073]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0197, -0.0262,  0.0052,  ...,  0.0081, -0.0378, -0.0087],\n",
      "        [-0.0079, -0.0146,  0.1244,  ...,  0.0421, -0.0523,  0.0365],\n",
      "        [-0.1055,  0.0199,  0.0096,  ...,  0.0417, -0.0194,  0.0826],\n",
      "        ...,\n",
      "        [-0.0373, -0.0710,  0.0228,  ...,  0.0516,  0.1481, -0.0235],\n",
      "        [-0.0164,  0.0163, -0.0548,  ...,  0.0427,  0.0267,  0.0363],\n",
      "        [ 0.0066,  0.1573, -0.1508,  ..., -0.0057, -0.0282, -0.1634]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4980, 1.3459, 1.4473, 1.4358, 1.3913, 1.4258, 1.3466, 1.3956, 1.4740,\n",
      "        1.4381, 1.4112, 1.4339, 1.4113, 1.4300, 1.4728, 1.4411, 1.1943, 1.2046,\n",
      "        1.1835, 1.2129, 1.1931, 1.1500, 1.1957, 1.1494, 1.2792, 1.1394, 1.1760,\n",
      "        1.0981, 1.1760, 1.2063, 1.1164, 1.2555, 1.8288, 1.9685, 1.9895, 1.9297,\n",
      "        2.1100, 2.1907, 2.1788, 1.9598, 2.1310, 2.0346, 1.9360, 2.1141, 1.9997,\n",
      "        1.9785, 1.9648, 2.1194, 1.3960, 1.4999, 1.3750, 1.4157, 1.4814, 1.5020,\n",
      "        1.3688, 1.5273, 1.4835, 1.4501, 1.4853, 1.4578, 1.4939, 1.5067, 1.4857,\n",
      "        1.4171, 1.2774, 1.2409, 1.2623, 1.2956, 1.3209, 1.3307, 1.2779, 1.3276,\n",
      "        1.2602, 1.2174, 1.3099, 1.2621, 1.2084, 1.2641, 1.2588, 1.1698, 2.0404,\n",
      "        2.1187, 2.0826, 1.9912, 2.1269, 2.1338, 2.1966, 2.1638, 2.2216, 2.1042,\n",
      "        2.0221, 2.1139, 2.0291, 2.1846, 2.0846, 1.9232, 1.6377, 1.7291, 1.7390,\n",
      "        1.7090, 1.7661, 1.7659, 1.7432, 1.7142, 1.6639, 1.6418, 1.7013, 1.7508,\n",
      "        1.7101, 1.6589, 1.6387, 1.7484, 1.2062, 1.2426, 1.2586, 1.2244, 1.2592,\n",
      "        1.3000, 1.1969, 1.2025, 1.3070, 1.2519, 1.2038, 1.2704, 1.3148, 1.2617,\n",
      "        1.2592, 1.2307, 1.6067, 1.4729, 1.5090, 1.5377, 1.5189, 1.4314, 1.5557,\n",
      "        1.5550, 1.5509, 1.4745, 1.5585, 1.4381, 1.5450, 1.5911, 1.5353, 1.4696,\n",
      "        1.2607, 1.2757, 1.1858, 1.2939, 1.2295, 1.2569, 1.2915, 1.2513, 1.2909,\n",
      "        1.3173, 1.1631, 1.3012, 1.2709, 1.2663, 1.2420, 1.2154, 1.1433, 1.2282,\n",
      "        1.2999, 1.1342, 1.1834, 1.1620, 1.2391, 1.1815, 1.1959, 1.1553, 1.1661,\n",
      "        1.2237, 1.2253, 1.3122, 1.1865, 1.2240, 1.4584, 1.4568, 1.4475, 1.4286,\n",
      "        1.4856, 1.4470, 1.4348, 1.4726, 1.4814, 1.4087, 1.5119, 1.4882, 1.4415,\n",
      "        1.4326, 1.4308, 1.5150, 1.5702, 1.5611, 1.6107, 1.6394, 1.5555, 1.5646,\n",
      "        1.5111, 1.5860, 1.5927, 1.6342, 1.5680, 1.5987, 1.5860, 1.5844, 1.5843,\n",
      "        1.5492, 1.4515, 1.4995, 1.5403, 1.5012, 1.4267, 1.5219, 1.5294, 1.5072,\n",
      "        1.6551, 1.5566, 1.4999, 1.4341, 1.5469, 1.4206, 1.5184, 1.5231, 1.3572,\n",
      "        1.3593, 1.3325, 1.3854, 1.3571, 1.4159, 1.4355, 1.4141, 1.3958, 1.4192,\n",
      "        1.4696, 1.3837, 1.3532, 1.3965, 1.3966, 1.3738, 1.6144, 1.6316, 1.6704,\n",
      "        1.6964, 1.7117, 1.6431, 1.6655, 1.5849, 1.6665, 1.6786, 1.7127, 1.6865,\n",
      "        1.6185, 1.7515, 1.6403, 1.6968, 1.8723, 1.8087, 1.7256, 1.7091, 1.7462,\n",
      "        1.8303, 1.7958, 1.8147, 1.8122, 1.8251, 1.8571, 1.8262, 1.8690, 1.7861,\n",
      "        1.6899, 1.7560, 1.7648, 1.7503, 1.9527, 1.8088, 1.8292, 1.8069, 1.8912,\n",
      "        1.7744, 1.7893, 1.7516, 1.8652, 1.7254, 1.7266, 1.8316, 1.8588, 1.7919,\n",
      "        1.7756, 1.8021, 1.8023, 1.8010, 1.8481, 1.7843, 1.7818, 1.8256, 1.8149,\n",
      "        1.6881, 1.7637, 1.8266, 1.7904, 1.8124, 1.7440, 1.8805, 1.4873, 1.4176,\n",
      "        1.4244, 1.3782, 1.4729, 1.3896, 1.4025, 1.3679, 1.4368, 1.3750, 1.4059,\n",
      "        1.3538, 1.3856, 1.4647, 1.4448, 1.5029], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0222,  0.0224,  ...,  0.0101,  0.0533, -0.0518],\n",
      "        [ 0.0227, -0.0480,  0.0123,  ...,  0.0455, -0.0377,  0.0202],\n",
      "        [ 0.0099,  0.0050, -0.0066,  ..., -0.0239,  0.0389, -0.0145],\n",
      "        ...,\n",
      "        [-0.0525, -0.0258,  0.0031,  ..., -0.0879,  0.0053,  0.0270],\n",
      "        [ 0.0043,  0.0049,  0.0244,  ...,  0.0091,  0.0857, -0.0087],\n",
      "        [ 0.0048, -0.0286, -0.0033,  ..., -0.0186,  0.0499, -0.0678]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0699, -0.0754,  0.0219,  ...,  0.0680, -0.0979, -0.0380],\n",
      "        [ 0.0222,  0.0479, -0.0024,  ...,  0.0479, -0.0308,  0.0669],\n",
      "        [ 0.0789,  0.0041,  0.0304,  ..., -0.0284,  0.0059, -0.0273],\n",
      "        ...,\n",
      "        [ 0.0269,  0.0711, -0.0231,  ...,  0.0981,  0.0372,  0.0509],\n",
      "        [ 0.0127,  0.0197,  0.0026,  ...,  0.0688, -0.0037, -0.0093],\n",
      "        [ 0.5940, -0.0378,  0.2032,  ...,  0.0810,  0.0980,  0.1319]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5017, 1.3348, 1.4557, 1.5515, 1.5730, 1.3890, 1.4217, 1.3872, 1.3844,\n",
      "        1.5491, 1.4278, 1.5350, 1.4248, 1.3817, 1.4057, 1.5236, 1.5381, 1.5122,\n",
      "        1.3979, 1.4319, 1.5571, 1.3928, 1.5441, 1.4488, 1.4208, 1.5494, 1.4653,\n",
      "        1.4478, 1.5200, 1.5324, 1.4738, 1.4753, 1.4613, 1.4356, 1.4540, 1.5696,\n",
      "        1.5998, 1.5053, 1.5695, 1.5046, 1.4315, 1.4927, 1.7001, 1.5625, 1.4244,\n",
      "        1.3825, 1.3145, 1.5076, 1.5389, 1.5490, 1.4521, 1.4387, 1.4519, 1.4844,\n",
      "        1.4565, 1.4971, 1.4035, 1.5016, 1.4489, 1.4327, 1.5005, 1.8010, 1.4406,\n",
      "        1.5980, 1.5243, 1.4550, 1.4112, 1.4501, 1.4538, 1.5796, 1.4548, 1.6206,\n",
      "        1.5958, 1.5160, 1.4537, 1.3340, 1.4609, 1.4703, 1.4276, 1.4735, 1.5058,\n",
      "        1.5631, 1.4336, 1.5360, 1.5922, 1.5253, 1.4387, 1.3703, 1.3609, 1.5956,\n",
      "        1.4412, 1.5321, 1.5645, 1.4062, 1.5134, 1.4783, 1.2997, 1.5408, 1.6227,\n",
      "        1.3970, 1.4782, 1.3789, 1.4151, 1.3146, 1.4456, 1.5722, 1.4935, 1.4081,\n",
      "        1.6215, 1.4184, 1.4402, 1.4856, 1.5544, 1.5291, 1.4436, 1.4682, 1.6035,\n",
      "        1.5418, 1.4652, 1.4597, 1.5233, 1.5839, 1.4628, 1.4946, 1.4858, 1.3861,\n",
      "        1.3928, 1.4512, 1.4620, 1.6742, 1.4592, 1.5305, 1.4766, 1.4869, 1.4565,\n",
      "        1.2850, 1.5033, 1.3575, 1.5451, 1.3872, 1.4509, 1.5233, 1.4467, 1.4184,\n",
      "        1.4185, 1.4106, 1.3943, 1.4288, 1.4935, 1.4826, 1.7352, 1.5121, 1.4623,\n",
      "        1.3521, 1.4324, 1.4864, 1.5864, 1.5271, 1.5507, 1.5103, 1.3882, 1.4910,\n",
      "        1.4856, 1.4616, 1.4890, 1.5005, 1.4750, 1.4226, 1.4069, 1.4865, 1.4920,\n",
      "        1.4959, 1.5237, 1.5382, 1.5554, 1.4507, 1.5220, 1.4799, 1.3936, 1.4417,\n",
      "        1.4713, 1.3843, 1.6368, 1.5296, 1.4537, 1.4887, 1.5235, 1.3477, 1.3948,\n",
      "        1.4150, 1.3907, 1.4642, 2.2830, 1.4287, 1.4877, 1.4324, 1.4421, 1.4250,\n",
      "        1.4568, 1.4854, 1.4586, 1.4920, 1.3931, 1.4875, 1.4075, 1.4117, 1.5979,\n",
      "        1.4611, 1.5325, 1.3526, 1.5216, 1.3862, 1.4600, 1.4651, 1.4202, 1.5174,\n",
      "        1.4706, 1.4642, 1.4751, 1.5125, 1.4629, 1.5553, 1.5431, 1.4479, 1.3771,\n",
      "        1.3374, 1.4546, 1.4461, 1.4929, 1.5388, 1.5058, 1.4566, 1.4710, 1.4280,\n",
      "        1.4451, 1.5151, 1.4833, 1.2989, 1.4596, 1.5329, 1.5531, 1.3593, 1.5034,\n",
      "        1.5055, 1.4199, 1.3569, 1.3553, 1.4771, 1.4486, 1.4853, 1.4104, 1.5438,\n",
      "        1.4865, 1.5443, 1.4281, 1.5061, 1.5819, 1.3899, 1.3341, 1.5003, 1.4953,\n",
      "        1.4423, 1.4588, 1.5079, 1.6188, 1.5684, 1.5438, 1.5535, 1.4586, 1.4856,\n",
      "        1.4503, 1.4786, 1.6876, 1.4415, 1.5083, 1.5201, 1.4955, 1.4365, 1.4477,\n",
      "        1.4546, 1.4780, 1.5449, 1.3886, 1.5185, 1.4341, 1.4793, 1.4304, 1.4436,\n",
      "        1.2968, 1.5297, 1.5393, 1.5855, 1.3822, 1.4893, 1.4967, 1.5099, 1.5831,\n",
      "        1.4870, 1.4372, 1.4805, 1.4345, 1.5428, 1.4000, 1.3519, 1.4385, 1.4646,\n",
      "        1.5905, 1.5174, 1.4593, 1.3578, 1.4713, 1.5128, 1.4325, 1.5718, 1.4143,\n",
      "        1.5713, 1.5181, 1.4250, 1.4072, 2.1116], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1897,  0.1444, -0.0601,  ...,  0.1208, -0.0411,  0.1693],\n",
      "        [ 0.1106, -0.1279,  0.0166,  ...,  0.0435,  0.0459, -0.0275],\n",
      "        [-0.0351, -0.0751, -0.0027,  ...,  0.0003, -0.1235, -0.0907],\n",
      "        ...,\n",
      "        [ 0.0315, -0.0510,  0.0915,  ..., -0.0235,  0.0670,  0.0321],\n",
      "        [-0.0569,  0.0056,  0.0759,  ..., -0.1676, -0.0737, -0.0191],\n",
      "        [-0.1071, -0.0135,  0.1023,  ...,  0.0200,  0.1014,  0.0301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-1.9415e-02, -3.0802e-02,  3.7045e-02,  ...,  1.2388e-01,\n",
      "          5.8074e-02,  3.7667e-02],\n",
      "        [ 2.2984e-02,  1.1339e-01,  3.0278e-03,  ...,  9.6700e-02,\n",
      "          8.4788e-02,  1.1016e-01],\n",
      "        [ 2.8255e-02, -2.6010e-02,  9.8865e-03,  ..., -1.2348e-02,\n",
      "         -4.1980e-02,  1.8962e-02],\n",
      "        ...,\n",
      "        [-1.3478e-01, -1.8110e-01,  1.9137e-01,  ...,  2.2181e-02,\n",
      "         -8.2965e-02, -1.8213e-02],\n",
      "        [ 3.8945e-02,  1.4990e-01, -3.3312e-01,  ...,  1.3039e-01,\n",
      "          1.4722e-01, -6.6580e-02],\n",
      "        [-1.3467e-01, -1.4184e-01,  4.9619e-02,  ...,  3.7615e-02,\n",
      "          7.1831e-02,  2.6942e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6495, 2.6144, 2.0923, 1.2663, 2.4449, 4.1128, 4.0464, 4.0822, 1.6418,\n",
      "        1.7398, 2.3901, 2.2299, 1.8270, 4.0006, 3.8445, 3.9535, 1.6899, 1.9972,\n",
      "        2.5525, 2.4626, 2.4197, 2.8368, 2.4949, 2.8939, 2.1522, 2.0523, 1.9412,\n",
      "        1.9591, 1.9815, 2.1155, 2.6463, 3.0366, 2.0924, 1.9519, 1.9135, 1.3717,\n",
      "        1.9336, 2.9369, 2.5860, 2.5326, 2.3171, 1.6125, 1.7838, 2.2947, 2.6071,\n",
      "        1.9390, 2.4219, 2.3957, 1.2522, 1.4052, 2.0674, 1.9370, 1.6648, 2.5130,\n",
      "        2.3807, 2.6832, 1.7377, 2.3225, 1.9634, 1.8986, 1.9682, 1.9425, 2.2501,\n",
      "        2.6913, 2.1484, 2.1123, 1.7639, 2.0286, 2.1186, 2.2937, 2.7662, 3.8656,\n",
      "        2.4026, 2.2081, 2.1102, 1.9174, 1.7959, 2.3483, 2.9658, 2.7738, 1.9227,\n",
      "        2.1042, 2.0690, 1.9647, 2.0851, 2.3690, 2.5679, 2.0241, 1.8270, 1.6958,\n",
      "        2.2732, 2.1671, 2.0249, 2.0650, 2.6504, 2.8148, 2.7256, 2.0201, 2.3212,\n",
      "        2.3504, 2.1281, 2.4978, 2.7551, 2.7805, 1.6057, 2.2123, 1.8004, 2.3013,\n",
      "        2.6184, 3.3338, 2.7687, 3.1015, 3.1543, 1.7773, 2.4207, 2.0360, 2.1830,\n",
      "        2.8891, 2.4851, 2.5311, 1.7617, 2.2906, 1.6699, 2.4805, 2.7635, 2.4349,\n",
      "        2.8071, 2.8553, 1.2718, 1.8109, 1.7713, 2.0664, 2.2396, 2.5660, 2.2657,\n",
      "        2.8725, 2.3810, 1.7232, 2.2653, 1.7358, 1.9742, 2.1835, 2.4735, 3.1659,\n",
      "        2.2275, 1.5316, 1.7490, 2.3351, 2.7477, 2.5349, 2.5800, 2.5983, 1.8945,\n",
      "        1.4549, 1.8616, 1.7386, 2.0547, 2.0583, 2.5462, 2.6193, 1.9031, 2.0368,\n",
      "        2.1479, 2.2566, 2.0079, 1.7242, 2.6139, 2.8391, 1.8368, 2.0845, 2.4292,\n",
      "        2.0815, 2.6271, 2.4523, 2.6958, 1.9641, 2.0994, 2.4425, 1.9681, 2.6334,\n",
      "        3.3529, 3.3632, 2.8961, 3.0801, 2.4249, 2.1216, 2.4391, 2.2884, 2.2374,\n",
      "        2.7303, 2.7596, 2.9757, 1.9266, 1.6893, 1.9801, 1.6159, 2.1873, 3.1525,\n",
      "        2.7997, 2.9109, 2.6979, 2.2458, 2.1388, 2.7122, 2.8330, 2.3681, 2.5511,\n",
      "        2.6233, 2.3600, 2.1750, 2.2240, 2.1401, 1.9611, 2.6368, 2.8951, 2.7490,\n",
      "        2.0636, 2.1809, 2.0105, 2.2464, 2.3964, 2.6847, 2.9179, 2.9543, 1.2775,\n",
      "        1.4900, 2.3056, 1.6133, 2.3192, 1.7255, 2.2733, 2.2277, 1.6507, 1.6870,\n",
      "        1.3529, 2.0694, 1.6068, 2.3528, 2.1011, 2.3101, 2.1997, 1.8327, 1.8519,\n",
      "        1.8385, 1.7826, 1.8731, 2.4475, 2.6061, 1.9085, 1.7990, 2.0775, 2.4626,\n",
      "        2.1533, 2.2617, 2.4516, 2.3303, 1.9336, 2.2999, 2.7084, 2.2813, 2.1295,\n",
      "        2.7011, 2.8912, 3.2627, 2.6091, 2.0597, 1.8857, 2.2190, 2.5810, 3.0999,\n",
      "        3.1366, 3.6367, 2.1977, 2.1439, 2.1812, 2.0905, 1.8529, 2.2315, 3.7129,\n",
      "        3.8967, 2.2103, 2.2557, 2.6557, 2.3096, 2.0927, 2.0087, 3.6656, 3.6867,\n",
      "        2.9027, 1.8847, 1.6723, 2.2194, 2.2961, 2.3742, 2.8923, 2.8787, 1.7463,\n",
      "        1.7686, 2.1835, 1.9271, 1.9526, 2.7359, 2.7723, 3.0096, 2.0436, 1.9872,\n",
      "        2.2731, 2.8552, 2.3956, 2.3333, 3.2278, 2.8162, 2.2047, 2.0217, 2.1248,\n",
      "        1.9222, 2.8409, 2.7430, 2.7870, 2.3523], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0500,  0.0367, -0.1410,  ...,  0.1743, -0.0096,  0.2134],\n",
      "        [ 0.0673, -0.0120, -0.0263,  ...,  0.1661, -0.0988, -0.1892],\n",
      "        [-0.0827, -0.0236,  0.0409,  ..., -0.0563, -0.0041,  0.1286],\n",
      "        ...,\n",
      "        [ 0.0416, -0.1141, -0.0563,  ..., -0.0755, -0.1019, -0.0351],\n",
      "        [-0.0106,  0.0886,  0.1031,  ..., -0.1101,  0.0183, -0.0582],\n",
      "        [ 0.2231, -0.0744,  0.1027,  ..., -0.0649, -0.0113, -0.0082]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141, -0.0454,  0.0057,  ..., -0.0188,  0.0038,  0.0191],\n",
      "        [ 0.0571,  0.0436, -0.0234,  ...,  0.0247, -0.1909, -0.2112],\n",
      "        [ 0.0445,  0.0218,  0.1265,  ..., -0.0322, -0.0156, -0.1175],\n",
      "        ...,\n",
      "        [-0.0849, -0.1355,  0.1477,  ...,  0.0361,  0.0411,  0.1307],\n",
      "        [-0.1064, -0.1564, -0.2834,  ..., -0.1471, -0.0404,  0.2403],\n",
      "        [ 0.0911, -0.0271,  0.0647,  ...,  0.0427,  0.2251,  0.1965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0793, 2.5442, 2.3540, 1.9580, 2.0947, 2.9668, 3.0822, 3.2043, 2.0321,\n",
      "        1.8978, 2.1931, 1.7006, 1.8006, 2.9096, 2.9934, 3.1883, 1.7706, 1.6959,\n",
      "        1.7769, 1.4557, 1.4466, 1.5502, 2.4281, 2.1244, 1.8274, 2.3831, 2.1472,\n",
      "        2.7797, 2.7033, 2.7936, 2.1874, 2.0652, 2.2348, 1.7275, 2.1146, 2.4233,\n",
      "        1.8946, 1.4516, 2.4257, 1.8548, 2.1352, 1.9024, 1.8633, 1.5362, 1.6844,\n",
      "        2.5594, 2.3211, 2.1430, 1.2303, 2.1816, 1.9798, 1.3834, 2.5260, 1.4670,\n",
      "        2.2720, 1.9820, 1.5280, 1.3478, 1.5541, 2.1650, 1.2909, 2.4384, 2.0820,\n",
      "        2.0018, 2.0357, 2.1039, 1.9218, 2.1927, 1.7991, 1.9380, 2.2556, 2.1697,\n",
      "        2.2817, 2.2250, 1.9968, 1.7323, 1.9625, 1.9532, 2.3242, 1.8753, 1.7879,\n",
      "        2.1644, 2.4467, 2.0333, 2.0129, 1.7532, 2.1370, 2.1353, 1.8837, 1.5389,\n",
      "        1.9164, 2.0265, 1.9623, 2.4082, 2.4402, 2.5236, 1.8690, 1.8526, 1.9993,\n",
      "        2.8850, 3.0240, 2.9510, 2.7143, 2.7029, 2.3952, 2.2387, 2.0162, 1.8027,\n",
      "        1.5500, 1.6638, 2.5698, 2.1683, 2.0655, 1.8332, 2.2394, 2.8401, 3.2795,\n",
      "        1.6035, 2.2678, 2.4907, 2.4999, 2.0698, 1.5869, 1.5862, 1.3818, 2.7155,\n",
      "        2.3508, 2.1278, 1.8412, 1.7707, 2.1987, 1.5185, 1.6337, 1.8006, 2.2282,\n",
      "        2.1152, 1.2382, 1.7002, 1.7470, 2.0186, 2.0072, 2.1672, 2.3772, 2.1212,\n",
      "        2.1688, 1.8210, 2.2809, 1.4070, 1.3524, 1.6107, 2.2615, 2.0743, 1.8620,\n",
      "        1.5988, 1.6275, 2.4134, 2.7282, 2.2205, 2.3348, 1.9452, 1.9873, 2.0682,\n",
      "        2.4391, 2.0440, 2.3092, 2.2915, 2.1585, 2.5369, 1.5104, 2.1322, 2.2597,\n",
      "        2.3189, 1.9889, 2.1624, 2.4252, 2.1554, 2.2700, 2.0741, 1.9190, 1.9291,\n",
      "        1.6265, 1.6005, 2.7915, 2.7372, 2.2100, 2.3545, 2.2805, 2.4966, 2.3441,\n",
      "        2.5899, 2.6791, 2.6998, 2.1339, 1.7781, 2.0061, 2.1823, 2.4102, 1.5506,\n",
      "        2.1535, 2.1468, 1.9080, 2.0525, 1.9541, 1.3706, 1.3432, 2.1246, 2.5483,\n",
      "        2.3490, 2.0671, 2.2180, 2.1230, 2.3011, 2.0029, 1.9655, 2.6712, 2.7159,\n",
      "        2.1323, 2.1638, 2.0403, 1.9926, 1.7621, 1.9377, 2.6548, 2.7543, 1.4470,\n",
      "        1.7856, 1.3633, 2.3535, 1.2309, 2.3134, 2.1170, 2.1004, 1.6081, 1.5316,\n",
      "        2.4258, 1.3032, 2.4427, 1.3130, 1.9826, 1.8881, 1.9640, 1.8179, 1.7906,\n",
      "        2.1343, 2.4397, 2.5613, 2.2650, 2.1448, 2.0950, 1.7971, 1.9852, 1.5358,\n",
      "        1.3759, 1.4370, 2.3595, 1.9340, 1.9675, 1.9765, 1.7728, 1.7976, 2.8042,\n",
      "        2.3958, 2.6748, 2.1281, 2.2027, 2.0739, 2.3718, 2.4037, 1.5837, 2.2386,\n",
      "        2.6574, 2.1748, 2.0852, 2.1502, 2.0357, 2.1396, 1.9317, 2.2147, 3.0343,\n",
      "        3.0321, 2.3202, 2.3938, 2.5252, 2.1948, 2.0657, 1.9904, 3.0063, 3.0462,\n",
      "        2.0403, 1.9066, 2.4615, 1.7370, 1.3764, 2.3821, 2.3170, 2.2560, 2.1020,\n",
      "        1.7692, 1.5508, 1.6553, 2.5132, 1.6593, 2.2081, 2.2577, 2.2588, 1.8993,\n",
      "        2.1746, 2.2868, 1.9393, 2.3400, 2.3158, 2.7784, 1.7887, 2.0533, 2.1199,\n",
      "        2.2876, 2.3033, 2.2717, 2.8608, 2.3375], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0288, -0.0201, -0.0456,  ...,  0.1469, -0.0759, -0.0684],\n",
      "        [-0.0304, -0.0814,  0.0367,  ..., -0.0041,  0.0160,  0.0169],\n",
      "        [-0.0677,  0.1499,  0.0201,  ..., -0.0365, -0.1146, -0.0570],\n",
      "        ...,\n",
      "        [-0.0368, -0.0433,  0.0684,  ..., -0.0629, -0.0465,  0.0074],\n",
      "        [-0.0696, -0.0594,  0.0368,  ...,  0.0569, -0.0507,  0.0033],\n",
      "        [-0.0476,  0.0658, -0.0835,  ..., -0.0380,  0.0564, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.1009,  0.0422,  0.0744,  ..., -0.0690,  0.0039,  0.0293],\n",
      "        [ 0.0631, -0.2107, -0.0640,  ..., -0.0489, -0.0206,  0.0946],\n",
      "        [ 0.1240,  0.1947, -0.0428,  ...,  0.0176,  0.2068,  0.0217],\n",
      "        ...,\n",
      "        [ 0.0440,  0.0606, -0.0811,  ...,  0.0092,  0.0140, -0.0459],\n",
      "        [-0.1132,  0.0147,  0.0120,  ..., -0.0290,  0.0403,  0.0715],\n",
      "        [ 0.1267, -0.0766, -0.0246,  ..., -0.0502, -0.0462,  0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5237, 1.6380, 1.5909, 1.5373, 1.6410, 1.6303, 1.5066, 1.5455, 1.5264,\n",
      "        1.5900, 1.5219, 1.5666, 1.5115, 1.5389, 1.5558, 1.5440, 1.1689, 1.2181,\n",
      "        1.2120, 1.1211, 1.2291, 1.1781, 1.2347, 1.1810, 1.2587, 1.1774, 1.2028,\n",
      "        1.1693, 1.1764, 1.1862, 1.1704, 1.2264, 1.6736, 1.4945, 1.6123, 1.6292,\n",
      "        1.6166, 1.5468, 1.6582, 1.6193, 1.6017, 1.5828, 1.6127, 1.6441, 1.6247,\n",
      "        1.6412, 1.6417, 1.6128, 1.2785, 1.1570, 1.2661, 1.2270, 1.1938, 1.2148,\n",
      "        1.2249, 1.2382, 1.2601, 1.2769, 1.1751, 1.2436, 1.2156, 1.2205, 1.2555,\n",
      "        1.2170, 1.8320, 1.8457, 1.8351, 1.7394, 1.8790, 1.8049, 1.8405, 1.9301,\n",
      "        1.8848, 1.9098, 1.8283, 1.9672, 1.8562, 1.9477, 1.8806, 1.9459, 1.7929,\n",
      "        1.7863, 1.8406, 1.7410, 1.7440, 1.8983, 1.7523, 1.7559, 1.8501, 1.6784,\n",
      "        1.7680, 1.8315, 1.7797, 1.8618, 1.8402, 1.8362, 1.6506, 1.7759, 1.6428,\n",
      "        1.6796, 1.7780, 1.6657, 1.6824, 1.7052, 1.7319, 1.6564, 1.6668, 1.6512,\n",
      "        1.6643, 1.7300, 1.6339, 1.6688, 1.6180, 1.5663, 1.6039, 1.5467, 1.5501,\n",
      "        1.5765, 1.6244, 1.5800, 1.5934, 1.5740, 1.5941, 1.5254, 1.6167, 1.5758,\n",
      "        1.5669, 1.6327, 1.2452, 1.1876, 1.2376, 1.2357, 1.2244, 1.1834, 1.1787,\n",
      "        1.1754, 1.2060, 1.1574, 1.1616, 1.1757, 1.2127, 1.1926, 1.1651, 1.2091,\n",
      "        1.5834, 1.6092, 1.5880, 1.5917, 1.5367, 1.5804, 1.5876, 1.5490, 1.5769,\n",
      "        1.6101, 1.6322, 1.6483, 1.5669, 1.6292, 1.6192, 1.6547, 1.7856, 1.8543,\n",
      "        1.7574, 1.8590, 1.8113, 1.7640, 1.7554, 1.8429, 1.7273, 1.8047, 1.6980,\n",
      "        1.8044, 1.6846, 1.8038, 1.6839, 1.7869, 1.7251, 1.7426, 1.6602, 1.6381,\n",
      "        1.7714, 1.7228, 1.7506, 1.8162, 1.7294, 1.7619, 1.7026, 1.6313, 1.7297,\n",
      "        1.6644, 1.7589, 1.6497, 1.5183, 1.5592, 1.4837, 1.5197, 1.5177, 1.5479,\n",
      "        1.5514, 1.5530, 1.5441, 1.5787, 1.6513, 1.5932, 1.6224, 1.5820, 1.5263,\n",
      "        1.5076, 1.8011, 1.8360, 1.7349, 1.7691, 1.8147, 1.8794, 1.7126, 1.7388,\n",
      "        1.6939, 1.8265, 1.8655, 1.7353, 1.7816, 1.7472, 1.8009, 1.7903, 1.3405,\n",
      "        1.3327, 1.2976, 1.3638, 1.3436, 1.4620, 1.3607, 1.3682, 1.4538, 1.4819,\n",
      "        1.3863, 1.3462, 1.3702, 1.3104, 1.3549, 1.3847, 1.6643, 1.6504, 1.6803,\n",
      "        1.6373, 1.5873, 1.6462, 1.6596, 1.6927, 1.6154, 1.6603, 1.6730, 1.6267,\n",
      "        1.7073, 1.6567, 1.7175, 1.6536, 1.2674, 1.2777, 1.2885, 1.2327, 1.2414,\n",
      "        1.2075, 1.2551, 1.2171, 1.2736, 1.2409, 1.2649, 1.1671, 1.2889, 1.2685,\n",
      "        1.2630, 1.2291, 1.8232, 1.6227, 1.6922, 1.6783, 1.6564, 1.6695, 1.6106,\n",
      "        1.8201, 1.5714, 1.6413, 1.7176, 1.7646, 1.7243, 1.6156, 1.7404, 1.7740,\n",
      "        1.3033, 1.2989, 1.2630, 1.2814, 1.2807, 1.3029, 1.3530, 1.2194, 1.2728,\n",
      "        1.3257, 1.2967, 1.3116, 1.2969, 1.2997, 1.2598, 1.3316, 1.4795, 1.5311,\n",
      "        1.5504, 1.4317, 1.5504, 1.5279, 1.4412, 1.4876, 1.5327, 1.4673, 1.5108,\n",
      "        1.4187, 1.4377, 1.5406, 1.4656, 1.6510], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0107,  0.0294, -0.0998,  ...,  0.0848, -0.0658,  0.2637],\n",
      "        [ 0.0298, -0.2141,  0.0243,  ...,  0.1150,  0.0791,  0.0398],\n",
      "        [ 0.1201, -0.0364, -0.0535,  ..., -0.0532, -0.0891,  0.1107],\n",
      "        ...,\n",
      "        [-0.0448,  0.1025,  0.0266,  ..., -0.0527, -0.0337, -0.0475],\n",
      "        [ 0.0427,  0.0838, -0.0343,  ..., -0.0642, -0.0347, -0.0093],\n",
      "        [-0.0318, -0.0439,  0.0419,  ...,  0.1552, -0.1432,  0.0283]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0852,  0.0132,  0.0308,  ..., -0.0494, -0.0430,  0.1691],\n",
      "        [-0.0361,  0.1662,  0.1051,  ..., -0.0820,  0.0795, -0.1699],\n",
      "        [ 0.0602,  0.0065,  0.0425,  ...,  0.0193, -0.0457,  0.0358],\n",
      "        ...,\n",
      "        [ 0.0220, -0.1076, -0.0351,  ...,  0.0743,  0.1189, -0.0309],\n",
      "        [ 0.0220, -0.1038,  0.0549,  ...,  0.0006, -0.0522,  0.0158],\n",
      "        [ 0.0709, -0.1168, -0.0378,  ..., -0.2090, -0.0591,  0.0829]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4847, 1.5854, 1.5281, 1.5112, 1.6205, 1.3933, 1.4293, 1.5220, 1.4344,\n",
      "        1.4322, 1.4330, 1.5356, 1.5584, 1.4024, 1.6372, 1.4345, 1.3345, 1.5364,\n",
      "        1.4364, 1.4626, 1.5345, 1.4153, 1.6026, 1.4375, 1.4696, 1.4832, 1.4734,\n",
      "        1.5345, 1.4947, 1.4880, 1.4454, 1.4259, 1.6303, 1.4785, 1.4093, 1.4467,\n",
      "        1.6177, 1.4142, 1.3959, 1.4692, 1.6344, 1.4974, 1.4720, 1.4923, 1.3762,\n",
      "        1.4122, 1.3920, 1.6090, 1.5128, 1.5003, 1.4581, 1.5089, 1.3121, 1.3747,\n",
      "        1.3892, 1.3606, 1.3440, 1.4343, 1.6283, 1.5520, 1.3893, 1.6759, 2.7844,\n",
      "        1.4561, 1.4414, 1.4795, 1.4250, 1.5668, 1.5379, 1.5108, 1.3348, 1.4412,\n",
      "        1.4062, 1.4302, 1.4888, 1.5939, 1.4416, 1.5165, 1.4689, 1.5040, 1.4802,\n",
      "        1.5338, 1.5007, 1.5733, 1.4101, 1.4544, 1.4441, 1.5479, 1.2997, 1.5089,\n",
      "        1.4162, 1.5413, 1.5944, 1.5202, 1.4379, 1.5832, 1.4857, 1.5958, 1.5407,\n",
      "        1.5192, 1.5830, 1.5442, 1.6174, 1.4657, 1.5545, 1.6074, 1.4277, 1.4065,\n",
      "        1.4201, 1.4251, 1.5887, 1.3608, 1.4386, 1.4492, 1.5470, 1.4966, 1.5836,\n",
      "        1.4753, 1.4542, 1.4340, 1.4909, 1.5836, 1.5379, 1.5224, 1.3440, 1.5710,\n",
      "        1.5226, 1.5317, 1.4819, 1.4899, 1.3567, 1.3921, 1.5082, 1.5816, 1.5643,\n",
      "        1.4018, 1.4542, 1.4932, 1.3423, 1.3792, 1.4234, 1.4151, 1.4774, 1.4146,\n",
      "        1.4055, 1.4890, 1.4656, 1.5486, 1.4744, 1.5527, 2.1154, 1.5793, 1.5607,\n",
      "        1.4953, 1.4764, 1.5145, 1.4657, 1.5617, 1.5575, 1.4825, 1.3502, 1.4525,\n",
      "        1.5726, 1.5779, 1.6583, 1.5692, 1.3804, 1.4436, 1.4081, 1.5196, 1.4325,\n",
      "        1.6387, 1.5334, 1.4450, 1.4886, 1.3273, 1.4974, 1.5533, 1.4687, 1.5539,\n",
      "        1.4422, 1.3911, 1.4604, 1.4915, 1.5441, 1.4027, 1.4639, 1.4745, 1.4918,\n",
      "        1.6241, 1.3499, 1.3536, 2.1478, 1.3057, 1.4600, 1.4698, 1.4978, 1.4253,\n",
      "        1.5231, 1.5591, 1.5982, 1.3092, 1.4334, 1.3613, 1.3678, 1.5513, 1.5309,\n",
      "        1.3400, 1.5387, 1.6581, 1.5580, 1.4863, 1.4336, 1.6080, 1.4570, 1.5346,\n",
      "        1.5540, 1.5155, 1.3515, 1.5483, 1.4277, 1.5748, 1.5334, 1.4417, 1.4742,\n",
      "        1.4410, 1.3977, 1.4992, 1.4367, 1.4513, 1.3513, 1.3961, 1.3730, 1.4724,\n",
      "        1.4606, 1.7224, 1.4556, 1.3458, 1.4811, 1.6254, 1.5191, 1.4462, 1.3226,\n",
      "        1.3853, 1.3522, 1.5353, 1.4177, 1.5309, 1.4781, 1.4179, 1.5149, 1.4159,\n",
      "        1.4979, 1.3967, 1.4373, 1.4593, 1.3772, 1.4508, 1.2212, 1.5037, 1.4556,\n",
      "        1.5011, 1.6198, 1.5268, 1.4054, 1.4593, 1.5355, 1.6802, 1.3931, 1.5964,\n",
      "        1.3429, 1.5394, 1.2852, 1.6051, 1.4286, 1.6176, 1.5341, 1.3497, 1.5703,\n",
      "        1.4810, 1.4809, 1.4689, 1.6007, 1.6133, 1.5608, 1.3981, 1.4171, 1.4585,\n",
      "        1.2427, 1.3542, 1.5318, 1.5489, 1.5062, 1.5158, 1.2936, 1.4616, 1.5901,\n",
      "        1.5646, 1.4796, 1.4929, 1.5041, 1.5503, 1.4835, 1.3523, 1.4823, 1.4329,\n",
      "        1.4578, 1.5483, 1.4357, 1.4534, 1.4646, 1.3408, 1.5548, 1.5341, 1.5688,\n",
      "        1.5005, 1.5777, 1.5705, 1.3821, 1.6774], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0948,  0.0598, -0.1138,  ..., -0.1099, -0.0847,  0.1816],\n",
      "        [ 0.1103, -0.1547, -0.0565,  ...,  0.1228, -0.0392, -0.0935],\n",
      "        [ 0.1795, -0.0441, -0.1028,  ..., -0.0675, -0.0260,  0.0195],\n",
      "        ...,\n",
      "        [-0.0545, -0.0109,  0.0828,  ..., -0.0864,  0.1112,  0.0080],\n",
      "        [ 0.0687,  0.0949,  0.0336,  ...,  0.0302, -0.0092, -0.0079],\n",
      "        [ 0.0093,  0.0754, -0.1319,  ...,  0.0140,  0.0235,  0.0426]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0682, -0.0174, -0.0538,  ..., -0.0117,  0.0471,  0.0644],\n",
      "        [-0.1031, -0.0088, -0.0234,  ..., -0.0370,  0.0062, -0.0071],\n",
      "        [ 0.0909, -0.0037, -0.1149,  ...,  0.0268, -0.0139,  0.0319],\n",
      "        ...,\n",
      "        [-0.1392, -0.0707,  0.0049,  ...,  0.0538, -0.0339,  0.0174],\n",
      "        [-0.0306, -0.0796, -0.1533,  ...,  0.0644, -0.0339, -0.0305],\n",
      "        [ 0.1289,  0.1313, -0.0153,  ..., -0.0077, -0.0347, -0.0208]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6552, 1.4334, 1.4333, 1.3825, 1.4168, 2.3624, 2.0710, 2.5610, 1.8070,\n",
      "        1.4649, 1.3482, 1.3601, 1.5074, 1.2783, 2.5928, 2.0364, 1.1000, 1.1304,\n",
      "        1.4394, 1.9744, 2.4227, 4.6512, 4.2595, 3.4382, 1.0111, 1.2298, 1.2887,\n",
      "        2.0301, 2.6273, 4.4372, 4.2133, 3.6077, 1.4863, 1.5198, 1.7810, 1.7904,\n",
      "        1.8810, 1.5664, 2.2300, 1.8756, 1.4852, 1.5696, 2.1647, 1.8271, 1.8696,\n",
      "        1.7276, 2.2294, 2.2891, 1.5805, 1.9161, 1.9421, 1.8423, 2.0988, 1.7557,\n",
      "        2.3040, 2.3112, 1.5298, 1.9603, 1.8596, 1.9622, 2.2476, 2.5642, 2.5681,\n",
      "        2.3505, 2.1444, 1.7776, 2.3910, 2.2577, 2.2775, 2.1773, 2.7792, 2.3904,\n",
      "        1.4821, 1.8719, 2.1337, 2.0356, 2.0097, 2.5207, 2.6754, 2.4400, 2.2276,\n",
      "        1.8581, 1.4236, 1.7332, 1.6358, 2.0797, 2.3105, 2.2091, 2.1493, 1.8424,\n",
      "        1.7077, 1.6115, 2.4484, 2.4536, 2.1115, 2.2423, 1.2833, 1.4388, 1.4090,\n",
      "        1.3854, 1.4278, 2.3062, 1.9912, 2.5803, 1.4052, 1.5144, 1.4000, 1.2931,\n",
      "        1.4113, 1.4254, 2.0248, 2.1956, 2.0416, 1.6295, 1.5176, 1.5757, 1.2364,\n",
      "        2.0242, 2.1945, 2.1233, 1.9329, 1.3427, 1.3791, 1.5646, 2.0294, 1.5717,\n",
      "        1.9742, 2.5319, 1.8474, 1.3917, 1.4993, 1.4128, 1.4397, 2.0798, 1.9736,\n",
      "        2.5238, 1.4915, 1.6802, 1.5838, 1.4165, 1.9504, 1.6635, 2.0834, 2.4967,\n",
      "        1.7435, 1.6351, 2.1095, 1.9533, 2.2135, 1.8848, 2.4154, 2.1751, 2.4308,\n",
      "        1.9679, 1.7467, 2.2406, 1.7439, 2.4684, 2.5535, 2.6872, 1.9586, 1.7359,\n",
      "        2.0742, 2.0076, 2.4434, 2.5920, 2.9749, 2.8062, 1.6446, 1.9224, 1.9818,\n",
      "        2.2306, 2.0848, 2.5432, 2.7834, 2.7860, 2.0137, 1.5935, 1.7468, 2.1187,\n",
      "        1.7261, 1.9407, 2.3628, 2.5211, 1.7424, 1.7850, 2.0001, 1.8485, 1.7472,\n",
      "        1.7776, 2.2005, 2.4120, 1.5175, 1.5551, 1.4119, 1.4831, 1.8009, 2.2293,\n",
      "        2.0382, 2.7584, 1.6488, 1.5253, 1.7321, 1.6944, 1.7926, 1.7162, 1.9397,\n",
      "        2.3349, 1.1478, 1.1457, 1.4768, 1.7365, 2.5167, 3.6357, 3.7704, 3.2782,\n",
      "        0.8232, 1.1941, 1.2854, 2.0714, 2.2399, 3.7320, 3.5234, 3.3439, 1.6643,\n",
      "        1.9483, 1.9040, 2.0350, 1.9639, 2.1692, 2.4454, 2.3579, 1.9448, 1.8434,\n",
      "        1.9090, 2.0098, 1.9273, 1.9125, 2.1907, 2.4258, 1.8691, 1.4603, 1.7656,\n",
      "        1.8131, 1.9404, 1.6375, 2.3291, 2.0268, 1.4493, 1.7165, 1.8529, 1.7386,\n",
      "        1.9589, 1.6711, 2.3684, 2.3801, 1.3841, 1.3492, 1.7807, 1.7038, 1.6484,\n",
      "        1.7553, 2.4681, 2.1278, 1.3672, 1.5706, 1.7322, 1.7799, 1.6928, 1.9181,\n",
      "        2.0598, 2.0556, 1.4114, 1.4788, 1.8002, 1.6682, 1.6220, 1.8274, 2.2851,\n",
      "        2.3713, 1.4840, 1.4003, 1.4672, 1.6401, 1.5482, 1.7767, 2.5248, 2.5006,\n",
      "        1.8956, 1.7507, 1.7005, 1.4926, 1.6595, 1.8658, 2.3257, 2.2957, 1.6053,\n",
      "        1.4602, 1.6022, 1.8346, 1.5352, 1.9717, 2.2161, 2.4296, 1.5886, 1.1851,\n",
      "        1.2203, 1.4174, 1.3647, 1.8373, 2.0729, 2.3338, 1.4978, 1.3226, 1.4443,\n",
      "        1.6841, 1.6486, 1.7464, 1.9548, 2.0678], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1013,  0.0492,  0.0783,  ...,  0.0711,  0.1158, -0.0316],\n",
      "        [ 0.0913, -0.0953, -0.0129,  ...,  0.0360,  0.0673, -0.0089],\n",
      "        [ 0.0834, -0.0333,  0.0348,  ...,  0.1749,  0.0360, -0.1717],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0785,  0.0348,  ..., -0.0383,  0.0202, -0.0574],\n",
      "        [-0.0494,  0.1110, -0.0394,  ..., -0.0033, -0.0134, -0.0389],\n",
      "        [ 0.1508, -0.1824, -0.0611,  ...,  0.0301,  0.0138, -0.0965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-2.2536e-03,  1.6058e-02, -5.2478e-02,  ...,  2.3720e-02,\n",
      "          4.3420e-02,  3.8817e-02],\n",
      "        [-4.3882e-02, -9.3136e-03, -5.4000e-02,  ..., -2.1725e-02,\n",
      "         -5.7059e-03,  2.7946e-02],\n",
      "        [-3.8626e-02,  9.5391e-02, -4.3173e-02,  ...,  2.4294e-02,\n",
      "          3.1823e-02,  1.7045e-02],\n",
      "        ...,\n",
      "        [ 1.5301e-03, -2.2620e-02,  2.9410e-02,  ...,  7.5472e-02,\n",
      "         -1.1191e-02, -2.1986e-02],\n",
      "        [-1.9202e-01, -2.1342e-02, -8.4856e-02,  ...,  3.0970e-02,\n",
      "         -2.9097e-05, -5.5118e-02],\n",
      "        [ 2.5200e-01, -2.2403e-02, -1.2297e-02,  ...,  2.3491e-02,\n",
      "         -7.7852e-03, -2.0245e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6889, 1.4271, 1.7430, 1.6628, 1.6534, 1.2899, 1.8223, 1.7646, 1.8019,\n",
      "        1.7449, 1.6390, 1.7138, 1.6659, 2.3107, 1.6661, 1.7704, 1.1473, 1.3915,\n",
      "        1.3758, 2.0068, 1.9256, 3.2060, 2.9245, 2.7808, 1.1138, 0.9388, 1.4358,\n",
      "        1.8676, 2.2222, 3.0254, 3.0221, 2.7178, 1.4727, 1.5844, 1.9981, 1.8033,\n",
      "        1.9005, 2.2584, 1.9988, 1.9343, 1.4264, 1.4540, 1.8902, 1.8841, 1.8554,\n",
      "        1.6541, 2.1052, 2.1458, 1.6813, 2.0848, 1.8382, 1.7060, 1.8021, 2.2735,\n",
      "        2.2901, 2.0180, 1.4035, 1.6622, 1.9498, 1.9802, 1.8883, 1.5377, 2.1773,\n",
      "        2.0368, 1.8880, 1.7127, 2.0528, 2.0940, 1.8528, 2.0759, 2.4393, 2.1081,\n",
      "        1.7533, 1.9557, 2.2812, 2.0530, 2.0005, 1.8960, 2.5726, 2.3136, 1.7890,\n",
      "        1.8889, 1.7278, 1.7179, 2.0000, 1.9749, 2.0090, 2.0776, 2.2909, 1.8528,\n",
      "        1.9292, 1.8684, 1.3181, 1.4653, 2.0984, 1.9083, 1.4736, 1.5217, 1.7262,\n",
      "        1.9033, 1.4419, 1.2665, 1.8880, 1.5187, 1.3202, 1.7243, 1.6979, 1.5558,\n",
      "        1.7915, 1.9450, 1.8671, 1.6432, 1.8415, 1.6661, 1.7453, 1.7020, 1.9645,\n",
      "        1.4424, 1.9818, 2.0917, 2.0265, 1.7816, 1.7390, 1.8201, 1.3807, 2.2870,\n",
      "        2.1284, 1.7866, 1.8202, 1.6078, 1.6488, 1.6817, 1.8529, 1.3601, 1.9022,\n",
      "        1.8078, 1.4124, 1.6446, 1.8692, 1.7385, 1.3284, 1.9982, 2.0476, 1.8216,\n",
      "        2.0397, 1.7566, 1.9442, 2.0140, 1.7768, 2.1294, 1.9944, 1.9886, 1.5791,\n",
      "        1.7224, 1.7992, 1.9633, 2.0390, 1.8915, 2.2578, 2.2700, 1.9004, 1.8885,\n",
      "        1.8815, 2.0661, 2.0135, 1.9570, 2.7777, 2.6218, 1.6407, 1.7246, 2.1530,\n",
      "        2.0494, 2.0387, 2.2814, 2.5121, 2.3586, 1.7866, 1.5010, 1.9422, 1.9570,\n",
      "        1.9640, 2.0323, 2.4325, 2.4010, 1.8795, 1.8733, 1.7822, 2.0121, 1.7464,\n",
      "        1.7709, 2.1043, 2.2803, 1.7662, 1.6333, 1.7373, 1.8647, 1.5324, 1.2077,\n",
      "        1.9634, 1.8604, 1.9159, 1.7432, 1.5445, 1.3137, 1.3502, 2.2007, 1.9211,\n",
      "        1.9771, 1.1066, 1.2730, 1.5260, 1.9362, 2.1939, 2.7678, 2.8119, 2.5205,\n",
      "        0.8824, 1.1725, 1.1172, 1.7185, 1.8117, 2.8365, 2.5728, 2.6282, 1.7312,\n",
      "        1.8346, 1.9135, 1.8828, 1.6636, 1.7983, 2.2877, 2.2648, 1.7312, 1.8848,\n",
      "        1.8329, 2.0321, 1.8790, 1.7116, 1.9831, 2.1841, 1.7704, 1.5616, 1.7671,\n",
      "        1.8773, 1.8690, 1.9465, 2.1781, 1.9954, 1.3467, 1.6612, 1.8440, 1.7460,\n",
      "        1.9353, 1.8570, 2.2530, 2.3184, 1.2619, 1.5797, 1.8240, 1.8519, 1.7561,\n",
      "        1.8521, 1.9934, 2.1386, 1.5619, 1.4646, 1.7888, 1.8055, 1.9259, 1.7539,\n",
      "        2.5143, 1.9833, 1.2787, 1.3773, 1.7145, 1.7669, 1.6839, 1.7099, 2.1194,\n",
      "        1.8643, 1.0018, 1.3337, 1.6767, 1.9264, 1.7412, 1.7326, 1.9674, 1.9704,\n",
      "        1.8140, 1.7674, 1.7366, 1.7669, 1.7734, 1.7101, 2.3058, 2.0764, 1.6758,\n",
      "        1.6545, 1.7996, 1.8239, 1.7054, 1.6117, 2.1010, 2.2289, 1.4085, 1.5210,\n",
      "        1.4740, 1.3472, 1.8246, 1.2400, 1.4690, 1.5208, 1.7546, 1.4491, 1.4017,\n",
      "        1.2756, 1.1156, 1.2434, 1.6302, 1.5776], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.3299,  0.0627,  0.0467,  ..., -0.0733,  0.0088,  0.0062],\n",
      "        [ 0.0204,  0.1075,  0.0079,  ...,  0.0863, -0.0849,  0.0835],\n",
      "        [-0.0494, -0.0335, -0.3117,  ...,  0.0966, -0.0074,  0.0293],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0343, -0.0824,  ..., -0.0599,  0.1646,  0.0819],\n",
      "        [-0.0434, -0.0323,  0.0117,  ...,  0.0464, -0.1104,  0.1330],\n",
      "        [ 0.0307,  0.0402, -0.0509,  ..., -0.0433,  0.0360,  0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0920,  0.1429,  0.1498,  ...,  0.0309,  0.2395, -0.0015],\n",
      "        [-0.1530, -0.0796,  0.0126,  ..., -0.0116,  0.0841, -0.0048],\n",
      "        [-0.0298,  0.0040, -0.0071,  ...,  0.1056,  0.0821,  0.0290],\n",
      "        ...,\n",
      "        [-0.1107, -0.0566, -0.0015,  ..., -0.0197, -0.0874,  0.0378],\n",
      "        [-0.0951,  0.1062, -0.0328,  ..., -0.0955, -0.0743, -0.0265],\n",
      "        [-0.1280,  0.0642,  0.0136,  ...,  0.0097, -0.1531, -0.1130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9910, 1.7859, 1.7181, 1.8752, 1.9573, 1.7906, 1.7669, 1.8068, 1.7691,\n",
      "        1.9023, 1.7144, 1.8018, 1.7877, 1.8022, 1.9825, 1.8459, 1.5629, 1.4503,\n",
      "        1.5721, 1.4834, 1.5047, 1.4777, 1.4249, 1.4403, 1.4609, 1.4187, 1.4352,\n",
      "        1.4818, 1.5416, 1.4619, 1.4833, 1.4405, 1.7743, 1.8952, 1.8282, 1.7398,\n",
      "        1.8591, 1.7324, 1.8634, 1.6844, 1.8550, 1.9357, 1.7557, 1.7681, 1.7066,\n",
      "        1.7661, 1.8067, 1.8372, 1.7756, 1.7685, 2.2151, 1.9220, 1.8615, 1.8839,\n",
      "        1.8420, 1.8436, 1.9874, 1.9857, 1.7812, 1.8052, 2.0072, 2.1669, 1.8810,\n",
      "        1.8712, 2.1752, 2.0793, 1.7751, 2.3139, 1.8010, 2.3051, 1.7924, 1.9537,\n",
      "        1.9208, 1.9720, 1.9665, 1.9238, 1.8328, 1.9078, 2.0223, 1.9621, 1.7919,\n",
      "        1.9148, 1.7307, 1.7685, 1.7281, 1.7405, 1.7336, 1.7506, 1.8781, 1.7557,\n",
      "        1.7685, 1.6618, 1.7410, 1.7996, 1.7391, 1.7953, 1.7625, 1.8009, 1.8637,\n",
      "        1.8832, 1.8602, 1.8920, 1.7451, 1.9282, 1.9451, 1.8833, 1.9162, 1.8563,\n",
      "        1.9071, 1.9149, 1.9026, 1.8479, 1.8409, 1.8171, 1.7042, 1.8710, 1.8583,\n",
      "        1.8760, 1.7786, 1.8398, 1.7976, 1.8047, 1.8330, 1.7827, 1.8125, 1.7038,\n",
      "        1.7855, 1.8503, 1.8570, 1.8230, 1.7993, 1.6955, 1.8503, 1.7523, 1.7433,\n",
      "        1.9098, 1.7231, 1.8333, 1.7466, 1.7698, 1.7607, 1.8340, 1.9074, 1.8020,\n",
      "        1.6698, 1.6084, 1.7089, 1.6531, 1.7456, 1.6656, 1.7196, 1.6576, 1.7034,\n",
      "        1.6911, 1.7480, 1.7944, 1.7231, 1.7571, 1.6898, 1.7747, 1.8855, 1.8563,\n",
      "        1.8743, 1.8496, 2.0436, 2.0675, 1.8706, 1.8486, 1.9140, 1.8655, 1.9353,\n",
      "        1.9089, 1.9487, 1.8241, 1.9167, 1.9539, 1.9157, 2.0385, 1.8803, 1.9441,\n",
      "        2.0407, 1.8543, 1.9363, 1.9550, 1.9103, 1.9249, 1.9537, 1.8689, 1.9965,\n",
      "        1.9404, 1.9471, 1.9209, 1.3920, 1.4053, 1.3236, 1.4026, 1.2973, 1.3371,\n",
      "        1.4223, 1.3296, 1.3952, 1.3427, 1.5071, 1.4203, 1.4299, 1.4072, 1.3403,\n",
      "        1.3813, 1.5942, 1.5461, 1.4136, 1.5487, 1.4926, 1.5218, 1.5217, 1.4937,\n",
      "        1.4831, 1.3992, 1.4020, 1.5040, 1.4087, 1.4507, 1.4973, 1.4373, 2.1330,\n",
      "        1.9655, 2.0132, 1.9046, 2.1477, 1.9870, 1.8481, 1.9034, 1.8565, 1.9019,\n",
      "        1.8099, 2.1112, 2.1387, 1.9774, 1.9240, 1.8725, 1.7300, 1.7895, 1.7128,\n",
      "        1.7626, 1.7251, 1.7140, 1.8034, 1.7685, 1.7610, 1.7741, 1.8405, 1.6437,\n",
      "        1.7217, 1.7438, 1.7286, 1.8240, 1.8331, 1.7401, 1.8251, 1.7745, 1.7199,\n",
      "        1.6723, 1.7602, 1.8064, 1.7394, 1.7077, 1.7129, 1.8042, 1.8022, 1.7928,\n",
      "        1.6652, 1.7366, 1.8956, 1.9773, 1.8821, 2.0162, 1.8606, 1.8993, 1.9118,\n",
      "        1.8570, 1.7689, 1.8904, 1.8602, 1.8927, 1.9566, 2.0228, 1.9236, 1.9475,\n",
      "        1.9208, 1.8269, 1.8951, 1.8767, 1.9128, 2.0383, 1.8567, 1.9909, 1.8191,\n",
      "        1.9111, 1.9571, 1.8844, 1.9293, 1.9161, 1.8280, 1.9185, 1.7560, 1.7776,\n",
      "        1.8200, 1.6379, 2.0614, 1.7395, 1.6892, 1.8435, 1.8097, 1.7288, 1.8897,\n",
      "        1.6953, 1.7442, 1.7315, 1.7611, 1.8561], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.2656, -0.0917,  0.0655,  ..., -0.0740, -0.1863,  0.1651],\n",
      "        [-0.0688,  0.0296,  0.0274,  ..., -0.1426,  0.0641,  0.0861],\n",
      "        [-0.0090, -0.0202,  0.0656,  ...,  0.0323,  0.0653, -0.0444],\n",
      "        ...,\n",
      "        [-0.0234,  0.0651,  0.1240,  ..., -0.0122,  0.1006, -0.0528],\n",
      "        [ 0.0212, -0.0394, -0.0730,  ..., -0.0338, -0.0896, -0.0396],\n",
      "        [-0.1534, -0.0358,  0.0791,  ...,  0.0288,  0.0590, -0.0004]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.0284e-02,  3.0753e-03, -3.8465e-02,  ...,  5.6222e-02,\n",
      "          8.0556e-02,  4.7121e-02],\n",
      "        [-6.1613e-02, -1.4256e-01, -2.7052e-02,  ..., -1.4203e-02,\n",
      "         -1.4489e-02,  8.0996e-02],\n",
      "        [ 3.5126e-02, -8.7188e-02,  3.3745e-03,  ...,  5.1317e-02,\n",
      "         -5.1557e-02, -1.4658e-02],\n",
      "        ...,\n",
      "        [-1.0158e-02,  2.5906e-02,  6.1291e-03,  ..., -7.0126e-02,\n",
      "          6.1601e-02,  1.5138e-04],\n",
      "        [ 1.0797e-02, -1.5308e-01, -1.5853e-01,  ...,  1.2545e-01,\n",
      "         -9.3582e-02, -5.8525e-02],\n",
      "        [-1.5178e-03, -7.7993e-02,  2.2972e-02,  ..., -6.3244e-03,\n",
      "         -2.8605e-02,  1.5632e-01]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5955, 1.7774, 1.6411, 1.8565, 1.7713, 1.6490, 1.6777, 1.5707, 1.7964,\n",
      "        1.8246, 1.3758, 1.5962, 1.7137, 1.6887, 1.7496, 1.6856, 1.8570, 1.7961,\n",
      "        1.6562, 1.8484, 1.8222, 1.6783, 1.7030, 1.5851, 1.6452, 1.5345, 1.5934,\n",
      "        1.7014, 1.7638, 1.8334, 1.6760, 1.4294, 1.6589, 1.4353, 1.6987, 1.7667,\n",
      "        1.6623, 1.6831, 1.5934, 1.6539, 1.6826, 1.6921, 1.7623, 1.6043, 1.7830,\n",
      "        1.5819, 1.6015, 1.5426, 1.6916, 1.7561, 1.7366, 1.7634, 1.4993, 1.6625,\n",
      "        1.5742, 1.6977, 1.5489, 1.7028, 1.7906, 1.6264, 1.7208, 1.7474, 5.9713,\n",
      "        1.7458, 1.7300, 1.7038, 1.5947, 1.7354, 1.7214, 1.8418, 1.7113, 1.8144,\n",
      "        1.5569, 1.7767, 1.6403, 1.7845, 1.7405, 1.6785, 1.7531, 1.7531, 1.5310,\n",
      "        1.6492, 1.6948, 1.6424, 1.5773, 1.6855, 1.6548, 1.7195, 1.4733, 1.8451,\n",
      "        1.8094, 1.6022, 1.9097, 1.6485, 1.7164, 1.7464, 1.7125, 1.8256, 1.7713,\n",
      "        1.8616, 1.8821, 1.6654, 1.7550, 1.6193, 1.7847, 1.8291, 1.5213, 1.6521,\n",
      "        1.7260, 1.6569, 1.6522, 1.6711, 1.6025, 1.5468, 1.8519, 1.5736, 1.6658,\n",
      "        1.6979, 1.5867, 1.8101, 1.6108, 1.7021, 1.7064, 1.7419, 1.6317, 1.7966,\n",
      "        1.6672, 1.6655, 1.5100, 1.7696, 1.5108, 1.8478, 1.7174, 1.7817, 1.7482,\n",
      "        1.6621, 1.7180, 1.6757, 1.5812, 1.7535, 1.5516, 1.6415, 1.7088, 1.6439,\n",
      "        1.6568, 1.8728, 1.6958, 1.6390, 1.6714, 1.6700, 1.9461, 1.7414, 1.9013,\n",
      "        1.7572, 1.8281, 1.6806, 1.5423, 1.7415, 1.9297, 1.6975, 1.6655, 1.7206,\n",
      "        1.6213, 1.7000, 1.7465, 1.6649, 1.6983, 1.7315, 1.7622, 1.7193, 1.5828,\n",
      "        1.7184, 1.6800, 1.6066, 1.7018, 1.7054, 1.5293, 1.7079, 1.7337, 1.6082,\n",
      "        1.6103, 1.6586, 1.6661, 1.7270, 1.6324, 1.6296, 1.5839, 1.6382, 1.6109,\n",
      "        1.6628, 1.6732, 1.6020, 2.5911, 1.5976, 1.4699, 1.6618, 1.6716, 1.7346,\n",
      "        1.6751, 1.8661, 1.7660, 1.7797, 1.7193, 1.5756, 1.8248, 1.6417, 1.7320,\n",
      "        1.3543, 1.6818, 1.6336, 1.6644, 1.6058, 1.6006, 1.7863, 1.6508, 1.8074,\n",
      "        1.6615, 1.8319, 1.4196, 1.5631, 1.5192, 1.6476, 1.6225, 1.7474, 1.7357,\n",
      "        1.6482, 1.6599, 1.5845, 1.6166, 1.6875, 1.6143, 1.5869, 1.6801, 1.7017,\n",
      "        1.5875, 1.6446, 1.6187, 1.5729, 1.7308, 1.7823, 1.5599, 1.6769, 1.6657,\n",
      "        1.6443, 1.5579, 1.8099, 1.6075, 1.6483, 1.7434, 1.7089, 1.4043, 1.6873,\n",
      "        1.7122, 1.5987, 1.6250, 1.7545, 1.7020, 1.5411, 1.4943, 1.7379, 1.6249,\n",
      "        1.6571, 1.7278, 1.6129, 1.6526, 1.7350, 1.7500, 2.0525, 1.5756, 1.6635,\n",
      "        1.5913, 1.7879, 1.6143, 1.6523, 1.8038, 1.6544, 1.7428, 1.5644, 1.7499,\n",
      "        1.8054, 1.6897, 1.6909, 1.6986, 1.7602, 1.7452, 1.7076, 1.8191, 1.6954,\n",
      "        1.4708, 1.6595, 1.6592, 1.8116, 1.7024, 1.7261, 1.4623, 1.7947, 1.7470,\n",
      "        1.6415, 1.5469, 1.8144, 1.7742, 1.6891, 1.6152, 1.5693, 1.6135, 1.6632,\n",
      "        1.7055, 1.6798, 1.5891, 1.6839, 1.6884, 1.4870, 1.6161, 1.6982, 1.7105,\n",
      "        1.7106, 1.7921, 1.6304, 1.6943, 1.6345], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0533,  0.0625,  0.1111,  ...,  0.0668, -0.0847,  0.0618],\n",
      "        [-0.1108,  0.1348, -0.0355,  ..., -0.1056, -0.1544,  0.0290],\n",
      "        [ 0.0574, -0.0030, -0.0396,  ...,  0.0564, -0.1296, -0.0356],\n",
      "        ...,\n",
      "        [-0.0012,  0.0894,  0.1241,  ..., -0.0101, -0.0140, -0.0335],\n",
      "        [ 0.0125,  0.1294,  0.0398,  ...,  0.1606, -0.0676, -0.0041],\n",
      "        [ 0.0019,  0.0132, -0.0200,  ..., -0.0212,  0.0234,  0.0203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0204,  0.0916, -0.0315,  ..., -0.0114, -0.0107,  0.0267],\n",
      "        [-0.0031,  0.0262,  0.0189,  ...,  0.0338, -0.0096, -0.0121],\n",
      "        [-0.0315,  0.0781, -0.0404,  ...,  0.1034, -0.0275,  0.0202],\n",
      "        ...,\n",
      "        [-0.0392, -0.0384,  0.0073,  ..., -0.0457, -0.0576,  0.0726],\n",
      "        [-0.0173,  0.0006,  0.1081,  ..., -0.0750, -0.0350,  0.1080],\n",
      "        [-0.0203, -0.0470, -0.0835,  ...,  0.1310,  0.0393,  0.0021]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6348, 1.7580, 1.7600, 1.8987, 1.9073, 1.6733, 2.1742, 2.6053, 1.5479,\n",
      "        1.6980, 1.9652, 2.0713, 1.9909, 1.6580, 2.4569, 2.1450, 1.5354, 1.3555,\n",
      "        1.7000, 1.6704, 2.5079, 3.3870, 3.5893, 4.1186, 1.0323, 1.4894, 1.6665,\n",
      "        1.9888, 2.2814, 3.4984, 3.6289, 3.8487, 1.3646, 1.3216, 0.9955, 1.4157,\n",
      "        1.9318, 1.9584, 1.6417, 1.8326, 1.1083, 0.7401, 1.3810, 1.2650, 1.2892,\n",
      "        1.4124, 1.6810, 1.8815, 1.5584, 1.6734, 2.0461, 1.9555, 2.2413, 3.5153,\n",
      "        3.3176, 3.5692, 1.3070, 1.7736, 1.9741, 1.8774, 2.2711, 3.4681, 3.2261,\n",
      "        3.3209, 1.6160, 2.2388, 1.6578, 1.3719, 1.8077, 1.7066, 1.9959, 2.0373,\n",
      "        1.9473, 1.8989, 1.7558, 1.7326, 1.4083, 1.8565, 2.0642, 2.1348, 1.3966,\n",
      "        1.1396, 1.3960, 1.6090, 2.2096, 2.0403, 1.9043, 2.0704, 1.6033, 1.4882,\n",
      "        1.2290, 1.2406, 1.3760, 1.6762, 1.9929, 1.9341, 1.7229, 1.7364, 1.7947,\n",
      "        1.7437, 1.6513, 1.6086, 2.3612, 2.2471, 2.0393, 2.0386, 2.0181, 1.5519,\n",
      "        2.0385, 1.4618, 2.4900, 2.2070, 1.6887, 1.5130, 1.5078, 1.0071, 1.4091,\n",
      "        2.0931, 1.8544, 1.8918, 1.6560, 1.2705, 0.9788, 1.7273, 1.4331, 1.5869,\n",
      "        1.8908, 1.8766, 1.4347, 1.5045, 1.3517, 1.3812, 1.2570, 1.8235, 1.8054,\n",
      "        1.3317, 1.7539, 1.6626, 1.6450, 1.2972, 1.6370, 1.3301, 2.0044, 1.8913,\n",
      "        1.0565, 1.1074, 1.0726, 1.6159, 1.4091, 1.5235, 1.7917, 2.3176, 1.5910,\n",
      "        1.1359, 1.5162, 1.4123, 2.0761, 1.9423, 1.8269, 1.9580, 1.8525, 1.3937,\n",
      "        1.0158, 1.2760, 1.8018, 1.5525, 1.7616, 1.9545, 1.2163, 1.3962, 1.4957,\n",
      "        1.5052, 1.4415, 2.0264, 1.9120, 1.9618, 2.1249, 2.3317, 2.2198, 1.9617,\n",
      "        1.9704, 1.7355, 2.3967, 2.3548, 1.7853, 1.8450, 1.9076, 1.8547, 1.6410,\n",
      "        2.6102, 2.3785, 2.3255, 1.8591, 1.2332, 1.5919, 1.2847, 1.9284, 2.0518,\n",
      "        1.7799, 1.9947, 0.9066, 1.3266, 1.0756, 1.7669, 1.4615, 1.4817, 1.7480,\n",
      "        2.0130, 2.0156, 1.7369, 1.5396, 1.5977, 1.7004, 1.8698, 1.9593, 2.2848,\n",
      "        1.7541, 2.0033, 2.0069, 1.8192, 1.6435, 2.1934, 1.5391, 2.2392, 1.0392,\n",
      "        1.2093, 1.6377, 1.9484, 1.8447, 1.4341, 1.8449, 1.8253, 1.6858, 1.0493,\n",
      "        1.1410, 1.2426, 1.3798, 2.0106, 1.7852, 2.1957, 1.9850, 1.7412, 1.8574,\n",
      "        1.8661, 1.8031, 2.3466, 2.9997, 3.0253, 2.0872, 2.0299, 2.0917, 1.9352,\n",
      "        2.4777, 2.3653, 3.0240, 2.9702, 1.5602, 1.3140, 1.1655, 1.0991, 2.1892,\n",
      "        1.5726, 1.9698, 1.8761, 1.7897, 1.0264, 1.5203, 2.1242, 1.5288, 2.0092,\n",
      "        1.7991, 1.9230, 1.6340, 1.3481, 1.5435, 1.2237, 1.2811, 1.9219, 1.8515,\n",
      "        1.9168, 1.5493, 1.5348, 1.3427, 1.6986, 1.8454, 1.4895, 1.7584, 1.8982,\n",
      "        1.6191, 1.2692, 1.2172, 1.7493, 1.9760, 2.2467, 2.0327, 2.0167, 1.9440,\n",
      "        1.3115, 1.3252, 1.2610, 1.6757, 1.4898, 1.9572, 1.9318, 2.1588, 1.5117,\n",
      "        1.5889, 1.9241, 2.2671, 2.2252, 1.8919, 2.2643, 1.0607, 1.4866, 1.4783,\n",
      "        1.6324, 1.5742, 1.8355, 1.9991, 2.2522], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0421,  0.0021,  0.0544,  ..., -0.0900,  0.0365,  0.0097],\n",
      "        [ 0.1137, -0.0258, -0.0686,  ...,  0.0793,  0.1475, -0.0688],\n",
      "        [ 0.0153, -0.0830,  0.1688,  ..., -0.0317,  0.0222,  0.0120],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0593, -0.0321,  ..., -0.0472,  0.0927, -0.0035],\n",
      "        [-0.0721,  0.0438,  0.0404,  ...,  0.1253, -0.0052, -0.0427],\n",
      "        [-0.0041,  0.0302,  0.0469,  ..., -0.0585, -0.0741, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141,  0.0679,  0.0643,  ..., -0.0474, -0.0354, -0.0297],\n",
      "        [-0.0238,  0.0461,  0.0108,  ..., -0.0805, -0.0284,  0.0932],\n",
      "        [ 0.0274,  0.0133,  0.0273,  ...,  0.0278, -0.0944, -0.0970],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0287, -0.1295,  ...,  0.1143, -0.0046,  0.0036],\n",
      "        [ 0.0929,  0.1265,  0.0153,  ..., -0.1550,  0.0848, -0.0352],\n",
      "        [-0.2411, -0.0324, -0.0354,  ...,  0.0552,  0.0189, -0.0306]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5536, 1.6816, 1.9042, 2.0716, 1.8858, 1.7257, 2.2499, 2.1195, 1.6863,\n",
      "        1.6731, 1.8040, 1.7963, 1.9987, 2.2336, 2.0888, 2.0066, 1.4894, 1.3723,\n",
      "        1.6265, 1.8277, 2.2012, 2.5397, 2.6738, 2.5855, 0.8681, 1.4053, 1.6286,\n",
      "        1.7931, 1.9907, 2.4652, 2.6671, 2.2722, 1.5785, 1.3352, 1.4237, 1.2357,\n",
      "        1.0469, 1.2093, 1.5947, 1.6913, 1.5363, 1.5405, 1.6118, 1.4568, 1.9306,\n",
      "        1.8896, 1.6171, 1.5869, 1.4570, 1.6241, 1.9119, 2.0824, 2.0316, 2.5976,\n",
      "        2.7515, 2.7973, 1.4126, 1.8107, 2.0275, 1.7897, 2.1093, 2.4162, 2.7039,\n",
      "        2.6874, 1.7641, 1.9721, 1.7654, 1.5692, 1.5022, 2.0413, 1.9107, 1.9334,\n",
      "        1.7602, 1.9761, 1.8772, 2.0069, 1.7361, 1.4872, 1.9491, 1.9623, 1.8603,\n",
      "        1.4981, 1.6829, 1.2947, 1.1256, 1.2755, 1.8204, 1.7343, 1.6326, 1.7542,\n",
      "        1.5185, 1.7682, 2.2894, 1.9047, 1.8048, 1.8163, 1.7625, 1.5840, 1.7491,\n",
      "        1.8754, 2.1086, 2.0706, 2.1636, 2.0922, 1.8712, 2.0121, 2.0101, 1.8445,\n",
      "        1.5731, 1.7000, 2.0915, 2.0706, 1.6826, 1.7709, 1.5696, 2.0106, 2.1213,\n",
      "        1.2891, 1.7646, 1.8229, 1.7205, 1.5759, 1.8092, 1.3200, 1.2581, 2.1016,\n",
      "        1.7918, 1.7562, 1.5662, 1.6547, 1.7322, 1.7514, 1.7226, 1.3767, 1.7225,\n",
      "        1.7857, 1.5723, 1.7444, 1.6497, 1.5467, 1.5653, 1.8414, 1.9007, 1.8063,\n",
      "        1.4640, 1.4229, 1.7060, 1.4072, 2.1158, 1.8636, 1.7304, 1.5449, 1.7545,\n",
      "        1.6200, 1.4899, 1.3729, 1.1498, 1.2309, 1.7266, 1.7108, 1.6178, 1.4281,\n",
      "        1.5278, 1.5410, 1.2036, 2.1763, 1.7051, 1.6497, 2.0421, 1.8508, 1.5932,\n",
      "        1.4825, 1.8101, 1.2307, 1.7226, 2.0706, 1.9593, 2.0603, 2.0549, 1.9018,\n",
      "        1.4942, 2.3109, 2.2576, 2.2654, 1.9011, 1.9925, 1.9706, 1.9669, 2.2415,\n",
      "        1.3770, 2.1620, 2.1251, 1.7700, 1.5246, 1.4617, 1.9063, 1.2533, 1.1436,\n",
      "        1.6237, 1.7813, 1.6624, 1.7156, 1.7583, 1.2114, 1.7833, 2.1020, 1.6106,\n",
      "        1.7715, 1.6870, 1.8602, 1.7306, 1.6342, 1.4546, 1.9430, 1.9743, 2.0345,\n",
      "        1.9950, 1.8160, 1.7939, 1.5938, 1.7092, 1.4822, 1.6834, 1.9933, 1.2194,\n",
      "        1.5691, 1.6256, 1.1583, 1.1055, 2.0742, 1.7112, 1.7251, 2.1369, 1.4342,\n",
      "        1.4548, 1.8314, 1.9962, 1.2296, 1.7255, 1.8570, 2.1658, 1.8280, 2.0589,\n",
      "        1.7702, 1.9085, 1.7693, 2.2831, 2.4908, 1.9063, 1.9562, 1.8910, 2.0011,\n",
      "        1.7704, 1.7396, 2.4638, 2.2630, 1.7626, 1.6832, 1.8605, 1.9925, 1.1350,\n",
      "        1.9958, 1.7153, 1.8086, 1.8552, 1.6254, 1.5218, 1.0843, 1.9864, 1.1807,\n",
      "        1.7365, 1.8613, 1.7112, 1.6000, 1.8047, 1.6983, 2.2184, 1.1924, 1.7662,\n",
      "        1.8420, 1.7115, 1.8174, 1.5433, 1.2747, 1.1495, 2.0921, 1.7776, 1.8112,\n",
      "        1.9601, 1.6811, 1.6068, 1.3809, 1.2137, 1.1520, 1.9185, 1.8082, 1.7144,\n",
      "        1.6394, 1.7922, 1.5821, 1.6538, 2.1146, 1.7697, 1.8755, 1.8261, 1.6772,\n",
      "        1.6253, 1.4242, 1.2031, 1.2849, 1.8295, 1.8200, 1.5124, 1.5177, 1.6446,\n",
      "        1.6937, 2.1035, 2.1817, 1.8018, 1.7916], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1715, -0.0842,  0.0623,  ..., -0.0756, -0.0031, -0.1606],\n",
      "        [ 0.0817, -0.1410, -0.0496,  ..., -0.0089, -0.0691, -0.1001],\n",
      "        [ 0.0820,  0.2034,  0.0725,  ..., -0.1266,  0.1318, -0.0540],\n",
      "        ...,\n",
      "        [-0.0061, -0.0810, -0.0924,  ...,  0.0680,  0.0238,  0.0275],\n",
      "        [ 0.1704,  0.0787,  0.0281,  ..., -0.0454, -0.0855, -0.1911],\n",
      "        [-0.2309, -0.0218, -0.0763,  ...,  0.1687,  0.1254, -0.0525]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0212, -0.0068,  0.0331,  ..., -0.0212,  0.0122, -0.0030],\n",
      "        [-0.0332,  0.0130, -0.0306,  ..., -0.0494, -0.0249, -0.0276],\n",
      "        [-0.0666,  0.0183, -0.0298,  ..., -0.0298, -0.0221, -0.0577],\n",
      "        ...,\n",
      "        [ 0.1483,  0.0577,  0.0132,  ...,  0.0972,  0.1014,  0.0527],\n",
      "        [ 0.0901, -0.0900,  0.0795,  ..., -0.0050,  0.0649, -0.0170],\n",
      "        [-0.1823,  0.1025,  0.0497,  ..., -0.0007,  0.0304, -0.0460]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0495, 2.2048, 2.1120, 2.1693, 2.1060, 2.1021, 2.1868, 2.2377, 2.2128,\n",
      "        2.2043, 2.1598, 2.0417, 2.2390, 2.3311, 2.1067, 2.0563, 1.9171, 2.0404,\n",
      "        1.9799, 1.9873, 1.9744, 2.0415, 1.9896, 2.0045, 1.9913, 1.9401, 1.9581,\n",
      "        2.0183, 1.9174, 1.9384, 2.0045, 1.9150, 2.2766, 2.0638, 2.2254, 2.0308,\n",
      "        2.2499, 2.1712, 2.2782, 2.1155, 2.1624, 2.1638, 2.1577, 2.3273, 2.1918,\n",
      "        2.2545, 2.1759, 2.1963, 2.1347, 2.0653, 2.0541, 2.0570, 2.1073, 2.0817,\n",
      "        2.0449, 2.1140, 2.0666, 2.0831, 2.0142, 2.1385, 2.1106, 2.1883, 2.1007,\n",
      "        2.0872, 2.0205, 2.1767, 2.0134, 2.1241, 2.2175, 2.1041, 2.0752, 2.1008,\n",
      "        2.0314, 2.1187, 2.1195, 2.1005, 2.1170, 2.0785, 2.0752, 2.0460, 2.1669,\n",
      "        2.1353, 2.1647, 2.1658, 2.0014, 2.1286, 2.0516, 1.9847, 2.2515, 2.0725,\n",
      "        2.0402, 1.9245, 2.1495, 1.9749, 2.2374, 2.0468, 2.2730, 2.0603, 1.9387,\n",
      "        2.0699, 2.1515, 2.0536, 2.0301, 2.0969, 2.1480, 2.1383, 2.0986, 2.1326,\n",
      "        2.1154, 2.0511, 2.1418, 2.0887, 2.1814, 2.1556, 2.1672, 2.1582, 2.1819,\n",
      "        2.1460, 2.0467, 2.1432, 2.2585, 2.2070, 2.1642, 2.1787, 2.0933, 2.1587,\n",
      "        2.0944, 2.1431, 2.2903, 2.3285, 2.3718, 2.2128, 2.4165, 2.3178, 2.2839,\n",
      "        2.3518, 2.2228, 2.3738, 2.1087, 2.4056, 2.2179, 2.3715, 2.2003, 2.4125,\n",
      "        1.9146, 2.0916, 2.1712, 2.2652, 1.9171, 2.0636, 2.1150, 2.0638, 1.9520,\n",
      "        2.1379, 2.0530, 2.0866, 2.1546, 2.1500, 1.9942, 2.1712, 1.9708, 2.0548,\n",
      "        2.1380, 2.1717, 2.0747, 2.0236, 2.0393, 2.0816, 2.0894, 2.0867, 2.0580,\n",
      "        2.0386, 2.1921, 2.0320, 2.0395, 2.1253, 2.0965, 2.1400, 2.1609, 2.1872,\n",
      "        2.2712, 2.1265, 2.1808, 2.2425, 2.1028, 2.3416, 2.1107, 2.1525, 2.3117,\n",
      "        2.1140, 2.0935, 2.1631, 1.9732, 1.9822, 2.0123, 2.0756, 2.0586, 2.1165,\n",
      "        2.0382, 1.9871, 2.0904, 1.9565, 2.1668, 2.0140, 2.0896, 2.0407, 2.0649,\n",
      "        1.9673, 2.1920, 2.0060, 2.1001, 2.0976, 2.1416, 2.1732, 2.0442, 2.1553,\n",
      "        2.1796, 2.3227, 2.2859, 2.2147, 2.1200, 2.1530, 2.2035, 2.2011, 1.9450,\n",
      "        1.9687, 2.0582, 1.9553, 1.9166, 2.0437, 2.0069, 1.8648, 2.0188, 1.9949,\n",
      "        2.0728, 1.9337, 1.9205, 2.0275, 2.0149, 1.9903, 2.1992, 2.1705, 2.1031,\n",
      "        2.3225, 2.1443, 2.3063, 2.0864, 1.7365, 2.1415, 2.3493, 2.0829, 2.1643,\n",
      "        2.1815, 2.1439, 2.2507, 2.0906, 2.1452, 2.1071, 2.1356, 2.0505, 2.0215,\n",
      "        2.1501, 2.0770, 2.0448, 2.0266, 2.1436, 1.9286, 2.0773, 2.1543, 2.0217,\n",
      "        2.0664, 2.0946, 1.9948, 2.0498, 2.1070, 2.0138, 2.0716, 2.1482, 2.2025,\n",
      "        2.1179, 2.1650, 2.0878, 2.0716, 2.0906, 2.0693, 1.9299, 1.9989, 2.1120,\n",
      "        2.0810, 2.0685, 2.1059, 2.1710, 2.0967, 2.1544, 2.1586, 2.0304, 2.0635,\n",
      "        2.0298, 2.1248, 2.0035, 2.0301, 2.1508, 1.9285, 2.0460, 1.9622, 1.7745,\n",
      "        1.8431, 1.7964, 1.7838, 1.9629, 1.7669, 1.8178, 1.7955, 1.8170, 1.8814,\n",
      "        1.8242, 1.8466, 1.8700, 1.7794, 1.8211], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 8.8324e-05,  2.0355e-01,  5.2049e-02,  ..., -4.1894e-02,\n",
      "         -7.8651e-02, -3.7493e-03],\n",
      "        [-4.9087e-02,  2.3672e-02,  5.3354e-02,  ..., -6.8793e-02,\n",
      "         -6.4622e-02, -9.6315e-03],\n",
      "        [ 5.7368e-02,  2.3672e-02, -1.5432e-02,  ..., -1.9427e-02,\n",
      "          6.9371e-02, -1.9224e-02],\n",
      "        ...,\n",
      "        [ 1.0113e-01, -3.3027e-02, -7.1275e-02,  ...,  4.1312e-02,\n",
      "         -1.3080e-02,  3.2657e-02],\n",
      "        [-6.8477e-03,  7.8059e-02,  8.9468e-03,  ...,  1.3031e-02,\n",
      "          2.7940e-02, -8.5010e-02],\n",
      "        [-2.0587e-02, -1.0060e-02, -8.8134e-02,  ...,  5.5471e-02,\n",
      "          2.6415e-02, -2.2054e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0090, -0.0695,  0.0390,  ..., -0.0564, -0.0452,  0.0496],\n",
      "        [ 0.0317,  0.1405, -0.1798,  ..., -0.0167, -0.0521, -0.0025],\n",
      "        [ 0.0198,  0.1143, -0.0069,  ..., -0.0293, -0.1098,  0.0112],\n",
      "        ...,\n",
      "        [-0.0177,  0.0038,  0.0465,  ...,  0.0580,  0.0385, -0.0033],\n",
      "        [-0.0204, -0.0618, -0.0123,  ..., -0.0552, -0.0003, -0.0503],\n",
      "        [-0.0456,  0.0811, -0.1216,  ...,  0.0442, -0.0663, -0.0061]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0055, 1.9557, 1.9704, 2.1324, 2.1731, 1.9505, 2.0974, 1.9793, 2.0461,\n",
      "        1.9577, 1.8013, 1.9536, 1.9740, 1.8011, 2.1541, 2.0805, 1.9135, 1.9814,\n",
      "        1.8274, 1.8958, 2.1822, 1.9660, 1.9524, 2.0149, 1.8457, 1.9096, 1.8501,\n",
      "        2.0354, 2.1854, 2.1081, 1.9647, 1.6484, 2.1421, 1.8790, 1.9481, 1.8430,\n",
      "        1.8972, 1.8930, 1.8771, 2.1814, 1.9473, 2.0182, 2.0468, 2.0100, 2.0133,\n",
      "        2.0035, 2.9223, 1.9116, 1.9517, 1.9880, 2.0135, 1.9278, 1.9356, 1.8451,\n",
      "        1.9900, 1.9375, 2.1638, 2.1308, 2.3529, 2.0649, 1.8935, 1.9455, 4.8716,\n",
      "        1.8821, 2.0843, 2.1245, 2.1351, 2.0274, 1.8840, 2.0495, 1.9643, 1.9253,\n",
      "        1.8507, 2.2194, 1.9492, 2.1264, 1.9499, 1.9875, 1.9122, 1.7431, 1.8252,\n",
      "        1.9525, 1.8802, 2.1915, 1.8790, 1.8753, 1.7320, 2.0096, 1.7238, 2.0467,\n",
      "        2.1768, 1.9937, 2.2495, 1.9270, 1.9573, 2.0583, 1.9605, 2.1692, 2.1222,\n",
      "        2.1558, 2.0593, 2.1935, 1.9482, 2.0171, 1.9617, 2.0094, 2.0740, 2.0508,\n",
      "        1.8458, 1.7966, 2.2040, 2.1126, 2.0663, 1.9918, 2.1703, 1.9680, 2.0676,\n",
      "        1.9163, 1.9293, 2.2184, 2.0688, 1.9553, 2.1763, 2.0546, 1.8338, 2.1322,\n",
      "        1.9444, 1.9715, 1.9406, 1.9044, 1.8161, 2.1147, 2.1509, 2.0006, 2.1967,\n",
      "        1.9711, 1.8554, 2.0119, 1.8858, 1.9758, 1.9901, 1.8955, 2.3196, 2.1101,\n",
      "        1.9873, 1.9814, 1.9739, 2.1033, 1.9955, 1.8833, 2.1361, 2.0371, 1.9852,\n",
      "        1.9350, 2.1060, 1.9405, 1.8593, 1.9374, 2.1718, 1.9165, 1.9951, 2.0135,\n",
      "        2.1132, 1.9389, 2.0485, 2.1346, 2.1194, 2.0623, 2.0735, 1.9688, 1.9620,\n",
      "        1.7365, 2.0689, 2.0171, 1.8450, 1.8389, 1.8285, 2.0580, 2.3147, 1.9832,\n",
      "        1.8122, 1.9919, 2.0453, 2.0067, 1.9990, 1.9914, 1.9614, 2.0083, 1.9909,\n",
      "        1.9493, 2.0185, 1.8169, 2.7180, 2.0940, 1.9456, 2.0006, 1.9487, 1.8783,\n",
      "        2.0361, 1.9904, 1.9744, 2.0577, 1.9463, 2.1006, 1.9936, 2.0922, 2.0284,\n",
      "        1.7363, 1.9648, 1.9240, 1.8725, 2.1117, 2.0740, 2.1431, 2.2522, 2.0319,\n",
      "        1.8962, 2.0656, 1.6674, 1.8803, 1.8633, 2.0120, 1.9570, 2.1855, 2.1416,\n",
      "        2.0462, 2.0886, 2.0041, 1.9346, 2.0826, 1.8729, 1.7602, 2.0428, 1.8642,\n",
      "        2.0166, 2.0407, 1.8838, 1.8824, 2.0865, 2.0063, 1.8915, 1.8490, 2.0172,\n",
      "        2.0545, 2.0240, 1.8180, 2.0169, 2.0037, 1.9446, 1.9863, 1.9482, 1.9289,\n",
      "        2.0845, 1.7917, 1.9263, 1.9173, 1.9728, 1.7890, 1.7846, 2.0817, 2.1906,\n",
      "        2.1338, 2.0852, 1.9841, 1.9849, 2.0283, 2.0234, 2.5428, 2.0396, 1.9104,\n",
      "        1.9049, 2.1833, 1.8690, 2.0071, 1.9823, 1.9348, 2.1952, 1.8549, 1.9779,\n",
      "        2.0817, 1.9344, 1.9139, 2.0625, 2.0004, 2.0117, 2.0327, 2.0320, 1.8527,\n",
      "        1.9051, 1.8836, 1.8895, 2.1753, 1.9715, 2.1408, 1.7530, 2.0435, 2.1139,\n",
      "        1.9584, 1.8592, 1.9205, 2.0673, 2.0071, 2.1712, 1.9832, 2.1144, 1.9808,\n",
      "        2.0780, 1.7150, 1.8597, 2.1245, 2.0491, 1.8093, 1.7969, 2.1929, 2.1235,\n",
      "        1.9569, 2.0337, 1.9640, 2.1792, 1.9460], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0073,  0.0102,  0.0462,  ...,  0.0315,  0.0441,  0.0013],\n",
      "        [ 0.0333,  0.0555,  0.0503,  ..., -0.1330,  0.1519, -0.0530],\n",
      "        [ 0.1017, -0.0370,  0.1119,  ..., -0.0176, -0.0636,  0.0295],\n",
      "        ...,\n",
      "        [ 0.0825, -0.0387, -0.0156,  ..., -0.0756,  0.1528, -0.0153],\n",
      "        [-0.1052,  0.0206, -0.0937,  ...,  0.0618, -0.0624, -0.0180],\n",
      "        [ 0.0490,  0.0565, -0.0574,  ..., -0.0635, -0.0364,  0.0447]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0461,  0.0262,  0.0022,  ..., -0.0061,  0.0090, -0.0286],\n",
      "        [-0.0102,  0.0213,  0.0306,  ..., -0.0180, -0.0194, -0.0525],\n",
      "        [-0.1038,  0.0173, -0.0120,  ..., -0.0310,  0.0195,  0.0772],\n",
      "        ...,\n",
      "        [-0.0309, -0.0474, -0.0235,  ..., -0.0311,  0.0514, -0.0992],\n",
      "        [-0.0016,  0.0119,  0.0048,  ...,  0.0858, -0.0234,  0.0073],\n",
      "        [-0.0076,  0.0255, -0.0022,  ...,  0.0099, -0.0004,  0.0467]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5368, 1.3225, 1.3641, 1.3156, 1.5666, 1.7158, 1.6279, 1.6214, 1.5950,\n",
      "        1.4265, 1.1316, 1.3534, 1.4139, 1.3207, 1.6320, 1.5699, 1.5069, 1.3517,\n",
      "        1.0071, 1.3447, 1.6910, 1.4730, 1.8885, 1.9736, 1.0800, 1.0347, 1.3305,\n",
      "        1.1702, 1.1228, 1.9557, 2.0854, 1.8484, 1.0515, 1.1953, 1.1974, 1.6993,\n",
      "        2.1686, 1.5645, 1.8078, 2.5822, 2.0143, 1.5149, 1.6445, 1.4798, 1.4966,\n",
      "        2.1797, 1.9514, 1.9484, 1.6748, 1.7555, 1.7091, 1.5560, 1.4525, 1.5003,\n",
      "        1.8639, 1.9671, 1.7003, 1.7868, 1.5899, 1.7639, 1.7015, 1.5566, 1.8937,\n",
      "        1.9436, 1.1612, 1.6655, 1.4198, 1.9186, 2.0157, 2.1018, 2.3118, 2.0316,\n",
      "        1.2119, 1.2488, 1.5115, 2.0779, 2.0502, 1.6387, 2.2858, 2.1165, 1.7823,\n",
      "        1.3058, 1.6804, 1.0797, 1.8355, 1.7230, 1.3282, 1.4588, 1.6455, 1.7354,\n",
      "        1.3877, 1.8082, 1.1443, 1.1742, 1.3140, 1.3171, 1.1484, 0.9841, 1.0700,\n",
      "        1.1459, 1.4953, 1.4697, 1.4394, 1.5985, 1.1349, 0.9326, 1.2745, 1.3835,\n",
      "        1.2613, 1.5509, 1.4289, 1.4626, 1.5476, 1.6488, 1.9058, 1.7081, 1.8283,\n",
      "        1.6734, 2.2480, 2.3427, 1.5089, 1.6913, 1.7863, 1.8000, 1.5815, 1.9787,\n",
      "        2.2678, 2.3637, 2.0180, 1.7015, 1.6983, 1.5177, 1.6967, 2.2796, 1.9591,\n",
      "        1.9252, 1.6387, 1.7445, 1.6532, 1.3730, 1.7039, 1.3887, 1.8727, 1.9169,\n",
      "        1.9107, 1.2786, 1.6920, 1.2270, 1.2931, 1.4013, 1.6026, 2.1462, 1.2905,\n",
      "        1.2760, 1.1794, 1.8154, 1.9418, 2.0207, 1.7277, 1.7967, 1.4730, 1.4159,\n",
      "        1.5970, 1.7577, 1.5353, 2.2971, 1.8043, 1.8251, 1.6383, 1.4362, 1.6954,\n",
      "        1.7452, 1.8870, 1.4961, 1.6804, 1.9680, 0.7657, 1.3476, 1.5195, 1.4450,\n",
      "        2.0980, 2.0155, 2.4045, 2.0078, 1.3796, 1.1228, 1.2158, 1.6612, 1.9276,\n",
      "        2.0324, 2.3394, 1.9491, 1.6883, 1.2101, 1.6318, 1.8729, 1.8518, 1.8774,\n",
      "        1.6447, 1.9473, 1.2597, 1.4344, 1.0772, 1.2358, 1.2471, 1.3985, 1.6674,\n",
      "        1.6312, 1.0052, 1.3318, 1.2853, 1.2822, 1.2366, 1.2028, 1.7407, 1.7831,\n",
      "        1.2223, 1.1921, 1.2935, 1.3818, 1.1879, 1.7481, 1.9144, 1.8125, 1.4456,\n",
      "        1.3235, 1.7326, 1.7165, 1.2700, 1.3193, 1.6807, 1.6340, 1.6753, 1.4890,\n",
      "        1.0858, 1.0995, 1.6785, 1.9240, 1.5711, 1.8783, 0.7041, 1.5450, 1.4543,\n",
      "        1.6946, 2.1882, 1.7497, 2.1172, 2.2867, 1.3589, 1.1821, 1.4575, 1.5449,\n",
      "        2.2424, 1.7564, 2.2913, 2.3966, 1.9503, 1.9917, 1.9599, 1.6376, 1.3974,\n",
      "        2.1447, 1.8159, 2.0288, 1.8508, 1.8440, 1.8045, 1.3187, 1.7615, 1.5278,\n",
      "        1.9940, 1.9049, 1.8235, 1.9191, 1.9034, 1.8488, 1.4945, 2.1589, 2.1034,\n",
      "        2.0661, 1.7055, 1.7328, 1.9938, 1.9696, 1.7322, 1.5763, 2.2130, 2.0086,\n",
      "        1.2941, 1.3094, 1.3938, 1.2779, 1.3335, 1.1334, 2.0082, 2.0317, 1.2359,\n",
      "        1.2616, 1.2682, 1.3733, 1.2583, 1.5971, 1.5906, 1.3886, 0.7830, 0.7899,\n",
      "        0.8347, 1.1984, 1.9244, 2.0819, 2.0385, 1.9636, 1.7728, 1.4158, 1.5102,\n",
      "        1.5398, 1.7841, 2.0747, 1.9819, 1.6824], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0215,  0.0150,  0.0091,  ...,  0.0519,  0.0153, -0.0416],\n",
      "        [-0.0860,  0.0242, -0.0667,  ...,  0.1001, -0.0442,  0.0452],\n",
      "        [-0.0295, -0.0235, -0.0832,  ..., -0.0746,  0.1778, -0.1176],\n",
      "        ...,\n",
      "        [ 0.0532, -0.1309,  0.0575,  ...,  0.0162,  0.0261, -0.0626],\n",
      "        [-0.0855, -0.1336,  0.0984,  ..., -0.1099,  0.0492, -0.0313],\n",
      "        [-0.0704, -0.1061,  0.0029,  ...,  0.0134,  0.0346, -0.0062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0783,  0.0690,  0.0403,  ...,  0.0509,  0.0027, -0.0378],\n",
      "        [-0.0576,  0.0044,  0.0273,  ..., -0.0060,  0.0540, -0.0076],\n",
      "        [-0.0898, -0.0435, -0.0597,  ...,  0.0474,  0.0171, -0.0014],\n",
      "        ...,\n",
      "        [-0.0914, -0.0074,  0.0715,  ..., -0.0711, -0.0640, -0.0445],\n",
      "        [ 0.0785, -0.0027,  0.0033,  ..., -0.0626,  0.0321, -0.0602],\n",
      "        [-0.0413, -0.0573,  0.0439,  ...,  0.0396,  0.0621, -0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4116, 1.2869, 1.4040, 1.4386, 1.1972, 1.1681, 1.5470, 1.5688, 1.4929,\n",
      "        1.4154, 1.3172, 1.3301, 1.4167, 1.8665, 1.5428, 1.5289, 1.1147, 1.5148,\n",
      "        1.0565, 1.2031, 1.3732, 1.3777, 1.5738, 1.4400, 1.2858, 1.0459, 1.5223,\n",
      "        1.2367, 1.2028, 1.3299, 1.7910, 1.6500, 1.5675, 1.5081, 1.5430, 1.3184,\n",
      "        1.2879, 2.0940, 1.7373, 1.8801, 1.4800, 1.4605, 1.4369, 1.7065, 2.0743,\n",
      "        1.3102, 1.8638, 1.8796, 1.6820, 1.7304, 1.7476, 1.5021, 1.5088, 1.4430,\n",
      "        1.7225, 1.8481, 1.6303, 1.8364, 1.7775, 1.7184, 1.4339, 2.0440, 1.7543,\n",
      "        1.9274, 1.1919, 1.5750, 1.5811, 1.7357, 1.5912, 1.3472, 1.8635, 1.7683,\n",
      "        1.2279, 1.2720, 1.6522, 1.7685, 1.4027, 1.6574, 1.6959, 1.8719, 1.8123,\n",
      "        1.4573, 1.6272, 1.5664, 0.9547, 1.0160, 1.3885, 1.4747, 1.5920, 1.6941,\n",
      "        1.4292, 1.2681, 1.5888, 1.3839, 1.3415, 1.3113, 1.0193, 1.2440, 1.2518,\n",
      "        1.3264, 1.0544, 1.0992, 1.3848, 1.4612, 1.2096, 1.1463, 1.3280, 1.0917,\n",
      "        1.1314, 1.2770, 1.3709, 1.4434, 1.4747, 1.5877, 1.9298, 1.7948, 1.7309,\n",
      "        1.7952, 1.9564, 1.8071, 1.5686, 1.7315, 1.7642, 1.8259, 1.7535, 1.6207,\n",
      "        1.9943, 1.8868, 1.9234, 1.7467, 1.8033, 1.6020, 1.2421, 1.1475, 1.8421,\n",
      "        1.8383, 1.7106, 1.8518, 1.7749, 1.5160, 1.4104, 2.3289, 1.7604, 1.7623,\n",
      "        1.8187, 1.4724, 1.3239, 1.8488, 1.8319, 1.9336, 1.5368, 1.7297, 1.4966,\n",
      "        1.4935, 1.5658, 1.1262, 1.0515, 1.2040, 1.6920, 1.6968, 1.5927, 1.5324,\n",
      "        1.4527, 1.3844, 1.6690, 1.1685, 1.6447, 1.7316, 1.5451, 1.6106, 1.4540,\n",
      "        1.4350, 1.1940, 1.9879, 1.7021, 1.7158, 1.5575, 1.0656, 1.3163, 1.4897,\n",
      "        1.6273, 1.8750, 1.9520, 1.8415, 0.8389, 1.5049, 1.3271, 1.3535, 1.7081,\n",
      "        1.8360, 1.9656, 1.8309, 1.7724, 1.4108, 1.3970, 1.1305, 1.0311, 1.1721,\n",
      "        1.6011, 1.7636, 1.3853, 1.5272, 1.5339, 1.5868, 1.9726, 1.8663, 1.6444,\n",
      "        1.5473, 0.8886, 0.9861, 1.0051, 1.3035, 1.1549, 1.2811, 1.4831, 1.4566,\n",
      "        1.0308, 1.1621, 1.3777, 1.2034, 1.1838, 1.2269, 1.7718, 1.3476, 1.5553,\n",
      "        1.4274, 1.6282, 1.0651, 1.8447, 1.9128, 1.6386, 1.5973, 1.7890, 1.5965,\n",
      "        1.3439, 1.7148, 1.0031, 1.0843, 1.5286, 1.6319, 1.5151, 1.5535, 1.5597,\n",
      "        1.6597, 1.6226, 1.7640, 1.9543, 1.9166, 1.1147, 0.9685, 1.1389, 1.2499,\n",
      "        1.8571, 1.7111, 2.0848, 1.9509, 1.9312, 1.9202, 1.9729, 1.7157, 1.9367,\n",
      "        1.2632, 1.6160, 1.7558, 1.7952, 1.8941, 1.8133, 1.5970, 1.4772, 2.1867,\n",
      "        1.8879, 1.7051, 1.7840, 1.9081, 1.9374, 1.9089, 1.6112, 1.4665, 1.9838,\n",
      "        1.8579, 1.6893, 1.6945, 1.8617, 1.8429, 1.5650, 2.0964, 2.0671, 1.9823,\n",
      "        0.9965, 0.9309, 1.1761, 1.1208, 1.3269, 1.2626, 1.6881, 1.4547, 0.8535,\n",
      "        1.0226, 1.2652, 1.3274, 1.1132, 1.1905, 1.4763, 1.4421, 1.1819, 1.5136,\n",
      "        1.6333, 1.6202, 1.6708, 1.8065, 1.8563, 1.8628, 1.4555, 1.1405, 1.0376,\n",
      "        1.5225, 1.7374, 1.8638, 1.8448, 1.7264], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1150,  0.1392,  0.0754,  ...,  0.1355, -0.0420,  0.1357],\n",
      "        [ 0.0094,  0.1272, -0.1467,  ..., -0.2259,  0.0004, -0.0703],\n",
      "        [ 0.0644, -0.0123, -0.0300,  ..., -0.0635,  0.0930,  0.0264],\n",
      "        ...,\n",
      "        [-0.0049, -0.0458, -0.2506,  ..., -0.0136, -0.0258, -0.0807],\n",
      "        [-0.1246, -0.0504,  0.1183,  ..., -0.0251, -0.0156,  0.0329],\n",
      "        [ 0.0340, -0.1423, -0.0712,  ..., -0.0710,  0.0840, -0.1588]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0761, -0.0069, -0.0053,  ..., -0.0090, -0.1210, -0.0851],\n",
      "        [ 0.0666,  0.0496,  0.0566,  ...,  0.0779, -0.0479,  0.0591],\n",
      "        [ 0.0380, -0.0183,  0.0653,  ..., -0.0602, -0.0586,  0.0154],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0399, -0.0262,  ..., -0.0599,  0.1558, -0.1293],\n",
      "        [ 0.0307,  0.1448,  0.0184,  ...,  0.0054,  0.0137,  0.0574],\n",
      "        [-0.1246,  0.0679,  0.0135,  ...,  0.0936,  0.0072, -0.0776]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0442, 2.0853, 2.0471, 2.0901, 2.3486, 2.2211, 2.0851, 1.9905, 2.1028,\n",
      "        2.1358, 2.1735, 2.0286, 2.1258, 2.1664, 2.0550, 2.1849, 2.2911, 2.2386,\n",
      "        2.2585, 2.3480, 2.2424, 2.1293, 2.3298, 2.4110, 2.2936, 2.2989, 2.2159,\n",
      "        2.1354, 2.2864, 2.1896, 2.2933, 2.2576, 2.0436, 1.9744, 2.1105, 2.0963,\n",
      "        1.9682, 2.0158, 2.0849, 2.0885, 2.0617, 2.0828, 1.9925, 2.0324, 2.0425,\n",
      "        2.0312, 2.0150, 1.9847, 2.1889, 2.2438, 2.2250, 2.3072, 1.8944, 2.1396,\n",
      "        2.2150, 2.0761, 2.0216, 2.1733, 2.2568, 2.2428, 2.2746, 2.0169, 2.3392,\n",
      "        2.2099, 2.2647, 2.3458, 2.1150, 2.1306, 2.4575, 2.2004, 2.2174, 2.1677,\n",
      "        2.2060, 2.2350, 2.2470, 2.1667, 2.2380, 2.1511, 2.1586, 2.2338, 2.1038,\n",
      "        2.1432, 2.2130, 2.1341, 2.2741, 2.1409, 2.1226, 2.1318, 2.2851, 2.0852,\n",
      "        2.1748, 2.2007, 2.1417, 2.1999, 2.0864, 2.0789, 2.3388, 2.0160, 2.2409,\n",
      "        2.1115, 2.2180, 2.1312, 2.0715, 2.1348, 2.1959, 2.1153, 2.2379, 2.2033,\n",
      "        2.3930, 2.2008, 2.1358, 2.1759, 2.1748, 2.2629, 2.0810, 2.1339, 2.0520,\n",
      "        2.1913, 2.2353, 2.2570, 2.5342, 2.1002, 2.1875, 2.2594, 2.0362, 2.1932,\n",
      "        2.2333, 2.2806, 2.2092, 2.0789, 2.2171, 2.2364, 2.2626, 2.2793, 2.0792,\n",
      "        2.2267, 2.1276, 2.2702, 2.1248, 2.3065, 2.2737, 2.2561, 2.1163, 2.1424,\n",
      "        2.0718, 2.1055, 1.8319, 1.9468, 2.0511, 2.1004, 2.1169, 1.9696, 2.2386,\n",
      "        2.1631, 2.0404, 2.0989, 2.0110, 2.2007, 2.0452, 2.0343, 2.1601, 2.1030,\n",
      "        2.0542, 2.0920, 2.1077, 2.1149, 2.1249, 2.1001, 2.0537, 2.0069, 2.1051,\n",
      "        2.1377, 2.0574, 1.9637, 2.0481, 2.1575, 1.9833, 1.8287, 2.0211, 1.9624,\n",
      "        1.8355, 1.9022, 1.9274, 1.9562, 2.0097, 2.0032, 1.9591, 1.8686, 1.9321,\n",
      "        1.9571, 1.9369, 1.8656, 2.0766, 1.8880, 2.0142, 2.1909, 2.1615, 2.0364,\n",
      "        2.0369, 1.9778, 2.1755, 2.2215, 2.0100, 2.1371, 2.1592, 2.0551, 1.8469,\n",
      "        2.1685, 2.5375, 2.5440, 2.5067, 2.5049, 2.5299, 2.9297, 2.5395, 2.2732,\n",
      "        2.1209, 2.3802, 2.2883, 2.2952, 2.2291, 2.4940, 2.3730, 2.3485, 2.1469,\n",
      "        2.1571, 2.2396, 2.1710, 2.1478, 1.9849, 2.1935, 2.1474, 2.2911, 2.0965,\n",
      "        2.1456, 2.1554, 2.0864, 2.1692, 2.1087, 1.9452, 2.0201, 2.0361, 1.9028,\n",
      "        1.9347, 1.9515, 1.9447, 1.9039, 1.9894, 1.9496, 1.9135, 1.9692, 1.9135,\n",
      "        1.8777, 1.8615, 1.9766, 1.9654, 2.2727, 2.2128, 1.9967, 2.2745, 2.1988,\n",
      "        2.1535, 2.0757, 2.2224, 2.2121, 2.2413, 2.1406, 2.1887, 2.2318, 2.1231,\n",
      "        2.1738, 2.0930, 2.2212, 2.1576, 2.1767, 2.2876, 2.1582, 2.2211, 2.2092,\n",
      "        2.1175, 2.2415, 2.2014, 2.1048, 2.0464, 2.0210, 2.3349, 2.1261, 2.2483,\n",
      "        3.5255, 2.7152, 2.6703, 3.1695, 2.3523, 2.8169, 2.5482, 2.6783, 3.5530,\n",
      "        2.5467, 2.7356, 2.6696, 3.0308, 2.7923, 2.7934, 2.7396, 2.1105, 1.9799,\n",
      "        1.8996, 1.8759, 1.9248, 1.9094, 1.9289, 2.1587, 1.9692, 1.9847, 1.9027,\n",
      "        1.9387, 2.1132, 1.9424, 1.9731, 1.9452], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0915, -0.0714,  0.0588,  ..., -0.0032, -0.0169, -0.0294],\n",
      "        [ 0.0116,  0.0402,  0.0047,  ..., -0.0320,  0.0208,  0.0288],\n",
      "        [-0.0471,  0.0851, -0.0107,  ..., -0.0535,  0.0484, -0.0171],\n",
      "        ...,\n",
      "        [ 0.0645, -0.1493,  0.0411,  ...,  0.0696,  0.0132, -0.0090],\n",
      "        [-0.0267,  0.0851,  0.0993,  ..., -0.0058,  0.0118, -0.0687],\n",
      "        [-0.0289,  0.1402,  0.0259,  ..., -0.0161, -0.0446,  0.0596]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0415,  0.0729,  0.1431,  ..., -0.0753, -0.1106, -0.0613],\n",
      "        [-0.0254, -0.0852,  0.1126,  ...,  0.0349, -0.0273, -0.0711],\n",
      "        [ 0.0217,  0.0666, -0.1168,  ..., -0.0975,  0.0970,  0.0671],\n",
      "        ...,\n",
      "        [-0.0635,  0.0283, -0.0731,  ...,  0.0655, -0.1097,  0.0682],\n",
      "        [ 0.0096,  0.0344,  0.0452,  ...,  0.1059,  0.1194,  0.0186],\n",
      "        [ 0.0120, -0.0659,  0.0237,  ...,  0.0144,  0.0119, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2001, 2.0649, 2.1826, 2.2333, 2.5525, 2.2693, 2.1388, 2.3175, 2.2945,\n",
      "        2.1789, 2.1131, 2.4515, 2.5238, 2.0640, 2.3606, 2.1918, 2.2525, 2.1239,\n",
      "        2.4282, 2.0793, 2.4574, 2.4551, 2.0395, 2.1859, 2.1356, 1.9936, 2.2356,\n",
      "        2.2176, 2.4658, 2.2351, 2.1030, 2.0881, 2.4467, 2.1637, 2.1030, 2.0851,\n",
      "        2.1193, 2.2560, 2.3777, 2.1805, 2.1419, 2.3535, 2.3202, 2.2070, 2.3381,\n",
      "        2.1412, 3.1431, 1.9852, 2.2996, 2.5638, 2.2664, 2.2294, 2.0731, 2.3491,\n",
      "        2.0709, 2.1836, 2.2054, 2.3917, 2.6282, 2.4052, 2.3250, 2.1080, 6.9229,\n",
      "        2.1975, 2.1994, 2.1904, 2.3762, 2.6559, 2.3245, 2.1622, 2.2235, 2.1452,\n",
      "        2.0118, 2.3210, 2.1238, 2.3784, 2.2433, 2.4320, 1.9149, 1.9942, 2.0483,\n",
      "        2.1285, 2.3058, 2.2698, 2.0258, 2.1690, 2.4940, 2.4309, 1.9704, 2.2228,\n",
      "        2.1342, 2.0366, 2.4102, 2.2417, 2.2248, 2.2922, 2.1154, 2.2886, 2.1748,\n",
      "        2.2911, 2.3384, 2.2690, 2.4402, 2.4001, 2.3141, 2.3590, 2.1077, 2.2956,\n",
      "        2.0209, 2.3123, 2.2632, 2.2647, 2.2199, 2.1725, 2.4502, 2.1415, 2.2513,\n",
      "        2.3008, 2.4727, 2.5089, 2.4013, 2.2929, 2.2879, 2.2217, 2.0046, 2.5290,\n",
      "        2.1169, 2.1518, 2.2462, 2.3462, 2.1483, 2.0092, 2.1823, 2.1726, 2.4301,\n",
      "        2.1273, 2.2263, 2.4388, 2.1580, 2.3052, 2.1156, 2.1031, 2.2378, 2.3579,\n",
      "        2.3933, 2.2964, 2.1586, 2.3353, 2.1749, 2.2731, 2.4370, 2.2586, 2.3892,\n",
      "        2.1547, 2.2835, 2.2933, 2.0832, 2.3654, 2.2919, 2.1714, 2.0760, 2.2150,\n",
      "        2.3346, 2.3733, 2.3468, 2.3351, 2.3019, 2.1078, 2.3303, 2.1595, 2.3057,\n",
      "        2.3066, 2.2320, 2.2088, 2.2607, 2.0505, 2.2779, 2.2448, 2.1731, 2.0139,\n",
      "        2.0261, 2.0946, 2.1817, 2.1949, 2.3202, 2.1889, 2.3096, 2.2494, 1.9641,\n",
      "        2.2257, 2.1471, 2.0857, 3.3503, 2.1548, 2.2182, 2.2905, 2.4367, 2.2555,\n",
      "        2.3901, 2.2950, 2.2943, 2.3602, 2.2790, 2.2419, 2.2514, 2.3281, 1.9495,\n",
      "        2.2518, 2.6105, 2.1610, 2.1901, 2.3634, 2.3917, 2.3293, 2.3204, 2.1613,\n",
      "        2.1776, 2.0311, 2.0213, 2.1334, 2.0871, 2.1079, 2.4267, 2.4329, 2.3723,\n",
      "        2.2466, 2.3325, 2.1380, 1.9406, 2.3966, 2.1306, 2.1025, 2.0609, 2.3101,\n",
      "        2.0160, 2.1705, 2.2525, 2.2484, 2.0993, 2.2830, 2.1215, 2.0874, 2.3337,\n",
      "        2.0839, 2.2686, 2.0305, 2.3525, 1.9844, 2.0573, 2.3921, 2.2612, 2.1623,\n",
      "        2.0428, 1.9955, 2.2409, 2.2393, 2.3304, 2.0546, 2.3170, 2.3081, 2.1124,\n",
      "        2.1868, 2.2249, 2.1425, 2.1851, 2.1879, 2.3203, 2.8144, 2.3924, 2.1185,\n",
      "        1.9596, 2.1890, 2.0378, 2.4455, 2.4396, 2.3690, 2.1453, 2.0890, 2.2988,\n",
      "        2.1969, 2.2344, 2.1081, 2.2689, 2.5746, 2.1945, 2.2806, 2.2878, 2.3380,\n",
      "        2.1081, 2.1647, 2.1794, 2.3166, 2.2429, 2.3029, 2.0794, 2.0539, 2.1944,\n",
      "        2.0975, 2.0556, 2.2989, 2.2117, 2.1806, 2.1987, 2.1547, 2.3704, 2.2298,\n",
      "        2.3404, 2.0101, 2.0088, 2.0561, 2.3239, 1.9865, 1.9307, 2.3306, 2.3229,\n",
      "        2.1678, 2.3807, 2.1174, 2.2179, 2.0103], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1699, -0.8279,  0.0260,  ...,  0.0469, -0.0843,  0.0254],\n",
      "        [-0.3562,  0.0308,  0.0865,  ...,  0.0591,  0.2071, -0.2667],\n",
      "        [ 0.0750,  0.1241,  0.0656,  ..., -0.0567,  0.0863,  0.0597],\n",
      "        ...,\n",
      "        [-0.0034, -0.1483, -0.1708,  ..., -0.0852, -0.1563, -0.0351],\n",
      "        [ 0.0784, -0.0335,  0.0649,  ...,  0.0123, -0.1528, -0.1552],\n",
      "        [ 0.0145,  0.0106, -0.0876,  ...,  0.0228,  0.0360, -0.0452]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0273, -0.0062, -0.0049,  ...,  0.0532, -0.0117,  0.0456],\n",
      "        [ 0.1398,  0.0231, -0.0385,  ...,  0.0212,  0.0046,  0.0588],\n",
      "        [ 0.1207,  0.0045,  0.0326,  ...,  0.0863,  0.0458, -0.0689],\n",
      "        ...,\n",
      "        [ 0.0235, -0.0163,  0.0695,  ...,  0.0183,  0.0525, -0.0219],\n",
      "        [-0.0018,  0.0108,  0.0602,  ..., -0.0264,  0.1311,  0.0807],\n",
      "        [ 0.0329, -0.0148, -0.0398,  ..., -0.1342,  0.0230,  0.0035]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.5853, 0.9414, 0.8864, 1.0005, 0.8341, 1.6792, 0.8581, 1.3855, 0.7455,\n",
      "        0.6171, 0.8099, 1.1151, 1.2725, 0.9141, 1.2154, 1.8849, 2.3473, 1.6398,\n",
      "        1.3562, 0.8318, 2.1569, 0.5517, 1.0800, 1.0548, 0.8301, 0.7455, 1.4008,\n",
      "        1.6584, 1.0272, 2.0568, 1.1916, 2.2181, 1.3737, 1.6398, 1.6193, 1.6319,\n",
      "        1.5264, 0.9735, 1.3574, 1.1211, 1.3794, 0.7246, 0.9086, 0.7327, 0.9418,\n",
      "        1.7050, 1.5984, 1.0973, 0.5182, 0.8853, 0.7954, 0.7849, 1.0845, 1.2646,\n",
      "        0.8790, 1.1035, 0.6633, 0.8367, 1.0042, 1.2310, 1.3798, 0.4869, 1.3834,\n",
      "        1.1706, 1.1599, 0.8857, 0.9255, 1.6514, 1.8569, 0.6478, 1.6139, 1.1124,\n",
      "        1.7787, 1.4179, 1.4084, 0.7719, 0.4653, 1.7452, 1.6296, 1.4729, 1.1574,\n",
      "        1.2324, 0.9352, 1.2625, 0.6487, 1.1540, 1.5481, 1.6548, 1.6443, 1.1342,\n",
      "        1.5609, 1.3580, 1.6658, 1.8118, 1.0891, 1.5124, 0.5507, 0.5390, 0.6970,\n",
      "        0.8906, 0.6116, 1.2291, 2.0492, 1.5262, 0.6264, 0.5575, 0.5198, 1.3949,\n",
      "        1.0530, 1.1037, 1.2533, 1.6176, 0.4951, 0.4427, 0.6902, 0.5102, 0.6893,\n",
      "        1.0745, 1.0676, 0.9169, 0.4903, 0.5838, 0.6322, 0.6646, 1.2191, 0.5728,\n",
      "        1.4091, 1.2412, 0.8634, 0.6194, 1.5913, 1.5369, 1.0518, 0.8959, 1.0386,\n",
      "        0.7692, 0.3437, 0.8577, 1.3768, 0.9904, 1.1698, 1.2362, 1.0852, 0.7233,\n",
      "        1.8126, 1.2617, 1.5355, 1.6066, 2.2761, 0.4591, 1.5947, 1.8179, 1.6002,\n",
      "        1.2395, 1.2208, 0.5141, 0.4554, 2.5645, 1.4504, 0.9397, 0.7354, 0.8315,\n",
      "        1.0938, 1.1105, 1.3614, 1.5309, 1.5623, 0.7575, 1.0908, 1.0442, 0.8914,\n",
      "        1.3379, 1.8194, 1.3875, 0.7927, 0.7337, 1.2977, 0.9300, 1.2044, 1.9222,\n",
      "        1.4305, 1.3003, 0.9287, 1.6573, 0.7350, 0.8158, 1.0075, 1.9005, 1.2888,\n",
      "        1.2273, 1.4752, 1.2818, 0.5897, 1.1480, 0.8825, 1.1704, 1.2369, 0.6458,\n",
      "        1.0872, 1.6619, 0.4945, 0.9241, 1.2399, 0.9323, 0.7577, 1.0398, 1.7767,\n",
      "        0.9969, 0.4415, 0.6359, 1.4633, 0.9700, 0.8584, 0.7347, 0.6810, 0.8217,\n",
      "        0.5012, 0.5886, 0.3603, 1.1078, 1.1681, 1.3178, 0.7116, 0.6306, 3.4055,\n",
      "        2.2885, 2.3034, 2.3188, 1.4570, 2.1923, 1.5341, 1.7381, 3.1816, 2.2742,\n",
      "        2.9759, 1.6584, 2.8653, 1.8699, 2.1077, 2.2289, 1.1329, 0.8952, 0.5713,\n",
      "        1.4181, 1.8599, 0.8214, 1.1055, 1.2004, 0.6950, 1.1168, 1.4317, 1.1842,\n",
      "        0.5058, 1.8753, 1.0431, 0.8566, 0.5702, 1.1380, 1.4764, 1.2667, 1.6866,\n",
      "        0.5480, 1.7491, 0.8235, 1.0170, 0.9379, 0.5901, 1.5741, 0.7238, 1.8894,\n",
      "        1.9161, 0.8679, 1.3350, 1.5763, 1.6763, 1.3810, 1.2394, 0.7780, 1.7551,\n",
      "        1.2473, 0.9061, 1.0333, 2.0561, 1.9544, 2.2742, 2.4918, 1.1108, 1.1175,\n",
      "        1.6286, 0.7643, 1.6727, 1.8822, 1.2415, 0.7075, 1.6238, 1.0862, 0.8523,\n",
      "        1.2977, 0.5647, 0.9873, 1.1112, 1.9990, 1.8551, 1.1154, 0.5343, 0.5222,\n",
      "        1.3942, 0.4759, 0.4580, 2.1503, 1.5154, 1.1260, 1.4921, 1.9562, 1.1058,\n",
      "        1.4934, 2.3807, 0.5520, 0.8688, 1.0518], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0983,  0.5507,  0.0746,  ..., -0.0111,  0.1217,  0.0652],\n",
      "        [ 0.0198,  0.0509,  0.0957,  ..., -0.0973, -0.3128, -0.1401],\n",
      "        [ 0.1018, -0.4264,  0.3920,  ...,  0.0732,  0.0144, -0.0555],\n",
      "        ...,\n",
      "        [-0.0891, -0.0386, -0.0503,  ..., -0.0899,  0.2477,  0.2061],\n",
      "        [ 0.0940,  0.0337, -0.0276,  ..., -0.1335, -0.0683,  0.0346],\n",
      "        [ 0.0818,  0.2765, -0.0031,  ...,  0.0455, -0.0677, -0.2436]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0168, -0.0270, -0.0437,  ...,  0.1121, -0.0504, -0.0876],\n",
      "        [-0.0428,  0.0307,  0.2387,  ...,  0.0859, -0.0233,  0.0573],\n",
      "        [-0.0325,  0.0235, -0.0456,  ..., -0.0668, -0.0114,  0.2030],\n",
      "        ...,\n",
      "        [ 0.2680, -0.1350,  0.0185,  ...,  0.1096,  0.0893, -0.1141],\n",
      "        [ 0.0432, -0.0036, -0.0414,  ...,  0.0023,  0.0168, -0.0585],\n",
      "        [-0.0194,  0.0331, -0.0021,  ...,  0.0606, -0.0588,  0.1131]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([0.8392, 1.5019, 1.3078, 1.5079, 1.3025, 1.5585, 0.8220, 1.3313, 1.1638,\n",
      "        1.3262, 1.5978, 1.8851, 1.3966, 1.2226, 1.2462, 1.8329, 2.2638, 1.5260,\n",
      "        1.4429, 1.9183, 1.6768, 1.8161, 1.1113, 1.0717, 1.4786, 1.1437, 2.3015,\n",
      "        2.0781, 2.0550, 1.5666, 1.1595, 2.2420, 2.3097, 2.3510, 2.3838, 1.6049,\n",
      "        1.5131, 1.2421, 1.7080, 1.1408, 1.3006, 1.9111, 1.5604, 1.8477, 1.3583,\n",
      "        1.7531, 1.8328, 1.1693, 0.9461, 0.7927, 1.3253, 1.8668, 1.6322, 1.5397,\n",
      "        0.9627, 1.2589, 1.0646, 1.5216, 1.7032, 1.2262, 2.1231, 1.0534, 1.3696,\n",
      "        1.7091, 1.7756, 0.9021, 1.7070, 1.9496, 1.9010, 1.7491, 1.6047, 1.0320,\n",
      "        1.2980, 2.1570, 2.0436, 1.7722, 1.3395, 1.3266, 1.7732, 1.8616, 2.0157,\n",
      "        2.3580, 1.5554, 1.7025, 1.5890, 1.2600, 2.0191, 1.9583, 2.3127, 1.5514,\n",
      "        2.3803, 1.8509, 1.2195, 1.7522, 0.8964, 2.0830, 0.8266, 1.1107, 1.2753,\n",
      "        1.0481, 1.2297, 1.6393, 1.9367, 1.8472, 0.9549, 0.9827, 1.1031, 1.5806,\n",
      "        1.4853, 1.6312, 1.5712, 1.6653, 0.5601, 0.8297, 0.9081, 0.8465, 1.2861,\n",
      "        1.8130, 1.1477, 0.7905, 0.6225, 0.7807, 1.1590, 1.6046, 1.8592, 1.0478,\n",
      "        1.3277, 1.8135, 1.4369, 0.8611, 1.3014, 1.3713, 0.6711, 0.6987, 1.0542,\n",
      "        0.9251, 0.4482, 1.1542, 1.2361, 0.8154, 0.7378, 0.5549, 1.1003, 0.6287,\n",
      "        2.1110, 1.7296, 1.9410, 1.9848, 1.4768, 2.6344, 1.5212, 1.8838, 2.4259,\n",
      "        2.2121, 2.2918, 1.9224, 2.1539, 2.0339, 1.5981, 0.9361, 1.6011, 1.8197,\n",
      "        1.8355, 1.7057, 1.9585, 1.5404, 2.1567, 0.7077, 1.4238, 1.4275, 1.6206,\n",
      "        2.2843, 1.7853, 1.7433, 1.0910, 0.7828, 1.2124, 1.5569, 2.1363, 2.1490,\n",
      "        1.7376, 1.5103, 1.2466, 1.8997, 1.8123, 1.5968, 1.4765, 2.0046, 1.7172,\n",
      "        1.5096, 1.2301, 1.4968, 0.8001, 1.5426, 1.1875, 1.3194, 1.7850, 1.0918,\n",
      "        1.1095, 2.1928, 0.9384, 0.8893, 1.4626, 1.7024, 1.4097, 1.5778, 2.0078,\n",
      "        1.3213, 0.9281, 0.8281, 1.7055, 1.1076, 1.0169, 0.6628, 0.8071, 0.8527,\n",
      "        0.8261, 0.9799, 0.4856, 1.2225, 1.2257, 0.7850, 0.6270, 0.8081, 2.9451,\n",
      "        2.7178, 2.8894, 1.7407, 2.3582, 1.3965, 1.4035, 1.6724, 2.2536, 1.7997,\n",
      "        1.8414, 2.2658, 1.4603, 2.2006, 2.0325, 1.8343, 2.1152, 1.6703, 1.5110,\n",
      "        1.6628, 1.5135, 1.3825, 1.3710, 1.3022, 0.9267, 1.8836, 2.3095, 1.9317,\n",
      "        1.7390, 1.9284, 0.9710, 0.9299, 1.3975, 0.8381, 1.8367, 1.6382, 1.6303,\n",
      "        1.2757, 1.4130, 0.7267, 0.9907, 1.6766, 1.1864, 1.5973, 1.5308, 1.6346,\n",
      "        2.1062, 0.8994, 1.7928, 1.8450, 1.1499, 1.5268, 1.7757, 2.7709, 1.5211,\n",
      "        1.3783, 0.9778, 1.4357, 2.6976, 2.2279, 2.0355, 1.5333, 1.2152, 1.0892,\n",
      "        2.2129, 2.0966, 1.8543, 2.1541, 1.4145, 1.1462, 1.7422, 1.1569, 2.0677,\n",
      "        1.8638, 2.2184, 1.8875, 1.6365, 1.9494, 1.9363, 1.2111, 2.1896, 1.7809,\n",
      "        2.1253, 1.5785, 2.5326, 1.8456, 1.9618, 1.5199, 1.7201, 2.9075, 1.7146,\n",
      "        1.6995, 1.5715, 2.1892, 0.9054, 0.9631], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1435, -0.0052,  0.2564,  ...,  0.0818, -0.1329, -0.0929],\n",
      "        [ 0.1267,  0.0044,  0.1405,  ...,  0.0048, -0.0224, -0.1540],\n",
      "        [ 0.0457,  0.0006,  0.0886,  ..., -0.0794, -0.0209, -0.0535],\n",
      "        ...,\n",
      "        [-0.0140,  0.0056,  0.1073,  ..., -0.0442, -0.0528, -0.0796],\n",
      "        [ 0.1323, -0.0084, -0.0060,  ...,  0.0288, -0.0053,  0.1176],\n",
      "        [ 0.0237, -0.0080,  0.1673,  ...,  0.1739, -0.1087,  0.0090]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0796,  0.0986,  0.1738,  ...,  0.0372,  0.0837, -0.0044],\n",
      "        [ 0.1285, -0.0206, -0.1360,  ..., -0.0217, -0.2396,  0.0991],\n",
      "        [ 0.0575,  0.0792, -0.1378,  ..., -0.0274,  0.1571, -0.2062],\n",
      "        ...,\n",
      "        [ 0.0055,  0.0528,  0.0342,  ..., -0.0391,  0.0607, -0.0446],\n",
      "        [-0.0733,  0.0174,  0.1513,  ...,  0.0083, -0.1267, -0.0529],\n",
      "        [ 0.2143,  0.1047,  0.0596,  ...,  0.0273,  0.0736,  0.1619]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4917, 1.6185, 1.4137, 1.6170, 1.3530, 1.5407, 1.5942, 1.3468, 1.5060,\n",
      "        1.4935, 1.3167, 1.4252, 1.4963, 1.5173, 1.4869, 1.3957, 0.9246, 1.0934,\n",
      "        1.0331, 1.1334, 0.8693, 1.0941, 1.2022, 0.3912, 1.2021, 0.8737, 1.2234,\n",
      "        0.9823, 1.2429, 0.9371, 1.1835, 1.1728, 0.9154, 1.1116, 1.1121, 1.0157,\n",
      "        1.0529, 1.1077, 1.0750, 1.0342, 0.9951, 0.9604, 0.9966, 1.1954, 1.1085,\n",
      "        1.1945, 1.0490, 0.9612, 1.5618, 1.6146, 1.4254, 1.1164, 1.5294, 1.3780,\n",
      "        1.5317, 1.5991, 0.9939, 1.4163, 1.2540, 1.7454, 1.5451, 1.5020, 1.0513,\n",
      "        1.3620, 0.9185, 1.2775, 1.1574, 1.1358, 1.1757, 1.2605, 1.3937, 1.1459,\n",
      "        1.2644, 1.2660, 1.1582, 1.3192, 1.2963, 1.2039, 1.2946, 1.2063, 0.9913,\n",
      "        1.0043, 1.2472, 1.0516, 1.1947, 1.1830, 1.2502, 1.1816, 1.2841, 1.0190,\n",
      "        1.1961, 1.1195, 1.1155, 1.0742, 1.0973, 1.1344, 1.6846, 1.6441, 1.4500,\n",
      "        1.5267, 1.4659, 1.5084, 1.7593, 1.6825, 1.4565, 1.3629, 1.0535, 1.6753,\n",
      "        1.5996, 1.2679, 1.4663, 1.6751, 1.7296, 1.8034, 1.6548, 1.7982, 2.0474,\n",
      "        1.8326, 1.9631, 2.1344, 1.6607, 1.8523, 1.6492, 1.7315, 1.6057, 1.7653,\n",
      "        1.7109, 2.0691, 1.0070, 1.1540, 1.2071, 1.1142, 1.2202, 1.4893, 1.1888,\n",
      "        1.5323, 1.1306, 1.4408, 1.3467, 1.2089, 1.1028, 1.2117, 1.1165, 1.4369,\n",
      "        1.1537, 1.0274, 1.0762, 1.1233, 1.1200, 1.0833, 1.1910, 1.2220, 1.0911,\n",
      "        1.1992, 1.2143, 1.0937, 1.1224, 0.9989, 1.0956, 1.0685, 1.2307, 1.4808,\n",
      "        1.1657, 1.2746, 1.3468, 1.2639, 1.3008, 1.2622, 1.5240, 1.6527, 1.3566,\n",
      "        1.3567, 1.2450, 1.1234, 1.2562, 1.1868, 1.0280, 1.0338, 0.8857, 1.0276,\n",
      "        0.9761, 1.0706, 1.3064, 1.0495, 0.8612, 0.9804, 1.2206, 1.0050, 1.1042,\n",
      "        0.9609, 1.1378, 1.1455, 1.7731, 1.4016, 0.8720, 1.3047, 1.8650, 1.2752,\n",
      "        1.3031, 1.6741, 1.3652, 1.7766, 1.7024, 1.9491, 1.6957, 1.7040, 1.7962,\n",
      "        1.6843, 1.2912, 1.2714, 1.3934, 1.4783, 1.3656, 1.5633, 1.2917, 1.4029,\n",
      "        1.5967, 1.5338, 1.3340, 1.4297, 1.2472, 1.5835, 1.1728, 1.1883, 0.7338,\n",
      "        0.8380, 0.6725, 0.7765, 0.7458, 0.6886, 0.9215, 0.7849, 0.8845, 0.9460,\n",
      "        0.7414, 0.5960, 0.7254, 0.7640, 0.7100, 0.6799, 1.1931, 1.2631, 1.2553,\n",
      "        1.3665, 1.0376, 1.2633, 1.2751, 1.2068, 1.3215, 1.1548, 1.2156, 1.2706,\n",
      "        1.3866, 1.3554, 1.2047, 1.1761, 1.3438, 1.2454, 1.3817, 1.3412, 1.2593,\n",
      "        1.2886, 1.8079, 1.2284, 1.3260, 1.2047, 1.3143, 1.3199, 1.5424, 1.4314,\n",
      "        1.3844, 1.1220, 0.8966, 0.7924, 0.9035, 1.0229, 0.9823, 0.9605, 1.0148,\n",
      "        0.9744, 0.9330, 0.9196, 0.9483, 1.0967, 0.9123, 0.9746, 0.9372, 0.8767,\n",
      "        1.0440, 1.0145, 0.9922, 1.0112, 1.0669, 0.9030, 0.9701, 0.9799, 0.9938,\n",
      "        1.0708, 0.9241, 1.0546, 1.1600, 1.0257, 1.1353, 0.9803, 1.1513, 1.0881,\n",
      "        1.0447, 1.0441, 1.0934, 1.2000, 1.2321, 1.1657, 1.0623, 1.1826, 1.1684,\n",
      "        1.0922, 1.2743, 1.1286, 1.2012, 1.1993], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 2.2875e-02, -5.7616e-02,  5.3130e-02,  ..., -1.5022e-02,\n",
      "         -5.6132e-02,  4.2808e-02],\n",
      "        [ 1.7193e-02, -4.6617e-02, -1.2302e-01,  ...,  9.1113e-02,\n",
      "          1.4257e-03,  1.8496e-02],\n",
      "        [-4.1950e-02,  1.8176e-02,  3.2392e-03,  ...,  4.5562e-02,\n",
      "          5.6913e-02,  3.7032e-03],\n",
      "        ...,\n",
      "        [ 3.2912e-02, -1.2532e-01,  5.4360e-02,  ...,  2.1749e-02,\n",
      "          2.1646e-02, -8.4060e-02],\n",
      "        [ 1.1246e-01,  1.1954e-04,  8.8178e-02,  ..., -2.4415e-02,\n",
      "         -3.7357e-02,  3.6012e-02],\n",
      "        [-7.4992e-03,  2.0190e-02, -7.8832e-03,  ...,  3.2634e-02,\n",
      "          1.2502e-01, -8.3001e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0694,  0.0207,  0.0094,  ..., -0.0309,  0.0413, -0.1416],\n",
      "        [ 0.0015, -0.0281, -0.0550,  ..., -0.0552, -0.1333,  0.0577],\n",
      "        [-0.0678, -0.0670, -0.0332,  ...,  0.0564,  0.0964,  0.0413],\n",
      "        ...,\n",
      "        [-0.0034,  0.0211, -0.0138,  ...,  0.0476,  0.0222,  0.0774],\n",
      "        [-0.0069, -0.0140,  0.0580,  ...,  0.0576,  0.0310, -0.0124],\n",
      "        [-0.0300, -0.0429, -0.0785,  ...,  0.0153,  0.0679, -0.0098]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3740, 1.3904, 1.4838, 1.5118, 1.4510, 1.5785, 1.4105, 1.4067, 1.3052,\n",
      "        1.4298, 1.3426, 1.5070, 1.5158, 1.3758, 1.3639, 1.3709, 1.4356, 1.3511,\n",
      "        1.4590, 1.3494, 1.4009, 1.4041, 1.4205, 1.3598, 1.3991, 1.4443, 1.4524,\n",
      "        1.3815, 1.4113, 1.4558, 1.3600, 1.3032, 1.4515, 1.2735, 1.3580, 1.3907,\n",
      "        1.4180, 1.3804, 1.4393, 1.4540, 1.4143, 1.4283, 1.3486, 1.4449, 1.4344,\n",
      "        1.4374, 1.8067, 1.3505, 1.5645, 1.4160, 1.4332, 1.3737, 1.4381, 1.5773,\n",
      "        1.3828, 1.2475, 1.3874, 1.4301, 1.3509, 1.3399, 1.4914, 1.6885, 1.2521,\n",
      "        1.5101, 1.3818, 1.3831, 1.4515, 1.4315, 1.3969, 1.3142, 1.4090, 1.4638,\n",
      "        1.3627, 1.5256, 1.5117, 1.4152, 1.4218, 1.3243, 1.3828, 1.4735, 1.3924,\n",
      "        1.3767, 1.3131, 1.4211, 1.4310, 1.5536, 1.3673, 1.4457, 1.3947, 1.4270,\n",
      "        1.4839, 1.3977, 1.4594, 1.4249, 1.3820, 1.4246, 1.3019, 1.4367, 1.3813,\n",
      "        1.3407, 1.3682, 1.4494, 1.3550, 1.2743, 1.5165, 1.3337, 1.5171, 1.4821,\n",
      "        1.4211, 1.3537, 1.3815, 1.6056, 1.4653, 1.4690, 1.5439, 1.4124, 1.3118,\n",
      "        1.4480, 1.3472, 1.5352, 1.5861, 1.4152, 1.3639, 1.3605, 1.3323, 1.4614,\n",
      "        1.3607, 1.4409, 1.3664, 1.3438, 1.4065, 1.5738, 1.4407, 1.4703, 1.3721,\n",
      "        1.4661, 1.3906, 1.3201, 1.3794, 1.6478, 1.3449, 1.5379, 1.3763, 1.3410,\n",
      "        1.1465, 1.3874, 1.3620, 1.5130, 1.3825, 1.3696, 1.4624, 1.5309, 1.5773,\n",
      "        1.3766, 1.3823, 1.3124, 1.4153, 1.3662, 1.4840, 1.3986, 1.3518, 1.4303,\n",
      "        1.4070, 1.4805, 1.4840, 1.4982, 1.4995, 1.4460, 1.3829, 1.3684, 1.4587,\n",
      "        1.6012, 1.4714, 1.3866, 1.4046, 1.4590, 1.3298, 1.3535, 1.3849, 1.4275,\n",
      "        1.3954, 1.5177, 1.3541, 1.3493, 1.4538, 1.3263, 1.4283, 1.2313, 1.4423,\n",
      "        1.3513, 1.3759, 1.3375, 1.4987, 0.9611, 1.3892, 1.3322, 1.3945, 1.3051,\n",
      "        1.3567, 1.3461, 1.4279, 1.3839, 1.3114, 1.3412, 1.3983, 1.4422, 1.3717,\n",
      "        1.4004, 1.4139, 1.4557, 1.3899, 1.3335, 1.3043, 1.4171, 1.4907, 1.4302,\n",
      "        1.3905, 1.4190, 1.3560, 1.3773, 1.3991, 1.3786, 1.4501, 1.3242, 1.3477,\n",
      "        1.4172, 1.3803, 1.4688, 1.3984, 1.4666, 1.4248, 1.3366, 1.4687, 1.3257,\n",
      "        1.4461, 1.4017, 1.4657, 1.3568, 1.4132, 1.4396, 1.4444, 1.4564, 1.5247,\n",
      "        1.4280, 1.4539, 1.3982, 1.3897, 1.4348, 1.4736, 1.3185, 1.3491, 1.4413,\n",
      "        1.4655, 1.4000, 1.3226, 1.4614, 1.4832, 1.6965, 1.4447, 1.3706, 1.4175,\n",
      "        1.4554, 1.4346, 1.3463, 1.4888, 1.4108, 1.5053, 1.4166, 1.4168, 1.4874,\n",
      "        1.2874, 1.2381, 1.4579, 1.3490, 1.2877, 1.3832, 1.4297, 1.4457, 1.4464,\n",
      "        1.3924, 1.4519, 1.4723, 1.3501, 1.6392, 1.4451, 1.2619, 1.2780, 1.5177,\n",
      "        1.3808, 1.5079, 1.3931, 1.7730, 1.3874, 1.4245, 1.4911, 1.3472, 1.2795,\n",
      "        1.3893, 1.4300, 1.4069, 1.3913, 1.4318, 1.3971, 1.3429, 1.5421, 1.4095,\n",
      "        1.3809, 1.4325, 1.5401, 1.3790, 1.4808, 1.4995, 1.4290, 1.4436, 1.5071,\n",
      "        1.4553, 1.3814, 1.3954, 1.3907, 1.3718], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 1.6870e-01,  4.9902e-02, -1.2189e-01,  ...,  1.6759e-01,\n",
      "         -1.6534e-01,  6.3929e-01],\n",
      "        [ 8.9402e-02, -2.6805e-02, -9.0301e-02,  ...,  1.2442e-01,\n",
      "         -1.6093e-02,  7.5032e-02],\n",
      "        [-1.1900e-01, -5.2743e-02, -1.0721e-01,  ..., -2.3550e-01,\n",
      "         -2.1219e-02, -5.0626e-04],\n",
      "        ...,\n",
      "        [-5.7376e-02, -2.9079e-02, -1.2264e-01,  ..., -1.8853e-02,\n",
      "         -9.9525e-02,  5.4936e-02],\n",
      "        [-7.9351e-02, -9.3222e-02, -8.0281e-02,  ...,  1.0874e-01,\n",
      "         -1.5452e-01, -1.4049e-01],\n",
      "        [-4.1216e-02, -1.4729e-01, -1.3642e-01,  ..., -1.2539e-01,\n",
      "          3.1789e-03,  3.3047e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0141,  0.0241,  0.0289,  ...,  0.0098,  0.0666, -0.0135],\n",
      "        [-0.1490, -0.1216, -0.0346,  ..., -0.0460, -0.0623,  0.0192],\n",
      "        [-0.1204, -0.1405, -0.0062,  ...,  0.0264, -0.0182,  0.0078],\n",
      "        ...,\n",
      "        [-0.0759, -0.0302, -0.1345,  ...,  0.0959, -0.0223,  0.0074],\n",
      "        [-0.0206,  0.1339,  0.1310,  ..., -0.0788, -0.0802,  0.0098],\n",
      "        [ 0.0677, -0.1423, -0.1050,  ...,  0.0419,  0.1962, -0.0726]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.3245, 1.7168, 1.7899, 1.6921, 2.5019, 3.0230, 2.9051, 2.6821, 2.2656,\n",
      "        1.9286, 1.9575, 2.6198, 2.1558, 2.3735, 2.8304, 2.5617, 3.4486, 2.4797,\n",
      "        1.9333, 2.3668, 2.5104, 2.6295, 3.1140, 3.6420, 1.8723, 2.0190, 2.8540,\n",
      "        2.0065, 2.3818, 2.5469, 3.1762, 3.1797, 2.4396, 2.1003, 2.8242, 2.7793,\n",
      "        2.9950, 2.3750, 3.3404, 3.9783, 1.9111, 2.1084, 2.1892, 2.5384, 2.8881,\n",
      "        2.8819, 3.3324, 3.0388, 1.3544, 1.9157, 1.0063, 1.3015, 2.2458, 2.4231,\n",
      "        2.7791, 2.6768, 3.4506, 1.8947, 2.6747, 2.7112, 2.5856, 2.4897, 3.0258,\n",
      "        2.8980, 1.9294, 2.0109, 3.3894, 2.7113, 2.5089, 3.1272, 2.7267, 3.3233,\n",
      "        2.3448, 1.2482, 1.0486, 1.5450, 2.0146, 2.4396, 3.1620, 3.0219, 2.3152,\n",
      "        2.3613, 2.9629, 2.9116, 2.8080, 3.2015, 3.3321, 3.4925, 2.2904, 2.4564,\n",
      "        2.4738, 2.6459, 2.7453, 2.6686, 3.3170, 2.8940, 2.4965, 2.3668, 3.4119,\n",
      "        3.8276, 3.7419, 2.3307, 2.3495, 3.7480, 2.6472, 3.0456, 3.8322, 3.6121,\n",
      "        3.0308, 3.8170, 3.1908, 4.1504, 1.3837, 1.2890, 2.6511, 2.5087, 2.4521,\n",
      "        2.5008, 2.7571, 2.6419, 1.8360, 2.5589, 1.9935, 1.9783, 2.1956, 2.5941,\n",
      "        2.7387, 3.2058, 1.9193, 1.9477, 2.1161, 1.8403, 2.3576, 2.8870, 2.8029,\n",
      "        2.8380, 2.0006, 1.8999, 2.4727, 3.4790, 2.5432, 2.4968, 2.8966, 2.8671,\n",
      "        3.2983, 1.9448, 2.6039, 2.1472, 2.3245, 2.2576, 2.9870, 2.8286, 1.0430,\n",
      "        1.5752, 1.8245, 2.1747, 2.2242, 2.9386, 2.9885, 3.2160, 1.9532, 2.3568,\n",
      "        2.0262, 2.2121, 2.3628, 2.9375, 2.8399, 2.9173, 3.0711, 2.0559, 2.6335,\n",
      "        2.3709, 2.7033, 2.5364, 2.9938, 3.3630, 1.4400, 1.4544, 2.3855, 1.6382,\n",
      "        1.9818, 2.7049, 2.4330, 2.7141, 1.2462, 1.5827, 1.4435, 2.3673, 2.0342,\n",
      "        2.1272, 2.5182, 2.7164, 1.2996, 1.7125, 1.7743, 1.4574, 2.0389, 1.9021,\n",
      "        2.8402, 2.6258, 2.2244, 2.0748, 2.1919, 2.6215, 2.0894, 2.2945, 2.8064,\n",
      "        2.8616, 2.6161, 2.3002, 2.3308, 3.2465, 3.3884, 1.9806, 3.0118, 3.1338,\n",
      "        1.8333, 3.0788, 3.6652, 4.3917, 3.5267, 4.1543, 3.2870, 3.7847, 1.3667,\n",
      "        1.4765, 2.5817, 1.3997, 1.8004, 2.1469, 2.4749, 3.5545, 4.4102, 2.1699,\n",
      "        1.3076, 2.3206, 2.1613, 3.0763, 2.5509, 3.0494, 2.1864, 2.4883, 2.1916,\n",
      "        3.3076, 2.0339, 2.6320, 3.0719, 3.0617, 2.2879, 1.9822, 2.0287, 1.3856,\n",
      "        2.7286, 2.5970, 2.9732, 2.9859, 3.0516, 2.1417, 2.2701, 1.9949, 2.1251,\n",
      "        2.3400, 3.3546, 3.1013, 2.5145, 2.4692, 2.3275, 3.0263, 2.7831, 2.2138,\n",
      "        3.4609, 2.4852, 1.9811, 2.4358, 2.9205, 2.3416, 1.9662, 2.5458, 3.3893,\n",
      "        3.3530, 2.0105, 2.7228, 2.9802, 3.0973, 3.1343, 2.6227, 3.1460, 3.0294,\n",
      "        2.1830, 3.0292, 2.9034, 3.2462, 2.1824, 3.1652, 3.3615, 3.4801, 2.8929,\n",
      "        3.4013, 2.8332, 2.3360, 3.2497, 2.4488, 3.8892, 3.3294, 1.4866, 2.1185,\n",
      "        2.3769, 2.0340, 2.0800, 2.3999, 2.4460, 2.8901, 1.6317, 1.4096, 1.4471,\n",
      "        2.7143, 2.0882, 2.6064, 2.5952, 2.6835], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0254, -0.0781,  0.0762,  ..., -0.0876,  0.0627, -0.8724],\n",
      "        [-0.0080,  0.0157, -0.0311,  ..., -0.1192,  0.1119, -0.0826],\n",
      "        [ 0.0734,  0.0899, -0.0601,  ...,  0.0186, -0.1439, -0.0848],\n",
      "        ...,\n",
      "        [-0.0558, -0.0730,  0.0440,  ..., -0.1581, -0.1455,  0.0570],\n",
      "        [ 0.0727, -0.0508, -0.0124,  ...,  0.0745,  0.0575, -0.0876],\n",
      "        [-0.0860,  0.0428,  0.0784,  ..., -0.0787,  0.0608,  0.0376]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0593,  0.0664, -0.0933,  ...,  0.1563,  0.0474,  0.0165],\n",
      "        [-0.1472,  0.1030, -0.1273,  ...,  0.0431,  0.0448,  0.0223],\n",
      "        [ 0.2234, -0.0241,  0.0058,  ...,  0.0161,  0.0079, -0.0367],\n",
      "        ...,\n",
      "        [ 0.1645, -0.0652,  0.0206,  ..., -0.0526, -0.0232, -0.0255],\n",
      "        [ 0.1108,  0.0107,  0.0389,  ...,  0.0542,  0.0508, -0.0125],\n",
      "        [-0.0878, -0.0834, -0.0796,  ...,  0.0009,  0.0354, -0.0484]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9300, 1.6594, 1.4977, 3.0943, 1.3151, 1.9870, 2.4968, 2.6338, 1.9344,\n",
      "        2.2295, 2.5972, 1.2599, 2.9110, 2.0880, 2.8159, 2.0986, 1.6854, 2.0358,\n",
      "        2.7459, 1.7867, 1.7394, 1.7103, 3.2583, 3.1709, 1.9329, 2.2107, 1.6039,\n",
      "        1.6938, 3.4331, 3.6160, 3.4965, 3.0764, 2.6695, 1.9023, 2.4984, 2.6565,\n",
      "        2.2615, 1.9451, 2.1478, 2.9513, 1.6700, 2.2433, 2.5989, 2.2520, 2.1870,\n",
      "        2.2697, 2.4098, 2.3007, 2.6485, 1.6586, 1.4965, 1.9027, 2.9195, 1.3350,\n",
      "        2.4299, 2.5055, 1.5202, 2.2669, 1.7688, 1.4574, 1.3019, 3.0451, 2.6566,\n",
      "        2.4623, 1.7950, 1.7287, 1.3536, 1.2326, 1.4801, 1.7067, 2.6765, 2.0548,\n",
      "        1.9986, 1.5348, 1.6435, 2.5852, 1.7279, 2.8056, 2.5023, 2.0610, 2.4779,\n",
      "        2.5367, 2.7675, 2.3908, 2.3919, 2.4414, 3.2886, 3.1418, 1.7161, 2.2924,\n",
      "        2.2264, 2.5200, 2.2085, 2.2493, 3.1418, 2.6015, 2.4477, 2.6953, 3.1755,\n",
      "        3.2549, 2.8033, 3.6047, 3.4769, 3.3705, 2.8548, 3.3037, 3.0578, 3.1547,\n",
      "        2.9186, 1.5723, 2.2129, 3.5157, 1.2725, 2.1193, 1.5701, 1.4799, 1.4237,\n",
      "        2.8391, 2.7166, 2.7548, 1.6344, 1.5087, 1.9889, 2.4190, 2.6733, 1.4955,\n",
      "        2.6702, 2.2463, 2.1765, 2.2354, 1.8153, 3.1809, 1.6192, 1.5404, 2.6178,\n",
      "        2.5148, 1.9515, 1.8926, 2.9066, 1.5735, 1.7372, 3.0129, 2.5642, 2.7293,\n",
      "        2.0537, 2.2480, 1.5403, 1.6119, 1.3753, 3.0500, 2.5977, 2.6608, 1.3663,\n",
      "        1.5327, 2.7925, 1.6589, 2.9992, 1.6027, 2.7083, 2.2353, 2.1649, 2.5114,\n",
      "        2.9816, 3.0002, 3.2522, 1.8592, 3.2529, 3.0077, 2.2953, 2.0367, 1.6566,\n",
      "        1.8081, 1.7440, 3.2761, 3.3680, 3.0972, 1.2509, 1.4573, 1.4197, 2.6669,\n",
      "        1.2936, 1.4062, 2.2339, 2.1945, 1.6679, 1.8526, 2.1087, 1.1746, 1.5446,\n",
      "        2.3620, 2.4067, 1.9295, 1.9584, 1.8850, 1.5768, 3.1045, 1.4623, 1.4757,\n",
      "        2.6626, 2.7667, 2.4256, 2.2508, 1.4692, 1.3203, 2.9583, 3.0773, 2.5310,\n",
      "        2.8126, 1.8787, 2.7114, 3.3920, 3.1376, 2.8005, 3.1771, 2.5933, 4.0284,\n",
      "        2.3364, 2.0400, 2.2781, 2.7932, 2.7753, 1.8393, 3.9381, 3.9458, 1.3425,\n",
      "        1.9988, 1.2551, 2.1113, 2.7417, 2.6113, 2.2380, 2.0766, 1.2947, 1.6646,\n",
      "        1.2516, 1.2782, 1.2255, 1.5256, 2.2886, 2.0154, 2.3515, 2.1413, 1.9988,\n",
      "        1.6107, 3.1144, 1.6842, 2.7934, 2.8004, 2.0608, 2.2068, 2.0929, 2.1452,\n",
      "        1.3882, 2.3526, 2.6642, 2.8483, 2.4208, 2.2372, 2.2080, 2.1844, 2.8083,\n",
      "        2.1781, 3.0061, 2.5230, 2.8054, 2.3766, 2.2933, 2.2169, 1.9492, 2.0266,\n",
      "        2.9048, 2.3151, 1.7494, 2.7849, 2.7082, 2.4644, 3.2154, 2.1386, 3.1371,\n",
      "        3.2422, 2.0058, 2.3608, 2.4633, 2.5312, 1.8178, 2.0440, 3.2848, 3.0267,\n",
      "        2.7804, 2.9552, 2.6353, 2.9414, 3.7376, 2.2175, 3.3149, 3.2174, 2.3222,\n",
      "        2.7310, 2.6773, 2.2565, 1.7349, 2.4219, 3.0909, 3.4695, 1.7518, 1.9688,\n",
      "        1.2404, 2.7240, 2.6984, 1.9412, 2.4008, 2.0697, 1.7143, 1.5165, 2.6549,\n",
      "        1.3040, 1.1566, 2.1132, 2.3006, 2.4035], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0298,  0.0448,  0.1184,  ...,  0.0129,  0.0675, -0.0011],\n",
      "        [-0.0980,  0.0223, -0.0524,  ..., -0.0727, -0.0518, -0.0085],\n",
      "        [ 0.0984,  0.0308,  0.1346,  ...,  0.0088, -0.0262,  0.0029],\n",
      "        ...,\n",
      "        [-0.0525, -0.1194,  0.0279,  ..., -0.0296,  0.0422,  0.0077],\n",
      "        [ 0.0403,  0.1124,  0.0035,  ..., -0.0498,  0.0148,  0.0041],\n",
      "        [ 0.0406,  0.0608,  0.0054,  ...,  0.0125,  0.0080, -0.0073]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0197, -0.0262,  0.0052,  ...,  0.0081, -0.0378, -0.0087],\n",
      "        [-0.0079, -0.0146,  0.1244,  ...,  0.0421, -0.0523,  0.0365],\n",
      "        [-0.1055,  0.0199,  0.0096,  ...,  0.0417, -0.0194,  0.0826],\n",
      "        ...,\n",
      "        [-0.0373, -0.0710,  0.0228,  ...,  0.0516,  0.1481, -0.0235],\n",
      "        [-0.0164,  0.0163, -0.0548,  ...,  0.0427,  0.0267,  0.0363],\n",
      "        [ 0.0066,  0.1573, -0.1508,  ..., -0.0057, -0.0282, -0.1634]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4980, 1.3459, 1.4473, 1.4358, 1.3913, 1.4258, 1.3466, 1.3956, 1.4740,\n",
      "        1.4381, 1.4112, 1.4339, 1.4113, 1.4300, 1.4728, 1.4411, 1.1943, 1.2046,\n",
      "        1.1835, 1.2129, 1.1931, 1.1500, 1.1957, 1.1494, 1.2792, 1.1394, 1.1760,\n",
      "        1.0981, 1.1760, 1.2063, 1.1164, 1.2555, 1.8288, 1.9685, 1.9895, 1.9297,\n",
      "        2.1100, 2.1907, 2.1788, 1.9598, 2.1310, 2.0346, 1.9360, 2.1141, 1.9997,\n",
      "        1.9785, 1.9648, 2.1194, 1.3960, 1.4999, 1.3750, 1.4157, 1.4814, 1.5020,\n",
      "        1.3688, 1.5273, 1.4835, 1.4501, 1.4853, 1.4578, 1.4939, 1.5067, 1.4857,\n",
      "        1.4171, 1.2774, 1.2409, 1.2623, 1.2956, 1.3209, 1.3307, 1.2779, 1.3276,\n",
      "        1.2602, 1.2174, 1.3099, 1.2621, 1.2084, 1.2641, 1.2588, 1.1698, 2.0404,\n",
      "        2.1187, 2.0826, 1.9912, 2.1269, 2.1338, 2.1966, 2.1638, 2.2216, 2.1042,\n",
      "        2.0221, 2.1139, 2.0291, 2.1846, 2.0846, 1.9232, 1.6377, 1.7291, 1.7390,\n",
      "        1.7090, 1.7661, 1.7659, 1.7432, 1.7142, 1.6639, 1.6418, 1.7013, 1.7508,\n",
      "        1.7101, 1.6589, 1.6387, 1.7484, 1.2062, 1.2426, 1.2586, 1.2244, 1.2592,\n",
      "        1.3000, 1.1969, 1.2025, 1.3070, 1.2519, 1.2038, 1.2704, 1.3148, 1.2617,\n",
      "        1.2592, 1.2307, 1.6067, 1.4729, 1.5090, 1.5377, 1.5189, 1.4314, 1.5557,\n",
      "        1.5550, 1.5509, 1.4745, 1.5585, 1.4381, 1.5450, 1.5911, 1.5353, 1.4696,\n",
      "        1.2607, 1.2757, 1.1858, 1.2939, 1.2295, 1.2569, 1.2915, 1.2513, 1.2909,\n",
      "        1.3173, 1.1631, 1.3012, 1.2709, 1.2663, 1.2420, 1.2154, 1.1433, 1.2282,\n",
      "        1.2999, 1.1342, 1.1834, 1.1620, 1.2391, 1.1815, 1.1959, 1.1553, 1.1661,\n",
      "        1.2237, 1.2253, 1.3122, 1.1865, 1.2240, 1.4584, 1.4568, 1.4475, 1.4286,\n",
      "        1.4856, 1.4470, 1.4348, 1.4726, 1.4814, 1.4087, 1.5119, 1.4882, 1.4415,\n",
      "        1.4326, 1.4308, 1.5150, 1.5702, 1.5611, 1.6107, 1.6394, 1.5555, 1.5646,\n",
      "        1.5111, 1.5860, 1.5927, 1.6342, 1.5680, 1.5987, 1.5860, 1.5844, 1.5843,\n",
      "        1.5492, 1.4515, 1.4995, 1.5403, 1.5012, 1.4267, 1.5219, 1.5294, 1.5072,\n",
      "        1.6551, 1.5566, 1.4999, 1.4341, 1.5469, 1.4206, 1.5184, 1.5231, 1.3572,\n",
      "        1.3593, 1.3325, 1.3854, 1.3571, 1.4159, 1.4355, 1.4141, 1.3958, 1.4192,\n",
      "        1.4696, 1.3837, 1.3532, 1.3965, 1.3966, 1.3738, 1.6144, 1.6316, 1.6704,\n",
      "        1.6964, 1.7117, 1.6431, 1.6655, 1.5849, 1.6665, 1.6786, 1.7127, 1.6865,\n",
      "        1.6185, 1.7515, 1.6403, 1.6968, 1.8723, 1.8087, 1.7256, 1.7091, 1.7462,\n",
      "        1.8303, 1.7958, 1.8147, 1.8122, 1.8251, 1.8571, 1.8262, 1.8690, 1.7861,\n",
      "        1.6899, 1.7560, 1.7648, 1.7503, 1.9527, 1.8088, 1.8292, 1.8069, 1.8912,\n",
      "        1.7744, 1.7893, 1.7516, 1.8652, 1.7254, 1.7266, 1.8316, 1.8588, 1.7919,\n",
      "        1.7756, 1.8021, 1.8023, 1.8010, 1.8481, 1.7843, 1.7818, 1.8256, 1.8149,\n",
      "        1.6881, 1.7637, 1.8266, 1.7904, 1.8124, 1.7440, 1.8805, 1.4873, 1.4176,\n",
      "        1.4244, 1.3782, 1.4729, 1.3896, 1.4025, 1.3679, 1.4368, 1.3750, 1.4059,\n",
      "        1.3538, 1.3856, 1.4647, 1.4448, 1.5029], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0212,  0.0222,  0.0224,  ...,  0.0101,  0.0533, -0.0518],\n",
      "        [ 0.0227, -0.0480,  0.0123,  ...,  0.0455, -0.0377,  0.0202],\n",
      "        [ 0.0099,  0.0050, -0.0066,  ..., -0.0239,  0.0389, -0.0145],\n",
      "        ...,\n",
      "        [-0.0525, -0.0258,  0.0031,  ..., -0.0879,  0.0053,  0.0270],\n",
      "        [ 0.0043,  0.0049,  0.0244,  ...,  0.0091,  0.0857, -0.0087],\n",
      "        [ 0.0048, -0.0286, -0.0033,  ..., -0.0186,  0.0499, -0.0678]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0699, -0.0754,  0.0219,  ...,  0.0680, -0.0979, -0.0380],\n",
      "        [ 0.0222,  0.0479, -0.0024,  ...,  0.0479, -0.0308,  0.0669],\n",
      "        [ 0.0789,  0.0041,  0.0304,  ..., -0.0284,  0.0059, -0.0273],\n",
      "        ...,\n",
      "        [ 0.0269,  0.0711, -0.0231,  ...,  0.0981,  0.0372,  0.0509],\n",
      "        [ 0.0127,  0.0197,  0.0026,  ...,  0.0688, -0.0037, -0.0093],\n",
      "        [ 0.5940, -0.0378,  0.2032,  ...,  0.0810,  0.0980,  0.1319]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5017, 1.3348, 1.4557, 1.5515, 1.5730, 1.3890, 1.4217, 1.3872, 1.3844,\n",
      "        1.5491, 1.4278, 1.5350, 1.4248, 1.3817, 1.4057, 1.5236, 1.5381, 1.5122,\n",
      "        1.3979, 1.4319, 1.5571, 1.3928, 1.5441, 1.4488, 1.4208, 1.5494, 1.4653,\n",
      "        1.4478, 1.5200, 1.5324, 1.4738, 1.4753, 1.4613, 1.4356, 1.4540, 1.5696,\n",
      "        1.5998, 1.5053, 1.5695, 1.5046, 1.4315, 1.4927, 1.7001, 1.5625, 1.4244,\n",
      "        1.3825, 1.3145, 1.5076, 1.5389, 1.5490, 1.4521, 1.4387, 1.4519, 1.4844,\n",
      "        1.4565, 1.4971, 1.4035, 1.5016, 1.4489, 1.4327, 1.5005, 1.8010, 1.4406,\n",
      "        1.5980, 1.5243, 1.4550, 1.4112, 1.4501, 1.4538, 1.5796, 1.4548, 1.6206,\n",
      "        1.5958, 1.5160, 1.4537, 1.3340, 1.4609, 1.4703, 1.4276, 1.4735, 1.5058,\n",
      "        1.5631, 1.4336, 1.5360, 1.5922, 1.5253, 1.4387, 1.3703, 1.3609, 1.5956,\n",
      "        1.4412, 1.5321, 1.5645, 1.4062, 1.5134, 1.4783, 1.2997, 1.5408, 1.6227,\n",
      "        1.3970, 1.4782, 1.3789, 1.4151, 1.3146, 1.4456, 1.5722, 1.4935, 1.4081,\n",
      "        1.6215, 1.4184, 1.4402, 1.4856, 1.5544, 1.5291, 1.4436, 1.4682, 1.6035,\n",
      "        1.5418, 1.4652, 1.4597, 1.5233, 1.5839, 1.4628, 1.4946, 1.4858, 1.3861,\n",
      "        1.3928, 1.4512, 1.4620, 1.6742, 1.4592, 1.5305, 1.4766, 1.4869, 1.4565,\n",
      "        1.2850, 1.5033, 1.3575, 1.5451, 1.3872, 1.4509, 1.5233, 1.4467, 1.4184,\n",
      "        1.4185, 1.4106, 1.3943, 1.4288, 1.4935, 1.4826, 1.7352, 1.5121, 1.4623,\n",
      "        1.3521, 1.4324, 1.4864, 1.5864, 1.5271, 1.5507, 1.5103, 1.3882, 1.4910,\n",
      "        1.4856, 1.4616, 1.4890, 1.5005, 1.4750, 1.4226, 1.4069, 1.4865, 1.4920,\n",
      "        1.4959, 1.5237, 1.5382, 1.5554, 1.4507, 1.5220, 1.4799, 1.3936, 1.4417,\n",
      "        1.4713, 1.3843, 1.6368, 1.5296, 1.4537, 1.4887, 1.5235, 1.3477, 1.3948,\n",
      "        1.4150, 1.3907, 1.4642, 2.2830, 1.4287, 1.4877, 1.4324, 1.4421, 1.4250,\n",
      "        1.4568, 1.4854, 1.4586, 1.4920, 1.3931, 1.4875, 1.4075, 1.4117, 1.5979,\n",
      "        1.4611, 1.5325, 1.3526, 1.5216, 1.3862, 1.4600, 1.4651, 1.4202, 1.5174,\n",
      "        1.4706, 1.4642, 1.4751, 1.5125, 1.4629, 1.5553, 1.5431, 1.4479, 1.3771,\n",
      "        1.3374, 1.4546, 1.4461, 1.4929, 1.5388, 1.5058, 1.4566, 1.4710, 1.4280,\n",
      "        1.4451, 1.5151, 1.4833, 1.2989, 1.4596, 1.5329, 1.5531, 1.3593, 1.5034,\n",
      "        1.5055, 1.4199, 1.3569, 1.3553, 1.4771, 1.4486, 1.4853, 1.4104, 1.5438,\n",
      "        1.4865, 1.5443, 1.4281, 1.5061, 1.5819, 1.3899, 1.3341, 1.5003, 1.4953,\n",
      "        1.4423, 1.4588, 1.5079, 1.6188, 1.5684, 1.5438, 1.5535, 1.4586, 1.4856,\n",
      "        1.4503, 1.4786, 1.6876, 1.4415, 1.5083, 1.5201, 1.4955, 1.4365, 1.4477,\n",
      "        1.4546, 1.4780, 1.5449, 1.3886, 1.5185, 1.4341, 1.4793, 1.4304, 1.4436,\n",
      "        1.2968, 1.5297, 1.5393, 1.5855, 1.3822, 1.4893, 1.4967, 1.5099, 1.5831,\n",
      "        1.4870, 1.4372, 1.4805, 1.4345, 1.5428, 1.4000, 1.3519, 1.4385, 1.4646,\n",
      "        1.5905, 1.5174, 1.4593, 1.3578, 1.4713, 1.5128, 1.4325, 1.5718, 1.4143,\n",
      "        1.5713, 1.5181, 1.4250, 1.4072, 2.1116], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.1897,  0.1444, -0.0601,  ...,  0.1208, -0.0411,  0.1693],\n",
      "        [ 0.1106, -0.1279,  0.0166,  ...,  0.0435,  0.0459, -0.0275],\n",
      "        [-0.0351, -0.0751, -0.0027,  ...,  0.0003, -0.1235, -0.0907],\n",
      "        ...,\n",
      "        [ 0.0315, -0.0510,  0.0915,  ..., -0.0235,  0.0670,  0.0321],\n",
      "        [-0.0569,  0.0056,  0.0759,  ..., -0.1676, -0.0737, -0.0191],\n",
      "        [-0.1071, -0.0135,  0.1023,  ...,  0.0200,  0.1014,  0.0301]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-1.9415e-02, -3.0802e-02,  3.7045e-02,  ...,  1.2388e-01,\n",
      "          5.8074e-02,  3.7667e-02],\n",
      "        [ 2.2984e-02,  1.1339e-01,  3.0278e-03,  ...,  9.6700e-02,\n",
      "          8.4788e-02,  1.1016e-01],\n",
      "        [ 2.8255e-02, -2.6010e-02,  9.8865e-03,  ..., -1.2348e-02,\n",
      "         -4.1980e-02,  1.8962e-02],\n",
      "        ...,\n",
      "        [-1.3478e-01, -1.8110e-01,  1.9137e-01,  ...,  2.2181e-02,\n",
      "         -8.2965e-02, -1.8213e-02],\n",
      "        [ 3.8945e-02,  1.4990e-01, -3.3312e-01,  ...,  1.3039e-01,\n",
      "          1.4722e-01, -6.6580e-02],\n",
      "        [-1.3467e-01, -1.4184e-01,  4.9619e-02,  ...,  3.7615e-02,\n",
      "          7.1831e-02,  2.6942e-04]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.6495, 2.6144, 2.0923, 1.2663, 2.4449, 4.1128, 4.0464, 4.0822, 1.6418,\n",
      "        1.7398, 2.3901, 2.2299, 1.8270, 4.0006, 3.8445, 3.9535, 1.6899, 1.9972,\n",
      "        2.5525, 2.4626, 2.4197, 2.8368, 2.4949, 2.8939, 2.1522, 2.0523, 1.9412,\n",
      "        1.9591, 1.9815, 2.1155, 2.6463, 3.0366, 2.0924, 1.9519, 1.9135, 1.3717,\n",
      "        1.9336, 2.9369, 2.5860, 2.5326, 2.3171, 1.6125, 1.7838, 2.2947, 2.6071,\n",
      "        1.9390, 2.4219, 2.3957, 1.2522, 1.4052, 2.0674, 1.9370, 1.6648, 2.5130,\n",
      "        2.3807, 2.6832, 1.7377, 2.3225, 1.9634, 1.8986, 1.9682, 1.9425, 2.2501,\n",
      "        2.6913, 2.1484, 2.1123, 1.7639, 2.0286, 2.1186, 2.2937, 2.7662, 3.8656,\n",
      "        2.4026, 2.2081, 2.1102, 1.9174, 1.7959, 2.3483, 2.9658, 2.7738, 1.9227,\n",
      "        2.1042, 2.0690, 1.9647, 2.0851, 2.3690, 2.5679, 2.0241, 1.8270, 1.6958,\n",
      "        2.2732, 2.1671, 2.0249, 2.0650, 2.6504, 2.8148, 2.7256, 2.0201, 2.3212,\n",
      "        2.3504, 2.1281, 2.4978, 2.7551, 2.7805, 1.6057, 2.2123, 1.8004, 2.3013,\n",
      "        2.6184, 3.3338, 2.7687, 3.1015, 3.1543, 1.7773, 2.4207, 2.0360, 2.1830,\n",
      "        2.8891, 2.4851, 2.5311, 1.7617, 2.2906, 1.6699, 2.4805, 2.7635, 2.4349,\n",
      "        2.8071, 2.8553, 1.2718, 1.8109, 1.7713, 2.0664, 2.2396, 2.5660, 2.2657,\n",
      "        2.8725, 2.3810, 1.7232, 2.2653, 1.7358, 1.9742, 2.1835, 2.4735, 3.1659,\n",
      "        2.2275, 1.5316, 1.7490, 2.3351, 2.7477, 2.5349, 2.5800, 2.5983, 1.8945,\n",
      "        1.4549, 1.8616, 1.7386, 2.0547, 2.0583, 2.5462, 2.6193, 1.9031, 2.0368,\n",
      "        2.1479, 2.2566, 2.0079, 1.7242, 2.6139, 2.8391, 1.8368, 2.0845, 2.4292,\n",
      "        2.0815, 2.6271, 2.4523, 2.6958, 1.9641, 2.0994, 2.4425, 1.9681, 2.6334,\n",
      "        3.3529, 3.3632, 2.8961, 3.0801, 2.4249, 2.1216, 2.4391, 2.2884, 2.2374,\n",
      "        2.7303, 2.7596, 2.9757, 1.9266, 1.6893, 1.9801, 1.6159, 2.1873, 3.1525,\n",
      "        2.7997, 2.9109, 2.6979, 2.2458, 2.1388, 2.7122, 2.8330, 2.3681, 2.5511,\n",
      "        2.6233, 2.3600, 2.1750, 2.2240, 2.1401, 1.9611, 2.6368, 2.8951, 2.7490,\n",
      "        2.0636, 2.1809, 2.0105, 2.2464, 2.3964, 2.6847, 2.9179, 2.9543, 1.2775,\n",
      "        1.4900, 2.3056, 1.6133, 2.3192, 1.7255, 2.2733, 2.2277, 1.6507, 1.6870,\n",
      "        1.3529, 2.0694, 1.6068, 2.3528, 2.1011, 2.3101, 2.1997, 1.8327, 1.8519,\n",
      "        1.8385, 1.7826, 1.8731, 2.4475, 2.6061, 1.9085, 1.7990, 2.0775, 2.4626,\n",
      "        2.1533, 2.2617, 2.4516, 2.3303, 1.9336, 2.2999, 2.7084, 2.2813, 2.1295,\n",
      "        2.7011, 2.8912, 3.2627, 2.6091, 2.0597, 1.8857, 2.2190, 2.5810, 3.0999,\n",
      "        3.1366, 3.6367, 2.1977, 2.1439, 2.1812, 2.0905, 1.8529, 2.2315, 3.7129,\n",
      "        3.8967, 2.2103, 2.2557, 2.6557, 2.3096, 2.0927, 2.0087, 3.6656, 3.6867,\n",
      "        2.9027, 1.8847, 1.6723, 2.2194, 2.2961, 2.3742, 2.8923, 2.8787, 1.7463,\n",
      "        1.7686, 2.1835, 1.9271, 1.9526, 2.7359, 2.7723, 3.0096, 2.0436, 1.9872,\n",
      "        2.2731, 2.8552, 2.3956, 2.3333, 3.2278, 2.8162, 2.2047, 2.0217, 2.1248,\n",
      "        1.9222, 2.8409, 2.7430, 2.7870, 2.3523], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0500,  0.0367, -0.1410,  ...,  0.1743, -0.0096,  0.2134],\n",
      "        [ 0.0673, -0.0120, -0.0263,  ...,  0.1661, -0.0988, -0.1892],\n",
      "        [-0.0827, -0.0236,  0.0409,  ..., -0.0563, -0.0041,  0.1286],\n",
      "        ...,\n",
      "        [ 0.0416, -0.1141, -0.0563,  ..., -0.0755, -0.1019, -0.0351],\n",
      "        [-0.0106,  0.0886,  0.1031,  ..., -0.1101,  0.0183, -0.0582],\n",
      "        [ 0.2231, -0.0744,  0.1027,  ..., -0.0649, -0.0113, -0.0082]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141, -0.0454,  0.0057,  ..., -0.0188,  0.0038,  0.0191],\n",
      "        [ 0.0571,  0.0436, -0.0234,  ...,  0.0247, -0.1909, -0.2112],\n",
      "        [ 0.0445,  0.0218,  0.1265,  ..., -0.0322, -0.0156, -0.1175],\n",
      "        ...,\n",
      "        [-0.0849, -0.1355,  0.1477,  ...,  0.0361,  0.0411,  0.1307],\n",
      "        [-0.1064, -0.1564, -0.2834,  ..., -0.1471, -0.0404,  0.2403],\n",
      "        [ 0.0911, -0.0271,  0.0647,  ...,  0.0427,  0.2251,  0.1965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0793, 2.5442, 2.3540, 1.9580, 2.0947, 2.9668, 3.0822, 3.2043, 2.0321,\n",
      "        1.8978, 2.1931, 1.7006, 1.8006, 2.9096, 2.9934, 3.1883, 1.7706, 1.6959,\n",
      "        1.7769, 1.4557, 1.4466, 1.5502, 2.4281, 2.1244, 1.8274, 2.3831, 2.1472,\n",
      "        2.7797, 2.7033, 2.7936, 2.1874, 2.0652, 2.2348, 1.7275, 2.1146, 2.4233,\n",
      "        1.8946, 1.4516, 2.4257, 1.8548, 2.1352, 1.9024, 1.8633, 1.5362, 1.6844,\n",
      "        2.5594, 2.3211, 2.1430, 1.2303, 2.1816, 1.9798, 1.3834, 2.5260, 1.4670,\n",
      "        2.2720, 1.9820, 1.5280, 1.3478, 1.5541, 2.1650, 1.2909, 2.4384, 2.0820,\n",
      "        2.0018, 2.0357, 2.1039, 1.9218, 2.1927, 1.7991, 1.9380, 2.2556, 2.1697,\n",
      "        2.2817, 2.2250, 1.9968, 1.7323, 1.9625, 1.9532, 2.3242, 1.8753, 1.7879,\n",
      "        2.1644, 2.4467, 2.0333, 2.0129, 1.7532, 2.1370, 2.1353, 1.8837, 1.5389,\n",
      "        1.9164, 2.0265, 1.9623, 2.4082, 2.4402, 2.5236, 1.8690, 1.8526, 1.9993,\n",
      "        2.8850, 3.0240, 2.9510, 2.7143, 2.7029, 2.3952, 2.2387, 2.0162, 1.8027,\n",
      "        1.5500, 1.6638, 2.5698, 2.1683, 2.0655, 1.8332, 2.2394, 2.8401, 3.2795,\n",
      "        1.6035, 2.2678, 2.4907, 2.4999, 2.0698, 1.5869, 1.5862, 1.3818, 2.7155,\n",
      "        2.3508, 2.1278, 1.8412, 1.7707, 2.1987, 1.5185, 1.6337, 1.8006, 2.2282,\n",
      "        2.1152, 1.2382, 1.7002, 1.7470, 2.0186, 2.0072, 2.1672, 2.3772, 2.1212,\n",
      "        2.1688, 1.8210, 2.2809, 1.4070, 1.3524, 1.6107, 2.2615, 2.0743, 1.8620,\n",
      "        1.5988, 1.6275, 2.4134, 2.7282, 2.2205, 2.3348, 1.9452, 1.9873, 2.0682,\n",
      "        2.4391, 2.0440, 2.3092, 2.2915, 2.1585, 2.5369, 1.5104, 2.1322, 2.2597,\n",
      "        2.3189, 1.9889, 2.1624, 2.4252, 2.1554, 2.2700, 2.0741, 1.9190, 1.9291,\n",
      "        1.6265, 1.6005, 2.7915, 2.7372, 2.2100, 2.3545, 2.2805, 2.4966, 2.3441,\n",
      "        2.5899, 2.6791, 2.6998, 2.1339, 1.7781, 2.0061, 2.1823, 2.4102, 1.5506,\n",
      "        2.1535, 2.1468, 1.9080, 2.0525, 1.9541, 1.3706, 1.3432, 2.1246, 2.5483,\n",
      "        2.3490, 2.0671, 2.2180, 2.1230, 2.3011, 2.0029, 1.9655, 2.6712, 2.7159,\n",
      "        2.1323, 2.1638, 2.0403, 1.9926, 1.7621, 1.9377, 2.6548, 2.7543, 1.4470,\n",
      "        1.7856, 1.3633, 2.3535, 1.2309, 2.3134, 2.1170, 2.1004, 1.6081, 1.5316,\n",
      "        2.4258, 1.3032, 2.4427, 1.3130, 1.9826, 1.8881, 1.9640, 1.8179, 1.7906,\n",
      "        2.1343, 2.4397, 2.5613, 2.2650, 2.1448, 2.0950, 1.7971, 1.9852, 1.5358,\n",
      "        1.3759, 1.4370, 2.3595, 1.9340, 1.9675, 1.9765, 1.7728, 1.7976, 2.8042,\n",
      "        2.3958, 2.6748, 2.1281, 2.2027, 2.0739, 2.3718, 2.4037, 1.5837, 2.2386,\n",
      "        2.6574, 2.1748, 2.0852, 2.1502, 2.0357, 2.1396, 1.9317, 2.2147, 3.0343,\n",
      "        3.0321, 2.3202, 2.3938, 2.5252, 2.1948, 2.0657, 1.9904, 3.0063, 3.0462,\n",
      "        2.0403, 1.9066, 2.4615, 1.7370, 1.3764, 2.3821, 2.3170, 2.2560, 2.1020,\n",
      "        1.7692, 1.5508, 1.6553, 2.5132, 1.6593, 2.2081, 2.2577, 2.2588, 1.8993,\n",
      "        2.1746, 2.2868, 1.9393, 2.3400, 2.3158, 2.7784, 1.7887, 2.0533, 2.1199,\n",
      "        2.2876, 2.3033, 2.2717, 2.8608, 2.3375], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0288, -0.0201, -0.0456,  ...,  0.1469, -0.0759, -0.0684],\n",
      "        [-0.0304, -0.0814,  0.0367,  ..., -0.0041,  0.0160,  0.0169],\n",
      "        [-0.0677,  0.1499,  0.0201,  ..., -0.0365, -0.1146, -0.0570],\n",
      "        ...,\n",
      "        [-0.0368, -0.0433,  0.0684,  ..., -0.0629, -0.0465,  0.0074],\n",
      "        [-0.0696, -0.0594,  0.0368,  ...,  0.0569, -0.0507,  0.0033],\n",
      "        [-0.0476,  0.0658, -0.0835,  ..., -0.0380,  0.0564, -0.0148]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.1009,  0.0422,  0.0744,  ..., -0.0690,  0.0039,  0.0293],\n",
      "        [ 0.0631, -0.2107, -0.0640,  ..., -0.0489, -0.0206,  0.0946],\n",
      "        [ 0.1240,  0.1947, -0.0428,  ...,  0.0176,  0.2068,  0.0217],\n",
      "        ...,\n",
      "        [ 0.0440,  0.0606, -0.0811,  ...,  0.0092,  0.0140, -0.0459],\n",
      "        [-0.1132,  0.0147,  0.0120,  ..., -0.0290,  0.0403,  0.0715],\n",
      "        [ 0.1267, -0.0766, -0.0246,  ..., -0.0502, -0.0462,  0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5237, 1.6380, 1.5909, 1.5373, 1.6410, 1.6303, 1.5066, 1.5455, 1.5264,\n",
      "        1.5900, 1.5219, 1.5666, 1.5115, 1.5389, 1.5558, 1.5440, 1.1689, 1.2181,\n",
      "        1.2120, 1.1211, 1.2291, 1.1781, 1.2347, 1.1810, 1.2587, 1.1774, 1.2028,\n",
      "        1.1693, 1.1764, 1.1862, 1.1704, 1.2264, 1.6736, 1.4945, 1.6123, 1.6292,\n",
      "        1.6166, 1.5468, 1.6582, 1.6193, 1.6017, 1.5828, 1.6127, 1.6441, 1.6247,\n",
      "        1.6412, 1.6417, 1.6128, 1.2785, 1.1570, 1.2661, 1.2270, 1.1938, 1.2148,\n",
      "        1.2249, 1.2382, 1.2601, 1.2769, 1.1751, 1.2436, 1.2156, 1.2205, 1.2555,\n",
      "        1.2170, 1.8320, 1.8457, 1.8351, 1.7394, 1.8790, 1.8049, 1.8405, 1.9301,\n",
      "        1.8848, 1.9098, 1.8283, 1.9672, 1.8562, 1.9477, 1.8806, 1.9459, 1.7929,\n",
      "        1.7863, 1.8406, 1.7410, 1.7440, 1.8983, 1.7523, 1.7559, 1.8501, 1.6784,\n",
      "        1.7680, 1.8315, 1.7797, 1.8618, 1.8402, 1.8362, 1.6506, 1.7759, 1.6428,\n",
      "        1.6796, 1.7780, 1.6657, 1.6824, 1.7052, 1.7319, 1.6564, 1.6668, 1.6512,\n",
      "        1.6643, 1.7300, 1.6339, 1.6688, 1.6180, 1.5663, 1.6039, 1.5467, 1.5501,\n",
      "        1.5765, 1.6244, 1.5800, 1.5934, 1.5740, 1.5941, 1.5254, 1.6167, 1.5758,\n",
      "        1.5669, 1.6327, 1.2452, 1.1876, 1.2376, 1.2357, 1.2244, 1.1834, 1.1787,\n",
      "        1.1754, 1.2060, 1.1574, 1.1616, 1.1757, 1.2127, 1.1926, 1.1651, 1.2091,\n",
      "        1.5834, 1.6092, 1.5880, 1.5917, 1.5367, 1.5804, 1.5876, 1.5490, 1.5769,\n",
      "        1.6101, 1.6322, 1.6483, 1.5669, 1.6292, 1.6192, 1.6547, 1.7856, 1.8543,\n",
      "        1.7574, 1.8590, 1.8113, 1.7640, 1.7554, 1.8429, 1.7273, 1.8047, 1.6980,\n",
      "        1.8044, 1.6846, 1.8038, 1.6839, 1.7869, 1.7251, 1.7426, 1.6602, 1.6381,\n",
      "        1.7714, 1.7228, 1.7506, 1.8162, 1.7294, 1.7619, 1.7026, 1.6313, 1.7297,\n",
      "        1.6644, 1.7589, 1.6497, 1.5183, 1.5592, 1.4837, 1.5197, 1.5177, 1.5479,\n",
      "        1.5514, 1.5530, 1.5441, 1.5787, 1.6513, 1.5932, 1.6224, 1.5820, 1.5263,\n",
      "        1.5076, 1.8011, 1.8360, 1.7349, 1.7691, 1.8147, 1.8794, 1.7126, 1.7388,\n",
      "        1.6939, 1.8265, 1.8655, 1.7353, 1.7816, 1.7472, 1.8009, 1.7903, 1.3405,\n",
      "        1.3327, 1.2976, 1.3638, 1.3436, 1.4620, 1.3607, 1.3682, 1.4538, 1.4819,\n",
      "        1.3863, 1.3462, 1.3702, 1.3104, 1.3549, 1.3847, 1.6643, 1.6504, 1.6803,\n",
      "        1.6373, 1.5873, 1.6462, 1.6596, 1.6927, 1.6154, 1.6603, 1.6730, 1.6267,\n",
      "        1.7073, 1.6567, 1.7175, 1.6536, 1.2674, 1.2777, 1.2885, 1.2327, 1.2414,\n",
      "        1.2075, 1.2551, 1.2171, 1.2736, 1.2409, 1.2649, 1.1671, 1.2889, 1.2685,\n",
      "        1.2630, 1.2291, 1.8232, 1.6227, 1.6922, 1.6783, 1.6564, 1.6695, 1.6106,\n",
      "        1.8201, 1.5714, 1.6413, 1.7176, 1.7646, 1.7243, 1.6156, 1.7404, 1.7740,\n",
      "        1.3033, 1.2989, 1.2630, 1.2814, 1.2807, 1.3029, 1.3530, 1.2194, 1.2728,\n",
      "        1.3257, 1.2967, 1.3116, 1.2969, 1.2997, 1.2598, 1.3316, 1.4795, 1.5311,\n",
      "        1.5504, 1.4317, 1.5504, 1.5279, 1.4412, 1.4876, 1.5327, 1.4673, 1.5108,\n",
      "        1.4187, 1.4377, 1.5406, 1.4656, 1.6510], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0107,  0.0294, -0.0998,  ...,  0.0848, -0.0658,  0.2637],\n",
      "        [ 0.0298, -0.2141,  0.0243,  ...,  0.1150,  0.0791,  0.0398],\n",
      "        [ 0.1201, -0.0364, -0.0535,  ..., -0.0532, -0.0891,  0.1107],\n",
      "        ...,\n",
      "        [-0.0448,  0.1025,  0.0266,  ..., -0.0527, -0.0337, -0.0475],\n",
      "        [ 0.0427,  0.0838, -0.0343,  ..., -0.0642, -0.0347, -0.0093],\n",
      "        [-0.0318, -0.0439,  0.0419,  ...,  0.1552, -0.1432,  0.0283]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0852,  0.0132,  0.0308,  ..., -0.0494, -0.0430,  0.1691],\n",
      "        [-0.0361,  0.1662,  0.1051,  ..., -0.0820,  0.0795, -0.1699],\n",
      "        [ 0.0602,  0.0065,  0.0425,  ...,  0.0193, -0.0457,  0.0358],\n",
      "        ...,\n",
      "        [ 0.0220, -0.1076, -0.0351,  ...,  0.0743,  0.1189, -0.0309],\n",
      "        [ 0.0220, -0.1038,  0.0549,  ...,  0.0006, -0.0522,  0.0158],\n",
      "        [ 0.0709, -0.1168, -0.0378,  ..., -0.2090, -0.0591,  0.0829]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4847, 1.5854, 1.5281, 1.5112, 1.6205, 1.3933, 1.4293, 1.5220, 1.4344,\n",
      "        1.4322, 1.4330, 1.5356, 1.5584, 1.4024, 1.6372, 1.4345, 1.3345, 1.5364,\n",
      "        1.4364, 1.4626, 1.5345, 1.4153, 1.6026, 1.4375, 1.4696, 1.4832, 1.4734,\n",
      "        1.5345, 1.4947, 1.4880, 1.4454, 1.4259, 1.6303, 1.4785, 1.4093, 1.4467,\n",
      "        1.6177, 1.4142, 1.3959, 1.4692, 1.6344, 1.4974, 1.4720, 1.4923, 1.3762,\n",
      "        1.4122, 1.3920, 1.6090, 1.5128, 1.5003, 1.4581, 1.5089, 1.3121, 1.3747,\n",
      "        1.3892, 1.3606, 1.3440, 1.4343, 1.6283, 1.5520, 1.3893, 1.6759, 2.7844,\n",
      "        1.4561, 1.4414, 1.4795, 1.4250, 1.5668, 1.5379, 1.5108, 1.3348, 1.4412,\n",
      "        1.4062, 1.4302, 1.4888, 1.5939, 1.4416, 1.5165, 1.4689, 1.5040, 1.4802,\n",
      "        1.5338, 1.5007, 1.5733, 1.4101, 1.4544, 1.4441, 1.5479, 1.2997, 1.5089,\n",
      "        1.4162, 1.5413, 1.5944, 1.5202, 1.4379, 1.5832, 1.4857, 1.5958, 1.5407,\n",
      "        1.5192, 1.5830, 1.5442, 1.6174, 1.4657, 1.5545, 1.6074, 1.4277, 1.4065,\n",
      "        1.4201, 1.4251, 1.5887, 1.3608, 1.4386, 1.4492, 1.5470, 1.4966, 1.5836,\n",
      "        1.4753, 1.4542, 1.4340, 1.4909, 1.5836, 1.5379, 1.5224, 1.3440, 1.5710,\n",
      "        1.5226, 1.5317, 1.4819, 1.4899, 1.3567, 1.3921, 1.5082, 1.5816, 1.5643,\n",
      "        1.4018, 1.4542, 1.4932, 1.3423, 1.3792, 1.4234, 1.4151, 1.4774, 1.4146,\n",
      "        1.4055, 1.4890, 1.4656, 1.5486, 1.4744, 1.5527, 2.1154, 1.5793, 1.5607,\n",
      "        1.4953, 1.4764, 1.5145, 1.4657, 1.5617, 1.5575, 1.4825, 1.3502, 1.4525,\n",
      "        1.5726, 1.5779, 1.6583, 1.5692, 1.3804, 1.4436, 1.4081, 1.5196, 1.4325,\n",
      "        1.6387, 1.5334, 1.4450, 1.4886, 1.3273, 1.4974, 1.5533, 1.4687, 1.5539,\n",
      "        1.4422, 1.3911, 1.4604, 1.4915, 1.5441, 1.4027, 1.4639, 1.4745, 1.4918,\n",
      "        1.6241, 1.3499, 1.3536, 2.1478, 1.3057, 1.4600, 1.4698, 1.4978, 1.4253,\n",
      "        1.5231, 1.5591, 1.5982, 1.3092, 1.4334, 1.3613, 1.3678, 1.5513, 1.5309,\n",
      "        1.3400, 1.5387, 1.6581, 1.5580, 1.4863, 1.4336, 1.6080, 1.4570, 1.5346,\n",
      "        1.5540, 1.5155, 1.3515, 1.5483, 1.4277, 1.5748, 1.5334, 1.4417, 1.4742,\n",
      "        1.4410, 1.3977, 1.4992, 1.4367, 1.4513, 1.3513, 1.3961, 1.3730, 1.4724,\n",
      "        1.4606, 1.7224, 1.4556, 1.3458, 1.4811, 1.6254, 1.5191, 1.4462, 1.3226,\n",
      "        1.3853, 1.3522, 1.5353, 1.4177, 1.5309, 1.4781, 1.4179, 1.5149, 1.4159,\n",
      "        1.4979, 1.3967, 1.4373, 1.4593, 1.3772, 1.4508, 1.2212, 1.5037, 1.4556,\n",
      "        1.5011, 1.6198, 1.5268, 1.4054, 1.4593, 1.5355, 1.6802, 1.3931, 1.5964,\n",
      "        1.3429, 1.5394, 1.2852, 1.6051, 1.4286, 1.6176, 1.5341, 1.3497, 1.5703,\n",
      "        1.4810, 1.4809, 1.4689, 1.6007, 1.6133, 1.5608, 1.3981, 1.4171, 1.4585,\n",
      "        1.2427, 1.3542, 1.5318, 1.5489, 1.5062, 1.5158, 1.2936, 1.4616, 1.5901,\n",
      "        1.5646, 1.4796, 1.4929, 1.5041, 1.5503, 1.4835, 1.3523, 1.4823, 1.4329,\n",
      "        1.4578, 1.5483, 1.4357, 1.4534, 1.4646, 1.3408, 1.5548, 1.5341, 1.5688,\n",
      "        1.5005, 1.5777, 1.5705, 1.3821, 1.6774], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0948,  0.0598, -0.1138,  ..., -0.1099, -0.0847,  0.1816],\n",
      "        [ 0.1103, -0.1547, -0.0565,  ...,  0.1228, -0.0392, -0.0935],\n",
      "        [ 0.1795, -0.0441, -0.1028,  ..., -0.0675, -0.0260,  0.0195],\n",
      "        ...,\n",
      "        [-0.0545, -0.0109,  0.0828,  ..., -0.0864,  0.1112,  0.0080],\n",
      "        [ 0.0687,  0.0949,  0.0336,  ...,  0.0302, -0.0092, -0.0079],\n",
      "        [ 0.0093,  0.0754, -0.1319,  ...,  0.0140,  0.0235,  0.0426]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0682, -0.0174, -0.0538,  ..., -0.0117,  0.0471,  0.0644],\n",
      "        [-0.1031, -0.0088, -0.0234,  ..., -0.0370,  0.0062, -0.0071],\n",
      "        [ 0.0909, -0.0037, -0.1149,  ...,  0.0268, -0.0139,  0.0319],\n",
      "        ...,\n",
      "        [-0.1392, -0.0707,  0.0049,  ...,  0.0538, -0.0339,  0.0174],\n",
      "        [-0.0306, -0.0796, -0.1533,  ...,  0.0644, -0.0339, -0.0305],\n",
      "        [ 0.1289,  0.1313, -0.0153,  ..., -0.0077, -0.0347, -0.0208]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6552, 1.4334, 1.4333, 1.3825, 1.4168, 2.3624, 2.0710, 2.5610, 1.8070,\n",
      "        1.4649, 1.3482, 1.3601, 1.5074, 1.2783, 2.5928, 2.0364, 1.1000, 1.1304,\n",
      "        1.4394, 1.9744, 2.4227, 4.6512, 4.2595, 3.4382, 1.0111, 1.2298, 1.2887,\n",
      "        2.0301, 2.6273, 4.4372, 4.2133, 3.6077, 1.4863, 1.5198, 1.7810, 1.7904,\n",
      "        1.8810, 1.5664, 2.2300, 1.8756, 1.4852, 1.5696, 2.1647, 1.8271, 1.8696,\n",
      "        1.7276, 2.2294, 2.2891, 1.5805, 1.9161, 1.9421, 1.8423, 2.0988, 1.7557,\n",
      "        2.3040, 2.3112, 1.5298, 1.9603, 1.8596, 1.9622, 2.2476, 2.5642, 2.5681,\n",
      "        2.3505, 2.1444, 1.7776, 2.3910, 2.2577, 2.2775, 2.1773, 2.7792, 2.3904,\n",
      "        1.4821, 1.8719, 2.1337, 2.0356, 2.0097, 2.5207, 2.6754, 2.4400, 2.2276,\n",
      "        1.8581, 1.4236, 1.7332, 1.6358, 2.0797, 2.3105, 2.2091, 2.1493, 1.8424,\n",
      "        1.7077, 1.6115, 2.4484, 2.4536, 2.1115, 2.2423, 1.2833, 1.4388, 1.4090,\n",
      "        1.3854, 1.4278, 2.3062, 1.9912, 2.5803, 1.4052, 1.5144, 1.4000, 1.2931,\n",
      "        1.4113, 1.4254, 2.0248, 2.1956, 2.0416, 1.6295, 1.5176, 1.5757, 1.2364,\n",
      "        2.0242, 2.1945, 2.1233, 1.9329, 1.3427, 1.3791, 1.5646, 2.0294, 1.5717,\n",
      "        1.9742, 2.5319, 1.8474, 1.3917, 1.4993, 1.4128, 1.4397, 2.0798, 1.9736,\n",
      "        2.5238, 1.4915, 1.6802, 1.5838, 1.4165, 1.9504, 1.6635, 2.0834, 2.4967,\n",
      "        1.7435, 1.6351, 2.1095, 1.9533, 2.2135, 1.8848, 2.4154, 2.1751, 2.4308,\n",
      "        1.9679, 1.7467, 2.2406, 1.7439, 2.4684, 2.5535, 2.6872, 1.9586, 1.7359,\n",
      "        2.0742, 2.0076, 2.4434, 2.5920, 2.9749, 2.8062, 1.6446, 1.9224, 1.9818,\n",
      "        2.2306, 2.0848, 2.5432, 2.7834, 2.7860, 2.0137, 1.5935, 1.7468, 2.1187,\n",
      "        1.7261, 1.9407, 2.3628, 2.5211, 1.7424, 1.7850, 2.0001, 1.8485, 1.7472,\n",
      "        1.7776, 2.2005, 2.4120, 1.5175, 1.5551, 1.4119, 1.4831, 1.8009, 2.2293,\n",
      "        2.0382, 2.7584, 1.6488, 1.5253, 1.7321, 1.6944, 1.7926, 1.7162, 1.9397,\n",
      "        2.3349, 1.1478, 1.1457, 1.4768, 1.7365, 2.5167, 3.6357, 3.7704, 3.2782,\n",
      "        0.8232, 1.1941, 1.2854, 2.0714, 2.2399, 3.7320, 3.5234, 3.3439, 1.6643,\n",
      "        1.9483, 1.9040, 2.0350, 1.9639, 2.1692, 2.4454, 2.3579, 1.9448, 1.8434,\n",
      "        1.9090, 2.0098, 1.9273, 1.9125, 2.1907, 2.4258, 1.8691, 1.4603, 1.7656,\n",
      "        1.8131, 1.9404, 1.6375, 2.3291, 2.0268, 1.4493, 1.7165, 1.8529, 1.7386,\n",
      "        1.9589, 1.6711, 2.3684, 2.3801, 1.3841, 1.3492, 1.7807, 1.7038, 1.6484,\n",
      "        1.7553, 2.4681, 2.1278, 1.3672, 1.5706, 1.7322, 1.7799, 1.6928, 1.9181,\n",
      "        2.0598, 2.0556, 1.4114, 1.4788, 1.8002, 1.6682, 1.6220, 1.8274, 2.2851,\n",
      "        2.3713, 1.4840, 1.4003, 1.4672, 1.6401, 1.5482, 1.7767, 2.5248, 2.5006,\n",
      "        1.8956, 1.7507, 1.7005, 1.4926, 1.6595, 1.8658, 2.3257, 2.2957, 1.6053,\n",
      "        1.4602, 1.6022, 1.8346, 1.5352, 1.9717, 2.2161, 2.4296, 1.5886, 1.1851,\n",
      "        1.2203, 1.4174, 1.3647, 1.8373, 2.0729, 2.3338, 1.4978, 1.3226, 1.4443,\n",
      "        1.6841, 1.6486, 1.7464, 1.9548, 2.0678], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1013,  0.0492,  0.0783,  ...,  0.0711,  0.1158, -0.0316],\n",
      "        [ 0.0913, -0.0953, -0.0129,  ...,  0.0360,  0.0673, -0.0089],\n",
      "        [ 0.0834, -0.0333,  0.0348,  ...,  0.1749,  0.0360, -0.1717],\n",
      "        ...,\n",
      "        [ 0.0136,  0.0785,  0.0348,  ..., -0.0383,  0.0202, -0.0574],\n",
      "        [-0.0494,  0.1110, -0.0394,  ..., -0.0033, -0.0134, -0.0389],\n",
      "        [ 0.1508, -0.1824, -0.0611,  ...,  0.0301,  0.0138, -0.0965]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-2.2536e-03,  1.6058e-02, -5.2478e-02,  ...,  2.3720e-02,\n",
      "          4.3420e-02,  3.8817e-02],\n",
      "        [-4.3882e-02, -9.3136e-03, -5.4000e-02,  ..., -2.1725e-02,\n",
      "         -5.7059e-03,  2.7946e-02],\n",
      "        [-3.8626e-02,  9.5391e-02, -4.3173e-02,  ...,  2.4294e-02,\n",
      "          3.1823e-02,  1.7045e-02],\n",
      "        ...,\n",
      "        [ 1.5301e-03, -2.2620e-02,  2.9410e-02,  ...,  7.5472e-02,\n",
      "         -1.1191e-02, -2.1986e-02],\n",
      "        [-1.9202e-01, -2.1342e-02, -8.4856e-02,  ...,  3.0970e-02,\n",
      "         -2.9097e-05, -5.5118e-02],\n",
      "        [ 2.5200e-01, -2.2403e-02, -1.2297e-02,  ...,  2.3491e-02,\n",
      "         -7.7852e-03, -2.0245e-02]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6889, 1.4271, 1.7430, 1.6628, 1.6534, 1.2899, 1.8223, 1.7646, 1.8019,\n",
      "        1.7449, 1.6390, 1.7138, 1.6659, 2.3107, 1.6661, 1.7704, 1.1473, 1.3915,\n",
      "        1.3758, 2.0068, 1.9256, 3.2060, 2.9245, 2.7808, 1.1138, 0.9388, 1.4358,\n",
      "        1.8676, 2.2222, 3.0254, 3.0221, 2.7178, 1.4727, 1.5844, 1.9981, 1.8033,\n",
      "        1.9005, 2.2584, 1.9988, 1.9343, 1.4264, 1.4540, 1.8902, 1.8841, 1.8554,\n",
      "        1.6541, 2.1052, 2.1458, 1.6813, 2.0848, 1.8382, 1.7060, 1.8021, 2.2735,\n",
      "        2.2901, 2.0180, 1.4035, 1.6622, 1.9498, 1.9802, 1.8883, 1.5377, 2.1773,\n",
      "        2.0368, 1.8880, 1.7127, 2.0528, 2.0940, 1.8528, 2.0759, 2.4393, 2.1081,\n",
      "        1.7533, 1.9557, 2.2812, 2.0530, 2.0005, 1.8960, 2.5726, 2.3136, 1.7890,\n",
      "        1.8889, 1.7278, 1.7179, 2.0000, 1.9749, 2.0090, 2.0776, 2.2909, 1.8528,\n",
      "        1.9292, 1.8684, 1.3181, 1.4653, 2.0984, 1.9083, 1.4736, 1.5217, 1.7262,\n",
      "        1.9033, 1.4419, 1.2665, 1.8880, 1.5187, 1.3202, 1.7243, 1.6979, 1.5558,\n",
      "        1.7915, 1.9450, 1.8671, 1.6432, 1.8415, 1.6661, 1.7453, 1.7020, 1.9645,\n",
      "        1.4424, 1.9818, 2.0917, 2.0265, 1.7816, 1.7390, 1.8201, 1.3807, 2.2870,\n",
      "        2.1284, 1.7866, 1.8202, 1.6078, 1.6488, 1.6817, 1.8529, 1.3601, 1.9022,\n",
      "        1.8078, 1.4124, 1.6446, 1.8692, 1.7385, 1.3284, 1.9982, 2.0476, 1.8216,\n",
      "        2.0397, 1.7566, 1.9442, 2.0140, 1.7768, 2.1294, 1.9944, 1.9886, 1.5791,\n",
      "        1.7224, 1.7992, 1.9633, 2.0390, 1.8915, 2.2578, 2.2700, 1.9004, 1.8885,\n",
      "        1.8815, 2.0661, 2.0135, 1.9570, 2.7777, 2.6218, 1.6407, 1.7246, 2.1530,\n",
      "        2.0494, 2.0387, 2.2814, 2.5121, 2.3586, 1.7866, 1.5010, 1.9422, 1.9570,\n",
      "        1.9640, 2.0323, 2.4325, 2.4010, 1.8795, 1.8733, 1.7822, 2.0121, 1.7464,\n",
      "        1.7709, 2.1043, 2.2803, 1.7662, 1.6333, 1.7373, 1.8647, 1.5324, 1.2077,\n",
      "        1.9634, 1.8604, 1.9159, 1.7432, 1.5445, 1.3137, 1.3502, 2.2007, 1.9211,\n",
      "        1.9771, 1.1066, 1.2730, 1.5260, 1.9362, 2.1939, 2.7678, 2.8119, 2.5205,\n",
      "        0.8824, 1.1725, 1.1172, 1.7185, 1.8117, 2.8365, 2.5728, 2.6282, 1.7312,\n",
      "        1.8346, 1.9135, 1.8828, 1.6636, 1.7983, 2.2877, 2.2648, 1.7312, 1.8848,\n",
      "        1.8329, 2.0321, 1.8790, 1.7116, 1.9831, 2.1841, 1.7704, 1.5616, 1.7671,\n",
      "        1.8773, 1.8690, 1.9465, 2.1781, 1.9954, 1.3467, 1.6612, 1.8440, 1.7460,\n",
      "        1.9353, 1.8570, 2.2530, 2.3184, 1.2619, 1.5797, 1.8240, 1.8519, 1.7561,\n",
      "        1.8521, 1.9934, 2.1386, 1.5619, 1.4646, 1.7888, 1.8055, 1.9259, 1.7539,\n",
      "        2.5143, 1.9833, 1.2787, 1.3773, 1.7145, 1.7669, 1.6839, 1.7099, 2.1194,\n",
      "        1.8643, 1.0018, 1.3337, 1.6767, 1.9264, 1.7412, 1.7326, 1.9674, 1.9704,\n",
      "        1.8140, 1.7674, 1.7366, 1.7669, 1.7734, 1.7101, 2.3058, 2.0764, 1.6758,\n",
      "        1.6545, 1.7996, 1.8239, 1.7054, 1.6117, 2.1010, 2.2289, 1.4085, 1.5210,\n",
      "        1.4740, 1.3472, 1.8246, 1.2400, 1.4690, 1.5208, 1.7546, 1.4491, 1.4017,\n",
      "        1.2756, 1.1156, 1.2434, 1.6302, 1.5776], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.3299,  0.0627,  0.0467,  ..., -0.0733,  0.0088,  0.0062],\n",
      "        [ 0.0204,  0.1075,  0.0079,  ...,  0.0863, -0.0849,  0.0835],\n",
      "        [-0.0494, -0.0335, -0.3117,  ...,  0.0966, -0.0074,  0.0293],\n",
      "        ...,\n",
      "        [ 0.0021, -0.0343, -0.0824,  ..., -0.0599,  0.1646,  0.0819],\n",
      "        [-0.0434, -0.0323,  0.0117,  ...,  0.0464, -0.1104,  0.1330],\n",
      "        [ 0.0307,  0.0402, -0.0509,  ..., -0.0433,  0.0360,  0.0036]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0920,  0.1429,  0.1498,  ...,  0.0309,  0.2395, -0.0015],\n",
      "        [-0.1530, -0.0796,  0.0126,  ..., -0.0116,  0.0841, -0.0048],\n",
      "        [-0.0298,  0.0040, -0.0071,  ...,  0.1056,  0.0821,  0.0290],\n",
      "        ...,\n",
      "        [-0.1107, -0.0566, -0.0015,  ..., -0.0197, -0.0874,  0.0378],\n",
      "        [-0.0951,  0.1062, -0.0328,  ..., -0.0955, -0.0743, -0.0265],\n",
      "        [-0.1280,  0.0642,  0.0136,  ...,  0.0097, -0.1531, -0.1130]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.9910, 1.7859, 1.7181, 1.8752, 1.9573, 1.7906, 1.7669, 1.8068, 1.7691,\n",
      "        1.9023, 1.7144, 1.8018, 1.7877, 1.8022, 1.9825, 1.8459, 1.5629, 1.4503,\n",
      "        1.5721, 1.4834, 1.5047, 1.4777, 1.4249, 1.4403, 1.4609, 1.4187, 1.4352,\n",
      "        1.4818, 1.5416, 1.4619, 1.4833, 1.4405, 1.7743, 1.8952, 1.8282, 1.7398,\n",
      "        1.8591, 1.7324, 1.8634, 1.6844, 1.8550, 1.9357, 1.7557, 1.7681, 1.7066,\n",
      "        1.7661, 1.8067, 1.8372, 1.7756, 1.7685, 2.2151, 1.9220, 1.8615, 1.8839,\n",
      "        1.8420, 1.8436, 1.9874, 1.9857, 1.7812, 1.8052, 2.0072, 2.1669, 1.8810,\n",
      "        1.8712, 2.1752, 2.0793, 1.7751, 2.3139, 1.8010, 2.3051, 1.7924, 1.9537,\n",
      "        1.9208, 1.9720, 1.9665, 1.9238, 1.8328, 1.9078, 2.0223, 1.9621, 1.7919,\n",
      "        1.9148, 1.7307, 1.7685, 1.7281, 1.7405, 1.7336, 1.7506, 1.8781, 1.7557,\n",
      "        1.7685, 1.6618, 1.7410, 1.7996, 1.7391, 1.7953, 1.7625, 1.8009, 1.8637,\n",
      "        1.8832, 1.8602, 1.8920, 1.7451, 1.9282, 1.9451, 1.8833, 1.9162, 1.8563,\n",
      "        1.9071, 1.9149, 1.9026, 1.8479, 1.8409, 1.8171, 1.7042, 1.8710, 1.8583,\n",
      "        1.8760, 1.7786, 1.8398, 1.7976, 1.8047, 1.8330, 1.7827, 1.8125, 1.7038,\n",
      "        1.7855, 1.8503, 1.8570, 1.8230, 1.7993, 1.6955, 1.8503, 1.7523, 1.7433,\n",
      "        1.9098, 1.7231, 1.8333, 1.7466, 1.7698, 1.7607, 1.8340, 1.9074, 1.8020,\n",
      "        1.6698, 1.6084, 1.7089, 1.6531, 1.7456, 1.6656, 1.7196, 1.6576, 1.7034,\n",
      "        1.6911, 1.7480, 1.7944, 1.7231, 1.7571, 1.6898, 1.7747, 1.8855, 1.8563,\n",
      "        1.8743, 1.8496, 2.0436, 2.0675, 1.8706, 1.8486, 1.9140, 1.8655, 1.9353,\n",
      "        1.9089, 1.9487, 1.8241, 1.9167, 1.9539, 1.9157, 2.0385, 1.8803, 1.9441,\n",
      "        2.0407, 1.8543, 1.9363, 1.9550, 1.9103, 1.9249, 1.9537, 1.8689, 1.9965,\n",
      "        1.9404, 1.9471, 1.9209, 1.3920, 1.4053, 1.3236, 1.4026, 1.2973, 1.3371,\n",
      "        1.4223, 1.3296, 1.3952, 1.3427, 1.5071, 1.4203, 1.4299, 1.4072, 1.3403,\n",
      "        1.3813, 1.5942, 1.5461, 1.4136, 1.5487, 1.4926, 1.5218, 1.5217, 1.4937,\n",
      "        1.4831, 1.3992, 1.4020, 1.5040, 1.4087, 1.4507, 1.4973, 1.4373, 2.1330,\n",
      "        1.9655, 2.0132, 1.9046, 2.1477, 1.9870, 1.8481, 1.9034, 1.8565, 1.9019,\n",
      "        1.8099, 2.1112, 2.1387, 1.9774, 1.9240, 1.8725, 1.7300, 1.7895, 1.7128,\n",
      "        1.7626, 1.7251, 1.7140, 1.8034, 1.7685, 1.7610, 1.7741, 1.8405, 1.6437,\n",
      "        1.7217, 1.7438, 1.7286, 1.8240, 1.8331, 1.7401, 1.8251, 1.7745, 1.7199,\n",
      "        1.6723, 1.7602, 1.8064, 1.7394, 1.7077, 1.7129, 1.8042, 1.8022, 1.7928,\n",
      "        1.6652, 1.7366, 1.8956, 1.9773, 1.8821, 2.0162, 1.8606, 1.8993, 1.9118,\n",
      "        1.8570, 1.7689, 1.8904, 1.8602, 1.8927, 1.9566, 2.0228, 1.9236, 1.9475,\n",
      "        1.9208, 1.8269, 1.8951, 1.8767, 1.9128, 2.0383, 1.8567, 1.9909, 1.8191,\n",
      "        1.9111, 1.9571, 1.8844, 1.9293, 1.9161, 1.8280, 1.9185, 1.7560, 1.7776,\n",
      "        1.8200, 1.6379, 2.0614, 1.7395, 1.6892, 1.8435, 1.8097, 1.7288, 1.8897,\n",
      "        1.6953, 1.7442, 1.7315, 1.7611, 1.8561], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.2656, -0.0917,  0.0655,  ..., -0.0740, -0.1863,  0.1651],\n",
      "        [-0.0688,  0.0296,  0.0274,  ..., -0.1426,  0.0641,  0.0861],\n",
      "        [-0.0090, -0.0202,  0.0656,  ...,  0.0323,  0.0653, -0.0444],\n",
      "        ...,\n",
      "        [-0.0234,  0.0651,  0.1240,  ..., -0.0122,  0.1006, -0.0528],\n",
      "        [ 0.0212, -0.0394, -0.0730,  ..., -0.0338, -0.0896, -0.0396],\n",
      "        [-0.1534, -0.0358,  0.0791,  ...,  0.0288,  0.0590, -0.0004]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 1.0284e-02,  3.0753e-03, -3.8465e-02,  ...,  5.6222e-02,\n",
      "          8.0556e-02,  4.7121e-02],\n",
      "        [-6.1613e-02, -1.4256e-01, -2.7052e-02,  ..., -1.4203e-02,\n",
      "         -1.4489e-02,  8.0996e-02],\n",
      "        [ 3.5126e-02, -8.7188e-02,  3.3745e-03,  ...,  5.1317e-02,\n",
      "         -5.1557e-02, -1.4658e-02],\n",
      "        ...,\n",
      "        [-1.0158e-02,  2.5906e-02,  6.1291e-03,  ..., -7.0126e-02,\n",
      "          6.1601e-02,  1.5138e-04],\n",
      "        [ 1.0797e-02, -1.5308e-01, -1.5853e-01,  ...,  1.2545e-01,\n",
      "         -9.3582e-02, -5.8525e-02],\n",
      "        [-1.5178e-03, -7.7993e-02,  2.2972e-02,  ..., -6.3244e-03,\n",
      "         -2.8605e-02,  1.5632e-01]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5955, 1.7774, 1.6411, 1.8565, 1.7713, 1.6490, 1.6777, 1.5707, 1.7964,\n",
      "        1.8246, 1.3758, 1.5962, 1.7137, 1.6887, 1.7496, 1.6856, 1.8570, 1.7961,\n",
      "        1.6562, 1.8484, 1.8222, 1.6783, 1.7030, 1.5851, 1.6452, 1.5345, 1.5934,\n",
      "        1.7014, 1.7638, 1.8334, 1.6760, 1.4294, 1.6589, 1.4353, 1.6987, 1.7667,\n",
      "        1.6623, 1.6831, 1.5934, 1.6539, 1.6826, 1.6921, 1.7623, 1.6043, 1.7830,\n",
      "        1.5819, 1.6015, 1.5426, 1.6916, 1.7561, 1.7366, 1.7634, 1.4993, 1.6625,\n",
      "        1.5742, 1.6977, 1.5489, 1.7028, 1.7906, 1.6264, 1.7208, 1.7474, 5.9713,\n",
      "        1.7458, 1.7300, 1.7038, 1.5947, 1.7354, 1.7214, 1.8418, 1.7113, 1.8144,\n",
      "        1.5569, 1.7767, 1.6403, 1.7845, 1.7405, 1.6785, 1.7531, 1.7531, 1.5310,\n",
      "        1.6492, 1.6948, 1.6424, 1.5773, 1.6855, 1.6548, 1.7195, 1.4733, 1.8451,\n",
      "        1.8094, 1.6022, 1.9097, 1.6485, 1.7164, 1.7464, 1.7125, 1.8256, 1.7713,\n",
      "        1.8616, 1.8821, 1.6654, 1.7550, 1.6193, 1.7847, 1.8291, 1.5213, 1.6521,\n",
      "        1.7260, 1.6569, 1.6522, 1.6711, 1.6025, 1.5468, 1.8519, 1.5736, 1.6658,\n",
      "        1.6979, 1.5867, 1.8101, 1.6108, 1.7021, 1.7064, 1.7419, 1.6317, 1.7966,\n",
      "        1.6672, 1.6655, 1.5100, 1.7696, 1.5108, 1.8478, 1.7174, 1.7817, 1.7482,\n",
      "        1.6621, 1.7180, 1.6757, 1.5812, 1.7535, 1.5516, 1.6415, 1.7088, 1.6439,\n",
      "        1.6568, 1.8728, 1.6958, 1.6390, 1.6714, 1.6700, 1.9461, 1.7414, 1.9013,\n",
      "        1.7572, 1.8281, 1.6806, 1.5423, 1.7415, 1.9297, 1.6975, 1.6655, 1.7206,\n",
      "        1.6213, 1.7000, 1.7465, 1.6649, 1.6983, 1.7315, 1.7622, 1.7193, 1.5828,\n",
      "        1.7184, 1.6800, 1.6066, 1.7018, 1.7054, 1.5293, 1.7079, 1.7337, 1.6082,\n",
      "        1.6103, 1.6586, 1.6661, 1.7270, 1.6324, 1.6296, 1.5839, 1.6382, 1.6109,\n",
      "        1.6628, 1.6732, 1.6020, 2.5911, 1.5976, 1.4699, 1.6618, 1.6716, 1.7346,\n",
      "        1.6751, 1.8661, 1.7660, 1.7797, 1.7193, 1.5756, 1.8248, 1.6417, 1.7320,\n",
      "        1.3543, 1.6818, 1.6336, 1.6644, 1.6058, 1.6006, 1.7863, 1.6508, 1.8074,\n",
      "        1.6615, 1.8319, 1.4196, 1.5631, 1.5192, 1.6476, 1.6225, 1.7474, 1.7357,\n",
      "        1.6482, 1.6599, 1.5845, 1.6166, 1.6875, 1.6143, 1.5869, 1.6801, 1.7017,\n",
      "        1.5875, 1.6446, 1.6187, 1.5729, 1.7308, 1.7823, 1.5599, 1.6769, 1.6657,\n",
      "        1.6443, 1.5579, 1.8099, 1.6075, 1.6483, 1.7434, 1.7089, 1.4043, 1.6873,\n",
      "        1.7122, 1.5987, 1.6250, 1.7545, 1.7020, 1.5411, 1.4943, 1.7379, 1.6249,\n",
      "        1.6571, 1.7278, 1.6129, 1.6526, 1.7350, 1.7500, 2.0525, 1.5756, 1.6635,\n",
      "        1.5913, 1.7879, 1.6143, 1.6523, 1.8038, 1.6544, 1.7428, 1.5644, 1.7499,\n",
      "        1.8054, 1.6897, 1.6909, 1.6986, 1.7602, 1.7452, 1.7076, 1.8191, 1.6954,\n",
      "        1.4708, 1.6595, 1.6592, 1.8116, 1.7024, 1.7261, 1.4623, 1.7947, 1.7470,\n",
      "        1.6415, 1.5469, 1.8144, 1.7742, 1.6891, 1.6152, 1.5693, 1.6135, 1.6632,\n",
      "        1.7055, 1.6798, 1.5891, 1.6839, 1.6884, 1.4870, 1.6161, 1.6982, 1.7105,\n",
      "        1.7106, 1.7921, 1.6304, 1.6943, 1.6345], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0533,  0.0625,  0.1111,  ...,  0.0668, -0.0847,  0.0618],\n",
      "        [-0.1108,  0.1348, -0.0355,  ..., -0.1056, -0.1544,  0.0290],\n",
      "        [ 0.0574, -0.0030, -0.0396,  ...,  0.0564, -0.1296, -0.0356],\n",
      "        ...,\n",
      "        [-0.0012,  0.0894,  0.1241,  ..., -0.0101, -0.0140, -0.0335],\n",
      "        [ 0.0125,  0.1294,  0.0398,  ...,  0.1606, -0.0676, -0.0041],\n",
      "        [ 0.0019,  0.0132, -0.0200,  ..., -0.0212,  0.0234,  0.0203]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0204,  0.0916, -0.0315,  ..., -0.0114, -0.0107,  0.0267],\n",
      "        [-0.0031,  0.0262,  0.0189,  ...,  0.0338, -0.0096, -0.0121],\n",
      "        [-0.0315,  0.0781, -0.0404,  ...,  0.1034, -0.0275,  0.0202],\n",
      "        ...,\n",
      "        [-0.0392, -0.0384,  0.0073,  ..., -0.0457, -0.0576,  0.0726],\n",
      "        [-0.0173,  0.0006,  0.1081,  ..., -0.0750, -0.0350,  0.1080],\n",
      "        [-0.0203, -0.0470, -0.0835,  ...,  0.1310,  0.0393,  0.0021]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.6348, 1.7580, 1.7600, 1.8987, 1.9073, 1.6733, 2.1742, 2.6053, 1.5479,\n",
      "        1.6980, 1.9652, 2.0713, 1.9909, 1.6580, 2.4569, 2.1450, 1.5354, 1.3555,\n",
      "        1.7000, 1.6704, 2.5079, 3.3870, 3.5893, 4.1186, 1.0323, 1.4894, 1.6665,\n",
      "        1.9888, 2.2814, 3.4984, 3.6289, 3.8487, 1.3646, 1.3216, 0.9955, 1.4157,\n",
      "        1.9318, 1.9584, 1.6417, 1.8326, 1.1083, 0.7401, 1.3810, 1.2650, 1.2892,\n",
      "        1.4124, 1.6810, 1.8815, 1.5584, 1.6734, 2.0461, 1.9555, 2.2413, 3.5153,\n",
      "        3.3176, 3.5692, 1.3070, 1.7736, 1.9741, 1.8774, 2.2711, 3.4681, 3.2261,\n",
      "        3.3209, 1.6160, 2.2388, 1.6578, 1.3719, 1.8077, 1.7066, 1.9959, 2.0373,\n",
      "        1.9473, 1.8989, 1.7558, 1.7326, 1.4083, 1.8565, 2.0642, 2.1348, 1.3966,\n",
      "        1.1396, 1.3960, 1.6090, 2.2096, 2.0403, 1.9043, 2.0704, 1.6033, 1.4882,\n",
      "        1.2290, 1.2406, 1.3760, 1.6762, 1.9929, 1.9341, 1.7229, 1.7364, 1.7947,\n",
      "        1.7437, 1.6513, 1.6086, 2.3612, 2.2471, 2.0393, 2.0386, 2.0181, 1.5519,\n",
      "        2.0385, 1.4618, 2.4900, 2.2070, 1.6887, 1.5130, 1.5078, 1.0071, 1.4091,\n",
      "        2.0931, 1.8544, 1.8918, 1.6560, 1.2705, 0.9788, 1.7273, 1.4331, 1.5869,\n",
      "        1.8908, 1.8766, 1.4347, 1.5045, 1.3517, 1.3812, 1.2570, 1.8235, 1.8054,\n",
      "        1.3317, 1.7539, 1.6626, 1.6450, 1.2972, 1.6370, 1.3301, 2.0044, 1.8913,\n",
      "        1.0565, 1.1074, 1.0726, 1.6159, 1.4091, 1.5235, 1.7917, 2.3176, 1.5910,\n",
      "        1.1359, 1.5162, 1.4123, 2.0761, 1.9423, 1.8269, 1.9580, 1.8525, 1.3937,\n",
      "        1.0158, 1.2760, 1.8018, 1.5525, 1.7616, 1.9545, 1.2163, 1.3962, 1.4957,\n",
      "        1.5052, 1.4415, 2.0264, 1.9120, 1.9618, 2.1249, 2.3317, 2.2198, 1.9617,\n",
      "        1.9704, 1.7355, 2.3967, 2.3548, 1.7853, 1.8450, 1.9076, 1.8547, 1.6410,\n",
      "        2.6102, 2.3785, 2.3255, 1.8591, 1.2332, 1.5919, 1.2847, 1.9284, 2.0518,\n",
      "        1.7799, 1.9947, 0.9066, 1.3266, 1.0756, 1.7669, 1.4615, 1.4817, 1.7480,\n",
      "        2.0130, 2.0156, 1.7369, 1.5396, 1.5977, 1.7004, 1.8698, 1.9593, 2.2848,\n",
      "        1.7541, 2.0033, 2.0069, 1.8192, 1.6435, 2.1934, 1.5391, 2.2392, 1.0392,\n",
      "        1.2093, 1.6377, 1.9484, 1.8447, 1.4341, 1.8449, 1.8253, 1.6858, 1.0493,\n",
      "        1.1410, 1.2426, 1.3798, 2.0106, 1.7852, 2.1957, 1.9850, 1.7412, 1.8574,\n",
      "        1.8661, 1.8031, 2.3466, 2.9997, 3.0253, 2.0872, 2.0299, 2.0917, 1.9352,\n",
      "        2.4777, 2.3653, 3.0240, 2.9702, 1.5602, 1.3140, 1.1655, 1.0991, 2.1892,\n",
      "        1.5726, 1.9698, 1.8761, 1.7897, 1.0264, 1.5203, 2.1242, 1.5288, 2.0092,\n",
      "        1.7991, 1.9230, 1.6340, 1.3481, 1.5435, 1.2237, 1.2811, 1.9219, 1.8515,\n",
      "        1.9168, 1.5493, 1.5348, 1.3427, 1.6986, 1.8454, 1.4895, 1.7584, 1.8982,\n",
      "        1.6191, 1.2692, 1.2172, 1.7493, 1.9760, 2.2467, 2.0327, 2.0167, 1.9440,\n",
      "        1.3115, 1.3252, 1.2610, 1.6757, 1.4898, 1.9572, 1.9318, 2.1588, 1.5117,\n",
      "        1.5889, 1.9241, 2.2671, 2.2252, 1.8919, 2.2643, 1.0607, 1.4866, 1.4783,\n",
      "        1.6324, 1.5742, 1.8355, 1.9991, 2.2522], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0421,  0.0021,  0.0544,  ..., -0.0900,  0.0365,  0.0097],\n",
      "        [ 0.1137, -0.0258, -0.0686,  ...,  0.0793,  0.1475, -0.0688],\n",
      "        [ 0.0153, -0.0830,  0.1688,  ..., -0.0317,  0.0222,  0.0120],\n",
      "        ...,\n",
      "        [ 0.0065,  0.0593, -0.0321,  ..., -0.0472,  0.0927, -0.0035],\n",
      "        [-0.0721,  0.0438,  0.0404,  ...,  0.1253, -0.0052, -0.0427],\n",
      "        [-0.0041,  0.0302,  0.0469,  ..., -0.0585, -0.0741, -0.0177]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0141,  0.0679,  0.0643,  ..., -0.0474, -0.0354, -0.0297],\n",
      "        [-0.0238,  0.0461,  0.0108,  ..., -0.0805, -0.0284,  0.0932],\n",
      "        [ 0.0274,  0.0133,  0.0273,  ...,  0.0278, -0.0944, -0.0970],\n",
      "        ...,\n",
      "        [ 0.0022,  0.0287, -0.1295,  ...,  0.1143, -0.0046,  0.0036],\n",
      "        [ 0.0929,  0.1265,  0.0153,  ..., -0.1550,  0.0848, -0.0352],\n",
      "        [-0.2411, -0.0324, -0.0354,  ...,  0.0552,  0.0189, -0.0306]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5536, 1.6816, 1.9042, 2.0716, 1.8858, 1.7257, 2.2499, 2.1195, 1.6863,\n",
      "        1.6731, 1.8040, 1.7963, 1.9987, 2.2336, 2.0888, 2.0066, 1.4894, 1.3723,\n",
      "        1.6265, 1.8277, 2.2012, 2.5397, 2.6738, 2.5855, 0.8681, 1.4053, 1.6286,\n",
      "        1.7931, 1.9907, 2.4652, 2.6671, 2.2722, 1.5785, 1.3352, 1.4237, 1.2357,\n",
      "        1.0469, 1.2093, 1.5947, 1.6913, 1.5363, 1.5405, 1.6118, 1.4568, 1.9306,\n",
      "        1.8896, 1.6171, 1.5869, 1.4570, 1.6241, 1.9119, 2.0824, 2.0316, 2.5976,\n",
      "        2.7515, 2.7973, 1.4126, 1.8107, 2.0275, 1.7897, 2.1093, 2.4162, 2.7039,\n",
      "        2.6874, 1.7641, 1.9721, 1.7654, 1.5692, 1.5022, 2.0413, 1.9107, 1.9334,\n",
      "        1.7602, 1.9761, 1.8772, 2.0069, 1.7361, 1.4872, 1.9491, 1.9623, 1.8603,\n",
      "        1.4981, 1.6829, 1.2947, 1.1256, 1.2755, 1.8204, 1.7343, 1.6326, 1.7542,\n",
      "        1.5185, 1.7682, 2.2894, 1.9047, 1.8048, 1.8163, 1.7625, 1.5840, 1.7491,\n",
      "        1.8754, 2.1086, 2.0706, 2.1636, 2.0922, 1.8712, 2.0121, 2.0101, 1.8445,\n",
      "        1.5731, 1.7000, 2.0915, 2.0706, 1.6826, 1.7709, 1.5696, 2.0106, 2.1213,\n",
      "        1.2891, 1.7646, 1.8229, 1.7205, 1.5759, 1.8092, 1.3200, 1.2581, 2.1016,\n",
      "        1.7918, 1.7562, 1.5662, 1.6547, 1.7322, 1.7514, 1.7226, 1.3767, 1.7225,\n",
      "        1.7857, 1.5723, 1.7444, 1.6497, 1.5467, 1.5653, 1.8414, 1.9007, 1.8063,\n",
      "        1.4640, 1.4229, 1.7060, 1.4072, 2.1158, 1.8636, 1.7304, 1.5449, 1.7545,\n",
      "        1.6200, 1.4899, 1.3729, 1.1498, 1.2309, 1.7266, 1.7108, 1.6178, 1.4281,\n",
      "        1.5278, 1.5410, 1.2036, 2.1763, 1.7051, 1.6497, 2.0421, 1.8508, 1.5932,\n",
      "        1.4825, 1.8101, 1.2307, 1.7226, 2.0706, 1.9593, 2.0603, 2.0549, 1.9018,\n",
      "        1.4942, 2.3109, 2.2576, 2.2654, 1.9011, 1.9925, 1.9706, 1.9669, 2.2415,\n",
      "        1.3770, 2.1620, 2.1251, 1.7700, 1.5246, 1.4617, 1.9063, 1.2533, 1.1436,\n",
      "        1.6237, 1.7813, 1.6624, 1.7156, 1.7583, 1.2114, 1.7833, 2.1020, 1.6106,\n",
      "        1.7715, 1.6870, 1.8602, 1.7306, 1.6342, 1.4546, 1.9430, 1.9743, 2.0345,\n",
      "        1.9950, 1.8160, 1.7939, 1.5938, 1.7092, 1.4822, 1.6834, 1.9933, 1.2194,\n",
      "        1.5691, 1.6256, 1.1583, 1.1055, 2.0742, 1.7112, 1.7251, 2.1369, 1.4342,\n",
      "        1.4548, 1.8314, 1.9962, 1.2296, 1.7255, 1.8570, 2.1658, 1.8280, 2.0589,\n",
      "        1.7702, 1.9085, 1.7693, 2.2831, 2.4908, 1.9063, 1.9562, 1.8910, 2.0011,\n",
      "        1.7704, 1.7396, 2.4638, 2.2630, 1.7626, 1.6832, 1.8605, 1.9925, 1.1350,\n",
      "        1.9958, 1.7153, 1.8086, 1.8552, 1.6254, 1.5218, 1.0843, 1.9864, 1.1807,\n",
      "        1.7365, 1.8613, 1.7112, 1.6000, 1.8047, 1.6983, 2.2184, 1.1924, 1.7662,\n",
      "        1.8420, 1.7115, 1.8174, 1.5433, 1.2747, 1.1495, 2.0921, 1.7776, 1.8112,\n",
      "        1.9601, 1.6811, 1.6068, 1.3809, 1.2137, 1.1520, 1.9185, 1.8082, 1.7144,\n",
      "        1.6394, 1.7922, 1.5821, 1.6538, 2.1146, 1.7697, 1.8755, 1.8261, 1.6772,\n",
      "        1.6253, 1.4242, 1.2031, 1.2849, 1.8295, 1.8200, 1.5124, 1.5177, 1.6446,\n",
      "        1.6937, 2.1035, 2.1817, 1.8018, 1.7916], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1715, -0.0842,  0.0623,  ..., -0.0756, -0.0031, -0.1606],\n",
      "        [ 0.0817, -0.1410, -0.0496,  ..., -0.0089, -0.0691, -0.1001],\n",
      "        [ 0.0820,  0.2034,  0.0725,  ..., -0.1266,  0.1318, -0.0540],\n",
      "        ...,\n",
      "        [-0.0061, -0.0810, -0.0924,  ...,  0.0680,  0.0238,  0.0275],\n",
      "        [ 0.1704,  0.0787,  0.0281,  ..., -0.0454, -0.0855, -0.1911],\n",
      "        [-0.2309, -0.0218, -0.0763,  ...,  0.1687,  0.1254, -0.0525]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0212, -0.0068,  0.0331,  ..., -0.0212,  0.0122, -0.0030],\n",
      "        [-0.0332,  0.0130, -0.0306,  ..., -0.0494, -0.0249, -0.0276],\n",
      "        [-0.0666,  0.0183, -0.0298,  ..., -0.0298, -0.0221, -0.0577],\n",
      "        ...,\n",
      "        [ 0.1483,  0.0577,  0.0132,  ...,  0.0972,  0.1014,  0.0527],\n",
      "        [ 0.0901, -0.0900,  0.0795,  ..., -0.0050,  0.0649, -0.0170],\n",
      "        [-0.1823,  0.1025,  0.0497,  ..., -0.0007,  0.0304, -0.0460]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0495, 2.2048, 2.1120, 2.1693, 2.1060, 2.1021, 2.1868, 2.2377, 2.2128,\n",
      "        2.2043, 2.1598, 2.0417, 2.2390, 2.3311, 2.1067, 2.0563, 1.9171, 2.0404,\n",
      "        1.9799, 1.9873, 1.9744, 2.0415, 1.9896, 2.0045, 1.9913, 1.9401, 1.9581,\n",
      "        2.0183, 1.9174, 1.9384, 2.0045, 1.9150, 2.2766, 2.0638, 2.2254, 2.0308,\n",
      "        2.2499, 2.1712, 2.2782, 2.1155, 2.1624, 2.1638, 2.1577, 2.3273, 2.1918,\n",
      "        2.2545, 2.1759, 2.1963, 2.1347, 2.0653, 2.0541, 2.0570, 2.1073, 2.0817,\n",
      "        2.0449, 2.1140, 2.0666, 2.0831, 2.0142, 2.1385, 2.1106, 2.1883, 2.1007,\n",
      "        2.0872, 2.0205, 2.1767, 2.0134, 2.1241, 2.2175, 2.1041, 2.0752, 2.1008,\n",
      "        2.0314, 2.1187, 2.1195, 2.1005, 2.1170, 2.0785, 2.0752, 2.0460, 2.1669,\n",
      "        2.1353, 2.1647, 2.1658, 2.0014, 2.1286, 2.0516, 1.9847, 2.2515, 2.0725,\n",
      "        2.0402, 1.9245, 2.1495, 1.9749, 2.2374, 2.0468, 2.2730, 2.0603, 1.9387,\n",
      "        2.0699, 2.1515, 2.0536, 2.0301, 2.0969, 2.1480, 2.1383, 2.0986, 2.1326,\n",
      "        2.1154, 2.0511, 2.1418, 2.0887, 2.1814, 2.1556, 2.1672, 2.1582, 2.1819,\n",
      "        2.1460, 2.0467, 2.1432, 2.2585, 2.2070, 2.1642, 2.1787, 2.0933, 2.1587,\n",
      "        2.0944, 2.1431, 2.2903, 2.3285, 2.3718, 2.2128, 2.4165, 2.3178, 2.2839,\n",
      "        2.3518, 2.2228, 2.3738, 2.1087, 2.4056, 2.2179, 2.3715, 2.2003, 2.4125,\n",
      "        1.9146, 2.0916, 2.1712, 2.2652, 1.9171, 2.0636, 2.1150, 2.0638, 1.9520,\n",
      "        2.1379, 2.0530, 2.0866, 2.1546, 2.1500, 1.9942, 2.1712, 1.9708, 2.0548,\n",
      "        2.1380, 2.1717, 2.0747, 2.0236, 2.0393, 2.0816, 2.0894, 2.0867, 2.0580,\n",
      "        2.0386, 2.1921, 2.0320, 2.0395, 2.1253, 2.0965, 2.1400, 2.1609, 2.1872,\n",
      "        2.2712, 2.1265, 2.1808, 2.2425, 2.1028, 2.3416, 2.1107, 2.1525, 2.3117,\n",
      "        2.1140, 2.0935, 2.1631, 1.9732, 1.9822, 2.0123, 2.0756, 2.0586, 2.1165,\n",
      "        2.0382, 1.9871, 2.0904, 1.9565, 2.1668, 2.0140, 2.0896, 2.0407, 2.0649,\n",
      "        1.9673, 2.1920, 2.0060, 2.1001, 2.0976, 2.1416, 2.1732, 2.0442, 2.1553,\n",
      "        2.1796, 2.3227, 2.2859, 2.2147, 2.1200, 2.1530, 2.2035, 2.2011, 1.9450,\n",
      "        1.9687, 2.0582, 1.9553, 1.9166, 2.0437, 2.0069, 1.8648, 2.0188, 1.9949,\n",
      "        2.0728, 1.9337, 1.9205, 2.0275, 2.0149, 1.9903, 2.1992, 2.1705, 2.1031,\n",
      "        2.3225, 2.1443, 2.3063, 2.0864, 1.7365, 2.1415, 2.3493, 2.0829, 2.1643,\n",
      "        2.1815, 2.1439, 2.2507, 2.0906, 2.1452, 2.1071, 2.1356, 2.0505, 2.0215,\n",
      "        2.1501, 2.0770, 2.0448, 2.0266, 2.1436, 1.9286, 2.0773, 2.1543, 2.0217,\n",
      "        2.0664, 2.0946, 1.9948, 2.0498, 2.1070, 2.0138, 2.0716, 2.1482, 2.2025,\n",
      "        2.1179, 2.1650, 2.0878, 2.0716, 2.0906, 2.0693, 1.9299, 1.9989, 2.1120,\n",
      "        2.0810, 2.0685, 2.1059, 2.1710, 2.0967, 2.1544, 2.1586, 2.0304, 2.0635,\n",
      "        2.0298, 2.1248, 2.0035, 2.0301, 2.1508, 1.9285, 2.0460, 1.9622, 1.7745,\n",
      "        1.8431, 1.7964, 1.7838, 1.9629, 1.7669, 1.8178, 1.7955, 1.8170, 1.8814,\n",
      "        1.8242, 1.8466, 1.8700, 1.7794, 1.8211], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 8.8324e-05,  2.0355e-01,  5.2049e-02,  ..., -4.1894e-02,\n",
      "         -7.8651e-02, -3.7493e-03],\n",
      "        [-4.9087e-02,  2.3672e-02,  5.3354e-02,  ..., -6.8793e-02,\n",
      "         -6.4622e-02, -9.6315e-03],\n",
      "        [ 5.7368e-02,  2.3672e-02, -1.5432e-02,  ..., -1.9427e-02,\n",
      "          6.9371e-02, -1.9224e-02],\n",
      "        ...,\n",
      "        [ 1.0113e-01, -3.3027e-02, -7.1275e-02,  ...,  4.1312e-02,\n",
      "         -1.3080e-02,  3.2657e-02],\n",
      "        [-6.8477e-03,  7.8059e-02,  8.9468e-03,  ...,  1.3031e-02,\n",
      "          2.7940e-02, -8.5010e-02],\n",
      "        [-2.0587e-02, -1.0060e-02, -8.8134e-02,  ...,  5.5471e-02,\n",
      "          2.6415e-02, -2.2054e-03]], device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0090, -0.0695,  0.0390,  ..., -0.0564, -0.0452,  0.0496],\n",
      "        [ 0.0317,  0.1405, -0.1798,  ..., -0.0167, -0.0521, -0.0025],\n",
      "        [ 0.0198,  0.1143, -0.0069,  ..., -0.0293, -0.1098,  0.0112],\n",
      "        ...,\n",
      "        [-0.0177,  0.0038,  0.0465,  ...,  0.0580,  0.0385, -0.0033],\n",
      "        [-0.0204, -0.0618, -0.0123,  ..., -0.0552, -0.0003, -0.0503],\n",
      "        [-0.0456,  0.0811, -0.1216,  ...,  0.0442, -0.0663, -0.0061]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0055, 1.9557, 1.9704, 2.1324, 2.1731, 1.9505, 2.0974, 1.9793, 2.0461,\n",
      "        1.9577, 1.8013, 1.9536, 1.9740, 1.8011, 2.1541, 2.0805, 1.9135, 1.9814,\n",
      "        1.8274, 1.8958, 2.1822, 1.9660, 1.9524, 2.0149, 1.8457, 1.9096, 1.8501,\n",
      "        2.0354, 2.1854, 2.1081, 1.9647, 1.6484, 2.1421, 1.8790, 1.9481, 1.8430,\n",
      "        1.8972, 1.8930, 1.8771, 2.1814, 1.9473, 2.0182, 2.0468, 2.0100, 2.0133,\n",
      "        2.0035, 2.9223, 1.9116, 1.9517, 1.9880, 2.0135, 1.9278, 1.9356, 1.8451,\n",
      "        1.9900, 1.9375, 2.1638, 2.1308, 2.3529, 2.0649, 1.8935, 1.9455, 4.8716,\n",
      "        1.8821, 2.0843, 2.1245, 2.1351, 2.0274, 1.8840, 2.0495, 1.9643, 1.9253,\n",
      "        1.8507, 2.2194, 1.9492, 2.1264, 1.9499, 1.9875, 1.9122, 1.7431, 1.8252,\n",
      "        1.9525, 1.8802, 2.1915, 1.8790, 1.8753, 1.7320, 2.0096, 1.7238, 2.0467,\n",
      "        2.1768, 1.9937, 2.2495, 1.9270, 1.9573, 2.0583, 1.9605, 2.1692, 2.1222,\n",
      "        2.1558, 2.0593, 2.1935, 1.9482, 2.0171, 1.9617, 2.0094, 2.0740, 2.0508,\n",
      "        1.8458, 1.7966, 2.2040, 2.1126, 2.0663, 1.9918, 2.1703, 1.9680, 2.0676,\n",
      "        1.9163, 1.9293, 2.2184, 2.0688, 1.9553, 2.1763, 2.0546, 1.8338, 2.1322,\n",
      "        1.9444, 1.9715, 1.9406, 1.9044, 1.8161, 2.1147, 2.1509, 2.0006, 2.1967,\n",
      "        1.9711, 1.8554, 2.0119, 1.8858, 1.9758, 1.9901, 1.8955, 2.3196, 2.1101,\n",
      "        1.9873, 1.9814, 1.9739, 2.1033, 1.9955, 1.8833, 2.1361, 2.0371, 1.9852,\n",
      "        1.9350, 2.1060, 1.9405, 1.8593, 1.9374, 2.1718, 1.9165, 1.9951, 2.0135,\n",
      "        2.1132, 1.9389, 2.0485, 2.1346, 2.1194, 2.0623, 2.0735, 1.9688, 1.9620,\n",
      "        1.7365, 2.0689, 2.0171, 1.8450, 1.8389, 1.8285, 2.0580, 2.3147, 1.9832,\n",
      "        1.8122, 1.9919, 2.0453, 2.0067, 1.9990, 1.9914, 1.9614, 2.0083, 1.9909,\n",
      "        1.9493, 2.0185, 1.8169, 2.7180, 2.0940, 1.9456, 2.0006, 1.9487, 1.8783,\n",
      "        2.0361, 1.9904, 1.9744, 2.0577, 1.9463, 2.1006, 1.9936, 2.0922, 2.0284,\n",
      "        1.7363, 1.9648, 1.9240, 1.8725, 2.1117, 2.0740, 2.1431, 2.2522, 2.0319,\n",
      "        1.8962, 2.0656, 1.6674, 1.8803, 1.8633, 2.0120, 1.9570, 2.1855, 2.1416,\n",
      "        2.0462, 2.0886, 2.0041, 1.9346, 2.0826, 1.8729, 1.7602, 2.0428, 1.8642,\n",
      "        2.0166, 2.0407, 1.8838, 1.8824, 2.0865, 2.0063, 1.8915, 1.8490, 2.0172,\n",
      "        2.0545, 2.0240, 1.8180, 2.0169, 2.0037, 1.9446, 1.9863, 1.9482, 1.9289,\n",
      "        2.0845, 1.7917, 1.9263, 1.9173, 1.9728, 1.7890, 1.7846, 2.0817, 2.1906,\n",
      "        2.1338, 2.0852, 1.9841, 1.9849, 2.0283, 2.0234, 2.5428, 2.0396, 1.9104,\n",
      "        1.9049, 2.1833, 1.8690, 2.0071, 1.9823, 1.9348, 2.1952, 1.8549, 1.9779,\n",
      "        2.0817, 1.9344, 1.9139, 2.0625, 2.0004, 2.0117, 2.0327, 2.0320, 1.8527,\n",
      "        1.9051, 1.8836, 1.8895, 2.1753, 1.9715, 2.1408, 1.7530, 2.0435, 2.1139,\n",
      "        1.9584, 1.8592, 1.9205, 2.0673, 2.0071, 2.1712, 1.9832, 2.1144, 1.9808,\n",
      "        2.0780, 1.7150, 1.8597, 2.1245, 2.0491, 1.8093, 1.7969, 2.1929, 2.1235,\n",
      "        1.9569, 2.0337, 1.9640, 2.1792, 1.9460], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0073,  0.0102,  0.0462,  ...,  0.0315,  0.0441,  0.0013],\n",
      "        [ 0.0333,  0.0555,  0.0503,  ..., -0.1330,  0.1519, -0.0530],\n",
      "        [ 0.1017, -0.0370,  0.1119,  ..., -0.0176, -0.0636,  0.0295],\n",
      "        ...,\n",
      "        [ 0.0825, -0.0387, -0.0156,  ..., -0.0756,  0.1528, -0.0153],\n",
      "        [-0.1052,  0.0206, -0.0937,  ...,  0.0618, -0.0624, -0.0180],\n",
      "        [ 0.0490,  0.0565, -0.0574,  ..., -0.0635, -0.0364,  0.0447]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0461,  0.0262,  0.0022,  ..., -0.0061,  0.0090, -0.0286],\n",
      "        [-0.0102,  0.0213,  0.0306,  ..., -0.0180, -0.0194, -0.0525],\n",
      "        [-0.1038,  0.0173, -0.0120,  ..., -0.0310,  0.0195,  0.0772],\n",
      "        ...,\n",
      "        [-0.0309, -0.0474, -0.0235,  ..., -0.0311,  0.0514, -0.0992],\n",
      "        [-0.0016,  0.0119,  0.0048,  ...,  0.0858, -0.0234,  0.0073],\n",
      "        [-0.0076,  0.0255, -0.0022,  ...,  0.0099, -0.0004,  0.0467]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.5368, 1.3225, 1.3641, 1.3156, 1.5666, 1.7158, 1.6279, 1.6214, 1.5950,\n",
      "        1.4265, 1.1316, 1.3534, 1.4139, 1.3207, 1.6320, 1.5699, 1.5069, 1.3517,\n",
      "        1.0071, 1.3447, 1.6910, 1.4730, 1.8885, 1.9736, 1.0800, 1.0347, 1.3305,\n",
      "        1.1702, 1.1228, 1.9557, 2.0854, 1.8484, 1.0515, 1.1953, 1.1974, 1.6993,\n",
      "        2.1686, 1.5645, 1.8078, 2.5822, 2.0143, 1.5149, 1.6445, 1.4798, 1.4966,\n",
      "        2.1797, 1.9514, 1.9484, 1.6748, 1.7555, 1.7091, 1.5560, 1.4525, 1.5003,\n",
      "        1.8639, 1.9671, 1.7003, 1.7868, 1.5899, 1.7639, 1.7015, 1.5566, 1.8937,\n",
      "        1.9436, 1.1612, 1.6655, 1.4198, 1.9186, 2.0157, 2.1018, 2.3118, 2.0316,\n",
      "        1.2119, 1.2488, 1.5115, 2.0779, 2.0502, 1.6387, 2.2858, 2.1165, 1.7823,\n",
      "        1.3058, 1.6804, 1.0797, 1.8355, 1.7230, 1.3282, 1.4588, 1.6455, 1.7354,\n",
      "        1.3877, 1.8082, 1.1443, 1.1742, 1.3140, 1.3171, 1.1484, 0.9841, 1.0700,\n",
      "        1.1459, 1.4953, 1.4697, 1.4394, 1.5985, 1.1349, 0.9326, 1.2745, 1.3835,\n",
      "        1.2613, 1.5509, 1.4289, 1.4626, 1.5476, 1.6488, 1.9058, 1.7081, 1.8283,\n",
      "        1.6734, 2.2480, 2.3427, 1.5089, 1.6913, 1.7863, 1.8000, 1.5815, 1.9787,\n",
      "        2.2678, 2.3637, 2.0180, 1.7015, 1.6983, 1.5177, 1.6967, 2.2796, 1.9591,\n",
      "        1.9252, 1.6387, 1.7445, 1.6532, 1.3730, 1.7039, 1.3887, 1.8727, 1.9169,\n",
      "        1.9107, 1.2786, 1.6920, 1.2270, 1.2931, 1.4013, 1.6026, 2.1462, 1.2905,\n",
      "        1.2760, 1.1794, 1.8154, 1.9418, 2.0207, 1.7277, 1.7967, 1.4730, 1.4159,\n",
      "        1.5970, 1.7577, 1.5353, 2.2971, 1.8043, 1.8251, 1.6383, 1.4362, 1.6954,\n",
      "        1.7452, 1.8870, 1.4961, 1.6804, 1.9680, 0.7657, 1.3476, 1.5195, 1.4450,\n",
      "        2.0980, 2.0155, 2.4045, 2.0078, 1.3796, 1.1228, 1.2158, 1.6612, 1.9276,\n",
      "        2.0324, 2.3394, 1.9491, 1.6883, 1.2101, 1.6318, 1.8729, 1.8518, 1.8774,\n",
      "        1.6447, 1.9473, 1.2597, 1.4344, 1.0772, 1.2358, 1.2471, 1.3985, 1.6674,\n",
      "        1.6312, 1.0052, 1.3318, 1.2853, 1.2822, 1.2366, 1.2028, 1.7407, 1.7831,\n",
      "        1.2223, 1.1921, 1.2935, 1.3818, 1.1879, 1.7481, 1.9144, 1.8125, 1.4456,\n",
      "        1.3235, 1.7326, 1.7165, 1.2700, 1.3193, 1.6807, 1.6340, 1.6753, 1.4890,\n",
      "        1.0858, 1.0995, 1.6785, 1.9240, 1.5711, 1.8783, 0.7041, 1.5450, 1.4543,\n",
      "        1.6946, 2.1882, 1.7497, 2.1172, 2.2867, 1.3589, 1.1821, 1.4575, 1.5449,\n",
      "        2.2424, 1.7564, 2.2913, 2.3966, 1.9503, 1.9917, 1.9599, 1.6376, 1.3974,\n",
      "        2.1447, 1.8159, 2.0288, 1.8508, 1.8440, 1.8045, 1.3187, 1.7615, 1.5278,\n",
      "        1.9940, 1.9049, 1.8235, 1.9191, 1.9034, 1.8488, 1.4945, 2.1589, 2.1034,\n",
      "        2.0661, 1.7055, 1.7328, 1.9938, 1.9696, 1.7322, 1.5763, 2.2130, 2.0086,\n",
      "        1.2941, 1.3094, 1.3938, 1.2779, 1.3335, 1.1334, 2.0082, 2.0317, 1.2359,\n",
      "        1.2616, 1.2682, 1.3733, 1.2583, 1.5971, 1.5906, 1.3886, 0.7830, 0.7899,\n",
      "        0.8347, 1.1984, 1.9244, 2.0819, 2.0385, 1.9636, 1.7728, 1.4158, 1.5102,\n",
      "        1.5398, 1.7841, 2.0747, 1.9819, 1.6824], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.0215,  0.0150,  0.0091,  ...,  0.0519,  0.0153, -0.0416],\n",
      "        [-0.0860,  0.0242, -0.0667,  ...,  0.1001, -0.0442,  0.0452],\n",
      "        [-0.0295, -0.0235, -0.0832,  ..., -0.0746,  0.1778, -0.1176],\n",
      "        ...,\n",
      "        [ 0.0532, -0.1309,  0.0575,  ...,  0.0162,  0.0261, -0.0626],\n",
      "        [-0.0855, -0.1336,  0.0984,  ..., -0.1099,  0.0492, -0.0313],\n",
      "        [-0.0704, -0.1061,  0.0029,  ...,  0.0134,  0.0346, -0.0062]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0783,  0.0690,  0.0403,  ...,  0.0509,  0.0027, -0.0378],\n",
      "        [-0.0576,  0.0044,  0.0273,  ..., -0.0060,  0.0540, -0.0076],\n",
      "        [-0.0898, -0.0435, -0.0597,  ...,  0.0474,  0.0171, -0.0014],\n",
      "        ...,\n",
      "        [-0.0914, -0.0074,  0.0715,  ..., -0.0711, -0.0640, -0.0445],\n",
      "        [ 0.0785, -0.0027,  0.0033,  ..., -0.0626,  0.0321, -0.0602],\n",
      "        [-0.0413, -0.0573,  0.0439,  ...,  0.0396,  0.0621, -0.0092]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([1.4116, 1.2869, 1.4040, 1.4386, 1.1972, 1.1681, 1.5470, 1.5688, 1.4929,\n",
      "        1.4154, 1.3172, 1.3301, 1.4167, 1.8665, 1.5428, 1.5289, 1.1147, 1.5148,\n",
      "        1.0565, 1.2031, 1.3732, 1.3777, 1.5738, 1.4400, 1.2858, 1.0459, 1.5223,\n",
      "        1.2367, 1.2028, 1.3299, 1.7910, 1.6500, 1.5675, 1.5081, 1.5430, 1.3184,\n",
      "        1.2879, 2.0940, 1.7373, 1.8801, 1.4800, 1.4605, 1.4369, 1.7065, 2.0743,\n",
      "        1.3102, 1.8638, 1.8796, 1.6820, 1.7304, 1.7476, 1.5021, 1.5088, 1.4430,\n",
      "        1.7225, 1.8481, 1.6303, 1.8364, 1.7775, 1.7184, 1.4339, 2.0440, 1.7543,\n",
      "        1.9274, 1.1919, 1.5750, 1.5811, 1.7357, 1.5912, 1.3472, 1.8635, 1.7683,\n",
      "        1.2279, 1.2720, 1.6522, 1.7685, 1.4027, 1.6574, 1.6959, 1.8719, 1.8123,\n",
      "        1.4573, 1.6272, 1.5664, 0.9547, 1.0160, 1.3885, 1.4747, 1.5920, 1.6941,\n",
      "        1.4292, 1.2681, 1.5888, 1.3839, 1.3415, 1.3113, 1.0193, 1.2440, 1.2518,\n",
      "        1.3264, 1.0544, 1.0992, 1.3848, 1.4612, 1.2096, 1.1463, 1.3280, 1.0917,\n",
      "        1.1314, 1.2770, 1.3709, 1.4434, 1.4747, 1.5877, 1.9298, 1.7948, 1.7309,\n",
      "        1.7952, 1.9564, 1.8071, 1.5686, 1.7315, 1.7642, 1.8259, 1.7535, 1.6207,\n",
      "        1.9943, 1.8868, 1.9234, 1.7467, 1.8033, 1.6020, 1.2421, 1.1475, 1.8421,\n",
      "        1.8383, 1.7106, 1.8518, 1.7749, 1.5160, 1.4104, 2.3289, 1.7604, 1.7623,\n",
      "        1.8187, 1.4724, 1.3239, 1.8488, 1.8319, 1.9336, 1.5368, 1.7297, 1.4966,\n",
      "        1.4935, 1.5658, 1.1262, 1.0515, 1.2040, 1.6920, 1.6968, 1.5927, 1.5324,\n",
      "        1.4527, 1.3844, 1.6690, 1.1685, 1.6447, 1.7316, 1.5451, 1.6106, 1.4540,\n",
      "        1.4350, 1.1940, 1.9879, 1.7021, 1.7158, 1.5575, 1.0656, 1.3163, 1.4897,\n",
      "        1.6273, 1.8750, 1.9520, 1.8415, 0.8389, 1.5049, 1.3271, 1.3535, 1.7081,\n",
      "        1.8360, 1.9656, 1.8309, 1.7724, 1.4108, 1.3970, 1.1305, 1.0311, 1.1721,\n",
      "        1.6011, 1.7636, 1.3853, 1.5272, 1.5339, 1.5868, 1.9726, 1.8663, 1.6444,\n",
      "        1.5473, 0.8886, 0.9861, 1.0051, 1.3035, 1.1549, 1.2811, 1.4831, 1.4566,\n",
      "        1.0308, 1.1621, 1.3777, 1.2034, 1.1838, 1.2269, 1.7718, 1.3476, 1.5553,\n",
      "        1.4274, 1.6282, 1.0651, 1.8447, 1.9128, 1.6386, 1.5973, 1.7890, 1.5965,\n",
      "        1.3439, 1.7148, 1.0031, 1.0843, 1.5286, 1.6319, 1.5151, 1.5535, 1.5597,\n",
      "        1.6597, 1.6226, 1.7640, 1.9543, 1.9166, 1.1147, 0.9685, 1.1389, 1.2499,\n",
      "        1.8571, 1.7111, 2.0848, 1.9509, 1.9312, 1.9202, 1.9729, 1.7157, 1.9367,\n",
      "        1.2632, 1.6160, 1.7558, 1.7952, 1.8941, 1.8133, 1.5970, 1.4772, 2.1867,\n",
      "        1.8879, 1.7051, 1.7840, 1.9081, 1.9374, 1.9089, 1.6112, 1.4665, 1.9838,\n",
      "        1.8579, 1.6893, 1.6945, 1.8617, 1.8429, 1.5650, 2.0964, 2.0671, 1.9823,\n",
      "        0.9965, 0.9309, 1.1761, 1.1208, 1.3269, 1.2626, 1.6881, 1.4547, 0.8535,\n",
      "        1.0226, 1.2652, 1.3274, 1.1132, 1.1905, 1.4763, 1.4421, 1.1819, 1.5136,\n",
      "        1.6333, 1.6202, 1.6708, 1.8065, 1.8563, 1.8628, 1.4555, 1.1405, 1.0376,\n",
      "        1.5225, 1.7374, 1.8638, 1.8448, 1.7264], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight Parameter containing:\n",
      "tensor([[-0.1150,  0.1392,  0.0754,  ...,  0.1355, -0.0420,  0.1357],\n",
      "        [ 0.0094,  0.1272, -0.1467,  ..., -0.2259,  0.0004, -0.0703],\n",
      "        [ 0.0644, -0.0123, -0.0300,  ..., -0.0635,  0.0930,  0.0264],\n",
      "        ...,\n",
      "        [-0.0049, -0.0458, -0.2506,  ..., -0.0136, -0.0258, -0.0807],\n",
      "        [-0.1246, -0.0504,  0.1183,  ..., -0.0251, -0.0156,  0.0329],\n",
      "        [ 0.0340, -0.1423, -0.0712,  ..., -0.0710,  0.0840, -0.1588]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight Parameter containing:\n",
      "tensor([[-0.0761, -0.0069, -0.0053,  ..., -0.0090, -0.1210, -0.0851],\n",
      "        [ 0.0666,  0.0496,  0.0566,  ...,  0.0779, -0.0479,  0.0591],\n",
      "        [ 0.0380, -0.0183,  0.0653,  ..., -0.0602, -0.0586,  0.0154],\n",
      "        ...,\n",
      "        [ 0.0011, -0.0399, -0.0262,  ..., -0.0599,  0.1558, -0.1293],\n",
      "        [ 0.0307,  0.1448,  0.0184,  ...,  0.0054,  0.0137,  0.0574],\n",
      "        [-0.1246,  0.0679,  0.0135,  ...,  0.0936,  0.0072, -0.0776]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.0442, 2.0853, 2.0471, 2.0901, 2.3486, 2.2211, 2.0851, 1.9905, 2.1028,\n",
      "        2.1358, 2.1735, 2.0286, 2.1258, 2.1664, 2.0550, 2.1849, 2.2911, 2.2386,\n",
      "        2.2585, 2.3480, 2.2424, 2.1293, 2.3298, 2.4110, 2.2936, 2.2989, 2.2159,\n",
      "        2.1354, 2.2864, 2.1896, 2.2933, 2.2576, 2.0436, 1.9744, 2.1105, 2.0963,\n",
      "        1.9682, 2.0158, 2.0849, 2.0885, 2.0617, 2.0828, 1.9925, 2.0324, 2.0425,\n",
      "        2.0312, 2.0150, 1.9847, 2.1889, 2.2438, 2.2250, 2.3072, 1.8944, 2.1396,\n",
      "        2.2150, 2.0761, 2.0216, 2.1733, 2.2568, 2.2428, 2.2746, 2.0169, 2.3392,\n",
      "        2.2099, 2.2647, 2.3458, 2.1150, 2.1306, 2.4575, 2.2004, 2.2174, 2.1677,\n",
      "        2.2060, 2.2350, 2.2470, 2.1667, 2.2380, 2.1511, 2.1586, 2.2338, 2.1038,\n",
      "        2.1432, 2.2130, 2.1341, 2.2741, 2.1409, 2.1226, 2.1318, 2.2851, 2.0852,\n",
      "        2.1748, 2.2007, 2.1417, 2.1999, 2.0864, 2.0789, 2.3388, 2.0160, 2.2409,\n",
      "        2.1115, 2.2180, 2.1312, 2.0715, 2.1348, 2.1959, 2.1153, 2.2379, 2.2033,\n",
      "        2.3930, 2.2008, 2.1358, 2.1759, 2.1748, 2.2629, 2.0810, 2.1339, 2.0520,\n",
      "        2.1913, 2.2353, 2.2570, 2.5342, 2.1002, 2.1875, 2.2594, 2.0362, 2.1932,\n",
      "        2.2333, 2.2806, 2.2092, 2.0789, 2.2171, 2.2364, 2.2626, 2.2793, 2.0792,\n",
      "        2.2267, 2.1276, 2.2702, 2.1248, 2.3065, 2.2737, 2.2561, 2.1163, 2.1424,\n",
      "        2.0718, 2.1055, 1.8319, 1.9468, 2.0511, 2.1004, 2.1169, 1.9696, 2.2386,\n",
      "        2.1631, 2.0404, 2.0989, 2.0110, 2.2007, 2.0452, 2.0343, 2.1601, 2.1030,\n",
      "        2.0542, 2.0920, 2.1077, 2.1149, 2.1249, 2.1001, 2.0537, 2.0069, 2.1051,\n",
      "        2.1377, 2.0574, 1.9637, 2.0481, 2.1575, 1.9833, 1.8287, 2.0211, 1.9624,\n",
      "        1.8355, 1.9022, 1.9274, 1.9562, 2.0097, 2.0032, 1.9591, 1.8686, 1.9321,\n",
      "        1.9571, 1.9369, 1.8656, 2.0766, 1.8880, 2.0142, 2.1909, 2.1615, 2.0364,\n",
      "        2.0369, 1.9778, 2.1755, 2.2215, 2.0100, 2.1371, 2.1592, 2.0551, 1.8469,\n",
      "        2.1685, 2.5375, 2.5440, 2.5067, 2.5049, 2.5299, 2.9297, 2.5395, 2.2732,\n",
      "        2.1209, 2.3802, 2.2883, 2.2952, 2.2291, 2.4940, 2.3730, 2.3485, 2.1469,\n",
      "        2.1571, 2.2396, 2.1710, 2.1478, 1.9849, 2.1935, 2.1474, 2.2911, 2.0965,\n",
      "        2.1456, 2.1554, 2.0864, 2.1692, 2.1087, 1.9452, 2.0201, 2.0361, 1.9028,\n",
      "        1.9347, 1.9515, 1.9447, 1.9039, 1.9894, 1.9496, 1.9135, 1.9692, 1.9135,\n",
      "        1.8777, 1.8615, 1.9766, 1.9654, 2.2727, 2.2128, 1.9967, 2.2745, 2.1988,\n",
      "        2.1535, 2.0757, 2.2224, 2.2121, 2.2413, 2.1406, 2.1887, 2.2318, 2.1231,\n",
      "        2.1738, 2.0930, 2.2212, 2.1576, 2.1767, 2.2876, 2.1582, 2.2211, 2.2092,\n",
      "        2.1175, 2.2415, 2.2014, 2.1048, 2.0464, 2.0210, 2.3349, 2.1261, 2.2483,\n",
      "        3.5255, 2.7152, 2.6703, 3.1695, 2.3523, 2.8169, 2.5482, 2.6783, 3.5530,\n",
      "        2.5467, 2.7356, 2.6696, 3.0308, 2.7923, 2.7934, 2.7396, 2.1105, 1.9799,\n",
      "        1.8996, 1.8759, 1.9248, 1.9094, 1.9289, 2.1587, 1.9692, 1.9847, 1.9027,\n",
      "        1.9387, 2.1132, 1.9424, 1.9731, 1.9452], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight Parameter containing:\n",
      "tensor([[ 0.0915, -0.0714,  0.0588,  ..., -0.0032, -0.0169, -0.0294],\n",
      "        [ 0.0116,  0.0402,  0.0047,  ..., -0.0320,  0.0208,  0.0288],\n",
      "        [-0.0471,  0.0851, -0.0107,  ..., -0.0535,  0.0484, -0.0171],\n",
      "        ...,\n",
      "        [ 0.0645, -0.1493,  0.0411,  ...,  0.0696,  0.0132, -0.0090],\n",
      "        [-0.0267,  0.0851,  0.0993,  ..., -0.0058,  0.0118, -0.0687],\n",
      "        [-0.0289,  0.1402,  0.0259,  ..., -0.0161, -0.0446,  0.0596]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight Parameter containing:\n",
      "tensor([[ 0.0415,  0.0729,  0.1431,  ..., -0.0753, -0.1106, -0.0613],\n",
      "        [-0.0254, -0.0852,  0.1126,  ...,  0.0349, -0.0273, -0.0711],\n",
      "        [ 0.0217,  0.0666, -0.1168,  ..., -0.0975,  0.0970,  0.0671],\n",
      "        ...,\n",
      "        [-0.0635,  0.0283, -0.0731,  ...,  0.0655, -0.1097,  0.0682],\n",
      "        [ 0.0096,  0.0344,  0.0452,  ...,  0.1059,  0.1194,  0.0186],\n",
      "        [ 0.0120, -0.0659,  0.0237,  ...,  0.0144,  0.0119, -0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default Parameter containing:\n",
      "tensor([2.2001, 2.0649, 2.1826, 2.2333, 2.5525, 2.2693, 2.1388, 2.3175, 2.2945,\n",
      "        2.1789, 2.1131, 2.4515, 2.5238, 2.0640, 2.3606, 2.1918, 2.2525, 2.1239,\n",
      "        2.4282, 2.0793, 2.4574, 2.4551, 2.0395, 2.1859, 2.1356, 1.9936, 2.2356,\n",
      "        2.2176, 2.4658, 2.2351, 2.1030, 2.0881, 2.4467, 2.1637, 2.1030, 2.0851,\n",
      "        2.1193, 2.2560, 2.3777, 2.1805, 2.1419, 2.3535, 2.3202, 2.2070, 2.3381,\n",
      "        2.1412, 3.1431, 1.9852, 2.2996, 2.5638, 2.2664, 2.2294, 2.0731, 2.3491,\n",
      "        2.0709, 2.1836, 2.2054, 2.3917, 2.6282, 2.4052, 2.3250, 2.1080, 6.9229,\n",
      "        2.1975, 2.1994, 2.1904, 2.3762, 2.6559, 2.3245, 2.1622, 2.2235, 2.1452,\n",
      "        2.0118, 2.3210, 2.1238, 2.3784, 2.2433, 2.4320, 1.9149, 1.9942, 2.0483,\n",
      "        2.1285, 2.3058, 2.2698, 2.0258, 2.1690, 2.4940, 2.4309, 1.9704, 2.2228,\n",
      "        2.1342, 2.0366, 2.4102, 2.2417, 2.2248, 2.2922, 2.1154, 2.2886, 2.1748,\n",
      "        2.2911, 2.3384, 2.2690, 2.4402, 2.4001, 2.3141, 2.3590, 2.1077, 2.2956,\n",
      "        2.0209, 2.3123, 2.2632, 2.2647, 2.2199, 2.1725, 2.4502, 2.1415, 2.2513,\n",
      "        2.3008, 2.4727, 2.5089, 2.4013, 2.2929, 2.2879, 2.2217, 2.0046, 2.5290,\n",
      "        2.1169, 2.1518, 2.2462, 2.3462, 2.1483, 2.0092, 2.1823, 2.1726, 2.4301,\n",
      "        2.1273, 2.2263, 2.4388, 2.1580, 2.3052, 2.1156, 2.1031, 2.2378, 2.3579,\n",
      "        2.3933, 2.2964, 2.1586, 2.3353, 2.1749, 2.2731, 2.4370, 2.2586, 2.3892,\n",
      "        2.1547, 2.2835, 2.2933, 2.0832, 2.3654, 2.2919, 2.1714, 2.0760, 2.2150,\n",
      "        2.3346, 2.3733, 2.3468, 2.3351, 2.3019, 2.1078, 2.3303, 2.1595, 2.3057,\n",
      "        2.3066, 2.2320, 2.2088, 2.2607, 2.0505, 2.2779, 2.2448, 2.1731, 2.0139,\n",
      "        2.0261, 2.0946, 2.1817, 2.1949, 2.3202, 2.1889, 2.3096, 2.2494, 1.9641,\n",
      "        2.2257, 2.1471, 2.0857, 3.3503, 2.1548, 2.2182, 2.2905, 2.4367, 2.2555,\n",
      "        2.3901, 2.2950, 2.2943, 2.3602, 2.2790, 2.2419, 2.2514, 2.3281, 1.9495,\n",
      "        2.2518, 2.6105, 2.1610, 2.1901, 2.3634, 2.3917, 2.3293, 2.3204, 2.1613,\n",
      "        2.1776, 2.0311, 2.0213, 2.1334, 2.0871, 2.1079, 2.4267, 2.4329, 2.3723,\n",
      "        2.2466, 2.3325, 2.1380, 1.9406, 2.3966, 2.1306, 2.1025, 2.0609, 2.3101,\n",
      "        2.0160, 2.1705, 2.2525, 2.2484, 2.0993, 2.2830, 2.1215, 2.0874, 2.3337,\n",
      "        2.0839, 2.2686, 2.0305, 2.3525, 1.9844, 2.0573, 2.3921, 2.2612, 2.1623,\n",
      "        2.0428, 1.9955, 2.2409, 2.2393, 2.3304, 2.0546, 2.3170, 2.3081, 2.1124,\n",
      "        2.1868, 2.2249, 2.1425, 2.1851, 2.1879, 2.3203, 2.8144, 2.3924, 2.1185,\n",
      "        1.9596, 2.1890, 2.0378, 2.4455, 2.4396, 2.3690, 2.1453, 2.0890, 2.2988,\n",
      "        2.1969, 2.2344, 2.1081, 2.2689, 2.5746, 2.1945, 2.2806, 2.2878, 2.3380,\n",
      "        2.1081, 2.1647, 2.1794, 2.3166, 2.2429, 2.3029, 2.0794, 2.0539, 2.1944,\n",
      "        2.0975, 2.0556, 2.2989, 2.2117, 2.1806, 2.1987, 2.1547, 2.3704, 2.2298,\n",
      "        2.3404, 2.0101, 2.0088, 2.0561, 2.3239, 1.9865, 1.9307, 2.3306, 2.3229,\n",
      "        2.1678, 2.3807, 2.1174, 2.2179, 2.0103], device='cuda:0',\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name, params in light_mod.model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.classifier.dense.original_module.weight\n",
      "base_model.model.classifier.dense.original_module.bias\n",
      "base_model.model.classifier.dense.modules_to_save.default.weight\n",
      "base_model.model.classifier.dense.modules_to_save.default.bias\n",
      "base_model.model.classifier.out_proj.original_module.weight\n",
      "base_model.model.classifier.out_proj.original_module.bias\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.weight\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in light_mod.model.named_parameters():\n",
    "    if \"lora\" in name or \"classifier\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.default\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.default.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.default\n",
      "base_model.model.classifier.dense.original_module.weight\n",
      "base_model.model.classifier.dense.original_module.bias\n",
      "base_model.model.classifier.dense.modules_to_save.default.weight\n",
      "base_model.model.classifier.dense.modules_to_save.default.bias\n",
      "base_model.model.classifier.out_proj.original_module.weight\n",
      "base_model.model.classifier.out_proj.original_module.bias\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.weight\n",
      "base_model.model.classifier.out_proj.modules_to_save.default.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in mod.model.named_parameters():\n",
    "    if \"lora\" in name or \"classifier\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.0.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.1.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.2.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.3.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.4.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.query.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.key.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.self.value.lora_magnitude_vector.initial\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_A.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_B.initial.weight\n",
      "base_model.model.esm.encoder.layer.5.attention.output.dense.lora_magnitude_vector.initial\n",
      "base_model.model.classifier.dense.original_module.weight\n",
      "base_model.model.classifier.dense.original_module.bias\n",
      "base_model.model.classifier.dense.modules_to_save.initial.weight\n",
      "base_model.model.classifier.dense.modules_to_save.initial.bias\n",
      "base_model.model.classifier.out_proj.original_module.weight\n",
      "base_model.model.classifier.out_proj.original_module.bias\n",
      "base_model.model.classifier.out_proj.modules_to_save.initial.weight\n",
      "base_model.model.classifier.out_proj.modules_to_save.initial.bias\n"
     ]
    }
   ],
   "source": [
    "for name, params in a.named_parameters():\n",
    "    if \"lora\" in name or \"classifier\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA RTX A4500') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (47) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | PeftModel | 8.1 M \n",
      "------------------------------------\n",
      "233 K     Trainable params\n",
      "7.8 M     Non-trainable params\n",
      "8.1 M     Total params\n",
      "32.299    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a86435e5a954be98a80c2e9ae4dad25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61190975eb684204a5aaf80d6015b682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ea31921b34e9c9b928b199abe1068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved. New best score: -0.069\n",
      "Epoch 0, global step 47: 'Val_MCC' reached -0.06901 (best -0.06901), saving model to 'model_checkpoint/lightning_logs/version_41/checkpoints/epoch=0-Val_MCC=-0.07.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c19e193f6343a3bfcb1fa300b428f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 94: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03c5cb3bc356417ab8de82692e538121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 141: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "951ea6a311ff4642936772c7606130ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.388 >= min_delta = 0.005. New best score: 0.319\n",
      "Epoch 3, global step 188: 'Val_MCC' reached 0.31944 (best 0.31944), saving model to 'model_checkpoint/lightning_logs/version_41/checkpoints/epoch=3-Val_MCC=0.32.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d2a3ad7d1436f97135ca207b8fd9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 235: 'Val_MCC' was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53970445cfa3437fa560a4421acf5be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.019 >= min_delta = 0.005. New best score: 0.338\n",
      "Epoch 5, global step 282: 'Val_MCC' reached 0.33806 (best 0.33806), saving model to 'model_checkpoint/lightning_logs/version_41/checkpoints/epoch=5-Val_MCC=0.34.ckpt' as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c671715a7940cbabc3ecee7b846691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric Val_MCC improved by 0.072 >= min_delta = 0.005. New best score: 0.410\n",
      "Epoch 6, global step 329: 'Val_MCC' reached 0.40992 (best 0.40992), saving model to 'model_checkpoint/lightning_logs/version_41/checkpoints/epoch=6-Val_MCC=0.41.ckpt' as top 1\n"
     ]
    }
   ],
   "source": [
    "trainer = ft.Trainer(callbacks=[checkpoint_callback, early_callback], default_root_dir=train_config.model_checkpoint_dir,\n",
    "                          fast_dev_run=bool(train_config.debug_mode_sample), max_epochs=10, \n",
    "                          max_time=train_config.max_time, precision=train_config.precision,\n",
    "                          accumulate_grad_batches=train_config.accumulate_grad_batches)\n",
    "\n",
    "#tuner = Tuner(trainer)\n",
    "#lr_finder = tuner.lr_find(light_mod, data_module, min_lr=1e-6, max_lr=1, num_training=1000, mode=\"exponential\", early_stop_threshold=4)\n",
    "\n",
    "trainer.fit(model=light_mod, datamodule=data_module)\n",
    "best_model_path = checkpoint_callback.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Val_Loss': tensor(0.6900),\n",
       " 'Val_Acc': tensor(0.5000),\n",
       " 'Val_F1': tensor(0.5000),\n",
       " 'Val_Precision': tensor(0.5000),\n",
       " 'Val_Recall': tensor(0.5000),\n",
       " 'Val_MCC': tensor(0.),\n",
       " 'Val_AUROC': tensor(0.3333),\n",
       " 'Val_Average_Precision': tensor(0.7917),\n",
       " 'Val_Cohen_Kappa': tensor(nan),\n",
       " 'Train_Loss': tensor(0.5812),\n",
       " 'Train_Acc': tensor(0.7957),\n",
       " 'Train_F1': tensor(0.7957),\n",
       " 'Train_Precision': tensor(0.7957),\n",
       " 'Train_Recall': tensor(0.7957),\n",
       " 'Train_MCC': tensor(0.5914),\n",
       " 'Train_AUROC': tensor(0.4086),\n",
       " 'Train_Average_Precision': tensor(0.8065),\n",
       " 'Train_Cohen_Kappa': tensor(nan)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.logged_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[0.9258]], device='cuda:0', dtype=torch.float16)\n",
    "torch.tensor(84., device='cuda:0').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adapter(peft_model: str, llm_config,\n",
    "                 use_adapter: str=\"initial\", adapters: dict[str, str] | None=None):\n",
    "    device = \"auto\" if llm_config.device == \"cuda\" else llm_config.device\n",
    "    model = AutoPeftModel.from_pretrained(peft_model, adapter_name=\"initial\", \n",
    "                                                                   low_cpu_mem_usage=True, device_map=device,\n",
    "                                                                   torch_dtype=llm_config.dtype)                                                                \n",
    "    if adapters:\n",
    "        for key, value in adapters.items():\n",
    "            model.load_adapter(value, adapter_name=key)\n",
    "    model.set_adapter(use_adapter)\n",
    "    model.merge_adapter()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ft.TransformerModule.load_from_checkpoint(best_model_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.prepare_data()\n",
    "data_module.setup(\"fit\")\n",
    "inputs = data_module.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[20,  4,  4,  ...,  1,  1,  1]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([1], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in inputs:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phastos/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "a = load_adapter(\"model\", ft.LLMConfig(), use_adapter=\"initial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2565,  0.2070]], device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2692,  0.2177]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 78.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/peft/peft_model.py:642\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    641\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_base_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:1103\u001b[0m, in \u001b[0;36mEsmForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1103\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1113\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1114\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:907\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    900\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    901\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    902\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    906\u001b[0m )\n\u001b[0;32m--> 907\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    920\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:612\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    602\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:502\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m ):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:436\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m ):\n\u001b[1;32m    435\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 436\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    446\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Programs/mambaforge/envs/bioml/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:360\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    356\u001b[0m         attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m relative_position_scores_query \u001b[38;5;241m+\u001b[39m relative_position_scores_key\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in EsmModel forward() function)\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 78.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "mod.model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
