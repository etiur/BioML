{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM, BitsAndBytesConfig\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# the minimum for the ESM2 is 650M if we want better performance than ESM1b with 650M as well.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.arange(40, dtype=torch.float32).view(10, 4), torch.tensor([i for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(40).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[12., 13., 14., 15.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]],\n",
      "\n",
      "        [[20., 21., 22., 23.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[ -0.4483,  -0.5224,  -0.5965],\n",
      "         [ -3.0493,  -3.3023,  -3.5554],\n",
      "         [ -0.2073,  -0.2387,  -0.2701],\n",
      "         [  9.0189,   9.7911,  10.5633],\n",
      "         [ -3.3314,  -3.5062,  -3.6811],\n",
      "         [ -1.9417,  -2.0309,  -2.1201],\n",
      "         [  6.3485,   6.7886,   7.2287],\n",
      "         [ -5.4109,  -5.9195,  -6.4282],\n",
      "         [ -1.8216,  -2.0154,  -2.2093],\n",
      "         [-12.6559, -13.6681, -14.6804]],\n",
      "\n",
      "        [[ -1.6338,  -1.7079,  -1.7820],\n",
      "         [ -7.0983,  -7.3513,  -7.6044],\n",
      "         [ -0.7093,  -0.7407,  -0.7721],\n",
      "         [ 21.3746,  22.1468,  22.9190],\n",
      "         [ -6.1290,  -6.3038,  -6.4787],\n",
      "         [ -3.3689,  -3.4581,  -3.5473],\n",
      "         [ 13.3904,  13.8305,  14.2706],\n",
      "         [-13.5488, -14.0574, -14.5660],\n",
      "         [ -4.9232,  -5.1170,  -5.3109],\n",
      "         [-28.8517, -29.8640, -30.8762]],\n",
      "\n",
      "        [[ -1.0410,  -1.1151,  -1.1892],\n",
      "         [ -5.0738,  -5.3268,  -5.5799],\n",
      "         [ -0.4583,  -0.4897,  -0.5211],\n",
      "         [ 15.1967,  15.9689,  16.7412],\n",
      "         [ -4.7302,  -4.9050,  -5.0799],\n",
      "         [ -2.6553,  -2.7445,  -2.8337],\n",
      "         [  9.8694,  10.3095,  10.7497],\n",
      "         [ -9.4799,  -9.9885, -10.4971],\n",
      "         [ -3.3724,  -3.5662,  -3.7601],\n",
      "         [-20.7538, -21.7661, -22.7783]],\n",
      "\n",
      "        [[ -0.1519,  -0.2260,  -0.3001],\n",
      "         [ -2.0370,  -2.2901,  -2.5431],\n",
      "         [ -0.0818,  -0.1132,  -0.1445],\n",
      "         [  5.9299,   6.7022,   7.4744],\n",
      "         [ -2.6320,  -2.8069,  -2.9817],\n",
      "         [ -1.5849,  -1.6741,  -1.7633],\n",
      "         [  4.5880,   5.0281,   5.4683],\n",
      "         [ -3.3765,  -3.8851,  -4.3937],\n",
      "         [ -1.0462,  -1.2400,  -1.4339],\n",
      "         [ -8.6069,  -9.6192, -10.6314]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[ 4.4093e-01,  3.6683e-01,  2.9273e-01],\n",
      "         [-1.2538e-02, -2.6560e-01, -5.1866e-01],\n",
      "         [ 1.6925e-01,  1.3787e-01,  1.0649e-01],\n",
      "         [-2.4793e-01,  5.2430e-01,  1.2965e+00],\n",
      "         [-1.2332e+00, -1.4081e+00, -1.5829e+00],\n",
      "         [-8.7126e-01, -9.6046e-01, -1.0497e+00],\n",
      "         [ 1.0671e+00,  1.5072e+00,  1.9473e+00],\n",
      "         [ 6.9249e-01,  1.8388e-01, -3.2474e-01],\n",
      "         [ 5.0463e-01,  3.1078e-01,  1.1693e-01],\n",
      "         [-5.0898e-01, -1.5212e+00, -2.5335e+00]],\n",
      "\n",
      "        [[-1.9302e+00, -2.0043e+00, -2.0784e+00],\n",
      "         [-8.1105e+00, -8.3636e+00, -8.6166e+00],\n",
      "         [-8.3486e-01, -8.6624e-01, -8.9762e-01],\n",
      "         [ 2.4464e+01,  2.5236e+01,  2.6008e+01],\n",
      "         [-6.8284e+00, -7.0032e+00, -7.1781e+00],\n",
      "         [-3.7257e+00, -3.8149e+00, -3.9041e+00],\n",
      "         [ 1.5151e+01,  1.5591e+01,  1.6031e+01],\n",
      "         [-1.5583e+01, -1.6092e+01, -1.6601e+01],\n",
      "         [-5.6986e+00, -5.8925e+00, -6.0863e+00],\n",
      "         [-3.2901e+01, -3.3913e+01, -3.4925e+01]],\n",
      "\n",
      "        [[ 1.4453e-01,  7.0436e-02, -3.6626e-03],\n",
      "         [-1.0248e+00, -1.2778e+00, -1.5309e+00],\n",
      "         [ 4.3732e-02,  1.2354e-02, -1.9025e-02],\n",
      "         [ 2.8410e+00,  3.6132e+00,  4.3855e+00],\n",
      "         [-1.9326e+00, -2.1075e+00, -2.2823e+00],\n",
      "         [-1.2281e+00, -1.3173e+00, -1.4065e+00],\n",
      "         [ 2.8276e+00,  3.2677e+00,  3.7078e+00],\n",
      "         [-1.3420e+00, -1.8506e+00, -2.3592e+00],\n",
      "         [-2.7078e-01, -4.6463e-01, -6.5848e-01],\n",
      "         [-4.5580e+00, -5.5702e+00, -6.5824e+00]],\n",
      "\n",
      "        [[-1.3374e+00, -1.4115e+00, -1.4856e+00],\n",
      "         [-6.0860e+00, -6.3391e+00, -6.5921e+00],\n",
      "         [-5.8384e-01, -6.1522e-01, -6.4659e-01],\n",
      "         [ 1.8286e+01,  1.9058e+01,  1.9830e+01],\n",
      "         [-5.4296e+00, -5.6044e+00, -5.7793e+00],\n",
      "         [-3.0121e+00, -3.1013e+00, -3.1905e+00],\n",
      "         [ 1.1630e+01,  1.2070e+01,  1.2510e+01],\n",
      "         [-1.1514e+01, -1.2023e+01, -1.2532e+01],\n",
      "         [-4.1478e+00, -4.3416e+00, -4.5355e+00],\n",
      "         [-2.4803e+01, -2.5815e+01, -2.6827e+01]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[36., 37., 38., 39.]],\n",
      "\n",
      "        [[16., 17., 18., 19.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ -2.2266,  -2.3007,  -2.3748],\n",
      "         [ -9.1227,  -9.3758,  -9.6289],\n",
      "         [ -0.9604,  -0.9918,  -1.0231],\n",
      "         [ 27.5524,  28.3247,  29.0969],\n",
      "         [ -7.5278,  -7.7026,  -7.8775],\n",
      "         [ -4.0825,  -4.1717,  -4.2609],\n",
      "         [ 16.9113,  17.3514,  17.7915],\n",
      "         [-17.6178, -18.1264, -18.6350],\n",
      "         [ -6.4740,  -6.6679,  -6.8617],\n",
      "         [-36.9497, -37.9619, -38.9742]],\n",
      "\n",
      "        [[ -0.7446,  -0.8187,  -0.8928],\n",
      "         [ -4.0615,  -4.3146,  -4.5676],\n",
      "         [ -0.3328,  -0.3642,  -0.3956],\n",
      "         [ 12.1078,  12.8800,  13.6523],\n",
      "         [ -4.0308,  -4.2056,  -4.3805],\n",
      "         [ -2.2985,  -2.3877,  -2.4769],\n",
      "         [  8.1090,   8.5491,   8.9892],\n",
      "         [ -7.4454,  -7.9540,  -8.4626],\n",
      "         [ -2.5970,  -2.7908,  -2.9847],\n",
      "         [-16.7048, -17.7171, -18.7293]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 1\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[12., 13., 14., 15.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[ 1.4453e-01,  7.0436e-02, -3.6626e-03],\n",
      "         [-1.0248e+00, -1.2778e+00, -1.5309e+00],\n",
      "         [ 4.3732e-02,  1.2354e-02, -1.9025e-02],\n",
      "         [ 2.8410e+00,  3.6132e+00,  4.3855e+00],\n",
      "         [-1.9326e+00, -2.1075e+00, -2.2823e+00],\n",
      "         [-1.2281e+00, -1.3173e+00, -1.4065e+00],\n",
      "         [ 2.8276e+00,  3.2677e+00,  3.7078e+00],\n",
      "         [-1.3420e+00, -1.8506e+00, -2.3592e+00],\n",
      "         [-2.7078e-01, -4.6463e-01, -6.5848e-01],\n",
      "         [-4.5580e+00, -5.5702e+00, -6.5824e+00]],\n",
      "\n",
      "        [[-4.4825e-01, -5.2235e-01, -5.9645e-01],\n",
      "         [-3.0493e+00, -3.3023e+00, -3.5554e+00],\n",
      "         [-2.0730e-01, -2.3867e-01, -2.7005e-01],\n",
      "         [ 9.0189e+00,  9.7911e+00,  1.0563e+01],\n",
      "         [-3.3314e+00, -3.5062e+00, -3.6811e+00],\n",
      "         [-1.9417e+00, -2.0309e+00, -2.1201e+00],\n",
      "         [ 6.3485e+00,  6.7886e+00,  7.2287e+00],\n",
      "         [-5.4109e+00, -5.9195e+00, -6.4282e+00],\n",
      "         [-1.8216e+00, -2.0154e+00, -2.2093e+00],\n",
      "         [-1.2656e+01, -1.3668e+01, -1.4680e+01]],\n",
      "\n",
      "        [[-1.5186e-01, -2.2596e-01, -3.0006e-01],\n",
      "         [-2.0370e+00, -2.2901e+00, -2.5431e+00],\n",
      "         [-8.1781e-02, -1.1316e-01, -1.4454e-01],\n",
      "         [ 5.9299e+00,  6.7022e+00,  7.4744e+00],\n",
      "         [-2.6320e+00, -2.8069e+00, -2.9817e+00],\n",
      "         [-1.5849e+00, -1.6741e+00, -1.7633e+00],\n",
      "         [ 4.5880e+00,  5.0281e+00,  5.4683e+00],\n",
      "         [-3.3765e+00, -3.8851e+00, -4.3937e+00],\n",
      "         [-1.0462e+00, -1.2400e+00, -1.4339e+00],\n",
      "         [-8.6069e+00, -9.6192e+00, -1.0631e+01]],\n",
      "\n",
      "        [[-1.6338e+00, -1.7079e+00, -1.7820e+00],\n",
      "         [-7.0983e+00, -7.3513e+00, -7.6044e+00],\n",
      "         [-7.0935e-01, -7.4073e-01, -7.7211e-01],\n",
      "         [ 2.1375e+01,  2.2147e+01,  2.2919e+01],\n",
      "         [-6.1290e+00, -6.3038e+00, -6.4787e+00],\n",
      "         [-3.3689e+00, -3.4581e+00, -3.5473e+00],\n",
      "         [ 1.3390e+01,  1.3830e+01,  1.4271e+01],\n",
      "         [-1.3549e+01, -1.4057e+01, -1.4566e+01],\n",
      "         [-4.9232e+00, -5.1170e+00, -5.3109e+00],\n",
      "         [-2.8852e+01, -2.9864e+01, -3.0876e+01]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[20., 21., 22., 23.]],\n",
      "\n",
      "        [[36., 37., 38., 39.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[-1.0410e+00, -1.1151e+00, -1.1892e+00],\n",
      "         [-5.0738e+00, -5.3268e+00, -5.5799e+00],\n",
      "         [-4.5832e-01, -4.8970e-01, -5.2108e-01],\n",
      "         [ 1.5197e+01,  1.5969e+01,  1.6741e+01],\n",
      "         [-4.7302e+00, -4.9050e+00, -5.0799e+00],\n",
      "         [-2.6553e+00, -2.7445e+00, -2.8337e+00],\n",
      "         [ 9.8694e+00,  1.0310e+01,  1.0750e+01],\n",
      "         [-9.4799e+00, -9.9885e+00, -1.0497e+01],\n",
      "         [-3.3724e+00, -3.5662e+00, -3.7601e+00],\n",
      "         [-2.0754e+01, -2.1766e+01, -2.2778e+01]],\n",
      "\n",
      "        [[-2.2266e+00, -2.3007e+00, -2.3748e+00],\n",
      "         [-9.1227e+00, -9.3758e+00, -9.6289e+00],\n",
      "         [-9.6038e-01, -9.9176e-01, -1.0231e+00],\n",
      "         [ 2.7552e+01,  2.8325e+01,  2.9097e+01],\n",
      "         [-7.5278e+00, -7.7026e+00, -7.8775e+00],\n",
      "         [-4.0825e+00, -4.1717e+00, -4.2609e+00],\n",
      "         [ 1.6911e+01,  1.7351e+01,  1.7792e+01],\n",
      "         [-1.7618e+01, -1.8126e+01, -1.8635e+01],\n",
      "         [-6.4740e+00, -6.6679e+00, -6.8617e+00],\n",
      "         [-3.6950e+01, -3.7962e+01, -3.8974e+01]],\n",
      "\n",
      "        [[ 4.4093e-01,  3.6683e-01,  2.9273e-01],\n",
      "         [-1.2538e-02, -2.6560e-01, -5.1866e-01],\n",
      "         [ 1.6925e-01,  1.3787e-01,  1.0649e-01],\n",
      "         [-2.4793e-01,  5.2430e-01,  1.2965e+00],\n",
      "         [-1.2332e+00, -1.4081e+00, -1.5829e+00],\n",
      "         [-8.7126e-01, -9.6046e-01, -1.0497e+00],\n",
      "         [ 1.0671e+00,  1.5072e+00,  1.9473e+00],\n",
      "         [ 6.9249e-01,  1.8388e-01, -3.2474e-01],\n",
      "         [ 5.0463e-01,  3.1078e-01,  1.1693e-01],\n",
      "         [-5.0898e-01, -1.5212e+00, -2.5335e+00]],\n",
      "\n",
      "        [[-1.3374e+00, -1.4115e+00, -1.4856e+00],\n",
      "         [-6.0860e+00, -6.3391e+00, -6.5921e+00],\n",
      "         [-5.8384e-01, -6.1522e-01, -6.4659e-01],\n",
      "         [ 1.8286e+01,  1.9058e+01,  1.9830e+01],\n",
      "         [-5.4296e+00, -5.6044e+00, -5.7793e+00],\n",
      "         [-3.0121e+00, -3.1013e+00, -3.1905e+00],\n",
      "         [ 1.1630e+01,  1.2070e+01,  1.2510e+01],\n",
      "         [-1.1514e+01, -1.2023e+01, -1.2532e+01],\n",
      "         [-4.1478e+00, -4.3416e+00, -4.5355e+00],\n",
      "         [-2.4803e+01, -2.5815e+01, -2.6827e+01]]],\n",
      "       grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[16., 17., 18., 19.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ -0.7446,  -0.8187,  -0.8928],\n",
      "         [ -4.0615,  -4.3146,  -4.5676],\n",
      "         [ -0.3328,  -0.3642,  -0.3956],\n",
      "         [ 12.1078,  12.8800,  13.6523],\n",
      "         [ -4.0308,  -4.2056,  -4.3805],\n",
      "         [ -2.2985,  -2.3877,  -2.4769],\n",
      "         [  8.1090,   8.5491,   8.9892],\n",
      "         [ -7.4454,  -7.9540,  -8.4626],\n",
      "         [ -2.5970,  -2.7908,  -2.9847],\n",
      "         [-16.7048, -17.7171, -18.7293]],\n",
      "\n",
      "        [[ -1.9302,  -2.0043,  -2.0784],\n",
      "         [ -8.1105,  -8.3636,  -8.6166],\n",
      "         [ -0.8349,  -0.8662,  -0.8976],\n",
      "         [ 24.4635,  25.2357,  26.0080],\n",
      "         [ -6.8284,  -7.0032,  -7.1781],\n",
      "         [ -3.7257,  -3.8149,  -3.9041],\n",
      "         [ 15.1508,  15.5909,  16.0310],\n",
      "         [-15.5833, -16.0919, -16.6005],\n",
      "         [ -5.6986,  -5.8925,  -6.0863],\n",
      "         [-32.9007, -33.9130, -34.9252]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = nn.Conv1d(1, 10, 2)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "dl = DataLoader(dataset, batch_size=4, worker_init_fn=seed_worker, generator=g, shuffle=True)\n",
    "for i in range(2):\n",
    "    print(\"Epoch\", i)\n",
    "    for batch, label in dl:\n",
    "       print(batch.unsqueeze(1).shape)\n",
    "       print(batch.unsqueeze(1))\n",
    "       o = a(batch.unsqueeze(1))\n",
    "       print(o.shape)\n",
    "       print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/whole_sequence.fasta', 'r') as f:\n",
    "    seqs = list(SeqIO.parse(f, 'fasta'))\n",
    "seq = {s.id:str(s.seq) for s in seqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(seq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EH1(72)': 'MLLPETRNLLDLMDAATRGGRPRLETLPHAVGRKAVDKMSEDGEADPPEVAEVANGGFAGPASEIRFRRYRPLGEAAGLLPTLIYYHGGGFVIGNIETHDSTCRRLANKSRCQVISIDYRLAPEHPFPAPIDDGIAAFRHIRDNAESFGADAARLAVGGDSAGGAMAAVVCQACRDAGETGPAFQMLIYPATDSSRESASRVAFAEGYFLSKALMDWFWEAYVPEDTDLTDLRLSPLLATDFTGLPPAFVLTAGYDPLRDEGRAYADRLIEAGIKTTYVNYPGTIHGFFSLTRFLSQGLKANDEAAAVMGAHFGT',\n",
       " 'EH2(71)': 'MGLQKLIVRTLMKLPESWILKLAGGTPVEIDGRTMDPRIQLLAAQGAKAPSMTSMSIEDARKSADEGLALLDAKPRRTVSILSRTIPGPAGDLHVRIYTPAGATGPLPGIVYYHMGGCVIGNLETCNTFCSILADDCRAIVVSVDYRLAPEHKFPAAMDDAVASFDWVSENAAALGIDPTRLGVGGDSAGGWLSAVVCQTRKAEGKTQPKAQLLIYPATDLDAKEGSMQSCAEIYPLTAEIMDWFMQQFLNSPEDAKDLKASPAHSEDLSGLAPALIMTAGFDVLRDQGEAYGNRLRDAGVPVTYRCYDSLSHAYTAFSGAVPAARQACEEIARDMARALG',\n",
       " 'EH3(69)': 'MPDTTSLNIADDVRMDPRLKAMLAAFPMMEQQTFQTREEQVANANTPEATAAREQLKMMMDMMDSEEFAPSDNLDISTREFTSSPDGNAIKIQFIRPKGKQKVPCVYYIHGGGMMIMSAFYGNYRAWGKMIANNGVAVAMVDFRNCLSPSSAPEVAPFPAGLNDCVSGLKWVSENADELSIDKNKIIIAGESGGGNLTLATGLKLKQDGNIDLVKGLYALCPYIAGKWPQDRFPSSSENNGIMIELHNNQGALAYGIEQLEAENPLAWPSFASAEDMQGLPPTVINVNECDPLRDEGIDFYRRLMAAGVPARCRQVMGTCHAGDMFVAVIPDVSADTAADIARTAKGG',\n",
       " 'CalB(68)': 'MALPSGSDPAFSQPKSVLDAGLTCQGASPSSVSKPILLVPGTGTTGPQSFDSNWIPLSTQLGYTPCWISPPPFMLNDTQVNTEYMVNAITALYAGSGNNKLPVLTWSQGGLVAQWGLTFFPSIRSKVDRLMAFAPDYKGTVLAGPLDALAVSAPSVWQQTTGSALTTALRNAGGLTQIVPTTNLYSATDEIVQPQVSNSPLDSSYLFNGKNVQAQAVCGPLFVIDHAGSLTSQFSYVVGRSALRSTTGQARSADYGITDCNPLPANDLTPEQKVAAAALLAPAAAAIVAGPKQNCEPDLMPYARPFAVGKRTCSGIVTPLE',\n",
       " 'EH4(67)': 'MSLQRMIVRTLLKLPDGLLVKMSGGKPLEIDGRTLDARVQLLASQGAKAPSMTTLPIEEARKGADDGLAMLDAKPRRNVSILSRSIPGPEGELHVRVYTPAGATGPLPGIVYYHMGGCVIGGLETCNTFCSILAEDCRAIVVSVDYRLAPEHKFPAAIDDAIASYDWVYQNATALGIDNTRLGLGGDSAGGWLSAVVCQHRKREGLPQPKAQLLIYPATDLQMTGGSMESCKDVYPLTREIMDWFMAQFLTSDADRSDWRGSPGQTADLSGLAPAIVATAGFDVLRDQGEAYANKLKAAGVPASYHCYDSLAHAFTAFSGTVPAAKQACEELAREMAKALNA',\n",
       " 'EH5(67)': 'MALNSQAEELLKRAAESGTPGLGEGTPEEGRAIFATTTQLLGLPAPDVKDTKEIQISGPNGPIRTLVITPDGVETNNLPLFIYYHGGGWVIGSPETHYEECCYYANEAQCIVLVPDYRLAPEYPFPAAPEDCYAVLQWAADNAESLGADKSRIAVGGDSAGGNLSAVVAQMTQQRNGPELALQLLIYPATRMGADTQSYKDFEDGYFLTAKAMNWFFGHYLKKAEDWDNLLASPLLNDDLAGLAPAYVVTAGFDPLRDEGRAYADKLKAAGVPVEYVCYEGQIHGFASMAGALDEARSFLDEAAKVLRKAFNK',\n",
       " 'EH6(66)': 'MPLHPQIEGLLQQMAAAGGKGFHQMEVDECRQTFGGLLNSLPPSQQKIASAQDRGIPSPNGPVKVRVYTPEGSGPFPVMAYFHGGGWVIGDLETHDSLCRELCGAVGMVVVSVDYRLAPEHKFPAAPDDCVAVTRWIAANAAALNADASRIAVGGDSAGGNLAAVVAQRLRDEDALKLAAQLLIYPVVHLDGVATPSMIENAEGYLLTRKDMEWFGGHYLASPADGQNASASPILAKSLAGLPPALVLTCEFDPLRDEGEKYGKALQAAGVPTTISRHDGTIHATFSFFTALEPGRRMADEAIRWLKEQLVK',\n",
       " 'EH7(64)': 'MEFPMAQSNIIAGMDLNRLDRIAEHLDRAYLHPGKLAGTMTLVARRGEVVYCQAQGLRDVERQLPVERDTLFRIYSMTKPITSIALMQLYEQGRFLLDEPVHKYIPTWKNLRVYKTGSHPQMLTTAPQRPMTIRDLLTHQSGLTYGFMNRTNVDAAYRSLKLDGGPGHTLDRLIDELARLPLEFSPGTAWNYSVATDVCGYLVQLLSGMSLDDYFSKHIFQPLGMPDTFFTVPAEKLSRFAACYEYQPGDSFSLQDDPQGSAFAKAHGYLSGGGGLVSCVDDYYRFAQALANGGELDGARIIGRKTLEFMRMNHLPDNKGLPDVAIGSFSETPYDGTGFGLGFSVKLDVAKSQTVGSVGEYGWGGMASTNFFIDPEEDLLMVFMTQLIPSSTYAVRQELRAIINGALVD',\n",
       " 'EH8(63)': 'MNPAVIERATVRALMSLPGPVLERLAAGLETHSRPHLDSRLRFLLALSGAKPTLDSGTVEQARQIYRSTLALLDMAPVSLPVVVDHQVSMEDGSQILVRRYRPADAPLVSPAIMFFHGGGFTIGGVEEYDRLCRYIAKRTNAVVLSVDYRLAPEHPAPAGMDDALEAWRWLLNNTAQLGLDPNRLAVMGDSAGGCMSAVVSQQAKLAGLALPALQVLIYPTTDAALAHPSVQTLGQGFGLDIPLLTWFRGHFVQDPAVIEDYRVSPLRNPDLTGLPEAIVITATDPLRDEGLEYAQKLREAGNTVTSLDYPELIHGFISMGGVVPAARKAINDICVETKRRL',\n",
       " 'EH9(61)': 'MEKKMALDKQAAEILKRAEESDTPGLGEGSPAEGREVFAGTTALLGLPTPEGQRISEVQIPGPSGDIRTRIIHPLEGLADNLPILIYYHGGGWVIGSPETHEGETCFYANEANCVVLVPDYRLAPEDPFPAAPDDCYAVLEWANANAETFGGDASRIAVAGDSAGGNLSAVVSQMAHANNGPDIALQLLIYPATRMGATTESYREFNDGYFLTGKAMDWFFNHYLKRPEDWDALKASPLLAPDLSGLPPAYIMTAGFDPLRDEGKAYAERLQQAGVPVDYVCYEEQIHGFVSMAGALDQGKQFLREAAAVLRRAFTS'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = islice(a, 10)\n",
    "dict(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tokenizer and the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low_cpu_mem_usage: when loading try not to use more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", low_mem_usage=True)\n",
    "tok = tokenizer(list(seq.values())[:2], padding=True, truncation=True, return_tensors=\"pt\", is_split_into_words=False)\n",
    "tok = tok.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.to(device)\n",
    "#model.eval()\n",
    "model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cpu\")\n",
    "model_masked_16 = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, device_map=\"cpu\", torch_dtype=torch.float16)\n",
    "model_16 = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cpu\", torch_dtype=torch.float16) \n",
    "# most models might not be able to do inference with float 16. Its errors is lower than bfloat16 but you cannot run it in CPUs maybe\n",
    "model_masked = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, device_map=\"cpu\", torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_8 = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=1.0)\n",
    "model_8bit = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cuda\", quantization_config=quant_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "finfo(resolution=0.001, min=-65504, max=65504, eps=0.000976562, smallest_normal=6.10352e-05, tiny=6.10352e-05, dtype=float16)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.finfo(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=30, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.333333343267440795898437500000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1/3, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.333251953125000000000000000000, dtype=torch.float16)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1/3, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.333984375000000000000000000000, dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1/3, dtype=torch.bfloat16) # los numeros que puede presentar antes de que sea 0 y la precision que puede presentar es diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#output.logits# logits es lo que le das al softmax para que lo convierta en probabilidad -> softmax(logits) es el output the last linear model\n",
    "# y que es last hidden state entonces? Es muy diferente a los logits? El shape es diferente -> para cada position devuelve la probabilidad de que sea uno de los tokens\n",
    "# EL maskedLM y el automodel hidden state es lo mismo -> pero la logits cambia. Cual debería usar para el embedding?\n",
    "#output.hidden_states[-1] # en el caso de MASkedLM si no le especifico de devolver hidden states, no los devuelve, pero en el caso de automodel si los devuelve aun sin lo del hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoModelForMaskedLM, BitsAndBytesConfig\n",
    "\n",
    "model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cpu\")\n",
    "model_16 = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cpu\", \n",
    "                                     torch_dtype=torch.float16) \n",
    "# the threshold determines the values that are considered outliers and are calculated using 16 bit precision\n",
    "# the smaller it is the less memory it will save because, at the eveyrhting will be calculated with 16 bits\n",
    "quant_8 = BitsAndBytesConfig(load_in_8bit=True, llm_int8_threshold=1.0) \n",
    "model_8bit = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, device_map=\"cuda\", \n",
    "                                       quantization_config=quant_8)\n",
    "\n",
    "output = model(**tok) # full 32 precision\n",
    "output_16 = model_16(**tok) # half 16 precision\n",
    "output_8 = model_8bit(**tok) # quatized to 8 bits\n",
    "data = {32: output.last_hidden_state, 16: output_16.last_hidden_state, 8: output_8.last_hidden_state}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0027, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[32] - data[16]).max(axis=1)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0470, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(data[32] - data[8]).max(axis=1)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_masked(**tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 33])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft = F.softmax(output.logits, dim=-1).detach()\n",
    "soft.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 336, 316, 336, 258, 145, 163, 114, 161, 124, 120,  98, 115, 256,\n",
       "         123, 212, 185, 144, 127, 119,   1, 286, 140, 103, 165, 165, 165, 165,\n",
       "         165, 165, 165, 165, 336],\n",
       "        [  0, 281, 342, 281, 269, 172, 190, 144, 188, 151, 181, 219,  97, 283,\n",
       "         150, 153, 212, 171, 154, 146,   1, 313, 167, 130, 192, 222, 192, 192,\n",
       "         192, 192, 192, 192, 281]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(soft, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max difference: 0.01247859\n",
      "mean difference: 0.00063062285\n"
     ]
    }
   ],
   "source": [
    "model_masked_16 = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, \n",
    "                                                       device_map=\"cpu\", torch_dtype=torch.float16)\n",
    "\n",
    "model_masked = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", output_hidden_states=True, \n",
    "                                                    device_map=\"cpu\", torch_dtype=torch.float32)\n",
    "\n",
    "output = model_masked(**tok)\n",
    "output_16 = model_masked_16(**tok)\n",
    "\n",
    "print(\"max difference:\", (output.hidden_states[-1] - output_16.hidden_states[-1]).max().detach().numpy())\n",
    "print(\"mean difference:\", (output.hidden_states[-1] - output_16.hidden_states[-1]).abs().mean().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 320])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tok)\n",
    "output # si no le especificas hidden states -> tampoco tiene hidden states, sino que tiene last_hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1421,  0.5840, -0.0724,  ...,  1.1748, -0.0931, -0.4209],\n",
       "         [ 0.4133,  0.0937, -0.1658,  ...,  0.8398, -0.2219, -0.2437],\n",
       "         [ 0.0767, -0.5088, -0.0298,  ...,  0.3887, -0.0302,  0.1070],\n",
       "         ...,\n",
       "         [-0.2573,  0.2483,  0.5522,  ...,  0.6455, -0.5259, -0.0944],\n",
       "         [-0.3516,  0.2590,  0.5864,  ...,  0.4985, -0.5669, -0.1550],\n",
       "         [-0.3301,  0.2368,  0.2407,  ...,  0.4675, -0.6846, -0.3098]],\n",
       "\n",
       "        [[ 0.0927,  0.6987, -0.0489,  ...,  1.0352, -0.1703, -0.3040],\n",
       "         [ 0.3230,  0.4792, -0.1464,  ...,  0.7700, -0.2129, -0.2917],\n",
       "         [ 0.0061, -0.2739,  0.2622,  ..., -0.0435,  0.2888,  0.1104],\n",
       "         ...,\n",
       "         [-0.4729, -0.2120, -0.2720,  ...,  0.9448, -0.3599,  0.2312],\n",
       "         [-0.0944, -0.4836,  0.0655,  ...,  0.3899, -0.1250, -0.1101],\n",
       "         [-0.0054,  0.0670,  0.1100,  ...,  0.6562, -0.6016, -0.2551]]],\n",
       "       dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_16 = model_16(**tok)\n",
    "output_16.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.483202"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_float16.get_memory_footprint()/1e+6\n",
    "#model.get_memory_footprint()/1e+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference_16 = output.last_hidden_state - output_16.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0125, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference_16.max(axis=1).values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 15.0938,  -7.5859,  -6.3984,  ..., -15.4062, -15.6328,  -7.5781],\n",
       "         [ -9.7422, -16.4844,  -9.3047,  ..., -15.8906, -16.1406, -16.4688],\n",
       "         [-11.9688, -21.8438, -12.3438,  ..., -15.7969, -15.8750, -21.8281],\n",
       "         ...,\n",
       "         [ -5.5430,  -6.7969,  14.7812,  ..., -16.7656, -16.5781,  -6.8281],\n",
       "         [ -5.4414,  -6.3516,  17.0000,  ..., -16.7031, -16.5156,  -6.3867],\n",
       "         [ -5.4102,  -6.6562,  16.5625,  ..., -16.6719, -16.4844,  -6.6953]],\n",
       "\n",
       "        [[ 16.1562,  -5.9922,  -6.4141,  ..., -15.2656, -15.4844,  -5.9805],\n",
       "         [ -9.0000, -15.7500,  -7.1562,  ..., -15.9609, -16.2344, -15.7500],\n",
       "         [-11.6250, -19.8281, -10.8594,  ..., -15.8203, -15.9297, -19.8125],\n",
       "         ...,\n",
       "         [-10.2578, -20.8750, -12.7188,  ..., -15.9688, -16.0312, -20.9062],\n",
       "         [-12.6641, -21.1250, -12.9375,  ..., -16.2031, -16.3438, -21.1094],\n",
       "         [ -6.0078,  -5.7773,  17.8594,  ..., -16.7188, -16.5938,  -5.8281]]],\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_8bit = model_8bit(**tok)\n",
    "output_8bit.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = output.logits - output_8bit.logits\n",
    "state = output.hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4023, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference[0,:,-11].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.7880e-05, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "difference.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 160])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.AvgPool1d(2)(output.last_hidden_state[-1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the attention mask to remove the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "mask = tok[\"attention_mask\"].bool()\n",
    "for num, x in enumerate(output.last_hidden_state):\n",
    "    masked_x = x[mask[num]]\n",
    "    results[num] = masked_x.mean(dim=0).detach().cpu().numpy()\n",
    "    # detach removes the tensor from the computation graph (the gradient won't be computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(results[0], dim=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data2.csv\"\n",
    "embeddings = pd.DataFrame(results).T\n",
    "embeddings.to_csv(path, mode='a', header=not Path(path).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 320])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[tok[\"attention_mask\"].bool()[1]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the code to Load large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# https://huggingface.co/docs/datasets/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to load files within datasets -> from local or remote files (in these formats json, csv, text, parquet)  \n",
    "Since we have a fasta file that is not supported (because it will treat each line as a row so it will double the rows, but in fasta the first line is an id).  \n",
    "So we can process it in-memory to pandas, generators, dictionaries or list of dictionaries and use Datasets instead of load_dataset.  \n",
    "The load dataset returns a dataset dict with different splits (train, test, val) as keys and then a dataset object as values.\n",
    "\n",
    "But we are using a dataset object directl\n",
    "\n",
    "To load fasta files use from generator beacause it is in-memory and the file might be too large to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 294 examples [00:00, 6184.43 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 294\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = load_dataset(\"text\", data_files=\"../data/whole_sequence.fasta\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "with open(\"../data/whole_sequence.fasta\", 'r') as f:\n",
    "    seqs = SeqIO.parse(f, 'fasta')\n",
    "    d = pd.Series({s.id:str(s.seq) for s in seqs}).to_frame()\n",
    "    d.columns = [\"sequences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 147 examples [00:00, 21001.52 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process or tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use map to apply the tokenizer function to the entire dataset\n",
    "The map will create and add the new columns ('input_ids', 'attention_mask') coming from the tokenizer to the datatset   \n",
    "but you will have to change its format to torch tensors for the models to read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = b.map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"pt\",padding=True, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = dataset.select_columns([\"id\",\"input_ids\", \"attention_mask\"])\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract the embeddings use the dataloader from pytorch to create the batches for you  \n",
    "It will only return the input_ids and the attention mask (the ids are lost, so yoou don't know which sequence is which)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20, 10,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "for batch in dataloader:\n",
    "    u = batch\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioML.deep.embeddings as emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f39e453b894286bd6382344628cda1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = emb.TokenizeFasta(emb.LLMConfig()).tokenize(\"../data/whole_sequence.fasta\")\n",
    "embed = emb.ExtractEmbeddings(emb.LLMConfig())\n",
    "seq_keys = list(data[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, batch in enumerate(DataLoader(data, batch_size=batch_size)):\n",
    "    batch_seq_keys = seq_keys[num*batch_size:(num+1)*batch_size]\n",
    "    results = embed.extract(batch_seq_keys, batch)\n",
    "    #embed.save(results, \"../data/embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(\"../data/embeddings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.nn.Linear(320, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m attention_weights\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "attention_scores = attention_weights(output.hidden_states[-1])\n",
    "attention_weights = torch.softmax(attention_scores, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 109760])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = output.hidden_states[-1].reshape(output.hidden_states[-1].shape[0], -1)\n",
    "_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  ...,  0.4682, -0.6849, -0.3094],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -107712)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, 2048 - _temp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.nn.functional.pad(_temp, (0, 2048 - _temp.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  0.3390, -0.1853, -0.0982, -0.9235,  0.1019,\n",
       "        -0.4527, -0.6959], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109670"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(o[0].detach().numpy()).intersection(_temp[0].detach().numpy()))xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(output.hidden_states[-1][0][0].detach().numpy()).intersection(_temp[0][:100].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training using the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioML.models.regression as regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(\"../data/embeddings.csv\", index_col=0)\n",
    "label = list(range(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-02-2024 12:07:16 INFO ------------------------------------------------------------------------------\n",
      "23-02-2024 12:07:16 INFO PycaretInterface parameters\n",
      "23-02-2024 12:07:16 INFO Seed: 200\n",
      "23-02-2024 12:07:16 INFO Budget time: 20\n",
      "23-02-2024 12:07:16 INFO The number of models to select: 3\n",
      "23-02-2024 12:07:16 INFO Output path: regression_training\n",
      "23-02-2024 12:07:16 INFO ----------------Trainer inputs-------------------------\n",
      "23-02-2024 12:07:16 INFO Number of kfolds: 5\n",
      "23-02-2024 12:07:16 INFO Number of iterations: 30\n"
     ]
    }
   ],
   "source": [
    "data = regression.DataParser(\"../data/embeddings.csv\", label)\n",
    "experiment = regression.PycaretInterface(\"regression\", 200, scaler= \"zscore\", budget_time=20, best_model=3, \n",
    "                                        output_path=\"regression_training\", optimize=\"RMSE\")\n",
    "\n",
    "regressor = regression.Regressor(test_size=0.2, optimize=\"RMSE\")\n",
    "training = regression.Trainer(experiment, regressor, 5, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data according to sequence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = regression.split.ClusterSpliter(\"../data/resultsDB_clu.tsv\", 5, random_state=experiment.seed, test_size=0.2)\n",
    "X_train, X_test = c.train_test_split(data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/23 12:18:52 INFO mlflow.tracking.fluent: Experiment with name 'Regression' does not exist. Creating a new experiment.\n",
      "23-02-2024 12:18:53 INFO --------------------------------------------------------\n",
      "23-02-2024 12:18:53 INFO Training regression models\n",
      "23-02-2024 12:18:53 INFO The models used ['lr', 'lasso', 'ridge', 'en', 'lar', 'llar', 'omp', 'br', 'par', 'huber', 'svm', 'knn', 'dt', 'rf', 'et', 'gbr', 'mlp', 'xgboost', 'catboost', 'dummy']\n",
      "23-02-2024 12:18:53 INFO Time budget is 20 minutes\n",
      "23-02-2024 12:20:27 INFO Training over: Total runtime 1.565 minutes\n",
      "23-02-2024 12:20:27 INFO Analyse the best models and plotting them\n",
      "23-02-2024 12:20:27 INFO Analyse the top 1 model: catboost\n",
      "23-02-2024 12:24:10 INFO Analyse the top 2 model: br\n",
      "23-02-2024 12:24:13 INFO Analyse the top 3 model: rf\n",
      "23-02-2024 12:24:27 INFO --------Stacking the best models--------\n",
      "23-02-2024 12:24:27 INFO ----------Stacking the best models--------------\n",
      "23-02-2024 12:27:58 INFO --------Creating an ensemble model--------\n",
      "23-02-2024 12:27:58 INFO ----------Creating a majority voting model--------------\n",
      "23-02-2024 12:27:58 INFO fold: 5\n",
      "23-02-2024 12:27:58 INFO weights: None\n",
      "23-02-2024 12:28:45 INFO --------Retuning the best models--------\n",
      "23-02-2024 12:28:45 INFO Retuning catboost\n",
      "23-02-2024 12:28:45 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:28:45 INFO num_iter: 30\n",
      "23-02-2024 12:28:45 INFO fold: 5\n",
      "23-02-2024 12:45:00 INFO Retuning br\n",
      "23-02-2024 12:45:00 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:45:00 INFO num_iter: 30\n",
      "23-02-2024 12:45:00 INFO fold: 5\n",
      "23-02-2024 12:45:16 INFO Retuning rf\n",
      "23-02-2024 12:45:16 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:45:16 INFO num_iter: 30\n",
      "23-02-2024 12:45:16 INFO fold: 5\n",
      "23-02-2024 12:46:25 INFO --------Stacking the best models--------\n",
      "23-02-2024 12:46:25 INFO ----------Stacking the best models--------------\n",
      "23-02-2024 12:46:44 INFO --------Creating an ensemble model--------\n",
      "23-02-2024 12:46:44 INFO ----------Creating a majority voting model--------------\n",
      "23-02-2024 12:46:44 INFO fold: 5\n",
      "23-02-2024 12:46:44 INFO weights: None\n"
     ]
    }
   ],
   "source": [
    "results, models_dict = training.generate_training_results(X_train, data.label, True,\n",
    "                                                          test_data=X_test, fold_strategy=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml_pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
