{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# the minimum for the ESM2 is 650M if we want better performance than ESM1b with 650M as well.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.arange(40, dtype=torch.float32).view(10, 4), torch.tensor([i for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(40).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[12., 13., 14., 15.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]],\n",
      "\n",
      "        [[20., 21., 22., 23.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  7.9515,   8.6100,   9.2686],\n",
      "         [ -5.7440,  -6.2499,  -6.7559],\n",
      "         [ -2.4015,  -2.5816,  -2.7617],\n",
      "         [-12.2291, -13.2001, -14.1711],\n",
      "         [ 10.4888,  11.3591,  12.2294],\n",
      "         [  3.9718,   4.3172,   4.6625],\n",
      "         [  1.2479,   1.3680,   1.4881],\n",
      "         [ -0.7694,  -0.8526,  -0.9357],\n",
      "         [-12.2041, -13.1510, -14.0978],\n",
      "         [ -2.9194,  -3.1050,  -3.2906]],\n",
      "\n",
      "        [[ 18.4879,  19.1464,  19.8049],\n",
      "         [-13.8394, -14.3453, -14.8513],\n",
      "         [ -5.2831,  -5.4632,  -5.6433],\n",
      "         [-27.7649, -28.7359, -29.7069],\n",
      "         [ 24.4138,  25.2841,  26.1544],\n",
      "         [  9.4968,   9.8421,  10.1874],\n",
      "         [  3.1696,   3.2897,   3.4098],\n",
      "         [ -2.0995,  -2.1826,  -2.2658],\n",
      "         [-27.3533, -28.3001, -29.2469],\n",
      "         [ -5.8897,  -6.0753,  -6.2610]],\n",
      "\n",
      "        [[ 13.2197,  13.8782,  14.5367],\n",
      "         [ -9.7917, -10.2976, -10.8036],\n",
      "         [ -3.8423,  -4.0224,  -4.2025],\n",
      "         [-19.9970, -20.9680, -21.9390],\n",
      "         [ 17.4513,  18.3216,  19.1919],\n",
      "         [  6.7343,   7.0796,   7.4249],\n",
      "         [  2.2088,   2.3289,   2.4490],\n",
      "         [ -1.4345,  -1.5176,  -1.6007],\n",
      "         [-19.7787, -20.7255, -21.6724],\n",
      "         [ -4.4045,  -4.5902,  -4.7758]],\n",
      "\n",
      "        [[  5.3174,   5.9760,   6.6345],\n",
      "         [ -3.7201,  -4.2261,  -4.7320],\n",
      "         [ -1.6810,  -1.8611,  -2.0412],\n",
      "         [ -8.3452,  -9.3162, -10.2872],\n",
      "         [  7.0076,   7.8779,   8.7482],\n",
      "         [  2.5906,   2.9359,   3.2812],\n",
      "         [  0.7675,   0.8876,   1.0077],\n",
      "         [ -0.4369,  -0.5201,  -0.6032],\n",
      "         [ -8.4169,  -9.3637, -10.3105],\n",
      "         [ -2.1768,  -2.3624,  -2.5481]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  0.0493,   0.7078,   1.3663],\n",
      "         [  0.3276,  -0.1784,  -0.6843],\n",
      "         [ -0.2402,  -0.4203,  -0.6004],\n",
      "         [ -0.5773,  -1.5483,  -2.5193],\n",
      "         [  0.0451,   0.9154,   1.7857],\n",
      "         [ -0.1719,   0.1734,   0.5188],\n",
      "         [ -0.1933,  -0.0732,   0.0469],\n",
      "         [  0.2281,   0.1450,   0.0619],\n",
      "         [ -0.8423,  -1.7891,  -2.7359],\n",
      "         [ -0.6916,  -0.8772,  -1.0629]],\n",
      "\n",
      "        [[ 21.1219,  21.7805,  22.4390],\n",
      "         [-15.8632, -16.3692, -16.8751],\n",
      "         [ -6.0035,  -6.1837,  -6.3638],\n",
      "         [-31.6488, -32.6198, -33.5908],\n",
      "         [ 27.8951,  28.7654,  29.6357],\n",
      "         [ 10.8780,  11.2233,  11.5686],\n",
      "         [  3.6500,   3.7701,   3.8902],\n",
      "         [ -2.4320,  -2.5152,  -2.5983],\n",
      "         [-31.1406, -32.0874, -33.0342],\n",
      "         [ -6.6323,  -6.8179,  -7.0036]],\n",
      "\n",
      "        [[  2.6834,   3.3419,   4.0004],\n",
      "         [ -1.6963,  -2.2022,  -2.7082],\n",
      "         [ -0.9606,  -1.1407,  -1.3208],\n",
      "         [ -4.4613,  -5.4322,  -6.4032],\n",
      "         [  3.5263,   4.3966,   5.2669],\n",
      "         [  1.2094,   1.5547,   1.9000],\n",
      "         [  0.2871,   0.4072,   0.5273],\n",
      "         [ -0.1044,  -0.1875,  -0.2707],\n",
      "         [ -4.6296,  -5.5764,  -6.5232],\n",
      "         [ -1.4342,  -1.6198,  -1.8055]],\n",
      "\n",
      "        [[ 15.8538,  16.5123,  17.1708],\n",
      "         [-11.8155, -12.3215, -12.8274],\n",
      "         [ -4.5627,  -4.7428,  -4.9229],\n",
      "         [-23.8810, -24.8519, -25.8229],\n",
      "         [ 20.9326,  21.8029,  22.6732],\n",
      "         [  8.1155,   8.4609,   8.8062],\n",
      "         [  2.6892,   2.8093,   2.9294],\n",
      "         [ -1.7670,  -1.8501,  -1.9333],\n",
      "         [-23.5660, -24.5128, -25.4597],\n",
      "         [ -5.1471,  -5.3328,  -5.5184]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[36., 37., 38., 39.]],\n",
      "\n",
      "        [[16., 17., 18., 19.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ 23.7560,  24.4145,  25.0731],\n",
      "         [-17.8871, -18.3930, -18.8990],\n",
      "         [ -6.7240,  -6.9041,  -7.0842],\n",
      "         [-35.5328, -36.5038, -37.4748],\n",
      "         [ 31.3763,  32.2466,  33.1169],\n",
      "         [ 12.2593,  12.6046,  12.9499],\n",
      "         [  4.1304,   4.2505,   4.3707],\n",
      "         [ -2.7645,  -2.8477,  -2.9308],\n",
      "         [-34.9279, -35.8747, -36.8215],\n",
      "         [ -7.3749,  -7.5605,  -7.7462]],\n",
      "\n",
      "        [[ 10.5856,  11.2441,  11.9026],\n",
      "         [ -7.7678,  -8.2738,  -8.7797],\n",
      "         [ -3.1219,  -3.3020,  -3.4821],\n",
      "         [-16.1131, -17.0841, -18.0550],\n",
      "         [ 13.9701,  14.8404,  15.7107],\n",
      "         [  5.3531,   5.6984,   6.0437],\n",
      "         [  1.7284,   1.8485,   1.9686],\n",
      "         [ -1.1020,  -1.1851,  -1.2682],\n",
      "         [-15.9914, -16.9383, -17.8851],\n",
      "         [ -3.6619,  -3.8476,  -4.0332]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 1\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[12., 13., 14., 15.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  2.6834,   3.3419,   4.0004],\n",
      "         [ -1.6963,  -2.2022,  -2.7082],\n",
      "         [ -0.9606,  -1.1407,  -1.3208],\n",
      "         [ -4.4613,  -5.4322,  -6.4032],\n",
      "         [  3.5263,   4.3966,   5.2669],\n",
      "         [  1.2094,   1.5547,   1.9000],\n",
      "         [  0.2871,   0.4072,   0.5273],\n",
      "         [ -0.1044,  -0.1875,  -0.2707],\n",
      "         [ -4.6296,  -5.5764,  -6.5232],\n",
      "         [ -1.4342,  -1.6198,  -1.8055]],\n",
      "\n",
      "        [[  7.9515,   8.6100,   9.2686],\n",
      "         [ -5.7440,  -6.2499,  -6.7559],\n",
      "         [ -2.4015,  -2.5816,  -2.7617],\n",
      "         [-12.2291, -13.2001, -14.1711],\n",
      "         [ 10.4888,  11.3591,  12.2294],\n",
      "         [  3.9718,   4.3172,   4.6625],\n",
      "         [  1.2479,   1.3680,   1.4881],\n",
      "         [ -0.7694,  -0.8526,  -0.9357],\n",
      "         [-12.2041, -13.1510, -14.0978],\n",
      "         [ -2.9194,  -3.1050,  -3.2906]],\n",
      "\n",
      "        [[  5.3174,   5.9760,   6.6345],\n",
      "         [ -3.7201,  -4.2261,  -4.7320],\n",
      "         [ -1.6810,  -1.8611,  -2.0412],\n",
      "         [ -8.3452,  -9.3162, -10.2872],\n",
      "         [  7.0076,   7.8779,   8.7482],\n",
      "         [  2.5906,   2.9359,   3.2812],\n",
      "         [  0.7675,   0.8876,   1.0077],\n",
      "         [ -0.4369,  -0.5201,  -0.6032],\n",
      "         [ -8.4169,  -9.3637, -10.3105],\n",
      "         [ -2.1768,  -2.3624,  -2.5481]],\n",
      "\n",
      "        [[ 18.4879,  19.1464,  19.8049],\n",
      "         [-13.8394, -14.3453, -14.8513],\n",
      "         [ -5.2831,  -5.4632,  -5.6433],\n",
      "         [-27.7649, -28.7359, -29.7069],\n",
      "         [ 24.4138,  25.2841,  26.1544],\n",
      "         [  9.4968,   9.8421,  10.1874],\n",
      "         [  3.1696,   3.2897,   3.4098],\n",
      "         [ -2.0995,  -2.1826,  -2.2658],\n",
      "         [-27.3533, -28.3001, -29.2469],\n",
      "         [ -5.8897,  -6.0753,  -6.2610]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[20., 21., 22., 23.]],\n",
      "\n",
      "        [[36., 37., 38., 39.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[ 13.2197,  13.8782,  14.5367],\n",
      "         [ -9.7917, -10.2976, -10.8036],\n",
      "         [ -3.8423,  -4.0224,  -4.2025],\n",
      "         [-19.9970, -20.9680, -21.9390],\n",
      "         [ 17.4513,  18.3216,  19.1919],\n",
      "         [  6.7343,   7.0796,   7.4249],\n",
      "         [  2.2088,   2.3289,   2.4490],\n",
      "         [ -1.4345,  -1.5176,  -1.6007],\n",
      "         [-19.7787, -20.7255, -21.6724],\n",
      "         [ -4.4045,  -4.5902,  -4.7758]],\n",
      "\n",
      "        [[ 23.7560,  24.4145,  25.0731],\n",
      "         [-17.8871, -18.3930, -18.8990],\n",
      "         [ -6.7240,  -6.9041,  -7.0842],\n",
      "         [-35.5328, -36.5038, -37.4748],\n",
      "         [ 31.3763,  32.2466,  33.1169],\n",
      "         [ 12.2593,  12.6046,  12.9499],\n",
      "         [  4.1304,   4.2505,   4.3707],\n",
      "         [ -2.7645,  -2.8477,  -2.9308],\n",
      "         [-34.9279, -35.8747, -36.8215],\n",
      "         [ -7.3749,  -7.5605,  -7.7462]],\n",
      "\n",
      "        [[  0.0493,   0.7078,   1.3663],\n",
      "         [  0.3276,  -0.1784,  -0.6843],\n",
      "         [ -0.2402,  -0.4203,  -0.6004],\n",
      "         [ -0.5773,  -1.5483,  -2.5193],\n",
      "         [  0.0451,   0.9154,   1.7857],\n",
      "         [ -0.1719,   0.1734,   0.5188],\n",
      "         [ -0.1933,  -0.0732,   0.0469],\n",
      "         [  0.2281,   0.1450,   0.0619],\n",
      "         [ -0.8423,  -1.7891,  -2.7359],\n",
      "         [ -0.6916,  -0.8772,  -1.0629]],\n",
      "\n",
      "        [[ 15.8538,  16.5123,  17.1708],\n",
      "         [-11.8155, -12.3215, -12.8274],\n",
      "         [ -4.5627,  -4.7428,  -4.9229],\n",
      "         [-23.8810, -24.8519, -25.8229],\n",
      "         [ 20.9326,  21.8029,  22.6732],\n",
      "         [  8.1155,   8.4609,   8.8062],\n",
      "         [  2.6892,   2.8093,   2.9294],\n",
      "         [ -1.7670,  -1.8501,  -1.9333],\n",
      "         [-23.5660, -24.5128, -25.4597],\n",
      "         [ -5.1471,  -5.3328,  -5.5184]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[16., 17., 18., 19.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ 10.5856,  11.2441,  11.9026],\n",
      "         [ -7.7678,  -8.2738,  -8.7797],\n",
      "         [ -3.1219,  -3.3020,  -3.4821],\n",
      "         [-16.1131, -17.0841, -18.0550],\n",
      "         [ 13.9701,  14.8404,  15.7107],\n",
      "         [  5.3531,   5.6984,   6.0437],\n",
      "         [  1.7284,   1.8485,   1.9686],\n",
      "         [ -1.1020,  -1.1851,  -1.2682],\n",
      "         [-15.9914, -16.9383, -17.8851],\n",
      "         [ -3.6619,  -3.8476,  -4.0332]],\n",
      "\n",
      "        [[ 21.1219,  21.7805,  22.4390],\n",
      "         [-15.8632, -16.3692, -16.8751],\n",
      "         [ -6.0035,  -6.1837,  -6.3638],\n",
      "         [-31.6488, -32.6198, -33.5908],\n",
      "         [ 27.8951,  28.7654,  29.6357],\n",
      "         [ 10.8780,  11.2233,  11.5686],\n",
      "         [  3.6500,   3.7701,   3.8902],\n",
      "         [ -2.4320,  -2.5152,  -2.5983],\n",
      "         [-31.1406, -32.0874, -33.0342],\n",
      "         [ -6.6323,  -6.8179,  -7.0036]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = nn.Conv1d(1, 10, 2)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "dl = DataLoader(dataset, batch_size=4, worker_init_fn=seed_worker, generator=g, shuffle=True, num_workers=2)\n",
    "for i in range(2):\n",
    "    print(\"Epoch\", i)\n",
    "    for batch, label in dl:\n",
    "       print(batch.unsqueeze(1).shape)\n",
    "       print(batch.unsqueeze(1))\n",
    "       o = a(batch.unsqueeze(1))\n",
    "       print(o.shape)\n",
    "       print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data and the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/whole_sequence.fasta', 'r') as f:\n",
    "    seqs = list(SeqIO.parse(f, 'fasta'))\n",
    "seq = {s.id:str(s.seq) for s in seqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(seq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EH1(72)': 'MLLPETRNLLDLMDAATRGGRPRLETLPHAVGRKAVDKMSEDGEADPPEVAEVANGGFAGPASEIRFRRYRPLGEAAGLLPTLIYYHGGGFVIGNIETHDSTCRRLANKSRCQVISIDYRLAPEHPFPAPIDDGIAAFRHIRDNAESFGADAARLAVGGDSAGGAMAAVVCQACRDAGETGPAFQMLIYPATDSSRESASRVAFAEGYFLSKALMDWFWEAYVPEDTDLTDLRLSPLLATDFTGLPPAFVLTAGYDPLRDEGRAYADRLIEAGIKTTYVNYPGTIHGFFSLTRFLSQGLKANDEAAAVMGAHFGT',\n",
       " 'EH2(71)': 'MGLQKLIVRTLMKLPESWILKLAGGTPVEIDGRTMDPRIQLLAAQGAKAPSMTSMSIEDARKSADEGLALLDAKPRRTVSILSRTIPGPAGDLHVRIYTPAGATGPLPGIVYYHMGGCVIGNLETCNTFCSILADDCRAIVVSVDYRLAPEHKFPAAMDDAVASFDWVSENAAALGIDPTRLGVGGDSAGGWLSAVVCQTRKAEGKTQPKAQLLIYPATDLDAKEGSMQSCAEIYPLTAEIMDWFMQQFLNSPEDAKDLKASPAHSEDLSGLAPALIMTAGFDVLRDQGEAYGNRLRDAGVPVTYRCYDSLSHAYTAFSGAVPAARQACEEIARDMARALG',\n",
       " 'EH3(69)': 'MPDTTSLNIADDVRMDPRLKAMLAAFPMMEQQTFQTREEQVANANTPEATAAREQLKMMMDMMDSEEFAPSDNLDISTREFTSSPDGNAIKIQFIRPKGKQKVPCVYYIHGGGMMIMSAFYGNYRAWGKMIANNGVAVAMVDFRNCLSPSSAPEVAPFPAGLNDCVSGLKWVSENADELSIDKNKIIIAGESGGGNLTLATGLKLKQDGNIDLVKGLYALCPYIAGKWPQDRFPSSSENNGIMIELHNNQGALAYGIEQLEAENPLAWPSFASAEDMQGLPPTVINVNECDPLRDEGIDFYRRLMAAGVPARCRQVMGTCHAGDMFVAVIPDVSADTAADIARTAKGG',\n",
       " 'CalB(68)': 'MALPSGSDPAFSQPKSVLDAGLTCQGASPSSVSKPILLVPGTGTTGPQSFDSNWIPLSTQLGYTPCWISPPPFMLNDTQVNTEYMVNAITALYAGSGNNKLPVLTWSQGGLVAQWGLTFFPSIRSKVDRLMAFAPDYKGTVLAGPLDALAVSAPSVWQQTTGSALTTALRNAGGLTQIVPTTNLYSATDEIVQPQVSNSPLDSSYLFNGKNVQAQAVCGPLFVIDHAGSLTSQFSYVVGRSALRSTTGQARSADYGITDCNPLPANDLTPEQKVAAAALLAPAAAAIVAGPKQNCEPDLMPYARPFAVGKRTCSGIVTPLE',\n",
       " 'EH4(67)': 'MSLQRMIVRTLLKLPDGLLVKMSGGKPLEIDGRTLDARVQLLASQGAKAPSMTTLPIEEARKGADDGLAMLDAKPRRNVSILSRSIPGPEGELHVRVYTPAGATGPLPGIVYYHMGGCVIGGLETCNTFCSILAEDCRAIVVSVDYRLAPEHKFPAAIDDAIASYDWVYQNATALGIDNTRLGLGGDSAGGWLSAVVCQHRKREGLPQPKAQLLIYPATDLQMTGGSMESCKDVYPLTREIMDWFMAQFLTSDADRSDWRGSPGQTADLSGLAPAIVATAGFDVLRDQGEAYANKLKAAGVPASYHCYDSLAHAFTAFSGTVPAAKQACEELAREMAKALNA',\n",
       " 'EH5(67)': 'MALNSQAEELLKRAAESGTPGLGEGTPEEGRAIFATTTQLLGLPAPDVKDTKEIQISGPNGPIRTLVITPDGVETNNLPLFIYYHGGGWVIGSPETHYEECCYYANEAQCIVLVPDYRLAPEYPFPAAPEDCYAVLQWAADNAESLGADKSRIAVGGDSAGGNLSAVVAQMTQQRNGPELALQLLIYPATRMGADTQSYKDFEDGYFLTAKAMNWFFGHYLKKAEDWDNLLASPLLNDDLAGLAPAYVVTAGFDPLRDEGRAYADKLKAAGVPVEYVCYEGQIHGFASMAGALDEARSFLDEAAKVLRKAFNK',\n",
       " 'EH6(66)': 'MPLHPQIEGLLQQMAAAGGKGFHQMEVDECRQTFGGLLNSLPPSQQKIASAQDRGIPSPNGPVKVRVYTPEGSGPFPVMAYFHGGGWVIGDLETHDSLCRELCGAVGMVVVSVDYRLAPEHKFPAAPDDCVAVTRWIAANAAALNADASRIAVGGDSAGGNLAAVVAQRLRDEDALKLAAQLLIYPVVHLDGVATPSMIENAEGYLLTRKDMEWFGGHYLASPADGQNASASPILAKSLAGLPPALVLTCEFDPLRDEGEKYGKALQAAGVPTTISRHDGTIHATFSFFTALEPGRRMADEAIRWLKEQLVK',\n",
       " 'EH7(64)': 'MEFPMAQSNIIAGMDLNRLDRIAEHLDRAYLHPGKLAGTMTLVARRGEVVYCQAQGLRDVERQLPVERDTLFRIYSMTKPITSIALMQLYEQGRFLLDEPVHKYIPTWKNLRVYKTGSHPQMLTTAPQRPMTIRDLLTHQSGLTYGFMNRTNVDAAYRSLKLDGGPGHTLDRLIDELARLPLEFSPGTAWNYSVATDVCGYLVQLLSGMSLDDYFSKHIFQPLGMPDTFFTVPAEKLSRFAACYEYQPGDSFSLQDDPQGSAFAKAHGYLSGGGGLVSCVDDYYRFAQALANGGELDGARIIGRKTLEFMRMNHLPDNKGLPDVAIGSFSETPYDGTGFGLGFSVKLDVAKSQTVGSVGEYGWGGMASTNFFIDPEEDLLMVFMTQLIPSSTYAVRQELRAIINGALVD',\n",
       " 'EH8(63)': 'MNPAVIERATVRALMSLPGPVLERLAAGLETHSRPHLDSRLRFLLALSGAKPTLDSGTVEQARQIYRSTLALLDMAPVSLPVVVDHQVSMEDGSQILVRRYRPADAPLVSPAIMFFHGGGFTIGGVEEYDRLCRYIAKRTNAVVLSVDYRLAPEHPAPAGMDDALEAWRWLLNNTAQLGLDPNRLAVMGDSAGGCMSAVVSQQAKLAGLALPALQVLIYPTTDAALAHPSVQTLGQGFGLDIPLLTWFRGHFVQDPAVIEDYRVSPLRNPDLTGLPEAIVITATDPLRDEGLEYAQKLREAGNTVTSLDYPELIHGFISMGGVVPAARKAINDICVETKRRL',\n",
       " 'EH9(61)': 'MEKKMALDKQAAEILKRAEESDTPGLGEGSPAEGREVFAGTTALLGLPTPEGQRISEVQIPGPSGDIRTRIIHPLEGLADNLPILIYYHGGGWVIGSPETHEGETCFYANEANCVVLVPDYRLAPEDPFPAAPDDCYAVLEWANANAETFGGDASRIAVAGDSAGGNLSAVVSQMAHANNGPDIALQLLIYPATRMGATTESYREFNDGYFLTGKAMDWFFNHYLKRPEDWDALKASPLLAPDLSGLPPAYIMTAGFDPLRDEGKAYAERLQQAGVPVDYVCYEEQIHGFVSMAGALDQGKQFLREAAAVLRRAFTS'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = islice(a, 10)\n",
    "dict(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the tokenizer and the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "low_cpu_mem_usage: when loading try not to use more memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", low_mem_usage=True)\n",
    "tok = tokenizer(list(seq.values())[:2], padding=True, truncation=True, return_tensors=\"pt\", is_split_into_words=False)\n",
    "tok = tok.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "EsmModel.__init__() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacebook/esm2_t6_8M_UR50D\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_pooling_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/bioml_pycaret/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/bioml_pycaret/lib/python3.10/site-packages/transformers/modeling_utils.py:3462\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3456\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3457\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3458\u001b[0m )\n\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3462\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3465\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "\u001b[0;31mTypeError\u001b[0m: EsmModel.__init__() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, output_hidden_states=True, low_cpu_mem_usage=True, dtype=torch.float32)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309.58004"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_memory_footprint() / 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1419,  0.5839, -0.0722,  ...,  1.1745, -0.0934, -0.4214],\n",
       "         [ 0.4127,  0.0940, -0.1655,  ...,  0.8397, -0.2216, -0.2433],\n",
       "         [ 0.0768, -0.5086, -0.0294,  ...,  0.3882, -0.0305,  0.1067],\n",
       "         ...,\n",
       "         [-0.2578,  0.2489,  0.5511,  ...,  0.6463, -0.5266, -0.0942],\n",
       "         [-0.3510,  0.2589,  0.5853,  ...,  0.4988, -0.5679, -0.1550],\n",
       "         [-0.3296,  0.2368,  0.2401,  ...,  0.4682, -0.6849, -0.3094]],\n",
       "\n",
       "        [[ 0.0936,  0.6978, -0.0490,  ...,  1.0340, -0.1707, -0.3038],\n",
       "         [ 0.3237,  0.4791, -0.1456,  ...,  0.7708, -0.2135, -0.2907],\n",
       "         [ 0.0065, -0.2744,  0.2615,  ..., -0.0429,  0.2885,  0.1101],\n",
       "         ...,\n",
       "         [-0.4722, -0.2125, -0.2723,  ...,  0.9457, -0.3602,  0.2324],\n",
       "         [-0.0936, -0.4826,  0.0658,  ...,  0.3896, -0.1248, -0.1090],\n",
       "         [-0.0047,  0.0668,  0.1097,  ...,  0.6548, -0.6016, -0.2547]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**tok)\n",
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 320])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 160])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.AvgPool1d(2)(output.last_hidden_state[-1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the attention mask to remove the padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "mask = tok[\"attention_mask\"].bool()\n",
    "for num, x in enumerate(output.last_hidden_state):\n",
    "    masked_x = x[mask[num]]\n",
    "    results[num] = masked_x.mean(dim=0).detach().cpu().numpy()\n",
    "    # detach removes the tensor from the computation graph (the gradient won't be computed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([320])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(results[0], dim=0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data2.csv\"\n",
    "embeddings = pd.DataFrame(results).T\n",
    "embeddings.to_csv(path, mode='a', header=not Path(path).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([343, 320])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[tok[\"attention_mask\"].bool()[1]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt the code to Load large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# https://huggingface.co/docs/datasets/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to load files within datasets -> from local or remote files (in these formats json, csv, text, parquet)  \n",
    "Since we have a fasta file that is not supported (because it will treat each line as a row so it will double the rows, but in fasta the first line is an id).  \n",
    "So we can process it in-memory to pandas, generators, dictionaries or list of dictionaries and use Datasets instead of load_dataset.  \n",
    "The load dataset returns a dataset dict with different splits (train, test, val) as keys and then a dataset object as values.\n",
    "\n",
    "But we are using a dataset object directl\n",
    "\n",
    "To load fasta files use from generator beacause it is in-memory and the file might be too large to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 294\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = load_dataset(\"text\", data_files=\"../data/whole_sequence.fasta\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "with open(\"../data/whole_sequence.fasta\", 'r') as f:\n",
    "    seqs = SeqIO.parse(f, 'fasta')\n",
    "    d = pd.Series({s.id:str(s.seq) for s in seqs}).to_frame()\n",
    "    d.columns = [\"sequences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process or tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use map to apply the tokenizer function to the entire dataset\n",
    "The map will create and add the new columns ('input_ids', 'attention_mask') coming from the tokenizer to the datatset   \n",
    "but you will have to change its format to torch tensors for the models to read it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = b.map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)\n",
    "type(dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = dataset.select_columns([\"input_ids\", \"attention_mask\"])\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to extract the embeddings use the dataloader from pytorch to create the batches for you  \n",
    "It will only return the input_ids and the attention mask (the ids are lost, so yoou don't know which sequence is which)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20, 10,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "for batch in dataloader:\n",
    "    u = batch\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioML.deep.embeddings as emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f0b02e36284dd68ee741ed7050c697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c268ce1f5b6c47518a30603bae5cb322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = emb.TokenizeFasta(emb.LLMConfig()).tokenize(\"../data/whole_sequence.fasta\")\n",
    "embed = emb.ExtractEmbeddings(emb.LLMConfig())\n",
    "seq_keys = list(data[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, batch in enumerate(DataLoader(data, batch_size=batch_size)):\n",
    "    batch_seq_keys = seq_keys[num*batch_size:(num+1)*batch_size]\n",
    "    results = embed.extract(batch_seq_keys, batch)\n",
    "    #embed.save(results, \"../data/embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.nn.Linear(320, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m attention_weights\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "attention_scores = attention_weights(output.hidden_states[-1])\n",
    "attention_weights = torch.softmax(attention_scores, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 109760])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = output.hidden_states[-1].reshape(output.hidden_states[-1].shape[0], -1)\n",
    "_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  ...,  0.4682, -0.6849, -0.3094],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -107712)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, 2048 - _temp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.nn.functional.pad(_temp, (0, 2048 - _temp.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  0.3390, -0.1853, -0.0982, -0.9235,  0.1019,\n",
       "        -0.4527, -0.6959], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109670"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(o[0].detach().numpy()).intersection(_temp[0].detach().numpy()))xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(output.hidden_states[-1][0][0].detach().numpy()).intersection(_temp[0][:100].detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test training using the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BioML.models.regression as regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv(\"../data/embeddings.csv\", index_col=0)\n",
    "label = list(range(len(embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-02-2024 12:07:16 INFO ------------------------------------------------------------------------------\n",
      "23-02-2024 12:07:16 INFO PycaretInterface parameters\n",
      "23-02-2024 12:07:16 INFO Seed: 200\n",
      "23-02-2024 12:07:16 INFO Budget time: 20\n",
      "23-02-2024 12:07:16 INFO The number of models to select: 3\n",
      "23-02-2024 12:07:16 INFO Output path: regression_training\n",
      "23-02-2024 12:07:16 INFO ----------------Trainer inputs-------------------------\n",
      "23-02-2024 12:07:16 INFO Number of kfolds: 5\n",
      "23-02-2024 12:07:16 INFO Number of iterations: 30\n"
     ]
    }
   ],
   "source": [
    "data = regression.DataParser(\"../data/embeddings.csv\", label)\n",
    "experiment = regression.PycaretInterface(\"regression\", 200, scaler= \"zscore\", budget_time=20, best_model=3, \n",
    "                                        output_path=\"regression_training\", optimize=\"RMSE\")\n",
    "\n",
    "regressor = regression.Regressor(test_size=0.2, optimize=\"RMSE\")\n",
    "training = regression.Trainer(experiment, regressor, 5, 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data according to sequence similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = regression.split.ClusterSpliter(\"../data/resultsDB_clu.tsv\", 5, random_state=experiment.seed, test_size=0.2)\n",
    "X_train, X_test = c.train_test_split(data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/02/23 12:18:52 INFO mlflow.tracking.fluent: Experiment with name 'Regression' does not exist. Creating a new experiment.\n",
      "23-02-2024 12:18:53 INFO --------------------------------------------------------\n",
      "23-02-2024 12:18:53 INFO Training regression models\n",
      "23-02-2024 12:18:53 INFO The models used ['lr', 'lasso', 'ridge', 'en', 'lar', 'llar', 'omp', 'br', 'par', 'huber', 'svm', 'knn', 'dt', 'rf', 'et', 'gbr', 'mlp', 'xgboost', 'catboost', 'dummy']\n",
      "23-02-2024 12:18:53 INFO Time budget is 20 minutes\n",
      "23-02-2024 12:20:27 INFO Training over: Total runtime 1.565 minutes\n",
      "23-02-2024 12:20:27 INFO Analyse the best models and plotting them\n",
      "23-02-2024 12:20:27 INFO Analyse the top 1 model: catboost\n",
      "23-02-2024 12:24:10 INFO Analyse the top 2 model: br\n",
      "23-02-2024 12:24:13 INFO Analyse the top 3 model: rf\n",
      "23-02-2024 12:24:27 INFO --------Stacking the best models--------\n",
      "23-02-2024 12:24:27 INFO ----------Stacking the best models--------------\n",
      "23-02-2024 12:27:58 INFO --------Creating an ensemble model--------\n",
      "23-02-2024 12:27:58 INFO ----------Creating a majority voting model--------------\n",
      "23-02-2024 12:27:58 INFO fold: 5\n",
      "23-02-2024 12:27:58 INFO weights: None\n",
      "23-02-2024 12:28:45 INFO --------Retuning the best models--------\n",
      "23-02-2024 12:28:45 INFO Retuning catboost\n",
      "23-02-2024 12:28:45 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:28:45 INFO num_iter: 30\n",
      "23-02-2024 12:28:45 INFO fold: 5\n",
      "23-02-2024 12:45:00 INFO Retuning br\n",
      "23-02-2024 12:45:00 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:45:00 INFO num_iter: 30\n",
      "23-02-2024 12:45:00 INFO fold: 5\n",
      "23-02-2024 12:45:16 INFO Retuning rf\n",
      "23-02-2024 12:45:16 INFO ---------Retuning the best models--------------\n",
      "23-02-2024 12:45:16 INFO num_iter: 30\n",
      "23-02-2024 12:45:16 INFO fold: 5\n",
      "23-02-2024 12:46:25 INFO --------Stacking the best models--------\n",
      "23-02-2024 12:46:25 INFO ----------Stacking the best models--------------\n",
      "23-02-2024 12:46:44 INFO --------Creating an ensemble model--------\n",
      "23-02-2024 12:46:44 INFO ----------Creating a majority voting model--------------\n",
      "23-02-2024 12:46:44 INFO fold: 5\n",
      "23-02-2024 12:46:44 INFO weights: None\n"
     ]
    }
   ],
   "source": [
    "results, models_dict = training.generate_training_results(X_train, data.label, True,\n",
    "                                                          test_data=X_test, fold_strategy=c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml_pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
