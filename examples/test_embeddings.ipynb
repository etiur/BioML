{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,  AutoModelForSequenceClassification\n",
    "from Bio import SeqIO\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "# https://huggingface.co/blog/AmelieSchreiber/esmbind\n",
    "# the minimum for the ESM2 is 650M if we want better performance than ESM1b with 650M as well.\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(torch.arange(40, dtype=torch.float32).view(10, 4), torch.tensor([i for i in range(10)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
       "        36, 37, 38, 39])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(40).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[12., 13., 14., 15.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]],\n",
      "\n",
      "        [[20., 21., 22., 23.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  7.9515,   8.6100,   9.2686],\n",
      "         [ -5.7440,  -6.2499,  -6.7559],\n",
      "         [ -2.4015,  -2.5816,  -2.7617],\n",
      "         [-12.2291, -13.2001, -14.1711],\n",
      "         [ 10.4888,  11.3591,  12.2294],\n",
      "         [  3.9718,   4.3172,   4.6625],\n",
      "         [  1.2479,   1.3680,   1.4881],\n",
      "         [ -0.7694,  -0.8526,  -0.9357],\n",
      "         [-12.2041, -13.1510, -14.0978],\n",
      "         [ -2.9194,  -3.1050,  -3.2906]],\n",
      "\n",
      "        [[ 18.4879,  19.1464,  19.8049],\n",
      "         [-13.8394, -14.3453, -14.8513],\n",
      "         [ -5.2831,  -5.4632,  -5.6433],\n",
      "         [-27.7649, -28.7359, -29.7069],\n",
      "         [ 24.4138,  25.2841,  26.1544],\n",
      "         [  9.4968,   9.8421,  10.1874],\n",
      "         [  3.1696,   3.2897,   3.4098],\n",
      "         [ -2.0995,  -2.1826,  -2.2658],\n",
      "         [-27.3533, -28.3001, -29.2469],\n",
      "         [ -5.8897,  -6.0753,  -6.2610]],\n",
      "\n",
      "        [[ 13.2197,  13.8782,  14.5367],\n",
      "         [ -9.7917, -10.2976, -10.8036],\n",
      "         [ -3.8423,  -4.0224,  -4.2025],\n",
      "         [-19.9970, -20.9680, -21.9390],\n",
      "         [ 17.4513,  18.3216,  19.1919],\n",
      "         [  6.7343,   7.0796,   7.4249],\n",
      "         [  2.2088,   2.3289,   2.4490],\n",
      "         [ -1.4345,  -1.5176,  -1.6007],\n",
      "         [-19.7787, -20.7255, -21.6724],\n",
      "         [ -4.4045,  -4.5902,  -4.7758]],\n",
      "\n",
      "        [[  5.3174,   5.9760,   6.6345],\n",
      "         [ -3.7201,  -4.2261,  -4.7320],\n",
      "         [ -1.6810,  -1.8611,  -2.0412],\n",
      "         [ -8.3452,  -9.3162, -10.2872],\n",
      "         [  7.0076,   7.8779,   8.7482],\n",
      "         [  2.5906,   2.9359,   3.2812],\n",
      "         [  0.7675,   0.8876,   1.0077],\n",
      "         [ -0.4369,  -0.5201,  -0.6032],\n",
      "         [ -8.4169,  -9.3637, -10.3105],\n",
      "         [ -2.1768,  -2.3624,  -2.5481]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]],\n",
      "\n",
      "        [[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  0.0493,   0.7078,   1.3663],\n",
      "         [  0.3276,  -0.1784,  -0.6843],\n",
      "         [ -0.2402,  -0.4203,  -0.6004],\n",
      "         [ -0.5773,  -1.5483,  -2.5193],\n",
      "         [  0.0451,   0.9154,   1.7857],\n",
      "         [ -0.1719,   0.1734,   0.5188],\n",
      "         [ -0.1933,  -0.0732,   0.0469],\n",
      "         [  0.2281,   0.1450,   0.0619],\n",
      "         [ -0.8423,  -1.7891,  -2.7359],\n",
      "         [ -0.6916,  -0.8772,  -1.0629]],\n",
      "\n",
      "        [[ 21.1219,  21.7805,  22.4390],\n",
      "         [-15.8632, -16.3692, -16.8751],\n",
      "         [ -6.0035,  -6.1837,  -6.3638],\n",
      "         [-31.6488, -32.6198, -33.5908],\n",
      "         [ 27.8951,  28.7654,  29.6357],\n",
      "         [ 10.8780,  11.2233,  11.5686],\n",
      "         [  3.6500,   3.7701,   3.8902],\n",
      "         [ -2.4320,  -2.5152,  -2.5983],\n",
      "         [-31.1406, -32.0874, -33.0342],\n",
      "         [ -6.6323,  -6.8179,  -7.0036]],\n",
      "\n",
      "        [[  2.6834,   3.3419,   4.0004],\n",
      "         [ -1.6963,  -2.2022,  -2.7082],\n",
      "         [ -0.9606,  -1.1407,  -1.3208],\n",
      "         [ -4.4613,  -5.4322,  -6.4032],\n",
      "         [  3.5263,   4.3966,   5.2669],\n",
      "         [  1.2094,   1.5547,   1.9000],\n",
      "         [  0.2871,   0.4072,   0.5273],\n",
      "         [ -0.1044,  -0.1875,  -0.2707],\n",
      "         [ -4.6296,  -5.5764,  -6.5232],\n",
      "         [ -1.4342,  -1.6198,  -1.8055]],\n",
      "\n",
      "        [[ 15.8538,  16.5123,  17.1708],\n",
      "         [-11.8155, -12.3215, -12.8274],\n",
      "         [ -4.5627,  -4.7428,  -4.9229],\n",
      "         [-23.8810, -24.8519, -25.8229],\n",
      "         [ 20.9326,  21.8029,  22.6732],\n",
      "         [  8.1155,   8.4609,   8.8062],\n",
      "         [  2.6892,   2.8093,   2.9294],\n",
      "         [ -1.7670,  -1.8501,  -1.9333],\n",
      "         [-23.5660, -24.5128, -25.4597],\n",
      "         [ -5.1471,  -5.3328,  -5.5184]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[36., 37., 38., 39.]],\n",
      "\n",
      "        [[16., 17., 18., 19.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ 23.7560,  24.4145,  25.0731],\n",
      "         [-17.8871, -18.3930, -18.8990],\n",
      "         [ -6.7240,  -6.9041,  -7.0842],\n",
      "         [-35.5328, -36.5038, -37.4748],\n",
      "         [ 31.3763,  32.2466,  33.1169],\n",
      "         [ 12.2593,  12.6046,  12.9499],\n",
      "         [  4.1304,   4.2505,   4.3707],\n",
      "         [ -2.7645,  -2.8477,  -2.9308],\n",
      "         [-34.9279, -35.8747, -36.8215],\n",
      "         [ -7.3749,  -7.5605,  -7.7462]],\n",
      "\n",
      "        [[ 10.5856,  11.2441,  11.9026],\n",
      "         [ -7.7678,  -8.2738,  -8.7797],\n",
      "         [ -3.1219,  -3.3020,  -3.4821],\n",
      "         [-16.1131, -17.0841, -18.0550],\n",
      "         [ 13.9701,  14.8404,  15.7107],\n",
      "         [  5.3531,   5.6984,   6.0437],\n",
      "         [  1.7284,   1.8485,   1.9686],\n",
      "         [ -1.1020,  -1.1851,  -1.2682],\n",
      "         [-15.9914, -16.9383, -17.8851],\n",
      "         [ -3.6619,  -3.8476,  -4.0332]]], grad_fn=<ConvolutionBackward0>)\n",
      "Epoch 1\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[ 4.,  5.,  6.,  7.]],\n",
      "\n",
      "        [[12., 13., 14., 15.]],\n",
      "\n",
      "        [[ 8.,  9., 10., 11.]],\n",
      "\n",
      "        [[28., 29., 30., 31.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[  2.6834,   3.3419,   4.0004],\n",
      "         [ -1.6963,  -2.2022,  -2.7082],\n",
      "         [ -0.9606,  -1.1407,  -1.3208],\n",
      "         [ -4.4613,  -5.4322,  -6.4032],\n",
      "         [  3.5263,   4.3966,   5.2669],\n",
      "         [  1.2094,   1.5547,   1.9000],\n",
      "         [  0.2871,   0.4072,   0.5273],\n",
      "         [ -0.1044,  -0.1875,  -0.2707],\n",
      "         [ -4.6296,  -5.5764,  -6.5232],\n",
      "         [ -1.4342,  -1.6198,  -1.8055]],\n",
      "\n",
      "        [[  7.9515,   8.6100,   9.2686],\n",
      "         [ -5.7440,  -6.2499,  -6.7559],\n",
      "         [ -2.4015,  -2.5816,  -2.7617],\n",
      "         [-12.2291, -13.2001, -14.1711],\n",
      "         [ 10.4888,  11.3591,  12.2294],\n",
      "         [  3.9718,   4.3172,   4.6625],\n",
      "         [  1.2479,   1.3680,   1.4881],\n",
      "         [ -0.7694,  -0.8526,  -0.9357],\n",
      "         [-12.2041, -13.1510, -14.0978],\n",
      "         [ -2.9194,  -3.1050,  -3.2906]],\n",
      "\n",
      "        [[  5.3174,   5.9760,   6.6345],\n",
      "         [ -3.7201,  -4.2261,  -4.7320],\n",
      "         [ -1.6810,  -1.8611,  -2.0412],\n",
      "         [ -8.3452,  -9.3162, -10.2872],\n",
      "         [  7.0076,   7.8779,   8.7482],\n",
      "         [  2.5906,   2.9359,   3.2812],\n",
      "         [  0.7675,   0.8876,   1.0077],\n",
      "         [ -0.4369,  -0.5201,  -0.6032],\n",
      "         [ -8.4169,  -9.3637, -10.3105],\n",
      "         [ -2.1768,  -2.3624,  -2.5481]],\n",
      "\n",
      "        [[ 18.4879,  19.1464,  19.8049],\n",
      "         [-13.8394, -14.3453, -14.8513],\n",
      "         [ -5.2831,  -5.4632,  -5.6433],\n",
      "         [-27.7649, -28.7359, -29.7069],\n",
      "         [ 24.4138,  25.2841,  26.1544],\n",
      "         [  9.4968,   9.8421,  10.1874],\n",
      "         [  3.1696,   3.2897,   3.4098],\n",
      "         [ -2.0995,  -2.1826,  -2.2658],\n",
      "         [-27.3533, -28.3001, -29.2469],\n",
      "         [ -5.8897,  -6.0753,  -6.2610]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([4, 1, 4])\n",
      "tensor([[[20., 21., 22., 23.]],\n",
      "\n",
      "        [[36., 37., 38., 39.]],\n",
      "\n",
      "        [[ 0.,  1.,  2.,  3.]],\n",
      "\n",
      "        [[24., 25., 26., 27.]]])\n",
      "torch.Size([4, 10, 3])\n",
      "tensor([[[ 13.2197,  13.8782,  14.5367],\n",
      "         [ -9.7917, -10.2976, -10.8036],\n",
      "         [ -3.8423,  -4.0224,  -4.2025],\n",
      "         [-19.9970, -20.9680, -21.9390],\n",
      "         [ 17.4513,  18.3216,  19.1919],\n",
      "         [  6.7343,   7.0796,   7.4249],\n",
      "         [  2.2088,   2.3289,   2.4490],\n",
      "         [ -1.4345,  -1.5176,  -1.6007],\n",
      "         [-19.7787, -20.7255, -21.6724],\n",
      "         [ -4.4045,  -4.5902,  -4.7758]],\n",
      "\n",
      "        [[ 23.7560,  24.4145,  25.0731],\n",
      "         [-17.8871, -18.3930, -18.8990],\n",
      "         [ -6.7240,  -6.9041,  -7.0842],\n",
      "         [-35.5328, -36.5038, -37.4748],\n",
      "         [ 31.3763,  32.2466,  33.1169],\n",
      "         [ 12.2593,  12.6046,  12.9499],\n",
      "         [  4.1304,   4.2505,   4.3707],\n",
      "         [ -2.7645,  -2.8477,  -2.9308],\n",
      "         [-34.9279, -35.8747, -36.8215],\n",
      "         [ -7.3749,  -7.5605,  -7.7462]],\n",
      "\n",
      "        [[  0.0493,   0.7078,   1.3663],\n",
      "         [  0.3276,  -0.1784,  -0.6843],\n",
      "         [ -0.2402,  -0.4203,  -0.6004],\n",
      "         [ -0.5773,  -1.5483,  -2.5193],\n",
      "         [  0.0451,   0.9154,   1.7857],\n",
      "         [ -0.1719,   0.1734,   0.5188],\n",
      "         [ -0.1933,  -0.0732,   0.0469],\n",
      "         [  0.2281,   0.1450,   0.0619],\n",
      "         [ -0.8423,  -1.7891,  -2.7359],\n",
      "         [ -0.6916,  -0.8772,  -1.0629]],\n",
      "\n",
      "        [[ 15.8538,  16.5123,  17.1708],\n",
      "         [-11.8155, -12.3215, -12.8274],\n",
      "         [ -4.5627,  -4.7428,  -4.9229],\n",
      "         [-23.8810, -24.8519, -25.8229],\n",
      "         [ 20.9326,  21.8029,  22.6732],\n",
      "         [  8.1155,   8.4609,   8.8062],\n",
      "         [  2.6892,   2.8093,   2.9294],\n",
      "         [ -1.7670,  -1.8501,  -1.9333],\n",
      "         [-23.5660, -24.5128, -25.4597],\n",
      "         [ -5.1471,  -5.3328,  -5.5184]]], grad_fn=<ConvolutionBackward0>)\n",
      "torch.Size([2, 1, 4])\n",
      "tensor([[[16., 17., 18., 19.]],\n",
      "\n",
      "        [[32., 33., 34., 35.]]])\n",
      "torch.Size([2, 10, 3])\n",
      "tensor([[[ 10.5856,  11.2441,  11.9026],\n",
      "         [ -7.7678,  -8.2738,  -8.7797],\n",
      "         [ -3.1219,  -3.3020,  -3.4821],\n",
      "         [-16.1131, -17.0841, -18.0550],\n",
      "         [ 13.9701,  14.8404,  15.7107],\n",
      "         [  5.3531,   5.6984,   6.0437],\n",
      "         [  1.7284,   1.8485,   1.9686],\n",
      "         [ -1.1020,  -1.1851,  -1.2682],\n",
      "         [-15.9914, -16.9383, -17.8851],\n",
      "         [ -3.6619,  -3.8476,  -4.0332]],\n",
      "\n",
      "        [[ 21.1219,  21.7805,  22.4390],\n",
      "         [-15.8632, -16.3692, -16.8751],\n",
      "         [ -6.0035,  -6.1837,  -6.3638],\n",
      "         [-31.6488, -32.6198, -33.5908],\n",
      "         [ 27.8951,  28.7654,  29.6357],\n",
      "         [ 10.8780,  11.2233,  11.5686],\n",
      "         [  3.6500,   3.7701,   3.8902],\n",
      "         [ -2.4320,  -2.5152,  -2.5983],\n",
      "         [-31.1406, -32.0874, -33.0342],\n",
      "         [ -6.6323,  -6.8179,  -7.0036]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = nn.Conv1d(1, 10, 2)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "dl = DataLoader(dataset, batch_size=4, worker_init_fn=seed_worker, generator=g, shuffle=True, num_workers=2)\n",
    "for i in range(2):\n",
    "    print(\"Epoch\", i)\n",
    "    for batch, label in dl:\n",
    "       print(batch.unsqueeze(1).shape)\n",
    "       print(batch.unsqueeze(1))\n",
    "       o = a(batch.unsqueeze(1))\n",
    "       print(o.shape)\n",
    "       print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/whole_sequence.fasta', 'r') as f:\n",
    "    seqs = list(SeqIO.parse(f, 'fasta'))\n",
    "seq = {s.id:str(s.seq) for s in seqs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = iter(seq.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EH1(72)': 'MLLPETRNLLDLMDAATRGGRPRLETLPHAVGRKAVDKMSEDGEADPPEVAEVANGGFAGPASEIRFRRYRPLGEAAGLLPTLIYYHGGGFVIGNIETHDSTCRRLANKSRCQVISIDYRLAPEHPFPAPIDDGIAAFRHIRDNAESFGADAARLAVGGDSAGGAMAAVVCQACRDAGETGPAFQMLIYPATDSSRESASRVAFAEGYFLSKALMDWFWEAYVPEDTDLTDLRLSPLLATDFTGLPPAFVLTAGYDPLRDEGRAYADRLIEAGIKTTYVNYPGTIHGFFSLTRFLSQGLKANDEAAAVMGAHFGT',\n",
       " 'EH2(71)': 'MGLQKLIVRTLMKLPESWILKLAGGTPVEIDGRTMDPRIQLLAAQGAKAPSMTSMSIEDARKSADEGLALLDAKPRRTVSILSRTIPGPAGDLHVRIYTPAGATGPLPGIVYYHMGGCVIGNLETCNTFCSILADDCRAIVVSVDYRLAPEHKFPAAMDDAVASFDWVSENAAALGIDPTRLGVGGDSAGGWLSAVVCQTRKAEGKTQPKAQLLIYPATDLDAKEGSMQSCAEIYPLTAEIMDWFMQQFLNSPEDAKDLKASPAHSEDLSGLAPALIMTAGFDVLRDQGEAYGNRLRDAGVPVTYRCYDSLSHAYTAFSGAVPAARQACEEIARDMARALG',\n",
       " 'EH3(69)': 'MPDTTSLNIADDVRMDPRLKAMLAAFPMMEQQTFQTREEQVANANTPEATAAREQLKMMMDMMDSEEFAPSDNLDISTREFTSSPDGNAIKIQFIRPKGKQKVPCVYYIHGGGMMIMSAFYGNYRAWGKMIANNGVAVAMVDFRNCLSPSSAPEVAPFPAGLNDCVSGLKWVSENADELSIDKNKIIIAGESGGGNLTLATGLKLKQDGNIDLVKGLYALCPYIAGKWPQDRFPSSSENNGIMIELHNNQGALAYGIEQLEAENPLAWPSFASAEDMQGLPPTVINVNECDPLRDEGIDFYRRLMAAGVPARCRQVMGTCHAGDMFVAVIPDVSADTAADIARTAKGG',\n",
       " 'CalB(68)': 'MALPSGSDPAFSQPKSVLDAGLTCQGASPSSVSKPILLVPGTGTTGPQSFDSNWIPLSTQLGYTPCWISPPPFMLNDTQVNTEYMVNAITALYAGSGNNKLPVLTWSQGGLVAQWGLTFFPSIRSKVDRLMAFAPDYKGTVLAGPLDALAVSAPSVWQQTTGSALTTALRNAGGLTQIVPTTNLYSATDEIVQPQVSNSPLDSSYLFNGKNVQAQAVCGPLFVIDHAGSLTSQFSYVVGRSALRSTTGQARSADYGITDCNPLPANDLTPEQKVAAAALLAPAAAAIVAGPKQNCEPDLMPYARPFAVGKRTCSGIVTPLE',\n",
       " 'EH4(67)': 'MSLQRMIVRTLLKLPDGLLVKMSGGKPLEIDGRTLDARVQLLASQGAKAPSMTTLPIEEARKGADDGLAMLDAKPRRNVSILSRSIPGPEGELHVRVYTPAGATGPLPGIVYYHMGGCVIGGLETCNTFCSILAEDCRAIVVSVDYRLAPEHKFPAAIDDAIASYDWVYQNATALGIDNTRLGLGGDSAGGWLSAVVCQHRKREGLPQPKAQLLIYPATDLQMTGGSMESCKDVYPLTREIMDWFMAQFLTSDADRSDWRGSPGQTADLSGLAPAIVATAGFDVLRDQGEAYANKLKAAGVPASYHCYDSLAHAFTAFSGTVPAAKQACEELAREMAKALNA',\n",
       " 'EH5(67)': 'MALNSQAEELLKRAAESGTPGLGEGTPEEGRAIFATTTQLLGLPAPDVKDTKEIQISGPNGPIRTLVITPDGVETNNLPLFIYYHGGGWVIGSPETHYEECCYYANEAQCIVLVPDYRLAPEYPFPAAPEDCYAVLQWAADNAESLGADKSRIAVGGDSAGGNLSAVVAQMTQQRNGPELALQLLIYPATRMGADTQSYKDFEDGYFLTAKAMNWFFGHYLKKAEDWDNLLASPLLNDDLAGLAPAYVVTAGFDPLRDEGRAYADKLKAAGVPVEYVCYEGQIHGFASMAGALDEARSFLDEAAKVLRKAFNK',\n",
       " 'EH6(66)': 'MPLHPQIEGLLQQMAAAGGKGFHQMEVDECRQTFGGLLNSLPPSQQKIASAQDRGIPSPNGPVKVRVYTPEGSGPFPVMAYFHGGGWVIGDLETHDSLCRELCGAVGMVVVSVDYRLAPEHKFPAAPDDCVAVTRWIAANAAALNADASRIAVGGDSAGGNLAAVVAQRLRDEDALKLAAQLLIYPVVHLDGVATPSMIENAEGYLLTRKDMEWFGGHYLASPADGQNASASPILAKSLAGLPPALVLTCEFDPLRDEGEKYGKALQAAGVPTTISRHDGTIHATFSFFTALEPGRRMADEAIRWLKEQLVK',\n",
       " 'EH7(64)': 'MEFPMAQSNIIAGMDLNRLDRIAEHLDRAYLHPGKLAGTMTLVARRGEVVYCQAQGLRDVERQLPVERDTLFRIYSMTKPITSIALMQLYEQGRFLLDEPVHKYIPTWKNLRVYKTGSHPQMLTTAPQRPMTIRDLLTHQSGLTYGFMNRTNVDAAYRSLKLDGGPGHTLDRLIDELARLPLEFSPGTAWNYSVATDVCGYLVQLLSGMSLDDYFSKHIFQPLGMPDTFFTVPAEKLSRFAACYEYQPGDSFSLQDDPQGSAFAKAHGYLSGGGGLVSCVDDYYRFAQALANGGELDGARIIGRKTLEFMRMNHLPDNKGLPDVAIGSFSETPYDGTGFGLGFSVKLDVAKSQTVGSVGEYGWGGMASTNFFIDPEEDLLMVFMTQLIPSSTYAVRQELRAIINGALVD',\n",
       " 'EH8(63)': 'MNPAVIERATVRALMSLPGPVLERLAAGLETHSRPHLDSRLRFLLALSGAKPTLDSGTVEQARQIYRSTLALLDMAPVSLPVVVDHQVSMEDGSQILVRRYRPADAPLVSPAIMFFHGGGFTIGGVEEYDRLCRYIAKRTNAVVLSVDYRLAPEHPAPAGMDDALEAWRWLLNNTAQLGLDPNRLAVMGDSAGGCMSAVVSQQAKLAGLALPALQVLIYPTTDAALAHPSVQTLGQGFGLDIPLLTWFRGHFVQDPAVIEDYRVSPLRNPDLTGLPEAIVITATDPLRDEGLEYAQKLREAGNTVTSLDYPELIHGFISMGGVVPAARKAINDICVETKRRL',\n",
       " 'EH9(61)': 'MEKKMALDKQAAEILKRAEESDTPGLGEGSPAEGREVFAGTTALLGLPTPEGQRISEVQIPGPSGDIRTRIIHPLEGLADNLPILIYYHGGGWVIGSPETHEGETCFYANEANCVVLVPDYRLAPEDPFPAAPDDCYAVLEWANANAETFGGDASRIAVAGDSAGGNLSAVVSQMAHANNGPDIALQLLIYPATRMGATTESYREFNDGYFLTGKAMDWFFNHYLKRPEDWDALKASPLLAPDLSGLPPAYIMTAGFDPLRDEGKAYAERLQQAGVPVDYVCYEEQIHGFVSMAGALDQGKQFLREAAAVLRRAFTS'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = islice(a, 10)\n",
    "dict(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n",
    "tok = tokenizer(list(seq.values())[:2], padding=True, truncation=True, return_tensors=\"pt\", is_split_into_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", add_pooling_layer=False, output_hidden_states=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "n = AutoModelForSequenceClassification.from_pretrained(\"facebook/esm2_t6_8M_UR50D\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = n(**tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**tok.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok[\"attention_mask\"]  = tok[\"attention_mask\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 320])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "mask = tok[\"attention_mask\"].bool()\n",
    "for num, x in enumerate(output.hidden_states[-1]):\n",
    "    masked_x = x[mask[num]]\n",
    "    results[num] = masked_x.mean(dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data2.csv\"\n",
    "embeddings = pd.DataFrame(results).T\n",
    "embeddings.to_csv(path, mode='a', header=not Path(path).exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([317, 320])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[tok[\"attention_mask\"].bool()[0]].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "# https://huggingface.co/docs/datasets/loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to load files within datasets -> from local or remote files (json, csv, text, parquet)\n",
    "Since we have a fasta file that is not supported (because it will treat each line as a row so it will double rows, but in fasta the first line is an id).\n",
    "So we can process it in-memory to pandas, generators, dictionaries or list of dictionaries and use Datasets instead of load_dataset.\n",
    "The load dataset it returns a dataset dict with different splits (train, test, val) as keys and then a dataset object as values.\n",
    "\n",
    "Using datatses we will be directly using datasets so without the splits\n",
    "\n",
    "To load fasta files use from generator beacause it is in-memory and the file might be too large to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 294\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = load_dataset(\"text\", data_files=\"../data/whole_sequence.fasta\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_generator(fasta_file: str=\"../data/whole_sequence.fasta\"):\n",
    "    with open(fasta_file, 'r') as f:\n",
    "        seqs = SeqIO.parse(f, 'fasta')\n",
    "        for seq in seqs:\n",
    "            yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "with open(\"../data/whole_sequence.fasta\", 'r') as f:\n",
    "    seqs = SeqIO.parse(f, 'fasta')\n",
    "    d = pd.Series({s.id:str(s.seq) for s in seqs}).to_frame()\n",
    "    d.columns = [\"sequences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Dataset.from_generator(fasta_generator, gen_kwargs={\"fasta_file\":\"../data/whole_sequence.fasta\"})\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process or tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use map to apply the tokenizer function to the entire dataset\n",
    "Then select the new columns generate to pass it to the model -> but you will have to change its format to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = b.map(lambda examples: tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = dataset.select_columns([\"input_ids\", \"attention_mask\"])\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'seq', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 147\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 0, 20, 10,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1],\n",
       "         [ 0, 20, 15,  ...,  1,  1,  1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "for batch in dataloader:\n",
    "    u = batch\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(slots=True)\n",
    "class LLMConfig:\n",
    "    \"\"\"\n",
    "    Configuration for the language model.\n",
    "\n",
    "    Parameters\n",
    "    ----------  \n",
    "    model_name : str\n",
    "        Name of the language model.\n",
    "    _device : str\n",
    "        Device to use for the language model.\n",
    "    disbale_gpu : bool\n",
    "        Whether to disable the GPU.\n",
    "    \"\"\"\n",
    "    model_name: str = \"facebook/esm2_t6_8M_UR50D\"\n",
    "    disbale_gpu: bool = False\n",
    "    _device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        if self.disbale_gpu:\n",
    "            return \"cpu\"\n",
    "        return self._device\n",
    "    \n",
    "\n",
    "@dataclass(slots=True)\n",
    "class TokenizeFasta:\n",
    "    config: LLMConfig\n",
    "    tokenizer: None = field(default=None, init=False)\n",
    "     \n",
    "    def __post_init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)\n",
    "\n",
    "    def chunks(self, fasta_file: str):\n",
    "        \"\"\"\n",
    "        Split the fasta file into individual examples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fasta_file : str\n",
    "            Path to the FASTA file.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        dict[str, str]\n",
    "            A sample of the fasta sequence.\n",
    "        \"\"\"\n",
    "        with open(fasta_file, 'r') as f:\n",
    "            seqs = SeqIO.parse(f, 'fasta')\n",
    "            for seq in seqs:\n",
    "                yield {\"id\":seq.id, \"seq\":str(seq.seq)}\n",
    "\n",
    "    def tokenize(self, fasta_file: str):\n",
    "        \"\"\"\n",
    "        Tokenize the batch of sequences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_seq : dict[str, str]\n",
    "            Batch of sequences.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, torch.Tensor]\n",
    "            Tokenized sequences.\n",
    "        \"\"\"\n",
    "        dataset = Dataset.from_generator(self.chunks, gen_kwargs={\"fasta_file\": fasta_file})\n",
    "        tok = dataset.map(lambda examples: self.tokenizer(examples[\"seq\"], return_tensors=\"np\",padding=True, truncation=True), batched=True)\n",
    "        tok.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"], device=self.config.device)\n",
    "        return tok\n",
    "\n",
    "@dataclass(slots=True)\n",
    "class ExtractEmbeddings:\n",
    "    config: LLMConfig\n",
    "    model: None = field(default=None, init=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(self.config.model_name, add_pooling_layer=False, output_hidden_states=True)\n",
    "        self.model.to(self.config.device)\n",
    "    \n",
    "    def extract(self, batch_seq_keys: list[str], tok: dict[str, torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Extract embeddings from the tokenized sequences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_seq_keys : list[str]\n",
    "            Keys for the batch of sequences.\n",
    "        tok : dict[str, torch.Tensor]\n",
    "            Tokenized sequences.\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, np.array]\n",
    "            Extracted embeddings.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        output = self.model(**tok)\n",
    "        mask = tok[\"attention_mask\"].bool()\n",
    "        for num, x in enumerate(output.last_hidden_state):\n",
    "            masked_x = x[mask[num]]\n",
    "            results[batch_seq_keys[num]] = masked_x.mean(dim=0).detach().cpu().numpy()\n",
    "        return results\n",
    "    \n",
    "    def save(self, results: dict[str, np.array], path: str):\n",
    "        \"\"\"\n",
    "        Save the embeddings to a CSV file.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        results : dict[str, np.array]\n",
    "            Embeddings to save.\n",
    "        path : str\n",
    "            Path to the CSV file.\n",
    "        \"\"\"\n",
    "        embeddings = pd.DataFrame(results).T\n",
    "        embeddings.to_csv(path, mode='a', header=not Path(path).exists())\n",
    "\n",
    "        \n",
    "def generate_embeddings(model_name, fasta_file, disable_gpu=False, batch_size=8, \n",
    "                        save_path = \"embeddings.csv\"):\n",
    "    \"\"\"\n",
    "    Generate embeddings from a FASTA file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict[str, np.array]\n",
    "        Extracted embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    config = LLMConfig(model_name, disbale_gpu=disable_gpu)\n",
    "    tokenizer = TokenizeFasta(config)\n",
    "    embeddings = ExtractEmbeddings(config)\n",
    "    tok = tokenizer.tokenize(fasta_file)\n",
    "\n",
    "    seq_keys = list(tok[\"id\"])\n",
    "    for num, batch in enumerate(DataLoader(tok, batch_size=batch_size)):\n",
    "        batch_seq_keys = seq_keys[num*batch_size:(num+1)*batch_size]\n",
    "        results = embeddings.extract(batch_seq_keys, batch)\n",
    "        embeddings.save(results, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TokenizeFasta(LLMConfig()).tokenize(\"../data/whole_sequence.fasta\")\n",
    "embed = ExtractEmbeddings(LLMConfig())\n",
    "seq_keys = list(data[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, batch in enumerate(DataLoader(data, batch_size=batch_size)):\n",
    "    batch_seq_keys = seq_keys[num*batch_size:(num+1)*batch_size]\n",
    "    results = embed.extract(batch_seq_keys, batch)\n",
    "    embed.save(results, \"embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create emebeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = torch.nn.Linear(320, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Tensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m attention_weights\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Tensor' object is not callable"
     ]
    }
   ],
   "source": [
    "attention_scores = attention_weights(output.hidden_states[-1])\n",
    "attention_weights = torch.softmax(attention_scores, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 343, 1])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 109760])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp = output.hidden_states[-1].reshape(output.hidden_states[-1].shape[0], -1)\n",
    "_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  ...,  0.4682, -0.6849, -0.3094],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, -107712)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0, 2048 - _temp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.nn.functional.pad(_temp, (0, 2048 - _temp.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1419,  0.5839, -0.0722,  0.3390, -0.1853, -0.0982, -0.9235,  0.1019,\n",
       "        -0.4527, -0.6959], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109670"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(o[0].detach().numpy()).intersection(_temp[0].detach().numpy()))xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(output.hidden_states[-1][0][0].detach().numpy()).intersection(_temp[0][:100].detach().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioml_pycaret",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
